# ...

---
updatedOn: '2024-05-13T13:24:36.612Z'
---

# Guides

Welcome to Neon guides! This folder contains the source code of the [Neon guides](https://neon.tech/guides/).

## Basic information

1. Every single Markdown file in this folder will be turned into a guides page.
2. Folder and file names should follow kebab-case.
3. `slug` is generated based on the folder structure and file names inside this folder. In order to see page `slug`, you can [start](../../README.md#run-the-website) and build the project with `npm run build` command that will display all generated pages.
4. Page `path` is generated by combining [`GUIDES_BASE_PATH`](../../src/constants/guides.js) and page `slug`.
5. There is no need to add `h1` to the page since it will be displayed automatically with the value from `title` field.

## Fields

Right now Markdown files accept the following fields:

1. `title` — title of the page (required)
2. `subtitle` — subtitle of the page.
3. `author` — your author ID.
4. `redirectFrom` — array of strings with paths to redirect from to the page, should start and end with a slash, e.g. `/guides/old-path/`
5. `isDraft` — flag that says the page is not ready yet. It won't appear in production but will appear in the development mode.
6. `enableTableOfContents` — flag that turns on the display of the outline for the page. The outline gets built out of second and third-level headings ([`h2`, `h3`]), thus appears as two-level nested max.
7. `ogImage` - the social preview image of the page.

> ⚠️ Please note that the project won't build if at least one of the Markdown files is missing a required field.

## Code blocks

All available languages for code blocks can be found [here](https://shiki.matsu.io/languages).

You can use fenced code blocks with three backticks (```) on the lines before and after the code block. And display code with options

- enable highlighting single lines, multiple lines, and ranges of code lines

  Examples:

  - Single line highlight

    ````md
    ```c++ {1}
    #include <iostream>

    int main() {
        std::cout << "Hello World";
        return 0;
    }
    ```
    ````

  - Multiple lines

    ````md
    ```c++ {1,2,5}
    #include <iostream>

    int main() {
        std::cout << "Hello World";
        return 0;
    }
    ```
    ````

  - Range of code lines

    ````md
    ```c++ {1-3,5}
    #include <iostream>

    int main() {
        std::cout << "Hello World";
        return 0;
    }
    ```
    ````

- use `[!code highlight]` to highlight a line.

  ```ts
  export function foo() {
    console.log('Highlighted'); // [!code highlight]
  }
  ```

- use `[!code word:xxx]` to highlight a word.

  ```ts
  export function foo() {
    // [!code word:Hello]
    const msg = 'Hello World';
    console.log(msg); // prints Hello World
  }
  ```

- `showLineNumbers` - flag to show on the line numbers in the code block.

  Example:

  ````md
  ```c++ showLineNumbers
  #include <iostream>

  int main() {
      std::cout << "Hello World";
      return 0;
  }
  ```
  ````

- `shouldWrap` - flag to enable code wrapping in the code block.

  Example:

  ````md
  ```powershell shouldWrap
  powershell -Command "Start-Process -FilePath powershell -Verb RunAs -ArgumentList '-NoProfile','-InputFormat None','-ExecutionPolicy Bypass','-Command ""iex (iwr -UseBasicParsing https://cli.configu.com/install.ps1)""'"
  ```
  ````

## Code Tabs

To display code tabs, wrap all pieces of code with `<CodeTabs></CodeTabs>` and write labels of code tabs in order:

````md
<CodeTabs labels={["Shell", "C++", "C#", "Java"]}>

```bash {2-4}
#!/bin/bash
STR="Hello World!"
echo $STR
```

```c++
#include <iostream>

int main() {
    std::cout << "Hello World";
    return 0;
}
```

```csharp
namespace HelloWorld
{
    class Hello {
        static void Main(string[] args)
        {
            System.Console.WriteLine("Hello World");
        }
    }
}
```

```java
import java.io.*;

class GFG {
    public static void main (String[] args) {
       System.out.println("Hello World");
    }
}
```

</CodeTabs>
````

<details>
<summary>Examples</summary>

![Code tabs example](../docs/code-tabs-example.jpg)

</details>

## Tabs

To display the tabs with content as image, video, code block, .etc, wrap the `TabItem` with `Tabs`

````md
<Tabs labels={["Content", "CLI"]}>

<TabItem>
In your config v3 project, head to the `/metadata/databases/databases.yaml` file and add the database configuration as below.

```bash showLineNumbers
- name: <db_name>
  kind: postgres
  configuration:
    connection_info:
      database_url:
        from_env: <DB_URL_ENV_VAR>
    pool_settings:
      idle_timeout: 180
      max_connections: 50
      retries: 1
  tables: []
  functions: []
```

Apply the Metadata by running:

```bash
hasura metadata apply
```

If you've spun up the Hasura Engine with Docker, you can access the Hasura Console by accessing it in a browser at the URL of your Hasura Engine instance, usually http://localhost:8080.

<Admonition type="note">
To access the Hasura Console via the URL the HASURA_GRAPHQL_ENABLE_CONSOLE environment variable or the `--enable-console` flag must be set to true.
</Admonition>

</TabItem>

<TabItem>
Alternatively, you can create read replicas using the Neon API or Neon CLI.

```bash
curl --request POST \
     --url https://console.neon.tech/api/v2/projects/late-bar-27572981/endpoints \
     --header 'Accept: application/json' \
     --header "Authorization: Bearer $NEON_API_KEY" \
     --header 'Content-Type: application/json' \
     --data '
{
  "endpoint": {
    "type": "read_only",
    "branch_id": "br-young-fire-15282225"
  }
}
' | jq
```

</TabItem>

</Tabs>
````

## Admonition

To improve the guides readability, one can leverage an Admonition custom component. Just wrap your piece of text with `<Admonition></Admonition>` and pass the type.

There are 6 types of Admonition: `note`, `important`, `tip`, `warning`, `info`, `comingSoon`; the default is `note`.

You may also specify an optional title with prop `title`.

Example:

```md
<Admonition type="note" title="Your title">
  The branch creation process does not increase load on the originating project. You can create a branch at any time without worrying about downtime or performance degradation.
</Admonition>

<Admonition type="info">
  The branch creation process does not increase load on the originating project. You can create a branch at any time without worrying about downtime or performance degradation.
</Admonition>
```

<details>
<summary>Examples</summary>

![Admonition example](../docs/admonition-example.jpg)

</details>

## CTA

This is a simple block with title, description text and one CTA button that accomplish certain actions.

```md
<CTA />
```

Check the example for default data of CTA block

<details>
<summary>Example</summary>

![CTA example](../docs/cta-example.jpg)

</details>

To change text in CTA block, you can pass to the component props `title`, `description`, `buttonText`, `buttonUrl`:

```md
<CTA title="Try it on Neon!" description="Neon is Serverless Postgres built for the cloud. Explore Postgres features and functions in our user-friendly SQL editor. Sign up for a free account to get started." buttonText="Sign Up" buttonUrl="https://console.neon.tech/signup" />
```

## Images

The images should be sourced in `public/guides` directory and be used in `.md` with the relative path, that begins with a `/` slash

Example file structure:

```md
├── public
│ ├── guides
│ │ ├── conceptual-guides
│ │ ├── neon_architecture_2.png // put images in a directory with the same name as the .md file
├── content
│ ├── guides
│ │ ├── conceptual-guides
│ │ ├── architecture-overview.md
```

To display images using Markdown syntax, use the following syntax: `![alt text](image url)`. Example content in `architecture-overview.md`:

```md
![Neon architecture diagram](/guides/conceptual-guides/neon_architecture_2.png)
```

If you need an image without border to show an annotated piece of UI, use the `"no-border"` attribute as in the example below:

```md
![Neon architecture diagram](/guides/conceptual-guides/neon_architecture_2.png 'no-border')
```

With this approach, all images on your guide pages will be displayed both on the production and GitHub preview.

## Definition list

Custom `mdx` component that makes possible using [extended markdown syntax for descriptions lists](https://www.markdownguide.org/extended-syntax/#definition-lists). Fully [WCAG-compliant](https://www.w3.org/TR/WCAG20-TECHS/H40.html). It provides an accessible way to make term lists, and it's a generally good way to add structure to a text when a writer needs more than bullets and less than headings.

The usage is pretty [straightforward](https://github.com/neondatabase/website/pull/231/commits/8f795eaf700c31794a2267fc5978c22bfc649a0c):

```md
[comment]: <> (other content here)

<DefinitionList>
[comment]: <> (required new line)
Scenario executor
: First definition
: Second definition

Soak test
: First and only definition

Smoke test
Another term for smoke test
: First definition for both terms
: Second definition for both terms
: ...n definition for both terms

[Stress test](/)
: First and **only** definition for both terms with additional markup <br/> Read more: [link](/)

[comment]: <> (other content here)
</DefinitionList>

[comment]: <> (other content here)
```

### Acceptable markup for term

- `*italic*`
- `[link](/)`
- `**strong**` - but that doesn't make sense, by default terms appearance is already bold
- `inlineCode` - but it doesn't alter it's change in this context

### Constraints

- using emojis in `dt` is prohibited, as it potentially can mess up with `id` attribute, and `href` at anchor. We can not be sure which range will be used to display a particular symbol (depends on editor OS) and if it is going to be stripped.
- if there are multiple terms for a given set of descriptions, only the first one will have an `id` and an `anchor`
- make absolutely sure your `dt` text content is unique across the page to avoid `id` collisions

### Acceptable markup for description

- everything for term
- emojis
- any inline html
- line breaks `<br/>` (recommended way to separate visually something inside a single description)

<details>
<summary>Examples</summary>

![Definition list example](../docs/definition-list-example.jpg)

</details>

## Detail Icon Cards

`DetailIconCards` is a custom MDX component that displays data in a card format. Each card contains icon, title, href and description. This layout is especially useful for presenting grouped information in a visually pleasing and easy-to-understand way.

```md
<DetailIconCards>

<a href="https://api-guides.neon.tech/reference/getting-started-with-neon-api" description="Collaborate on open-source projects" icon="github">Headless vector search</a>

<a href="https://api-guides.neon.tech/reference/getting-started-with-neon-api" description="Collaborate on open-source projects" icon="github">Open AI completions</a>

</DetailIconCards>
```

## Shared MDX components

Create a [markdown file](https://github.com/neondatabase/website/blob/main/content/docs/shared-content/need-help.md) in folder `content/docs/shared-content/`, add to `sharedMdxComponents` the name of component and the path to component.

```js
const sharedMdxComponents = {
  // name of component: path to component
  NeedHelp: '../docs/shared-content/need-help',
};

export default sharedMdxComponents;
```

Insert a shared markdown and render inline.

```md
## Resources

- [Open AI tiktoken source code on GitHub](https://github.com/openai/tiktoken)
- [pg_tiktoken source code on GitHub](https://github.com/kelvich/pg_tiktoken)

<NeedHelp/>
```

## Author data

Your author's data should be sourced in `content/guides/authors/data.json` file.

You should start with your name as the ID for the object in kebab-case. Then you can use these fields inside the object:

1. `name` — your name (required).
2. `position` — your position.
3. `bio` — your bio.
4. `link` — link to one of your social media accounts. Each link should have `title` and `url` fields.

Then you can add your photo to `public/guides/authors/` folder with the filename as your ID.

Please make sure that your ID is the same in your author's data, in `content/guides/authors/data.json`, and the photo filename.

Example usage:

```md
├── content
│ ├── guides
│ ├── next-server-actions.md // your guide
│ ├── authors
│ ├── data.json // your data here
├── public
│ ├── guides
│ ├── authors
│ ├── rishi-raj-jain.jpg // your photo
```

`content/guides/next-server-actions.md`:

```md
---
title: ...
author: rishi-raj-jain
---
```

`content/guides/authors/data.json`:

```json
{
  "rishi-raj-jain": {
    "name": "Rishi Raj Jain",
    "position": "Software Engineer",
    "bio": "Technical Writer",
    "link": {
      "title": "GitHub",
      "url": "https://github.com/rishi-raj-jain"
    }
  }
}
```

With this approach, all your data will be displayed in guide's author section.

## Contributing

For small changes and spelling fixes, we recommend using the GitHub UI because Markdown files are relatively easy to edit.

For larger contributions, consider [running the project locally](../../README.md#getting-started) to see how changes look like before making a pull request.


# Querying Neon Postgres with Natural Language via Amazon Q Business

---
title: Querying Neon Postgres with Natural Language via Amazon Q Business
subtitle: Learn how to set up Amazon Q Business to query your Neon Postgres database using natural language
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-11-23T00:00:00.000Z'
updatedOn: '2024-11-23T00:00:00.000Z'
---

Amazon Q Business enables you to build an interactive chat application that combines your Neon Postgres data with large language model capabilities.

Rather than querying your database directly, Amazon Q Business creates an index of your data which users can then interact with through a natural language interface.

In this guide, you'll learn how to set up Amazon Q Business to query your Neon Postgres database using natural language. We'll cover the following topics:

## Prerequisites

Before starting, you'll need:

- An [AWS Account](https://aws.amazon.com/q/) with Amazon Q Business subscription (Pro or Lite plan)
- A [Neon](https://console.neon.tech/signup) account and project
- [AWS IAM Identity Center](https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/idc-setup.html) configured (recommended) or Identity Federation through IAM
- Basic understanding of databases and SQL

## Setting Up Your Environment

In order to use Amazon Q, you will need to have an AWS Organization set up with the IAM Identity Center enabled. This will allow you to manage your users and permissions across multiple AWS accounts.

### Configure IAM Identity Center

To begin setting up the AWS Q Business application, you will have to configure the application environment to enable end-user access by integrating AWS IAM Identity Center for streamlined user management.

If you haven't set up IAM Identity Center yet, complete the setup process first following AWS's [detailed guide](https://docs.aws.amazon.com/singlesignon/latest/userguide/get-set-up-for-idc.html).

The exact steps may vary depending on your organization's requirements, existing identity sources, security policies, and other factors. It's recommended to work with your organization's IT or security team to ensure a smooth setup.

You can follow the [IAM Identity Center setup guide](https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/create-application.html) for detailed steps.

### Create Your Application Environment

Once IAM Identity Center is configured, next you will have to create your Amazon Q Business application:

1. Navigate to the [Amazon Q Business Console](https://us-east-1.console.aws.amazon.com/amazonq/business/welcome) and choose "Create application". You must add, assign, and subscribe at least one user to your Amazon Q Business application environment for it to work as intended.

2. Configure basic settings:

   - Name: Pick a unique name for your application
   - Outcome: Choose 'Web experience', this will allow you to access Q as a managed web experience
   - For Access management method select 'IAM Identity Center' or 'IAM Identity Provider' depending on your setup
   - Under the 'Quick start user' section, select the user that you've created in the IAM Identity Center or click the 'Add new users and groups' button to create a new user if you haven't done so already
   - Next to the selected user, make sure to select subscription plan (Pro or Lite) and assign the user to the application

3. Once ready, click the create button. Amazon Q Business will automatically create a web experience unless you opt out. For detailed steps, follow the [application creation guide](https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/create-app.html).

Now that your environment is set up, you can proceed to connecting your Neon database as a data source and configuring the natural language interface.

## Database Setup

If you already have data in your Neon Postgres database, you can skip this section. Otherwise, follow these steps to create a sample database and set up data syncing with Amazon Q Business.

Let's set up a sample database with customer data:

```sql
-- Create customers table
CREATE TABLE customers (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    subscription_tier VARCHAR(50),
    monthly_spend DECIMAL(10,2),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Insert sample data
INSERT INTO customers (name, email, subscription_tier, monthly_spend) VALUES
('TechWave Solutions', 'contact@example.com', 'Premium', 9500.00),
('BlueSky Innovations', 'hello@example.com', 'Growth', 2200.00),
('Greenfield Consulting', 'info@example.com', 'Standard', 600.00),
('BrightFuture Partners', 'support@example.com', 'Premium', 12000.00),
('UrbanTech Solutions', 'services@example.com', 'Basic', 300.00);
```

The above SQL script creates a `customers` table. The table stores customer information which we will use demonstrate data syncing with Amazon Q Business and querying through natural language.

## Configuring Data Source

Once your Neon database is set up, you can configure it as a data source in Amazon Q Business.

Head to the [Amazon Q Business Console](https://us-east-1.console.aws.amazon.com/amazonq/business/applications), click on the application you created, and then follow these steps:

1. Navigate to "Data sources" and click "Add data source"
2. Search for the "PostgreSQL" connector and click "Add"
3. Configure connection details:

   Under '**Source**' add the following details:

   - Data source name: Neon Database
   - Host: your-neon-hostname.neon.tech
   - Port: 5432
   - Instance: `your_neon_database`
   - Check the '**Enable SSL Certificate Location**' box but leave the '**SSL Certificate Location**' field empty

   Under '**Authentication**', click on the '**Create and add a secret**' button and add the following details:

   - Secret name: Neon Database Secret
   - Username: `your_neon_database_username`
   - Password: `your_neon_database_password`

   Click '**Create secret**' and then select the secret that you've just created from the dropdown.

   Under '**IAM role**', create a new role or select an existing one with the necessary permissions for Amazon Q Business to access your Neon credentials through the secret manager. You can click the '**Create a new service role**' button to create a new role if you don't have one already.

4. After setting up your database connection, you'll need to configure how Amazon Q Business should sync your data. Here's how to configure the sync scope:

   Enter your SQL query that defines what data to sync. For example:

   ```sql
   SELECT
       id,
       name,
       CONCAT(subscription_tier, ': ', monthly_spend) AS body
   FROM customers
   ```

   > Note: SQL queries must be less than 32 KB in size and must only use DQL (Data Query Language) operations.

   - Define the required columns:
     - **Primary key column**: `id`
     - **Title column**: `name`
     - **Body column**: `body`

   Amazon Q Business offers additional configuration options like change-detecting columns, user IDs, groups, timestamps, and more. These settings help you fine-tune how the sync process works and when data should be updated. For most use cases, the default settings will work well.

5. Once you've configured the sync scope, you can set up the '**Sync Mode**' and '**Sync Schedule**'. The sync mode determines how Amazon Q Business should handle data changes, while the sync schedule defines how often the sync job should run.

   - **Sync Mode**: Choose between '**Full Sync**' or '**New and Modified Content Sync**'. Full Sync will reindex all data each time, while New and Modified Content Sync will only update new or modified data.
   - **Sync Schedule**: Define the sync schedule. You can choose from options like on-demand, hourly, daily, weekly, monthly, or a custom cron expression. For most applications, a daily sync schedule is sufficient to keep data up-to-date unless you have a high amount of data changes.

Once you've configured your data source, click the '**Sync**' button and Amazon Q Business will start syncing your Neon database data. You can monitor the sync status and view the indexed data in the Amazon Q Business Console.

It can take from a few minutes to a few hours depending on the size of your database and the sync frequency you've set.

## Web Experience Setup

Once your data source is synced, you can access and share the Amazon Q Business web interface. The web experience provides a chat interface out of the box where users can query your Neon database using natural language.

### Finding Your Web Experience URL

The web experience URL is automatically generated by Amazon Q Business. To find it:

1. Navigate to your application in the Amazon Q Business Console
2. Look for the '**Web experience settings**' section
3. Find the '**Deployed URL**' - this is the URL you'll share with your authorized users

Once you have the URL, visit it in your browser to access the Amazon Q Business web interface. You will be prompted to log in using your IAM Identity Center credentials and then you can start querying your Neon database using natural language.

### Customizing the Experience

Before sharing with users, you can customize the web interface:

1. Click '**Customize web experience**' in your application settings
2. Configure the following:
   - Title: Your Database Assistant
   - Subtitle: Query your database using natural language
   - Welcome message: Ask questions about your data in plain English

### Example Queries

Once users access the web experience, they can select the '**Company Knowledge**' tab and ask questions like:

- "Show me all enterprise customers"
- "What's our total monthly spend by subscription tier?"
- "Who are our top customers by monthly spend?"

### Access Control

Only users who have been granted access through IAM Identity Center can access the web experience. Make sure to add users to the appropriate groups and configure permissions correctly based on your organization's policies.

After users click the web experience URL, they'll be prompted to log in using their IAM Identity Center credentials. Once authenticated, they can start querying your Neon database using natural language.

Keep in mind that Amazon Q Business is optimized for English language queries, so instruct your users to formulate their questions in English for best results.

## Conclusion

Amazon Q provides a natural language interface to query your Neon Postgres database, making it easier for team members who aren't SQL experts to access data.

Amazon Q Business also provides several methods to add data to your application beyond your primary Neon database connection, you can upload files like documentation, guides, or data dictionaries directly through the AWS Q console. Or use a Web Crawler to index web pages and documents directly from your website or the internet.

## Additional Resources

- [Neon Documentation](/docs)
- [Amazon Q Business Documentation](https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/what-is.html))
- [AWS IAM Identity Center Guide](https://docs.aws.amazon.com/singlesignon/latest/userguide/what-is.html)

<NeedHelp />


# Building a RESTful API with ASP.NET Core, Swagger, and Neon

---
title: Building a RESTful API with ASP.NET Core, Swagger, and Neon
subtitle: Learn how to connect your .NET applications to Neon's serverless Postgres database
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-11-03T00:00:00.000Z'
updatedOn: '2024-11-03T00:00:00.000Z'
---

In this guide, we'll walk through the process of developing a RESTful API using ASP.NET Core, connecting it to a Neon Postgres database. We will cover CRUD operations using Entity Framework Core (EF Core), generate interactive API documentation with Swagger, and explore best practices for testing your API endpoints. As a bonus, we'll also implement JWT authentication to secure your endpoints.

## Prerequisites

Before we start, make sure you have the following:

- [.NET SDK 8.0](https://dotnet.microsoft.com/download/dotnet/8.0)
- [Neon account](https://neon.tech) for setting up your Postgres database
- [Postman](https://www.postman.com/downloads/) for API testing
- Basic knowledge of C# and ASP.NET Core
- Familiarity with Entity Framework Core

## Setting Up Your ASP.NET Core Project

First, create a new ASP.NET Core Web API project:

```bash
dotnet new webapi -n NeonApi
cd NeonApi
```

Install the required NuGet packages using the `dotnet add package` command:

```bash
dotnet add package Microsoft.EntityFrameworkCore
dotnet add package Npgsql.EntityFrameworkCore.PostgreSQL
dotnet add package Swashbuckle.AspNetCore
dotnet add package Microsoft.AspNetCore.Authentication.JwtBearer
dotnet add package Microsoft.EntityFrameworkCore.Design
```

The above packages include:

- `Microsoft.EntityFrameworkCore` - Entity Framework Core for database operations
- `Npgsql.EntityFrameworkCore.PostgreSQL` - PostgreSQL provider for EF Core
- `Swashbuckle.AspNetCore` - Swagger for API documentation
- `Microsoft.AspNetCore.Authentication.JwtBearer` - JWT authentication for securing endpoints
- `Microsoft.EntityFrameworkCore.Design` - EF Core design tools for migrations

### Configuring the Neon Database

Head over to your [Neon Dashboard](https://neon.tech) and create a new project.

Once done, grab your database connection string and add it to your `appsettings.json`:

```json
"ConnectionStrings": {
  "NeonDb": "Host=<your-host>;Database=<your-database>;Username=<your-username>;Password=<your-password>;Port=5432"
}
```

While you're in `appsettings.json`, add a section for JWT authentication after the connection string:

```json
"Jwt": {
"SecretKey": "your-very-secure-secret-key"
}
```

We will cover JWT authentication in more detail later in this guide, but for now, let's focus on setting up the API.

Next, update your `Program.cs` file to include the database context, Swagger, and JWT authentication:

```csharp
using NeonApi.Data;
using Microsoft.AspNetCore.Authentication.JwtBearer;
using Microsoft.EntityFrameworkCore;
using Microsoft.IdentityModel.Tokens;
using System.Text;

var builder = WebApplication.CreateBuilder(args);
var configuration = builder.Configuration;

// Add the Neon database context using Npgsql provider
builder.Services.AddDbContext<NeonDbContext>(options =>
    options.UseNpgsql(configuration.GetConnectionString("NeonDb")));

// Register controllers for handling incoming HTTP requests
builder.Services.AddControllers();

// Enable API endpoint exploration and Swagger documentation
builder.Services.AddEndpointsApiExplorer();
builder.Services.AddSwaggerGen();

// Set up JWT authentication to secure API endpoints
builder.Services.AddAuthentication(JwtBearerDefaults.AuthenticationScheme)
    .AddJwtBearer(options =>
    {
        options.TokenValidationParameters = new TokenValidationParameters
        {
            // Validate that the token is signed with the specified key
            ValidateIssuerSigningKey = true,
            IssuerSigningKey = new SymmetricSecurityKey(Encoding.UTF8.GetBytes(configuration["Jwt:SecretKey"])),

            // Disable issuer and audience validation for testing purposes
            ValidateIssuer = false,
            ValidateAudience = false
        };
    });

var app = builder.Build();

// Configure the middleware pipeline
if (app.Environment.IsDevelopment())
{
    app.UseSwagger();
    app.UseSwaggerUI();
}

app.UseHttpsRedirection();
app.UseAuthentication();
app.UseAuthorization();
app.MapControllers();

app.Run();
```

In the above, we configure the necessary services directly within `Program.cs` to connect our ASP.NET Core API to Neon and secure it with JWT authentication:

1. We use `AddDbContext` to set up `NeonDbContext` with the Npgsql provider, connecting to the Neon database using the connection string defined in `appsettings.json`. Make sure to update `"NeonDb"` with your actual connection string key if it's named differently.

2. We register controllers with `AddControllers()`, which allows the application to handle incoming API requests and map them to their respective endpoints.

3. By adding `EndpointsApiExplorer` and `SwaggerGen`, we enable automatic generation of API documentation. This provides a user-friendly interface to interact with your API endpoints, accessible at `/swagger`.

4. For the JWT authentication setup, we are implementing the following:
   - Here, we set up JWT authentication to protect your API routes. We use `AddJwtBearer` to validate tokens sent by clients.
   - The `TokenValidationParameters` section allows us to make sure that the token is signed with the specified key. Replace `"your-secret-key"` in `appsettings.json` with a secure key unique to your application.
   - For simplicity, `ValidateIssuer` and `ValidateAudience` are set to `false`, which means the API won't check who issued the token or its intended audience. This is useful for local development but should be tightened for production environments.

To avoid hardcoding sensitive information like the secret key, consider using environment variables or a configuration management system to securely store secrets.

## Creating the Entity Framework Core Models

Data models define the structure of your database tables and the relationships between them. Here, we'll create a simple `Product` model to represent products in our Neon database.

In the `Models` folder, create a `Product.cs` file:

```csharp
namespace NeonApi.Models
{
    public class Product
    {
        public int Id { get; set; }
        public string Name { get; set; }
        public decimal Price { get; set; }
        public string Description { get; set; }
    }
}
```

Here, we define a simple `Product` model with four properties:

- `Id`: This is the primary key, which will auto-increment.
- `Name`: Stores the product's name.
- `Price`: Holds the product's price as a decimal value.
- `Description`: Provides additional details about the product.

Each property corresponds to a column in the database table that Entity Framework will generate for us.

### Creating the Database Context

Next, we need to create a database context class, which serves as a bridge between our C# code and the Neon database.

Create a new folder named `Data` and add a `NeonDbContext.cs` file:

```csharp
namespace NeonApi.Data
{
    public class NeonDbContext : DbContext
    {
        public NeonDbContext(DbContextOptions<NeonDbContext> options) : base(options) { }

        // This DbSet represents the Products table in the Neon database
        public DbSet<Product> Products { get; set; }
    }
}
```

The above code snippet does the following:

- The `NeonDbContext` class inherits from `DbContext`, which is part of Entity Framework Core.
- We pass `DbContextOptions` to the constructor to configure the connection to our Neon database.
- The `DbSet<Product>` property represents the `Products` table. This allows us to perform CRUD operations on the `Product` model directly through this context.

### Running Migrations to Create the Database Schema

Now that we have defined our model and context, let's generate the database schema using migrations. Open your terminal and run the following commands:

```bash
dotnet ef migrations add InitialCreate
```

The above command generates a migration file based on the changes made to the database schema. The migration file contains instructions to create the `Products` table.

Next, apply the migration to your Neon database:

```bash
dotnet ef database update
```

The `dotnet ef database update` command applies the migration to your Neon database, creating the `Products` table and any other necessary schema changes.

> **Note**: Make sure your database connection string in `appsettings.json` is correctly configured before running the migrations. That way the changes are applied to your Neon database instance.

At this point, your database is set up and ready to store product data!

## Building the API Endpoints

With the database schema in place, let's create the API endpoints to perform CRUD operations on the `Products` table.

In the `Controllers` folder, create a `ProductsController.cs` file with the following content:

```csharp
using System.Collections.Generic;
using System.Threading.Tasks;
using Microsoft.AspNetCore.Mvc;
using Microsoft.EntityFrameworkCore;
using NeonApi.Data;
using NeonApi.Models;

[Route("api/[controller]")]
[ApiController]
public class ProductsController : ControllerBase
{
    private readonly NeonDbContext _context;

    public ProductsController(NeonDbContext context)
    {
        _context = context;
    }

    [HttpGet]
    public async Task<ActionResult<IEnumerable<Product>>> GetProducts()
    {
        // Retrieve all products from the database
        return await _context.Products.ToListAsync();
    }

    [HttpGet("{id}")]
    public async Task<ActionResult<Product>> GetProduct(int id)
    {
        // Retrieve a single product by ID
        var product = await _context.Products.FindAsync(id);
        if (product == null) return NotFound(); // Return 404 if not found
        return product;
    }

    [HttpPost]
    public async Task<ActionResult<Product>> CreateProduct(Product product)
    {
        // Add a new product to the database
        _context.Products.Add(product);
        await _context.SaveChangesAsync();

        // Return 201 Created status with the newly created product
        return CreatedAtAction(nameof(GetProduct), new { id = product.Id }, product);
    }

    [HttpPut("{id}")]
    public async Task<IActionResult> UpdateProduct(int id, Product product)
    {
        // Ensure the ID in the URL matches the ID of the provided product
        if (id != product.Id) return BadRequest();

        // Mark the product as modified
        _context.Entry(product).State = EntityState.Modified;
        await _context.SaveChangesAsync();

        // Return 204 No Content status after a successful update
        return NoContent();
    }

    [HttpDelete("{id}")]
    public async Task<IActionResult> DeleteProduct(int id)
    {
        // Find the product by ID
        var product = await _context.Products.FindAsync(id);
        if (product == null) return NotFound(); // Return 404 if not found

        // Remove the product from the database
        _context.Products.Remove(product);
        await _context.SaveChangesAsync();

        // Return 204 No Content status after successful deletion
        return NoContent();
    }
}
```

In the code above, we define a `ProductsController` to handle all CRUD operations for our `Product` model. Here's a breakdown of how each endpoint works:

1. The `GetProducts` method handles `GET /api/products` requests, fetching all products stored in the Neon database.

2. The `GetProduct` method handles `GET /api/products/{id}` requests to retrieve a single product by its unique ID. If no product with the given ID is found, it responds with a `404 Not Found`. This ensures the client is notified when attempting to access a non-existent product.

3. The `CreateProduct` method handles `POST /api/products` requests to add a new product. The response uses `CreatedAtAction` to include a link to the newly created resource, following REST best practices.

4. The `UpdateProduct` method handles `PUT /api/products/{id}` requests to modify an existing product. Once the update is successful, it responds with a `204 No Content`, indicating the operation was successful without returning any additional data.

5. The `DeleteProduct` method handles `DELETE /api/products/{id}` requests to remove a product by its ID. If the product doesn't exist, it returns a `404 Not Found` response.

Each endpoint is fully asynchronous and interacts with the Neon database through the `NeonDbContext` context.

## Setting Up Swagger for API Documentation

To document our API and provide an interactive interface for testing, we'll integrate Swagger into our ASP.NET Core project.

Swagger automatically generates OpenAPI documentation, making it easy to explore your API and test its endpoints directly from your browser.

### Enabling Swagger in `Program.cs`

To set up Swagger, add the following code in the `Configure` method of your `Program.cs` file:

```csharp
app.UseSwagger();
app.UseSwaggerUI(c =>
{
    // Configure Swagger UI at the root URL
    c.SwaggerEndpoint("/swagger/v1/swagger.json", "Neon API V1");
    c.RoutePrefix = string.Empty;
});
```

The `UseSwagger` middleware generates the OpenAPI documentation for your API, while `UseSwaggerUI` sets up the Swagger interface for interacting with your endpoints.

### Running Your Application

Now that Swagger is set up, start your application using:

```bash
dotnet run
```

Once the application is running, you will see the port where your API is hosted (usually `https://localhost:5229`), eg.:

```
Building...
info: Microsoft.Hosting.Lifetime[14]
      Now listening on: http://localhost:5229
```

Visit `https://localhost:5229/swagger` in your browser to access the Swagger UI.

The Swagger UI will appear, displaying all your API endpoints with detailed documentation. You can test the endpoints by sending requests directly from the UI and viewing the responses, making it easy to verify that everything works as expected.

## Testing Your API with Postman

Now that your API is running, you can use Postman or any other API testing tool to interact with your endpoints. You can even use the Swagger UI to test the endpoints, but Postman provides a more robust environment for testing complex scenarios.

In this section, we'll walk through testing the CRUD operations using Postman but feel free to use any tool you're comfortable with like Insomnia or cURL.

Download and launch [Postman](https://www.postman.com/downloads/). Create a new request to interact with your API.

Open Postman and create the following requests:

1. `GET`: `/api/products`

   - **Description**: Fetches all products.
   - Set to `GET`, enter `https://localhost:5001/api/products`, and click **Send**.
   - You should receive a `200 OK` response with a list of products.

2. `POST`: `/api/products`

   - **Description**: Creates a new product.
   - Set to `POST`, enter `https://localhost:5001/api/products`, and go to **Body** → **raw** → **JSON**.
   - Add:
     ```json
     {
       "name": "New Product",
       "price": 19.99,
       "description": "Sample product"
     }
     ```
   - Click **Send**. Expect a `201 Created` response.

3. `PUT`: `/api/products/{id}`

   - **Description**: Updates a product.
   - Set to `PUT`, enter `https://localhost:5001/api/products/1`.
   - Add:
     ```json
     {
       "id": 1,
       "name": "Updated Product",
       "price": 29.99,
       "description": "Updated description"
     }
     ```
   - Click **Send**. You should receive a `204 No Content`.

4. `DELETE`: `/api/products/{id}`
   - **Description**: Deletes a product.
   - Set to `DELETE`, enter `https://localhost:5001/api/products/1`, and click **Send**.
   - Expect a `204 No Content`.

After testing, check that all changes are reflected in your Neon database. Use both Postman and Swagger UI to confirm the endpoints are functioning correctly.

## Securing Your API with JWT Authentication (Bonus)

To protect your API endpoints, we’ll use [JWT](https://jwt.io/) (JSON Web Token) authentication. By adding the `[Authorize]` attribute to specific controller actions, you can ensure that only authenticated users have access. Here’s how to secure the `GetProducts` endpoint:

```csharp
[Authorize]
[HttpGet]
public async Task<ActionResult<IEnumerable<Product>>> GetProducts()
{
    return await _context.Products.ToListAsync();
}
```

Now, any requests to `GetProducts` will require a valid JWT token, if you try to access the endpoint without a token, you will receive a `401 Unauthorized` response.

### Generating and Using JWT Tokens

When a user successfully logs in, the server generates a JWT token containing the user's authentication details. This token typically includes claims such as the user's ID, email, and a unique identifier. The token is then signed with a secret key to ensure its integrity and prevent tampering. For example:

```csharp
public string GenerateJwtToken(User user)
{
    var claims = new[]
    {
        new Claim(JwtRegisteredClaimNames.Sub, user.Id.ToString()),
        new Claim(JwtRegisteredClaimNames.Email, user.Email),
        new Claim(JwtRegisteredClaimNames.Jti, Guid.NewGuid().ToString())
    };

    var key = new SymmetricSecurityKey(Encoding.UTF8.GetBytes(configuration["Jwt:SecretKey"]));
    var creds = new SigningCredentials(key, SecurityAlgorithms.HmacSha256);

    var token = new JwtSecurityToken(
        issuer: null,
        audience: null,
        claims: claims,
        expires: DateTime.UtcNow.AddHours(1),
        signingCredentials: creds);

    return new JwtSecurityTokenHandler().WriteToken(token);
}
```

The token looks like this:

```
eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwiZW1haWwiOiJhZG1pbkBleGFtcGxlLmNvbSIsImp0aSI6IjMzYjQwMzIwLWUwZjItNDIwZi1iZjIwLWUwZjIwZmJmMjAwMCIsImV4cCI6MTY0MzUwNzQwMH0.7
```

If you were to go to [jwt.io](https://jwt.io/), you could paste the token and see its decoded contents.

Once the token is generated, the client stores it in local storage or session storage and includes it in the `Authorization` header for all subsequent requests to secured endpoints. For instance:

```
Authorization: Bearer <your-jwt-token>
```

With this header in place, the server can authenticate the user without requiring them to log in again for each request.

## Conclusion

In this guide, we covered the process of building a RESTful API with ASP.NET Core, connecting it to a Neon Postgres database, and securing it with JWT authentication. We explored CRUD operations using Entity Framework Core, generated interactive API documentation with Swagger, and tested our endpoints using Postman.

As a next step, consider expanding your API with additional features, such as pagination, filtering, or sorting. You can also explore adding testing frameworks like xUnit or NUnit to write unit tests for your API endpoints.

For more information, check out:

- [Neon Documentation](/docs)
- [Npgsql Documentation](https://www.npgsql.org/doc/index.html)
- [Entity Framework Core Documentation](https://docs.microsoft.com/en-us/ef/core/)

<NeedHelp />


# Authentication and Authorization in ASP.NET Core with ASP.NET Identity and Neon

---
title: Authentication and Authorization in ASP.NET Core with ASP.NET Identity and Neon
subtitle: Learn how to implement secure user authentication and authorization in ASP.NET Core applications using ASP.NET Identity with Neon Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-11-03T00:00:00.000Z'
updatedOn: '2024-11-03T00:00:00.000Z'
---

In this guide, we'll explore how to implement secure authentication and authorization in an ASP.NET Core application using ASP.NET Core Identity with Neon Postgres as the database backend. We'll cover user management, role-based authorization, and JWT token generation for secure API access.

## Prerequisites

Before we begin, ensure you have:

- .NET 8.0 or later installed
- A [Neon account](https://console.neon.tech/signup)
- Basic familiarity with ASP.NET Core and Entity Framework Core

## Project Setup

First, create a new ASP.NET Core Web API project with authentication:

```bash
dotnet new webapi -n NeonApi
cd NeonApi
```

With the project created, install the necessary packages:

```bash
dotnet add package Microsoft.AspNetCore.Identity.EntityFrameworkCore
dotnet add package Microsoft.AspNetCore.Authentication.JwtBearer
dotnet add package Npgsql.EntityFrameworkCore.PostgreSQL
dotnet add package Microsoft.EntityFrameworkCore.Design
dotnet add package Microsoft.EntityFrameworkCore
dotnet add package Microsoft.IdentityModel.Tokens
dotnet add package System.IdentityModel.Tokens.Jwt
```

The above packages provide support for ASP.NET Identity, JWT authentication, and PostgreSQL database integration.

### Configuring the Neon Database

Head over to your [Neon Dashboard](https://neon.tech) and create a new project.

Once done, grab your database connection string and add it to your `appsettings.json`:

```json
"ConnectionStrings": {
  "NeonConnection": "Host=<your-host>;Database=<your-database>;Username=<your-username>;Password=<your-password>;Port=5432"
}
```

The `appsettings.json` file is great for local development, but you should use environment variables or a secure vault for production.

While editing `appsettings.json`, add JWT configuration as well right below the connection string:

```json
"Jwt": {
  "Key": "your-very-secure-and-long-secret-key",
  "Issuer": "https://your-app.com",
  "Audience": "https://your-app.com"
}
```

The `Key` is a secret key used to sign and verify JWT tokens, while `Issuer` and `Audience` are used to validate the token's origin and intended recipient.

## Configuring ASP.NET Identity with Neon

In order to store additional information about users, we need to create a custom user class that extends the default Identity user provided by ASP.NET Core Identity.

This will allow us to add new properties, like `FirstName` and `LastName`, that are not included in the default `IdentityUser` class.

### Custom User Model

Let's create a new file named `ApplicationUser.cs` inside the `Models` folder with the following content:

```csharp
using Microsoft.AspNetCore.Identity;

public class ApplicationUser : IdentityUser
{
    public string FirstName { get; set; }
    public string LastName { get; set; }
    public DateTime CreatedAt { get; set; } = DateTime.UtcNow;
}
```

In the code above:

1. By inheriting from the `IdentityUser` class, `ApplicationUser` gets all the built-in properties like `UserName`, `Email`, `PasswordHash`, and more. This means we don't have to rewrite any of the existing authentication logic.

2. We added three new fields: `FirstName` and `LastName` allow us to store the user's personal details. And `CreatedAt` captures the date and time when the user was created, which can be helpful for tracking new sign-ups.

Why extend the `IdentityUser`? Well, the default user model is quite limited, and many real-world applications need to store more information than just usernames and emails. By creating a custom `ApplicationUser`, you can add more fields as you need to fit your application's requirements.

### Database Context Configuration

A database context is a class that represents a session with the database, allowing us to query and save data. In our case, we're setting up a context specifically for handling ASP.NET Identity and our custom `ApplicationUser` model.

Create the database context in `Data/ApplicationDbContext.cs`, inheriting from `IdentityDbContext<ApplicationUser>`:

```csharp
using Microsoft.AspNetCore.Identity;
using Microsoft.AspNetCore.Identity.EntityFrameworkCore;
using Microsoft.EntityFrameworkCore;

public class ApplicationDbContext : IdentityDbContext<ApplicationUser>
{
    public ApplicationDbContext(DbContextOptions<ApplicationDbContext> options) : base(options) { }

    protected override void OnModelCreating(ModelBuilder builder)
    {
        base.OnModelCreating(builder);

        // Custom configurations for Identity tables
        builder.Entity<ApplicationUser>(entity =>
        {
            entity.ToTable(name: "Users");
        });

        builder.Entity<IdentityRole>(entity =>
        {
            entity.ToTable(name: "Roles");
        });
    }
}
```

In this example, we create `ApplicationDbContext` by inheriting from `IdentityDbContext<ApplicationUser>`. This base class, `IdentityDbContext`, already includes all the necessary tables for ASP.NET Identity, such as tables for users, roles, and user claims. By specifying `ApplicationUser` as the type, we're telling ASP.NET Identity to use our custom user model.

The `OnModelCreating` method provides us with a chance to further configure the database schema. Here, we customize the names of the tables used by Identity to be more straightforward:

- **Users Table**: We rename the default user table to simply `Users` for clarity.
- **Roles Table**: Similarly, we rename the default roles table to `Roles`.

These configurations allow us to have simpler, more intuitive table names in the database, while still retaining all the built-in functionality of ASP.NET Identity.

This is not a requirement, but it can be helpful for keeping your database schema organized and easy to understand.

### Registering Services

Now that we have our `ApplicationDbContext` and user model set up, it's time to configure our application's services in `Program.cs` to enable Identity and authentication.

Open `Program.cs` and update it as follows:

```csharp
var builder = WebApplication.CreateBuilder(args);

// Add Neon database context
builder.Services.AddDbContext<ApplicationDbContext>(options =>
    options.UseNpgsql(builder.Configuration.GetConnectionString("NeonConnection")));

builder.Services.AddControllers();

// Configure Identity
builder.Services.AddIdentity<ApplicationUser, IdentityRole>(options =>
{
    // Password settings
    options.Password.RequireDigit = true;
    options.Password.RequireLowercase = true;
    options.Password.RequireUppercase = true;
    options.Password.RequireNonAlphanumeric = true;
    options.Password.RequiredLength = 8;

    // Lockout settings
    options.Lockout.DefaultLockoutTimeSpan = TimeSpan.FromMinutes(5);
    options.Lockout.MaxFailedAccessAttempts = 5;

    // User settings
    options.User.RequireUniqueEmail = true;
})
.AddEntityFrameworkStores<ApplicationDbContext>()
.AddDefaultTokenProviders();

// Configure JWT authentication
builder.Services.AddAuthentication(options =>
{
    options.DefaultAuthenticateScheme = JwtBearerDefaults.AuthenticationScheme;
    options.DefaultChallengeScheme = JwtBearerDefaults.AuthenticationScheme;
})
.AddJwtBearer(options =>
{
    options.TokenValidationParameters = new TokenValidationParameters
    {
        ValidateIssuer = true,
        ValidateAudience = true,
        ValidateLifetime = true,
        ValidateIssuerSigningKey = true,
        ValidIssuer = builder.Configuration["Jwt:Issuer"],
        ValidAudience = builder.Configuration["Jwt:Audience"],
        IssuerSigningKey = new SymmetricSecurityKey(
            Encoding.UTF8.GetBytes(builder.Configuration["Jwt:Key"]))
    };
});

// Add authorization
builder.Services.AddAuthorization(options =>
{
    options.AddPolicy("RequireAdminRole", policy =>
        policy.RequireRole("Admin"));
});

var app = builder.Build();

// Add roles on startup
using (var scope = app.Services.CreateScope())
{
    var roleManager = scope.ServiceProvider.GetRequiredService<RoleManager<IdentityRole>>();
    await RoleHelper.EnsureRolesCreated(roleManager);
}

// Map controllers
app.MapControllers();

// Configure middleware
app.UseAuthentication();
app.UseAuthorization();
```

We are doing quite a few things here:

1. First, we configure our database context using the connection string we defined earlier in `appsettings.json`. This connects our application to the Neon Postgres database, allowing it to store and retrieve user data.
1. Next, we set up ASP.NET Identity to manage user accounts which includes:
   - We enforce strong passwords.
   - We configure lockout settings to protect against brute force attacks.
   - We require users to have unique emails.
1. After setting up Identity, we configure the JWT authentication. This is where the `JWT` token configuration from the `appsettings.json` file comes into play as well. This allows our API to issue tokens to authenticated users, which can then be used to access secured endpoints.
1. Additionally, we define an authorization policy called `RequireAdminRole`, which restricts certain actions to users with the "Admin" role.
1. To make sure our application has the necessary roles, we include a piece of code that runs on startup to create roles like "Admin" and "User" if they don't already exist. This is done using a scoped service to access the `RoleManager`.
1. Finally, we map our controllers to handle HTTP requests and add the necessary middleware for authentication and authorization.

## Implementing Authentication Controllers

Now that we have configured our database and Identity services, let's create a controller to manage user registration and login. This controller will handle the core authentication flow for our application.

You can create this in a new file, `Controllers/AuthController.cs` with the following content:

```csharp
using System;
using System.IdentityModel.Tokens.Jwt;
using System.Security.Claims;
using System.Text;
using System.Threading.Tasks;
using Microsoft.AspNetCore.Authorization;
using Microsoft.AspNetCore.Identity;
using Microsoft.AspNetCore.Mvc;
using Microsoft.Extensions.Configuration;
using Microsoft.IdentityModel.Tokens;

[ApiController]
[Route("api/[controller]")]
public class AuthController : ControllerBase
{
    private readonly UserManager<ApplicationUser> _userManager;
    private readonly SignInManager<ApplicationUser> _signInManager;
    private readonly IConfiguration _configuration;

    public AuthController(
        UserManager<ApplicationUser> userManager,
        SignInManager<ApplicationUser> signInManager,
        IConfiguration configuration)
    {
        _userManager = userManager;
        _signInManager = signInManager;
        _configuration = configuration;
    }

    [HttpPost("register")]
    public async Task<IActionResult> Register(RegisterDto model)
    {
        var user = new ApplicationUser
        {
            UserName = model.Email,
            Email = model.Email,
            FirstName = model.FirstName,
            LastName = model.LastName
        };

        var result = await _userManager.CreateAsync(user, model.Password);
        if (result.Succeeded)
        {
            // Assign default role
            await _userManager.AddToRoleAsync(user, "User");
            return Ok(new { message = "Registration successful" });
        }

        return BadRequest(result.Errors);
    }

    [HttpPost("login")]
    public async Task<IActionResult> Login(LoginDto model)
    {
        var user = await _userManager.FindByEmailAsync(model.Email);
        if (user == null)
        {
            return BadRequest("Invalid credentials");
        }

        var result = await _signInManager.CheckPasswordSignInAsync(
            user, model.Password, lockoutOnFailure: false);

        if (result.Succeeded)
        {
            var token = GenerateJwtToken(user);
            return Ok(new { token });
        }

        return BadRequest("Invalid credentials");
    }

    private string GenerateJwtToken(ApplicationUser user)
    {
        var claims = new[]
        {
            new Claim(JwtRegisteredClaimNames.Sub, user.Email),
            new Claim(JwtRegisteredClaimNames.Jti, Guid.NewGuid().ToString()),
            new Claim(ClaimTypes.NameIdentifier, user.Id)
        };

        var key = new SymmetricSecurityKey(Encoding.UTF8.GetBytes(_configuration["Jwt:Key"]));
        var creds = new SigningCredentials(key, SecurityAlgorithms.HmacSha256);

        var token = new JwtSecurityToken(
            issuer: _configuration["Jwt:Issuer"],
            audience: _configuration["Jwt:Audience"],
            claims: claims,
            expires: DateTime.Now.AddHours(3),
            signingCredentials: creds);

        return new JwtSecurityTokenHandler().WriteToken(token);
    }
}
```

To begin, we create a new `AuthController` class that will handle user authentication. This includes two primary actions: user registration and user login. By using this controller, users will be able to create accounts and log in to receive a JWT, which they can use to access protected endpoints.

Here's how it works:

- The `AuthController` uses `UserManager` for user operations, `SignInManager` for handling sign-ins, and `IConfiguration` for accessing JWT settings.
- The `Register` method creates a new user with the provided details. Once created, the user is assigned the "User" role. If registration succeeds, an `Ok` response is returned; otherwise, a `BadRequest` with errors is sent.
- The `Login` method verifies if the email exists and checks the password. On success, a JWT token is generated and returned for authenticated access.
- The `GenerateJwtToken` method creates a token with the user's ID and email as claims. It signs the token using the secret key from `appsettings.json` and sets it to expire in 3 hours.

## Implementing Role-Based Authorization

To manage user roles effectively, we'll create a helper class that checks if the necessary roles are set up in your system. This is useful when you want to predefine certain roles like "Admin" or "User" and make them available as soon as the application starts instead of manually creating them.

You can create this in a new file, `Helpers/RoleHelper.cs` as follows:

```csharp
using Microsoft.AspNetCore.Identity;
using System.Threading.Tasks;

public static class RoleHelper
{
    public static async Task EnsureRolesCreated(RoleManager<IdentityRole> roleManager)
    {
        string[] roles = { "Admin", "User", "Manager" };

        foreach (var role in roles)
        {
            if (!await roleManager.RoleExistsAsync(role))
            {
                await roleManager.CreateAsync(new IdentityRole(role));
            }
        }
    }
}
```

In the `RoleHelper` class, we define a method called `EnsureRolesCreated` which:

- Accepts a `RoleManager` instance to interact with the roles in the database.
- Defines an array of roles we want to set up ("Admin", "User", and "Manager").
- For each role, it checks if the role already exists using `RoleExistsAsync()`. If the role doesn't exist, it creates the role with `CreateAsync()`.

This way, you only need to call this method once during application startup to ensure all required roles are available for assignment.

## Authorization Policies

In this section, we're adding authorization to protect certain API endpoints, so that only authenticated users or users with specific roles can access them.

Create a protected endpoint that requires authentication, and an admin-only endpoint. You can create this in a new file, `Controllers/SecureController.cs`:

```csharp
using Microsoft.AspNetCore.Authorization;
using Microsoft.AspNetCore.Mvc;

[Authorize]
[ApiController]
[Route("api/[controller]")]
public class SecureController : ControllerBase
{
    [HttpGet]
    public IActionResult Get()
    {
        return Ok(new { message = "This is a secure endpoint" });
    }

    [Authorize(Policy = "RequireAdminRole")]
    [HttpGet("admin")]
    public IActionResult AdminOnly()
    {
        return Ok(new { message = "This is an admin-only endpoint" });
    }
}
```

With the above, we created a new controller called `SecureController` with two endpoints:

1. General Protected Endpoint:

   - The `/api/secure` route is protected with `[Authorize]`, allowing access only to authenticated users with a valid JWT token.
   - If access is granted, it returns a confirmation message.

2. Admin-Only Endpoint:
   - The `/api/secure/admin` route is restricted to users with the "Admin" role using `[Authorize(Policy = "RequireAdminRole")]`.
   - Only "Admin" users can access this. Others will receive a `403 Forbidden` response.

Using the same approach, you can create additional policies for different roles or permissions. This allows you to control access to your API endpoints based on user roles.

## Database Migrations

To set up your database schema, we need to run migrations. Migrations help keep your database in sync with your data models, allowing you to make changes to your schema without losing data.

Run the following commands to create the database and apply migrations:

- Create the initial migration:

```bash
dotnet ef migrations add InitialCreate
```

- Apply the migration to the database:

```bash
dotnet ef database update
```

If you were to make changes to your data models in the future, you would create a new migration and apply it using the same commands. Via the Neon console, you will now see the tables created by ASP.NET Identity.

## Testing Authentication

You can test your authentication endpoints using Postman or `curl` to actually verify that everything is working correctly. Let's quickly do that using `curl`.

### 1. Register a New User

To create a new account, send a `POST` request to the `/api/auth/register` endpoint with the user details:

```json
curl -X POST http://localhost:5241/api/auth/register \
     -H "Content-Type: application/json" \
     -d '{
           "email": "test@example.com",
           "password": "SecurePass123!",
           "firstName": "John",
           "lastName": "Doe"
         }'
```

This should return a response confirming that the user was successfully registered. Make sure to use a strong password and valid email format as our password policy requires it.

### 2. Log In and Get a JWT Token

Once registered, log in using the credentials you just created. Send a `POST` request to the `/api/auth/login` endpoint:

```json
curl -X POST http://localhost:5241/api/auth/login \
     -H "Content-Type: application/json" \
     -d '{
           "email": "test@example.com",
           "password": "SecurePass123!"
         }'
```

If the login is successful, you'll receive a JSON response containing a JWT access token. Save this token securely, as it will be used to access protected routes.

### 3. Access a Protected Endpoint

With the JWT token from the previous step, you can now access secured endpoints. Send a `GET` request to `/api/secure` and include the token in the `Authorization` header:

```bash
curl -X GET http://localhost:5241/api/secure \
     -H "Authorization: Bearer <your-jwt-token>"
```

If the token is valid, you'll receive a response from the protected resource. If not, you'll get an authentication error, indicating the token has expired or is invalid.

## User Session Management with Refresh Tokens

As an optional step, to improve user session management, we'll implement a two-token authentication system using both access tokens and refresh tokens:

- An access token which is a short-lived JWT used to authenticate API requests
- A longer-lived tokens used to obtain new access tokens without requiring re-login

### Setting Up the Token System

First, create a model for refresh tokens in `Models/RefreshToken.cs` with an ID, token, and expiry date:

```csharp
public class RefreshToken
{
    public int Id { get; set; }
    public string Token { get; set; }
    public DateTime ExpiryDate { get; set; } = DateTime.UtcNow.AddDays(7);
}
```

Next, update the `ApplicationUser` model to store refresh tokens:

```csharp
public class ApplicationUser : IdentityUser
{
    public string FirstName { get; set; }
    public string LastName { get; set; }
    public List<RefreshToken> RefreshTokens { get; set; } = new List<RefreshToken>();
}
```

### Implementing Token Management

Modify the login endpoint to return both the access and the refresh tokens:

```csharp
[HttpPost("login")]
public async Task<IActionResult> Login(LoginDto model)
{
    var user = await _userManager.FindByEmailAsync(model.Email);
    if (user == null)
    {
        return BadRequest("Invalid credentials");
    }

    var result = await _signInManager.CheckPasswordSignInAsync(
        user, model.Password, lockoutOnFailure: false);

    if (result.Succeeded)
    {
        // Generate both tokens
        var accessToken = GenerateJwtToken(user);
        var refreshToken = GenerateRefreshToken();

        // Save refresh token to user
        user.RefreshTokens.Add(refreshToken);
        await _userManager.UpdateAsync(user);

        return Ok(new
        {
            accessToken = accessToken,
            refreshToken = refreshToken.Token
        });
    }

    return BadRequest("Invalid credentials");
}
```

Add the refresh token endpoint to obtain new tokens:

```csharp
[HttpPost("refresh-token")]
public async Task<IActionResult> RefreshToken([FromBody] RefreshTokenDto refreshTokenDto)
{
    var user = await _userManager.Users
        .SingleOrDefaultAsync(u => u.RefreshTokens
            .Any(t => t.Token == refreshTokenDto.Token &&
                     t.ExpiryDate > DateTime.UtcNow));

    if (user == null)
        return BadRequest("Invalid token");

    var newAccessToken = GenerateJwtToken(user);
    var newRefreshToken = GenerateRefreshToken();

    // Remove old refresh token
    user.RefreshTokens.RemoveAll(t => t.Token == refreshTokenDto.Token);
    user.RefreshTokens.Add(newRefreshToken);
    await _userManager.UpdateAsync(user);

    return Ok(new
    {
        accessToken = newAccessToken,
        refreshToken = newRefreshToken.Token
    });
}

private RefreshToken GenerateRefreshToken()
{
    return new RefreshToken
    {
        Token = Convert.ToBase64String(RandomNumberGenerator.GetBytes(64)),
        ExpiryDate = DateTime.UtcNow.AddDays(7)
    };
}

public class RefreshTokenDto
{
    [Required]
    public string Token { get; set; }
}
```

Here we've added a new endpoint `/api/auth/refresh-token` that accepts a refresh token and returns new access and refresh tokens. The refresh token is stored in the user's `RefreshTokens` list and used to generate new tokens when needed. The refresh token is valid for 7 days, after which the user will need to log in again.

### Using the Token System

Unlike the standard login flow, the new flow involves three steps:

1. User logs in with credentials:

   ```json
   curl -X POST http://localhost:5241/api/auth/login \
       -H "Content-Type: application/json" \
       -d '{
           "email": "user@example.com",
           "password": "password123"
           }'
   ```

   Response includes both tokens:

   ```json
   {
     "accessToken": "eyJhbG...",
     "refreshToken": "long-base64-string..."
   }
   ```

2. Use the access token for API requests:

   ```bash
   curl -X GET http://localhost:5241/api/protected-endpoint \
       -H "Authorization: Bearer eyJhbG..."
   ```

3. When the access token expires, use the refresh token to get new tokens:

   ```json
   curl -X POST http://localhost:5241/api/auth/refresh-token \
       -H "Content-Type: application/json" \
       -d '{
           "token": "long-base64-string..."
           }'
   ```

   This returns new access and refresh tokens:

   ```json
   {
     "accessToken": "new-eyJhbG...",
     "refreshToken": "new-long-base64-string..."
   }
   ```

As a security measure, store refresh tokens securely in your Neon database while also making sure that clients use secure methods like HTTP-only cookies. Also, keep access tokens short-lived, rotate refresh tokens on refresh, and implement token expiration and revocation to enhance security.

## Integrating Auth0 for Authentication and Authorization (Optional)

If you're looking to add an extra layer of security and use external identity providers, integrating your ASP.NET Core application with Auth0 is a good option. This allows your users to authenticate using social accounts (like Google, GitHub, etc.) or enterprise identity providers.

Auth0 offers a flexible platform for managing user authentication, with built-in JWT token support that integrates seamlessly with your existing ASP.NET Core application.

Let's quickly walk through setting up Auth0 with ASP.NET Core for secure authentication and authorization.

### Setting Up Auth0 with ASP.NET Core

To get started, follow these high-level steps:

1. Start by creating an Auth0 API:

   - Log in to your [Auth0 Dashboard](https://manage.auth0.com/).
   - Navigate to the "APIs" section and click **Create API**.
   - Provide a name and a unique identifier for your API (e.g., `https://your-app.com/api`). Keep the default signing algorithm as `RS256`.

2. In the API settings, you can define permissions (scopes) to control access to your API endpoints. For example, you can create a `read:messages` permission to restrict access to certain routes.

3. Open your `appsettings.json` and add the following configuration:

   ```json
   "Auth0": {
     "Domain": "your-auth0-domain",
     "Audience": "https://your-app.com/api"
   }
   ```

4. Make sure you have the required packages installed if you haven't already as in the previous steps:

   ```bash
   dotnet add package Microsoft.AspNetCore.Authentication.JwtBearer
   ```

5. Update the `Program.cs` file to add the authentication middleware:

   ```csharp
   var builder = WebApplication.CreateBuilder(args);
   var domain = $"https://{builder.Configuration["Auth0:Domain"]}/";

   builder.Services.AddAuthentication(JwtBearerDefaults.AuthenticationScheme)
       .AddJwtBearer(options =>
       {
           options.Authority = domain;
           options.Audience = builder.Configuration["Auth0:Audience"];
           options.TokenValidationParameters = new TokenValidationParameters
           {
               NameClaimType = ClaimTypes.NameIdentifier
           };
       });

   builder.Services.AddAuthorization(options =>
   {
       options.AddPolicy("read:messages", policy =>
           policy.Requirements.Add(new HasScopeRequirement("read:messages", domain)));
   });

   builder.Services.AddSingleton<IAuthorizationHandler, HasScopeHandler>();

   var app = builder.Build();
   app.UseAuthentication();
   app.UseAuthorization();
   app.MapControllers();
   ```

With all that in place, you can secure your API endpoints using Auth0, use the `[Authorize]` attribute:

```csharp
[ApiController]
[Route("api")]
public class ApiController : ControllerBase
{
    // Requires authentication
    [HttpGet("private")]
    [Authorize]
    public IActionResult PrivateEndpoint()
    {
        return Ok(new { Message = "Hello from a private endpoint!" });
    }

    // Requires specific scope
    [HttpGet("private-scoped")]
    [Authorize("read:messages")]
    public IActionResult ScopedEndpoint()
    {
        return Ok(new { Message = "Hello from a scoped endpoint!" });
    }
}
```

For a more information on integrating Auth0 with ASP.NET Core, refer to the [Auth0 Documentation](https://auth0.com/docs/quickstart/backend/aspnet-core-webapi/01-authorization). The documentation covers everything from setting up your Auth0 tenant to configuring scopes and securing your APIs.

## Conclusion

In this guide, we implemented a secure authentication and authorization system in an ASP.NET Core application using ASP.NET Identity with Neon Postgres as the backend. We walked through setting up user registration and login endpoints, securing API routes with JWT tokens, and implementing role-based authorization.

For more information, check out:

- [ASP.NET Core Identity Documentation](https://docs.microsoft.com/en-us/aspnet/core/security/authentication/identity)
- [JWT Authentication in ASP.NET Core](https://docs.microsoft.com/en-us/aspnet/core/security/authentication/jwt-bearer)
- [Neon Documentation](/docs)

<NeedHelp />


# Building AI-Powered Chatbots with Azure AI Studio and Neon

---
title: Building AI-Powered Chatbots with Azure AI Studio and Neon
subtitle: Learn how to create AI powered chatbot using Azure AI Studio with Neon Postgres as the backend database
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-11-24T00:00:00.000Z'
updatedOn: '2024-11-24T00:00:00.000Z'
---

In this guide, we'll walk through creating an AI-powered chatbot from scratch. We will be using Azure AI Studio, Neon Postgres as the backend database, React for the frontend interface and Express for the backend API.

We'll deploy a GPT-4 model to Azure AI Studio, which we will then use to build a support chatbot that can answer questions, store conversations, and learn from interactions over time.

## Prerequisites

Before we begin, make sure you have:

- An [Azure account](https://azure.microsoft.com/free/) with an active subscription
- A [Neon account](https://console.neon.tech/signup) and project
- Basic familiarity with SQL and JavaScript/TypeScript
- [Node.js](https://nodejs.org/) 18.x or later installed

## Setting up Your Development Environment

If you haven't already, follow these steps to set up your development environment:

### Create a Neon Project

1. Navigate to the [Neon Console](https://console.neon.tech)
2. Click "New Project"
3. Select Azure as your cloud provider
4. Choose East US 2 as your region
5. Give your project a name (e.g., "chatbot-db")
6. Click "Create Project"

Save your connection details - you'll need these to configure your chatbot's database connection.

### Create the Database Schema

A standard chatbot needs to store conversations and track how users interact with it. We'll create a database schema in Neon Postgres that stores messages, tracks user data, and helps us understand how well the chatbot is performing.

Our schema will include 4 tables:

- `users`: Stores user information
- `conversations`: Manages chat sessions
- `messages`: Stores the messages between users and the bot
- `feedback`: Records user ratings and comments

Connect to your Neon database and execute the following SQL statements to create the tables:

```sql
-- Create users table
CREATE TABLE users (
    user_id VARCHAR(255) PRIMARY KEY,
    first_name VARCHAR(100),
    last_name VARCHAR(100),
    email VARCHAR(255),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Create conversations table
CREATE TABLE conversations (
    conversation_id SERIAL PRIMARY KEY,
    user_id VARCHAR(255) REFERENCES users(user_id),
    started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_message_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    status VARCHAR(50) DEFAULT 'active' CHECK (status IN ('active', 'closed', 'archived'))
);

-- Create messages table
CREATE TABLE messages (
    message_id SERIAL PRIMARY KEY,
    conversation_id INTEGER REFERENCES conversations(conversation_id),
    sender_type VARCHAR(50) CHECK (sender_type IN ('user', 'bot')),
    content TEXT NOT NULL,
    sent_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    tokens_used INTEGER,
    completion_tokens INTEGER,
    total_tokens INTEGER
);

-- Create feedback table for message ratings
CREATE TABLE message_feedback (
    feedback_id SERIAL PRIMARY KEY,
    message_id INTEGER REFERENCES messages(message_id),
    user_id VARCHAR(255) REFERENCES users(user_id),
    rating INTEGER CHECK (rating >= 1 AND rating <= 5),
    comment TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

With our 4 tables in place, we have a schema which allows us to:

- Track user interactions and store user data
- Manage chat sessions and track when they started
- Store messages between users and the bot
- Collect feedback on messages to improve the chatbot

### Set Up Azure AI Studio Project

With your Neon database ready, let's set up Azure AI Studio to deploy our own GPT-4 model.

In order to access the Azure OpenAI Studio, you need to create an Azure OpenAI resource. Here's how you can do that:

1. Go to [the Azure OpenAI resources portal](https://oai.azure.com/portal)
1. Click the "Create new Azure OpenAI resource" button
1. Fill in the required fields like the subscription, resource group, region and name
1. Click the "Next" button
1. For the network settings, you can leave the default settings so that all networks can access the resource or you can restrict access to specific networks
1. Click "Next" and under "Review + create" click the "Create" button to create the resource

This will create a new Azure OpenAI resource for you. The deployment might take a few minutes to complete.

Once the deployment is completed, you can again visit the [Azure OpenAI portal](https://oai.azure.com/portal), and you should see your newly created resource there with type "OpenAI".

### Deploy the Azure OpenAI Model

With the Azure OpenAI resource set up, we can now deploy the GPT-4 model. To deploy the Azure OpenAI model:

1. Go to the [Azure OpenAI portal](https://oai.azure.com/portal) again
1. Click on your OpenAI resource that you created earlier
1. Click on the "Model catalog" tab
1. Find and click on the "gpt-4" model from the list
1. Click the "Deploy" button
1. Wait for deployment to complete - you'll receive an Endpoint URL and API key

There are other models available in the Azure OpenAI Studio, but for this guide, we'll use the GPT-4 model for our chatbot.

After deployment, click "Open in playground" to test the model. The playground is a web interface where you can:

1. Test your model by chatting with it directly
1. Add training data to help the model understand your specific needs
1. Adjust settings like:
   - Maximum response length (how long answers can be)
   - Temperature (higher = more creative, lower = more focused)
   - Top P and Presence Penalty (control response variety)

Feel free to experiment with these settings to see how they affect the model's responses.

#### Setting Up Model Instructions

You can give the model instructions about how it should behave. Think of this like training a new colleague - you're telling them:

- What they should do
- What they shouldn't do
- How they should talk to users
- What information they can access

For example, you might write:

```
You are a customer service agent for a tech company.
- Always be polite and professional
- Only answer questions about our products and services
- If you don't know something, say so
- Format prices as USD with two decimal places
- Include links to our documentation when relevant
```

These instructions will be included with every message to the model. The model will follow these instructions for all conversations.

#### Testing Your Instructions

After setting up instructions for the model, you can test them in the playground, for example:

1. Try different types of questions in the playground
1. Check if the model follows your guidelines
1. Adjust the instructions if needed
1. Save the instructions when you're happy with the responses

Additionally, you can add training data to help the model understand your specific needs. To learn more about training data, check the [Azure OpenAI Studio documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/fine-tuning).

## Building the Backend

With the Azure OpenAI model deployed, we can now build the backend API that will interact with the model and store chat data in our Neon database.

But before we start building our backend, let's quickly look at how to get the API code from Azure OpenAI Studio. This will help us make sure that we're using the correct API format.

### Getting the API Code from Azure OpenAI Studio

1. In the Azure OpenAI Studio playground, click "View code"
2. From the dropdown menu, select "JSON"
3. Under "Key authentication" you'll see a sample request like this:

   ```json
   {
     "messages": [
       {
         "role": "system",
         "content": [
           {
             "type": "text",
             "text": "You are a marketing writing assistant. You help come up with creative content ideas ...\"\n"
           }
         ]
       }
     ],
     "temperature": 0.7,
     "top_p": 0.95,
     "max_tokens": 800
   }
   ```

This shows us the exact format we need to use when making API calls to Azure OpenAI.

### Setting Up the Project

First, let's create a new Node.js project and install the dependencies that we'll need for our chatbot backend.

Create a new project folder and initialize a new Node.js project:

```bash
mkdir azure-neon-chatbot
cd azure-neon-chatbot
npm init -y
```

After that, install the required packages:

```bash
npm install express pg dotenv cors axios
```

The packages we're installing are:

- `express`: Web framework for building our API endpoints
- `pg`: PostgreSQL client for connecting to Neon
- `dotenv`: Environment variable management
- `cors`: Handles Cross-Origin Resource Sharing for our frontend
- `axios`: Makes HTTP requests to Azure OpenAI API

### Project Structure

Before we start, let's organize our project files in a way that makes our code easy to maintain and update. We'll use a standard Node.js project structure that separates our code into different directories based on functionality:

- `config`: Holds configuration files, including database connection settings
- `services`: Contains the core business logic for chat functionality and OpenAI integration
- `routes`: Manages API endpoints and request handling
- `utils`: Stores helper functions and shared utilities
- The `.env` file will store our sensitive configuration values like API keys

The project structure will look like this:

```sh
azure-neon-chatbot/
├── src/
│   ├── config/
│   │   └── database.js
│   ├── services/
│   │   ├── chatService.js
│   │   └── openaiService.js
│   ├── routes/
│   │   └── chatRoutes.js
│   └── utils/
│       └── logger.js
├── .env
└── server.js
```

This structure will help us keep our code organized and makes it easier for other developers to understand and work with the project.

### Environment Configuration

Before we start coding, let's set up our environment configuration.

Create a `.env` file in your project root with the following configuration:

```env
# Database Configuration
DATABASE_URL='postgresql://neondb_owner:<your_password>@<your_host>.eastus2.azure.neon.tech/neondb?sslmode=require'

# Azure OpenAI Configuration
AZURE_OPENAI_ENDPOINT=https://<your-resource-name>.openai.azure.com
AZURE_OPENAI_API_KEY=<your-api-key>
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4

# Server Configuration
PORT=3000
NODE_ENV=development
```

You'll need to replace `<your_password>`, `<your_host>`, `<your-resource-name>`, and `<your-api-key>` with your actual values.

You can get your Azure OpenAI API key from the Azure OpenAI Studio portal under the Chat playground.

### Database Configuration

Next, let's set up the database connection. We'll use the `pg` package to connect to our Neon Postgres database.

Create a `src/config/database.js` file with the following code:

```javascript
const { Pool } = require('pg');
require('dotenv').config();

const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  ssl: {
    rejectUnauthorized: false,
  },
});

pool.connect((err, client, release) => {
  if (err) {
    console.error('Error connecting to the database:', err);
    return;
  }
  console.log('Successfully connected to Neon database');
  release();
});

module.exports = {
  query: (text, params) => pool.query(text, params),
  pool,
};
```

This sets up a connection to the Neon Postgres database using the `pg` package. We use the `DATABASE_URL` environment variable to connect to the database.

### OpenAI Service

Next, let's create a service to interact with the Azure OpenAI API. This service will handle sending messages to the GPT-4 model that we deployed earlier.

Create a `src/services/openaiService.js` file with the following code:

```javascript
// src/services/openaiService.js
const axios = require('axios');
require('dotenv').config();

class OpenAIService {
  constructor() {
    if (!process.env.AZURE_OPENAI_API_KEY) {
      throw new Error('AZURE_OPENAI_API_KEY is required');
    }
    if (!process.env.AZURE_OPENAI_ENDPOINT) {
      throw new Error('AZURE_OPENAI_ENDPOINT is required');
    }
    if (!process.env.AZURE_OPENAI_DEPLOYMENT_NAME) {
      throw new Error('AZURE_OPENAI_DEPLOYMENT_NAME is required');
    }

    this.endpoint = process.env.AZURE_OPENAI_ENDPOINT;
    this.apiKey = process.env.AZURE_OPENAI_API_KEY;
    this.deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME;
  }

  async generateResponse(userMessage, conversationHistory = []) {
    try {
      const url = `${this.endpoint}/openai/deployments/${this.deploymentName}/chat/completions?api-version=2024-02-15-preview`;

      const payload = {
        messages: [
          {
            role: 'system',
            content: [
              {
                type: 'text',
                text: 'You are a marketing writing assistant. You help come up with creative content ideas and content like marketing emails, blog posts, tweets, ad copy and product descriptions. You write in a friendly yet professional tone but can tailor your writing style that best works for a user-specified audience. If you do not know the answer to a question, respond by saying "I do not know the answer to your question."\n',
              },
            ],
          },
          ...conversationHistory,
          {
            role: 'user',
            content: [
              {
                type: 'text',
                text: userMessage,
              },
            ],
          },
        ],
        temperature: 0.7,
        top_p: 0.95,
        max_tokens: 800,
      };

      const headers = {
        'Content-Type': 'application/json',
        'api-key': this.apiKey,
      };

      const response = await axios.post(url, payload, { headers });

      return response.data.choices[0].message.content;
    } catch (error) {
      console.error('Error calling Azure OpenAI:', error.response?.data || error.message);
      if (error.response?.status === 401) {
        console.error('Authentication failed. Please check your AZURE_OPENAI_API_KEY.');
      }
      throw error;
    }
  }
}

module.exports = new OpenAIService();
```

This creates a service that handles all communication with Azure OpenAI. It does two main things:

1. Checks that we have all the required Azure OpenAI settings (API key, endpoint, and deployment name) when the service starts up
2. Provides a `generateResponse` method that:
   - Takes a user's message and any previous conversation history
   - Sends it to our deployed GPT-4 model on Azure
   - Returns the model's response

The service includes the bot's base instructions (as a marketing assistant in this example) and error handling for common issues like authentication problems.

Feel free to adjust the instructions and settings to match your chatbot's needs.

### Chat Service

Next, let's create a service to manage chat interactions. This service will handle user messages, conversation history, and saving messages to the database.

Create `src/services/chatService.js` with the following code:

```javascript
// src/services/chatService.js
const db = require('../config/database');
const openai = require('./openaiService');

class ChatService {
  async ensureUserExists(userId) {
    // Check if user exists
    const existingUser = await db.query('SELECT user_id FROM users WHERE user_id = $1', [userId]);

    if (existingUser.rows.length === 0) {
      // Create new user if doesn't exist
      await db.query('INSERT INTO users (user_id, first_name, last_name) VALUES ($1, $2, $3)', [
        userId,
        'Anonymous',
        'User',
      ]);
    }
  }

  async saveMessage(conversationId, senderType, content) {
    const query = `
      INSERT INTO messages (conversation_id, sender_type, content)
      VALUES ($1, $2, $3)
      RETURNING *
    `;
    return db.query(query, [conversationId, senderType, content]);
  }

  async getConversationHistory(conversationId, limit = 10) {
    const query = `
      SELECT sender_type, content
      FROM messages
      WHERE conversation_id = $1
      ORDER BY sent_at DESC
      LIMIT $2
    `;
    return db.query(query, [conversationId, limit]);
  }

  async processMessage(userId, message) {
    try {
      await this.ensureUserExists(userId);

      await db.query('BEGIN');

      // Get or create conversation
      let conversationId;
      const existingConversation = await db.query(
        'SELECT conversation_id FROM conversations WHERE user_id = $1 ORDER BY started_at DESC LIMIT 1',
        [userId]
      );

      if (existingConversation.rows.length === 0) {
        const newConversation = await db.query(
          'INSERT INTO conversations (user_id) VALUES ($1) RETURNING conversation_id',
          [userId]
        );
        conversationId = newConversation.rows[0].conversation_id;
      } else {
        conversationId = existingConversation.rows[0].conversation_id;
      }

      // Save user message
      await this.saveMessage(conversationId, 'user', message);

      // Get conversation history
      const history = await this.getConversationHistory(conversationId);
      const formattedHistory = history.rows.map((msg) => ({
        role: msg.sender_type === 'user' ? 'user' : 'assistant',
        content: [{ type: 'text', text: msg.content }],
      }));

      // Generate AI response
      const aiResponse = await openai.generateResponse(message, formattedHistory);

      // Save AI response
      await this.saveMessage(conversationId, 'bot', aiResponse);

      // Commit transaction
      await db.query('COMMIT');

      return {
        conversationId,
        reply: aiResponse,
      };
    } catch (error) {
      // Rollback transaction on error
      await db.query('ROLLBACK');
      console.error('Error processing message:', error);
      throw error;
    }
  }

  async startConversation(userId) {
    try {
      await this.ensureUserExists(userId);

      // Create new conversation
      const result = await db.query(
        'INSERT INTO conversations (user_id) VALUES ($1) RETURNING conversation_id',
        [userId]
      );

      return {
        conversationId: result.rows[0].conversation_id,
        message: 'Conversation started successfully',
      };
    } catch (error) {
      console.error('Error starting conversation:', error);
      throw error;
    }
  }
}

module.exports = new ChatService();
```

This chat service manages all our conversations and messages. There is a lot going on in this service, so let's break it down:

1. Creates or finds users in our database
1. Message Handling:
   - Saves messages from both users and the bot
   - Retrieves conversation history
1. Conversation Flow:
   - Starts new conversations
   - Processes incoming messages
   - Gets responses from Azure OpenAI
   - Stores everything in our Neon database

We are also using database transactions to make sure that all related data (messages, user info, and conversations) is saved correctly, with rollback support if anything fails. This helps maintain data consistency in our chat application.

For this service you can think of it as the coordinator between our database, the Azure AI model, and our chat interface which we'll build next.

### Chat API Routes Implementation

With our services in place, let's create the API routes that will handle incoming requests from our chat interface.

Create a `src/routes/chatRoutes.js` file with the following:

```javascript
// src/routes/chatRoutes.js
const express = require('express');
const router = express.Router();
const chatService = require('../services/chatService');

// Initialize or continue a chat session
router.post('/start', async (req, res) => {
  const { userId } = req.body;
  try {
    const result = await chatService.startConversation(userId);
    res.json(result);
  } catch (error) {
    console.error('Error starting conversation:', error);
    res.status(500).json({ error: 'Failed to start conversation' });
  }
});

// Send a message and get a response
router.post('/message', async (req, res) => {
  const { userId, message } = req.body;
  try {
    const result = await chatService.processMessage(userId, message);
    res.json(result);
  } catch (error) {
    console.error('Error processing message:', error);
    res.status(500).json({ error: 'Failed to process message' });
  }
});

// Get conversation history
router.get('/history/:conversationId', async (req, res) => {
  const { conversationId } = req.params;
  try {
    const history = await chatService.getConversationHistory(conversationId);
    res.json(history.rows);
  } catch (error) {
    console.error('Error fetching history:', error);
    res.status(500).json({ error: 'Failed to fetch conversation history' });
  }
});

module.exports = router;
```

The above are our API endpoints that our chat interface will use to communicate with the backend. We set up three main routes:

1. `/start`: Creates a new conversation for a user
2. `/message`: Handles sending messages and getting responses from the bot
3. `/history`: Retrieves past messages from a conversation

Each route connects to our chat service to perform its specific task. We'll use these routes to build our chat interface in the next section.

### Server Setup

Finally, let's set up our Express server to run our chatbot API. We'll also add a health check endpoint and error handling middleware.

Create `server.js` in your project root with the following content:

```javascript
// server.js
const express = require('express');
const cors = require('cors');
const dotenv = require('dotenv');
const chatRoutes = require('./src/routes/chatRoutes');

// Load environment variables
dotenv.config();

const app = express();

// Middleware
app.use(cors());
app.use(express.json());

// Routes
app.use('/api/chat', chatRoutes);

// Health check endpoint
app.get('/health', (req, res) => {
  res.json({ status: 'ok', timestamp: new Date().toISOString() });
});

// Error handling middleware
app.use((err, req, res, next) => {
  console.error(err.stack);
  res.status(500).json({
    error: 'Internal Server Error',
    message: process.env.NODE_ENV === 'development' ? err.message : undefined,
  });
});

const PORT = process.env.PORT || 3000;
app.listen(PORT, () => {
  console.log(`Server running on port ${PORT}`);
  console.log(`Environment: ${process.env.NODE_ENV}`);
});
```

The above is our main application file that brings everything together. It sets up an Express server with:

- CORS support to allow frontend access
- JSON parsing for API requests
- Our chat routes at `/api/chat`
- A health check endpoint to monitor the server

We also include an error handling middleware to catch any unhandled exceptions and log them to the console for easier debugging.

### Running the Application

Starting the server is straightforward - just run `node server.js`. Once started, the server will:

- Connect to your Neon database
- Listen for chat requests
- Be ready to handle messages from the chat interface

You can now send requests to `http://localhost:3000/api/chat` (or whichever port you configured) to interact with your chatbot.

## Creating the React Frontend

With our backend API ready, let's create a React frontend for our chatbot using Tailwind CSS for styling. We'll use TypeScript for type safety and Vite for faster development.

### Create React Project

First, let's create a new React project using Vite:

```bash
npm create vite@latest chatbot-frontend -- --template react-ts
```

Then navigate to project directory:

```bash
cd chatbot-frontend
```

And install the dependencies for the project:

```bash
npm install
```

After that, let's install the necessary packages for our chatbot interface such as Tailwind CSS and Axios:

```bash
npm install -D tailwindcss postcss autoprefixer
```

Also, let's install some additional utilities for our chatbot interface like clsx, Heroicons, and date-fns:

```
npm install clsx @heroicons/react date-fns
```

The `clsx` package is used to conditionally apply CSS classes, `@heroicons/react` provides SVG icons, and `date-fns` helps with date formatting. Those are not required but will make our chat interface a bit more user-friendly.

### Configure Tailwind CSS

With Tailwind CSS installed, let's set it up in our project. Start by initializing a Tailwind CSS:

```bash
npx tailwindcss init -p
```

This will create a `tailwind.config.js` file in your project root. Update the file with the following configuration:

```javascript
/** @type {import('tailwindcss').Config} */
export default {
  content: ['./index.html', './src/**/*.{js,ts,jsx,tsx}'],
  theme: {
    extend: {
      colors: {
        chatbot: {
          primary: '#0EA5E9',
          secondary: '#0284C7',
          accent: '#38BDF8',
          background: '#F0F9FF',
        },
      },
    },
  },
  plugins: [],
};
```

The above configuration extends the default Tailwind theme with custom colors for our chatbot interface and also specifies the content files to process.

After that, add the Tailwind directives to the `src/index.css` file:

```css
@tailwind base;
@tailwind components;
@tailwind utilities;

@layer base {
  body {
    @apply bg-chatbot-background;
  }
}
```

This will apply the Tailwind CSS styles to our project, so we can use them in our components.

### Create Environment Configuration

Our chatbot interface will need to connect to the backend API to send and receive messages. Let's set up the API URL in our environment configuration.

Create `.env` file in project root:

```env
VITE_API_URL=http://localhost:3000/api
```

Make sure to replace the `VITE_API_URL` with the actual URL of your backend API. This will allow our chatbot interface to communicate with the backend application.

### Project Structure

For our chat interface, let's organize our React components into a maintainable structure:

- `components/Chat`: Contains all chat-related components like message bubbles and input fields
- `components/Layout`: Holds reusable layout components
- `hooks`: Stores custom React hooks for managing chat functionality
- `types`: Defines TypeScript interfaces for our chat data

This structure will allow us to separate our code into logical pieces, so that it will be easier to find and update specific parts of the application.

The project structure will look like this:

```
src/
├── components/
│   ├── Chat/
│   │   ├── ChatBubble.tsx
│   │   ├── ChatInput.tsx
│   │   └── ChatInterface.tsx
│   └── Layout/
│       └── Container.tsx
├── hooks/
│   └── useChat.ts
└── types/
    └── chat.ts
```

With our project structure in place, let's start building our chatbot interface. We will create the components for our chat interface starting with the types and basic components, then bringing it all together.

### 1. Define Message Types

First, let's define TypeScript types for our chat messages:

```typescript
// src/types/chat.ts
export interface Message {
  sender: 'user' | 'bot';
  content: string;
  timestamp?: Date;
}
```

This defines a `Message` interface that tracks:

- Who sent the message (`sender`)
- Message content (`content`)
- When it was sent (`timestamp`)

We'll use this type to manage chat messages in our application.

### 2. Create the Layout Container

Next, let's create a container component which will provide a consistent spacing and width for our chat interface:

```typescript
// src/components/Layout/Container.tsx
export const Container = ({ children }: { children: React.ReactNode }) => (
  <div className="max-w-4xl mx-auto px-4 py-8 min-h-screen">
    {children}
  </div>
);
```

The container is a simple component that wraps all our chat components in a centered, responsive layout.

### 3. Build the Message Bubble Component

Each chat message will be displayed as a bubble with different styles for user and bot messages:

```typescript
// src/components/Chat/ChatBubble.tsx
import { Message } from '../../types/chat';
import { format } from 'date-fns';
import clsx from 'clsx';

interface ChatBubbleProps {
  message: Message;
}

export const ChatBubble = ({ message }: ChatBubbleProps) => {
  const isUser = message.sender === 'user';

  return (
    <div
      className={clsx(
        'flex w-full mt-2 space-x-3 max-w-xs',
        isUser ? 'ml-auto justify-end' : ''
      )}
    >
      <div>
        <div
          className={clsx(
            'p-3 rounded-lg',
            isUser
              ? 'bg-chatbot-primary text-white'
              : 'bg-white text-gray-800 border border-gray-200'
          )}
        >
          <p className="text-sm">{message.content}</p>
        </div>
        {message.timestamp && (
          <span className="text-xs text-gray-500 leading-none">
            {format(message.timestamp, 'HH:mm')}
          </span>
        )}
      </div>
    </div>
  );
};
```

The chat bubble component:

- Takes a `message` prop with sender and content
- Uses different styles for user vs bot messages
- Shows message timestamp and aligns user messages to the right, bot messages to the left

We are going to use this component to render chat messages in the chat interface.

### 4. Create the Message Input Component

Next, let's build an input field for users to type messages and a submit button:

```typescript
// src/components/Chat/ChatInput.tsx
import { PaperAirplaneIcon } from '@heroicons/react/24/solid';
import clsx from 'clsx';

interface ChatInputProps {
  value: string;
  onChange: (value: string) => void;
  onSubmit: () => void;
  isLoading?: boolean;
}

export const ChatInput = ({ value, onChange, onSubmit, isLoading }: ChatInputProps) => {
  return (
    <div className="border-t bg-white p-4">
      <form
        className="flex space-x-4"
        onSubmit={(e) => {
          e.preventDefault();
          onSubmit();
        }}
      >
        <input
          type="text"
          value={value}
          onChange={(e) => onChange(e.target.value)}
          placeholder="Type your message..."
          className="flex-1 border border-gray-300 rounded-lg px-4 py-2 focus:outline-none focus:ring-2 focus:ring-chatbot-primary"
        />
        <button
          type="submit"
          disabled={isLoading}
          className={clsx(
            'rounded-lg px-4 py-2 text-white',
            isLoading
              ? 'bg-gray-400 cursor-not-allowed'
              : 'bg-chatbot-primary hover:bg-chatbot-secondary'
          )}
        >
          <PaperAirplaneIcon className="h-5 w-5" />
        </button>
      </form>
    </div>
  );
};
```

This component includes:

- A text input field for messages
- A submit button with loading state
- A simple form handling with `preventDefault()`

The input field will allow users to type messages and submit them to the chatbot over the Azure OpenAI API that we set up earlier.

### 5. Create the Chat Hook

After building the basic components, let's create a custom hook to manage chat state and interactions. This hook will handle sending messages, loading states, and API calls to the backend:

```typescript
// src/hooks/useChat.ts
import { useState, useCallback } from 'react';
import { Message } from '../types/chat';

export const useChat = () => {
  const [messages, setMessages] = useState<Message[]>([]);
  const [isLoading, setIsLoading] = useState(false);
  const userId = useState(() => 'user-' + Date.now())[0];

  const sendMessage = useCallback(
    async (content: string) => {
      try {
        setIsLoading(true);
        const userMessage: Message = {
          sender: 'user',
          content,
          timestamp: new Date(),
        };
        setMessages((prev) => [...prev, userMessage]);

        const response = await fetch(`${import.meta.env.VITE_API_URL}/chat/message`, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ userId, message: content }),
        });

        const data = await response.json();

        const botMessage: Message = {
          sender: 'bot',
          content: data.reply,
          timestamp: new Date(),
        };
        setMessages((prev) => [...prev, botMessage]);
      } catch (error) {
        console.error('Error sending message:', error);
        const errorMessage: Message = {
          sender: 'bot',
          content: 'Sorry, I encountered an error. Please try again.',
          timestamp: new Date(),
        };
        setMessages((prev) => [...prev, errorMessage]);
      } finally {
        setIsLoading(false);
      }
    },
    [userId]
  );

  return { messages, isLoading, sendMessage };
};
```

The hook handles:

- Message state management
- API calls to the backend
- Error handling with user feedback

This hook will be used in the main chat interface component to manage chat interactions and state updates.

### 6. Build the Main Chat Interface

Finally, let's combine everything into the main chat interface component:

```typescript
// src/components/Chat/ChatInterface.tsx
import { useState } from 'react';
import { ChatBubble } from './ChatBubble';
import { ChatInput } from './ChatInput';
import { useChat } from '../../hooks/useChat';

export const ChatInterface = () => {
  const [input, setInput] = useState('');
  const { messages, isLoading, sendMessage } = useChat();

  const handleSubmit = async () => {
    if (!input.trim()) return;
    const message = input;
    setInput('');
    await sendMessage(message);
  };

  return (
    <div className="flex flex-col h-[600px] bg-white rounded-lg shadow-lg">
      <div className="p-4 border-b bg-chatbot-primary text-white">
        <h2 className="text-xl font-bold">Neon AI Assistant</h2>
      </div>

      <div className="flex-1 overflow-y-auto p-4 space-y-4 bg-gray-50">
        {messages.map((message, idx) => (
          <ChatBubble key={idx} message={message} />
        ))}
      </div>

      <ChatInput
        value={input}
        onChange={setInput}
        onSubmit={handleSubmit}
        isLoading={isLoading}
      />
    </div>
  );
};
```

The main interface component:

- Uses our chat hook for state management
- Renders message history with `ChatBubble` components
- Handles message input with `ChatInput` component

This component will display the chat interface with message bubbles, input field, and submit button for users to interact with the chatbot.

### 7. Update App Component

Finally, we can update the main App component to use our chat interface and wrap it in a container for layout:

```typescript
// src/App.tsx
import { Container } from './components/Layout/Container';
import { ChatInterface } from './components/Chat/ChatInterface';

function App() {
  return (
    <Container>
      <ChatInterface />
    </Container>
  );
}

export default App;
```

This wraps our chat interface in the container component for proper layout and spacing.

You can now start the development server to see your chat interface:

```bash
npm run dev
```

Visit `http://localhost:5173` to test the chatbot interface. It will automatically connect to your backend API running on port 3000. Make sure your backend server is running before testing the chat interface.

## Conclusion

In this guide, we've built a simple AI-powered chatbot widget that combines Azure AI Studio with Neon's serverless Postgres database. This implementation works well for documentation websites and help systems, where the chatbot can be embedded as a widget to provide immediate assistance to users.

When the Azure OpenAI model is trained on your specific documentation or knowledge base, the chatbot can provide accurate, relevant responses about your product or service. This creates a seamless experience for anonymous users who can get quick answers without searching through documentation.

Also, by capturing chat interactions, user queries, bot responses, and feedback in your database, you can analyze where users face challenges and identify areas for documentation improvement.

As a next step, you can further train your Azure OpenAI model with more specific data to improve its accuracy and relevance and extend the chatbot's functionality to handle more complex queries and tasks.

## Additional Resources

- [Azure Bot Service Documentation](https://docs.microsoft.com/en-us/azure/bot-service/)
- [LUIS Documentation](https://docs.microsoft.com/en-us/azure/cognitive-services/luis/)
- [Neon Documentation](/docs)
- [Bot Framework SDK](https://learn.microsoft.com/en-us/azure/bot-service/index-bf-sdk)

<NeedHelp />


# Sentiment Analysis with Azure AI Services and Neon

---
title: Sentiment Analysis with Azure AI Services and Neon
subtitle: Learn how to analyze customer feedback using Azure AI Language and store results in Neon Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-11-30T00:00:00.000Z'
updatedOn: '2024-11-30T00:00:00.000Z'
---

Analyzing customer sentiment can help you understand your customer satisfaction and identify areas for improvement. The Azure AI Language Services provide tools for sentiment analysis, key phrase extraction, and language detection which can be used to analyze customer feedback and extract valuable insights.

In this guide, you'll learn how to use Azure AI Language Services to analyze customer feedback and save the results in Neon Postgres. We'll go through setting up your environment, creating a database to store feedback and analysis results, and running the analysis to get useful insights.

## Prerequisites

- An [Azure account](https://azure.microsoft.com/free/) with an active subscription
- A [Neon account](https://console.neon.tech/signup) and project
- Node.js 18.x or later
- Basic familiarity with SQL and JavaScript

## Setting Up Your Development Environment

If you haven't already, follow these steps to set up your development environment:

### Create a Neon Project

1. Navigate to the [Neon Console](https://console.neon.tech)
2. Click "New Project"
3. Select Azure as your cloud provider
4. Choose East US 2 as your region
5. Name your project (e.g., "sentiment-analysis")
6. Click "Create Project"

Save your connection details, you'll need them later to connect to your Neon database.

### Create Database Schema

Next, you'll set up the database tables to store customer feedback and sentiment analysis results. These tables will hold the feedback text, analysis scores, sentiment labels, key phrases, and timestamps.

Connect to your Neon database and create tables for storing our customer feedback and sentiment analysis results from the Azure AI Language service:

```sql
CREATE TABLE customer_feedback (
    feedback_id SERIAL PRIMARY KEY,
    customer_id VARCHAR(50),
    feedback_text TEXT NOT NULL,
    product_id VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE sentiment_results (
    result_id SERIAL PRIMARY KEY,
    feedback_id INTEGER REFERENCES customer_feedback(feedback_id),
    sentiment_score DECIMAL(4,3),
    sentiment_label VARCHAR(20),
    key_phrases TEXT[],
    language_code VARCHAR(10),
    analyzed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

With the tables created, let's move on to setting up the Azure AI Language service.

### Set Up Azure AI Language

The [Azure AI Language service](https://learn.microsoft.com/en-us/azure/ai-services/language-service/overview) provides a set of tools for text analytics, including sentiment analysis, key phrase extraction, and language detection. To use the service, you'll need to create a new Language Service resource in your Azure account.

1. Go to the [Azure portal](https://portal.azure.com/)
1. Search for "Azure AI Services" in the search bar
1. From the list of services, select "Language Service"
1. Click the "Create" button to create a new Language Service resource
1. Select your subscription and resource group
1. Choose a region (East US 2 for proximity to Neon)
1. Select a pricing tier (Free tier for testing)
1. Create the resource
1. Once created, copy the endpoint URL and access key

### Project Setup

For the sake of this guide, we'll create a Node.js script that analyzes existing feedback stored in the Neon database. In a real-world app, you could integrate this process directly so that whenever a user posts a review, a queued job or a scheduled task automatically analyzes the feedback right away.

Let's start by creating a new Node.js project:

```bash
mkdir sentiment-analysis
cd sentiment-analysis
npm init -y
```

After creating the project, install the required packages:

```bash
npm install @azure/ai-language-text pg dotenv
```

The packages we're using are:

- `@azure/ai-language-text`: [Azure AI Language client library for JavaScript](https://www.npmjs.com/package/@azure/ai-language-text), this will allow us to interact with the Azure AI Language service to analyze text instead of using the REST API directly.
- `pg`: A PostgreSQL client for Node.js, this will allow us to connect to the Neon database and store the analysis results.
- `dotenv`: A package for loading environment variables from a `.env` file.

With the packages installed, create a `.env` file in the project root and add your Azure AI Language service key and endpoint, as well as your Neon database connection URL:

```env
AZURE_LANGUAGE_KEY=your_key_here
AZURE_LANGUAGE_ENDPOINT=your_endpoint_here
DATABASE_URL=postgres://user:password@your-neon-host.cloud/dbname
```

Change the `DATABASE_URL` to match your Neon database connection details. Also, replace `your_key_here` and `your_endpoint_here` with your Azure AI Language service key and endpoint which you can find in the Azure portal under your Language Service resource.

## Implementation

With everything set up, let's start implementing the sentiment analysis script. We'll create separate modules for database connection, Azure AI Language client, analysis script, and report generation.

### Database Connection

In this step, we'll set up a connection to our Neon Postgres database using the `pg` package. This connection will allow you to read customer feedback and store sentiment analysis results whenever the analysis script runs.

We'll use a connection pool to manage multiple database connections efficiently, which is especially useful when running scripts that perform multiple queries.

Create a new file `src/database.js` and add the following code:

```javascript
const { Pool } = require('pg');
require('dotenv').config();

const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  ssl: true,
});

module.exports = pool;
```

The `pool` object will be used to connect to the database and execute queries. We'll get the connection details from the `.env` file using the `dotenv` package.

### Azure AI Language Client

Now let's set up the Azure AI Language client to perform our sentiment analysis and extract key phrases from customer feedback stored in our Neon database. Here is where we'll use the `@azure/ai-language-text` package to interact with the Azure AI Language service.

Create a new file `src/textAnalytics.js` and add the following code:

```javascript
const { TextAnalysisClient, AzureKeyCredential } = require('@azure/ai-language-text');
require('dotenv').config();

const client = new TextAnalysisClient(
  process.env.AZURE_LANGUAGE_ENDPOINT,
  new AzureKeyCredential(process.env.AZURE_LANGUAGE_KEY)
);

async function analyzeSentiment(text) {
  const actions = [
    {
      kind: 'SentimentAnalysis',
    },
  ];

  const [result] = await client.analyze('SentimentAnalysis', [text]);

  return {
    score: result.confidenceScores[result.sentiment],
    label: result.sentiment,
    language: result.language,
  };
}

async function extractKeyPhrases(text) {
  const [result] = await client.analyze('KeyPhraseExtraction', [text]);
  return result.keyPhrases;
}

module.exports = {
  analyzeSentiment,
  extractKeyPhrases,
};
```

A quick overview of what we've done here:

- **Azure Client Setup**: We create a `TextAnalysisClient` using the Azure endpoint and API key from the environment variables. This client handles communication with the Azure AI Language service.
- **`analyzeSentiment` Function**: Analyzes the sentiment of the provided text and returns the sentiment label (positive, negative, mixed, or neutral), the sentiment score, and the detected language.
- **`extractKeyPhrases` Function**: Extracts key phrases from the given text, helping identify the main topics or themes.

Now that the Azure AI Language client is ready, let's move on to the main analysis script.

### Main Analysis Script

Now that we have our database connection and Azure AI Language client set up, let's create a script to process customer feedback, analyze it, and store the results in our Neon database.

Create a new file `src/analyze.js` and add the following code:

```javascript
const db = require('./database');
const { analyzeSentiment, extractKeyPhrases } = require('./textAnalytics');

async function processFeedback() {
  const client = await db.connect();

  try {
    await client.query('BEGIN');

    // Get unanalyzed feedback
    const feedbackResult = await client.query(`
      SELECT f.feedback_id, f.feedback_text 
      FROM customer_feedback f
      LEFT JOIN sentiment_results s ON f.feedback_id = s.feedback_id
      WHERE s.feedback_id IS NULL
    `);

    for (const row of feedbackResult.rows) {
      // Analyze sentiment and extract key phrases
      const [sentiment, keyPhrases] = await Promise.all([
        analyzeSentiment(row.feedback_text),
        extractKeyPhrases(row.feedback_text),
      ]);

      // Store results in the sentiment_results table
      await client.query(
        `
        INSERT INTO sentiment_results 
        (feedback_id, sentiment_score, sentiment_label, key_phrases, language_code)
        VALUES ($1, $2, $3, $4, $5)
      `,
        [row.feedback_id, sentiment.score, sentiment.label, keyPhrases, sentiment.language]
      );
    }

    await client.query('COMMIT');
    console.log(`Processed ${feedbackResult.rows.length} feedback items`);
  } catch (err) {
    await client.query('ROLLBACK');
    throw err;
  } finally {
    client.release();
  }
}

module.exports = { processFeedback };
```

Here we've implemented the following:

1. We start by fetching the customer feedback that hasn't been analyzed yet. We do this by selecting feedback entries that don't have corresponding sentiment analysis results in the `sentiment_results` table.
1. Next, for each feedback entry, it uses the `analyzeSentiment` function to get the sentiment and the `extractKeyPhrases` function to identify key phrases. These operations are performed in parallel using `Promise.all` to speed up the process.
1. After that, we insert the sentiment score, sentiment label, key phrases, and detected language into the `sentiment_results` table.

It is worth mentioning that, we are also using a database transaction (`BEGIN`, `COMMIT`, and `ROLLBACK`) to ensure data integrity. If an error occurs, changes are rolled back.

We can use this script and run it periodically or triggered whenever new feedback is received to keep the sentiment analysis up-to-date.

### Analysis Reports

With the sentiment analysis results stored in the database, we can generate reports to extract insights from the data. Let's create a module to generate sentiment analysis reports based on the stored results.

Create a new file `src/reports.js` and add the following code:

```javascript
const db = require('./database');

async function generateSentimentReport() {
  const client = await db.connect();

  try {
    // Overall sentiment distribution
    const sentimentDist = await client.query(`
      SELECT
        sentiment_label,
        COUNT(*) as count,
        AVG(sentiment_score) as avg_score
      FROM sentiment_results
      GROUP BY sentiment_label
    `);

    // Trending negative feedback topics
    const negativeTopics = await client.query(`
      SELECT
        UNNEST(key_phrases) as topic,
        COUNT(*) as mentions
      FROM sentiment_results
      WHERE sentiment_label = 'negative'
      GROUP BY topic
      ORDER BY mentions DESC
      LIMIT 10
    `);

    return {
      sentimentDistribution: sentimentDist.rows,
      topNegativeTopics: negativeTopics.rows,
    };
  } finally {
    client.release();
  }
}

module.exports = { generateSentimentReport };
```

Here we've defined two main functions that generate two main reports:

- **Sentiment Distribution**: This report shows the count and average sentiment score for each sentiment label (positive, negative, mixed, neutral).
- **Top Negative Topics**: This report lists the most common key phrases in negative feedback, helping identify recurring issues or topics that need attention.

These reports can be used to track customer sentiment trends, identify common complaints, and prioritize areas for improvement. For example, you can set up alerts like sending an email or a Slack message whenever the sentiment score drops below a certain threshold or when a specific topic is mentioned frequently.

## Running the Analysis

To put it all together, we'll create a script that processes customer feedback, analyzes it using Azure AI Language, and generates reports to summarize the insights.

Create a new file `index.js` and add the following code:

```javascript
const { processFeedback } = require('./src/analyze');
const { generateSentimentReport } = require('./src/reports');

async function main() {
  try {
    // Process new feedback
    await processFeedback();

    // Generate reports
    const report = await generateSentimentReport();
    console.log('Sentiment Distribution:', report.sentimentDistribution);
    console.log('Top Negative Topics:', report.topNegativeTopics);
  } catch (err) {
    console.error('Error:', err);
    process.exit(1);
  }
}

main();
```

This script will run the sentiment analysis on the customer feedback stored in the Neon database and generate reports based on the analysis results. You can run this script manually or set up a scheduled job to run it periodically.

The script can be extended to include additional reports, alerts, or integrations with other services based on the sentiment analysis results but for now, let's focus on running the analysis.

## Testing the Analysis

As a final step, let's test the sentiment analysis script by adding some sample customer feedback to the database and running the analysis script.

If you haven't already, create a few sample feedback entries in the `customer_feedback` table which we can use for testing. Here's an example SQL script to insert sample feedback with different sentiment labels:

```sql
INSERT INTO customer_feedback (customer_id, feedback_text, product_id) VALUES
-- Positive feedback
('CUST001', 'The new dashboard design is fantastic! Much easier to find everything I need and the analytics features are exactly what we were looking for.', 'SAAS-DASH'),
('CUST002', 'Your customer support team is incredible. Had an issue with our API integration and they helped us resolve it within minutes.', 'SAAS-SUPPORT'),
('CUST003', 'The automated reporting feature saves me hours every week. Best investment we''ve made this year for our analytics stack!', 'SAAS-REPORT'),

-- Mixed feedback
('CUST004', 'Love most of the new features, but the export functionality is a bit limited. Would love to see more format options.', 'SAAS-EXPORT'),
('CUST005', 'Great platform overall, though it can be slow during peak hours. The UI is intuitive but some advanced features are hard to find.', 'SAAS-PERF'),
('CUST006', 'Good value for enterprise plan, but smaller teams might find it pricey. The collaboration features make it worth it for us.', 'SAAS-PRICE'),

-- Negative feedback
('CUST007', 'Been experiencing frequent timeouts for the past week. Our team''s productivity has taken a hit and support hasn''t provided a clear timeline for resolution.', 'SAAS-PERF'),
('CUST008', 'The recent UI update is a massive step backward. Can''t find basic features anymore and my team is frustrated.', 'SAAS-UI'),
('CUST009', 'Pricing increased by 40% with barely any notice. Considering switching to alternatives as this no longer fits our budget.', 'SAAS-PRICE'),

-- Detailed feedback
('CUST010', 'We''ve been using the platform for 6 months now. The API documentation is comprehensive and integration was smooth. The webhook reliability has been perfect, and the custom event tracking is powerful. Only wish the dashboard had more customization options.', 'SAAS-API'),
('CUST011', 'While the data visualization options are powerful, the learning curve is steep. Took our team weeks to fully understand all features. Once mastered though, the insights we get are invaluable.', 'SAAS-VIS'),
('CUST012', 'The collaboration features are game-changing for our remote team. Real-time editing, commenting, and version control work flawlessly. However, the mobile app needs improvement.', 'SAAS-COLLAB'),

-- Technical feedback
('CUST013', 'REST API rate limits are too restrictive on the growth plan. Had to upgrade just for the API limits even though we don''t need the other enterprise features.', 'SAAS-API'),
('CUST014', 'SSO integration was unnecessarily complicated. Documentation is outdated and support couldn''t help with our specific Azure AD setup.', 'SAAS-AUTH'),
('CUST015', 'The new GraphQL API is amazing! Much more efficient than the REST endpoints. Query performance improved our app''s load time significantly.', 'SAAS-API');
```

The feedback entries include a mix of positive, mixed, and negative sentiments, as well as some feedback to test the sentiment analysis and key phrase extraction.

After running the SQL script to insert the sample feedback, let's run the sentiment analysis script:

```bash
node index.js
```

Check the console output for sentiment analysis results and reports:

```shell
Processed 15 feedback items
Sentiment Distribution: [
  {
    sentiment_label: 'negative',
    count: '4',
    avg_score: '0.84500000000000000000'
  },
  {
    sentiment_label: 'positive',
    count: '9',
    avg_score: '0.96777777777777777778'
  },
  { sentiment_label: 'mixed', count: '1', avg_score: null },
  {
    sentiment_label: 'neutral',
    count: '1',
    avg_score: '0.88000000000000000000'
  }
]
Top Negative Topics: [
  { topic: 'support', mentions: '2' },
  { topic: 'team', mentions: '2' },
  { topic: 'recent UI update', mentions: '1' },
  { topic: 'past week', mentions: '1' },
  { topic: 'clear timeline', mentions: '1' },
  { topic: 'SSO integration', mentions: '1' },
  { topic: 'productivity', mentions: '1' },
  { topic: 'Pricing', mentions: '1' },
  { topic: 'resolution', mentions: '1' },
  { topic: 'hit', mentions: '1' }
]
```

This output gives you a snapshot of customer sentiment and highlights recurring issues which can help you identify areas for improvement.

## Automating the Analysis

While running the sentiment analysis manually is useful for testing, in a production environment you'll want to automate this process.

One option is to integrate the sentiment analysis code into your application, so it runs whenever new feedback is submitted.

Alternatively, you can use a scheduled task to process feedback at regular intervals. For example, you could create an Azure Function that runs every few hours to analyze the new feedback and generate reports.

1. If you haven't already, follow the [Azure Functions Quickstart guide](https://learn.microsoft.com/azure/azure-functions/functions-create-first-function) to set up your development environment.

2. Create a new Azure Function with a timer trigger. The schedule expression `0 0 */2 * * *` will run the function every two hours.

3. Replace the default function code with the following to process the feedback from your Neon Postgres database:

   ```javascript
   const { processFeedback } = require('./src/analyze');

   module.exports = async function (context, myTimer) {
     const timeStamp = new Date().toISOString();

     if (myTimer.isPastDue) {
       context.log('Timer function is running late!');
     }

     try {
       await processFeedback();
       context.log('Sentiment analysis completed successfully:', timeStamp);
     } catch (err) {
       context.log.error('Error processing feedback:', err);
       throw err;
     }
   };
   ```

The timer schedule is defined in the `function.json` file as follows:

```json
{
  "bindings": [
    {
      "name": "sentimentAnalysisTrigger",
      "type": "timerTrigger",
      "direction": "in",
      "schedule": "0 0 */2 * * *"
    }
  ]
}
```

This configuration makes sure that the function runs every two hours. You can adjust the schedule as needed using a [cron expression](https://learn.microsoft.com/en-gb/azure/azure-functions/functions-bindings-timer).

For more details on connecting Azure Functions to a Postgres database and deploying the function to Azure, see the [Building a Serverless Referral System with Neon Postgres and Azure Functions](/guides/azure-functions-referral-system) guide.

## Analyzing Results

Now that you've processed customer feedback and stored the sentiment analysis results, you can run SQL queries to extract insights from the data. Here are some example queries to get you started:

1. This query shows how sentiment varies over time, giving you a sense of customer satisfaction trends:

   ```sql
   SELECT
       DATE_TRUNC('day', cf.created_at) AS date,
       AVG(sr.sentiment_score) AS avg_sentiment,
       COUNT(*) AS feedback_count
   FROM customer_feedback cf
   JOIN sentiment_results sr ON cf.feedback_id = sr.feedback_id
   GROUP BY date
   ORDER BY date;
   ```

2. Identify the most mentioned topics in negative feedback to spot recurring issues:

   ```sql
   SELECT
       kp AS topic,
       COUNT(*) AS mentions
   FROM sentiment_results sr,
        UNNEST(key_phrases) kp
   WHERE sentiment_label = 'negative'
   GROUP BY kp
   ORDER BY mentions DESC
   LIMIT 10;
   ```

## Conclusion

In this guide, we covered how to analyze customer feedback using Azure AI Language Services and store the results in a Neon Postgres database. This setup is just a starting point. You can expand it by adding real-time triggers, building dashboards, or supporting multiple languages.

The Azure AI Language service also includes SDKs for other languages like [Python](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-textanalytics-readme), [Java](https://learn.microsoft.com/en-us/java/api/overview/azure/ai-textanalytics-readme), and [.NET](https://learn.microsoft.com/en-us/dotnet/api/overview/azure/ai.textanalytics-readme), so you can integrate sentiment analysis into your existing applications.

You can extend this system by adding more analysis, visualizations, or multi-language support based on your needs.

## Additional Resources

- [Azure AI Language Documentation](https://learn.microsoft.com/azure/ai-services/language-service/)
- [Neon Documentation](/docs)
- [Azure AI Language Client Library](https://learn.microsoft.com/javascript/api/overview/azure/ai-language-text-readme)

<NeedHelp />


# Building a Serverless Referral System with Neon Postgres and Azure Functions

---
title: Building a Serverless Referral System with Neon Postgres and Azure Functions
subtitle: Learn how to create a serverless referral system using Neon Postgres and Azure Functions
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-11-24T00:00:00.000Z'
updatedOn: '2024-11-24T00:00:00.000Z'
---

Serverless computing makes it easier for developers to build and run apps without worrying about managing and scaling servers. By combining Azure Functions with Neon's serverless Postgres, you can create scalable and cost-effective applications that handle high volumes of traffic with ease.

In this guide, we'll explore how to create a serverless referral system using Azure Serverless Functions and Neon Postgres. We'll build a simple serverless referral system that allows users to create referral codes, track successful referrals, and award points to referrers.

## Prerequisites

Before we begin, make sure you have:

- [Node.js](https://nodejs.org/) 18.x or later installed
- [Visual Studio Code](https://code.visualstudio.com/) with the [Azure Functions extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurefunctions)
- An [Azure account](https://azure.microsoft.com/free/) with an active subscription
- A [Neon account](https://console.neon.tech/signup) and project
- [Azure Functions Core Tools version 4.x](https://learn.microsoft.com/en-gb/azure/azure-functions/create-first-function-vs-code-node?pivots=nodejs-model-v4#install-or-update-core-tools)

## Creating Your Neon Project

Neon is now available in Azure! You can create serverless Postgres databases that run on Azure infrastructure. To learn more about Neon's Azure launch, check out the [announcement post](https://neon.tech/blog/neon-is-coming-to-azure).

To create your Neon project on Azure:

1. Navigate to the [Neon Console](https://console.neon.tech).
2. Click **Create Project**.
3. Give your project a name.
4. Under **Cloud Service Provider**, select **Azure**.
5. Select a **Region** for your project.
6. Select the **Compute size** for your Neon database.
7. Click **Create Project**.

Once created, save your connection details - you'll need these to configure your Azure Functions connection to Neon Postgres.

## Database Schema Design

Let's start by designing the database schema for our referral system. We'll need tables to track users, referral codes, and rewards.

1. Create the `users` table:

   ```sql
   CREATE TABLE users (
       user_id SERIAL PRIMARY KEY,
       email VARCHAR(255) UNIQUE NOT NULL,
       name VARCHAR(100),
       points INT DEFAULT 0,
       created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
   );
   ```

2. Create the `referral_codes` table:

   ```sql
   CREATE TABLE referral_codes (
       code_id SERIAL PRIMARY KEY,
       referrer_id INT REFERENCES users(user_id),
       code VARCHAR(10) UNIQUE NOT NULL,
       times_used INT DEFAULT 0,
       max_uses INT DEFAULT 10,
       points_per_referral INT DEFAULT 100,
       expires_at TIMESTAMP,
       created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
       CONSTRAINT valid_points CHECK (points_per_referral > 0)
   );
   ```

3. Create the `referrals` table to track successful user referrals:

   ```sql
   CREATE TABLE referrals (
       referral_id SERIAL PRIMARY KEY,
       referrer_id INT REFERENCES users(user_id),
       referred_id INT REFERENCES users(user_id),
       code_id INT REFERENCES referral_codes(code_id),
       points_awarded INT,
       status VARCHAR(20) DEFAULT 'pending',
       created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
       CONSTRAINT unique_referral UNIQUE(referred_id),
       CONSTRAINT valid_status CHECK (status IN ('pending', 'completed', 'rejected'))
   );
   ```

With the three tables in place, our database schema:

- Tracks users and their total points
- Manages referral codes with usage limits and expiration dates
- Records each referral transaction
- Enforces data integrity through constraints
- Prevents duplicate referrals for the same user

## Setting Up Your Development Environment

To start building serverless applications with Azure Functions you need to set up your development environment first.

### Installing Required Tools

If you don't have the required tools installed, you can follow these steps to set up your development environment:

1. Install the **Azure Functions** extension for VS Code (if you haven't already):

   - Open VS Code
   - Click the Extensions icon or press `Ctrl+Shift+X` or `Cmd+Shift+X`
   - Search for "Azure Functions"
   - Install the extension from Microsoft

2. Install **Azure Functions Core Tools version 4.x**:

   In Visual Studio Code, select `F1` to open the command palette, and then search for and run the command 'Azure Functions: Install or Update Core Tools'.

   The Core Tools provide a local development environment and CLI commands for creating, testing, and deploying Azure Functions.

### Project Setup

With the required Azure tools installed, you're ready to create your first Azure Functions project.

1. Open a terminal and navigate to the directory where you want to create your project.

   Create a new directory for your project:

   ```bash
   mkdir referral-system
   ```

   Change to the project directory:

   ```bash
   cd referral-system
   ```

   Initialize a new Azure Functions project:

   ```bash
   func init
   ```

   When prompted, select:

   - Node.js as the runtime
   - JavaScript as the language

   This might take a few moments to complete, and it creates a basic project structure with the following files:

   - `host.json`: Contains global configuration options
   - `local.settings.json`: Stores app settings and connection strings for local development
   - `package.json`: Manages project dependencies

2. Install the required dependencies:

   ```bash
   npm install pg uuid dotenv
   ```

   We're using:

   - `pg` for Postgres connection to Neon
   - `uuid` for generating unique referral codes
   - `dotenv` for environment variables management

3. Configure your database connection by creating a `.env` file:

   ```bash
   NEON_CONNECTION_STRING=postgres://user:password@ep-xyz.region.azure.neon.tech/neondb
   ```

   Replace `user`, `password`, `ep-xyz.region`, and `neondb` with your Neon connection details.

4. Create a database utility file `src/utils/db.js` for our connection management:

   ```js
   const { Pool } = require('pg');
   require('dotenv').config();

   const pool = new Pool({
     connectionString: process.env.NEON_CONNECTION_STRING,
     ssl: true,
   });

   // Wrapper for queries with automatic connection handling
   async function query(text, params) {
     const client = await pool.connect();
     try {
       const result = await client.query(text, params);
       return result;
     } finally {
       client.release();
     }
   }

   module.exports = {
     query,
     pool,
   };
   ```

   We will use this utility to execute queries against the Neon Postgres database across our Azure Functions.

With your project set up, you're ready to start building the core functions for your referral system.

## Creating the Core Functions

We'll create several Azure Functions to handle different aspects of the referral system.

Each function will be an HTTP-triggered function that interacts with the Neon Postgres database to generate referral codes, process referrals, and retrieve referral statistics.

Other types of triggers, such as queue or timer triggers, can also be used depending on your requirements, but for this guide, we'll focus on HTTP triggers.

### 1. Generate Referral Code

Let's start by creating a function to generate unique referral codes for users.

Run the following command to create a new HTTP-triggered function:

```bash
func new --name generateReferralCode --template "HTTP trigger"
```

This will create a new function in `src/functions/generateReferralCode.js`. Open the file and replace the contents with the following code:

```js
const { app } = require('@azure/functions');
const { v4: uuidv4 } = require('uuid');
const { query } = require('../utils/db');

app.http('generateReferralCode', {
  methods: ['POST'],
  authLevel: 'anonymous',
  handler: async (request, context) => {
    try {
      const { userId } = await request.json();

      // Generate unique 8-character code
      const code = uuidv4().split('-')[0];

      // Insert new referral code
      const result = await query(
        `INSERT INTO referral_codes
                (referrer_id, code, max_uses, points_per_referral, expires_at)
                VALUES ($1, $2, $3, $4, NOW() + INTERVAL '30 days')
                RETURNING *`,
        [userId, code, 10, 100]
      );

      return {
        status: 201,
        body: JSON.stringify({
          code: result.rows[0].code,
          expiresAt: result.rows[0].expires_at,
        }),
      };
    } catch (error) {
      context.log('Error generating referral code:', error);
      return {
        status: 500,
        body: 'Error generating referral code',
      };
    }
  },
});
```

This function:

- Generates an 8-character referral code using `uuidv4`
- Inserts the code into the `referral_codes` table with a 30-day expiration
- Returns the generated code and expiration date

The endpoint will be available at `http://localhost:7071/api/generateReferralCode` and expects a JSON payload with the `userId` of the referrer. We'll be testing this function shortly once all functions are in place.

### 2. Process Referral

With the referral code generation function in place, let's create a function to process referrals when new users sign up.

Create a new HTTP-triggered function, again using the Azure Functions CLI:

```bash
func new --name processReferral --template "HTTP trigger"
```

This will create a new function in `src/functions/processReferral.js`. Replace the contents with the following code:

```js
const { app } = require('@azure/functions');
const { query } = require('../utils/db');

app.http('processReferral', {
  methods: ['POST'],
  authLevel: 'anonymous',
  handler: async (request, context) => {
    try {
      const { referralCode, newUserEmail, newUserName } = await request.json();

      // Start transaction
      await query('BEGIN');

      // Verify referral code
      const codeResult = await query(
        `SELECT * FROM referral_codes
                WHERE code = $1
                AND times_used < max_uses
                AND expires_at > NOW()`,
        [referralCode]
      );

      if (codeResult.rows.length === 0) {
        await query('ROLLBACK');
        return {
          status: 400,
          body: 'Invalid or expired referral code',
        };
      }

      const code = codeResult.rows[0];

      // Create new user
      const newUserResult = await query(
        `INSERT INTO users (email, name)
                VALUES ($1, $2)
                RETURNING user_id`,
        [newUserEmail, newUserName]
      );

      // Record referral
      await query(
        `INSERT INTO referrals
                (referrer_id, referred_id, code_id, points_awarded, status)
                VALUES ($1, $2, $3, $4, 'completed')`,
        [code.referrer_id, newUserResult.rows[0].user_id, code.code_id, code.points_per_referral]
      );

      // Update referral code usage
      await query(
        `UPDATE referral_codes
                SET times_used = times_used + 1
                WHERE code_id = $1`,
        [code.code_id]
      );

      // Award points to referrer
      await query(
        `UPDATE users
                SET points = points + $1
                WHERE user_id = $2`,
        [code.points_per_referral, code.referrer_id]
      );

      await query('COMMIT');

      return {
        status: 200,
        body: JSON.stringify({
          message: 'Referral processed successfully',
        }),
      };
    } catch (error) {
      await query('ROLLBACK');
      context.log('Error processing referral:', error);
      return {
        status: 500,
        body: 'Error processing referral',
      };
    }
  },
});
```

There is a bit more going on in this function, so let's break it down:

- The function accepts `POST` requests with the referral code, new user email, and name
- It starts a transaction to ensure data integrity
- Verifies the referral code is valid and not expired
- Creates a new user record
- Records the referral transaction
- Updates the referral code usage and awards points to the referrer
- Commits the transaction if successful, otherwise rolls back

The endpoint will be available at `http://localhost:7071/api/processReferral` and expects a JSON payload with the referral code, new user email, and name.

### 3. Get Referral Stats

Finally, let's create a function to retrieve referral statistics for a given user.

Create a new HTTP-triggered function using the Azure Functions CLI:

```bash
func new --name getReferralStats --template "HTTP trigger"
```

Open the generated file `src/functions/getReferralStats.js` and replace the contents with the following code:

```js
const { app } = require('@azure/functions');
const { query } = require('../utils/db');

app.http('getReferralStats', {
  methods: ['GET'],
  authLevel: 'anonymous',
  handler: async (request, context) => {
    try {
      const userId = request.query.get('userId');

      const stats = await query(
        `SELECT
                    u.points as total_points,
                    COUNT(r.referral_id) as total_referrals,
                    COUNT(CASE WHEN r.status = 'completed' THEN 1 END) as successful_referrals,
                    COUNT(CASE WHEN r.created_at > NOW() - INTERVAL '30 days' THEN 1 END) as recent_referrals
                FROM users u
                LEFT JOIN referrals r ON u.user_id = r.referrer_id
                WHERE u.user_id = $1
                GROUP BY u.user_id, u.points`,
        [userId]
      );

      return {
        status: 200,
        body: JSON.stringify(stats.rows[0]),
      };
    } catch (error) {
      context.log('Error fetching referral stats:', error);
      return {
        status: 500,
        body: 'Error fetching referral stats',
      };
    }
  },
});
```

Let's again break down the function:

- The function accepts `GET` requests with a `userId` query parameter
- It retrieves the total points, total referrals, successful referrals, and recent referrals for the given user
- The query joins the `users` and `referrals` tables to calculate the statistics
- The results are returned as JSON

The endpoint will be available at `http://localhost:7071/api/getReferralStats?userId=1`, where `userId` is the ID of the user to fetch statistics for.

## Testing Your Functions Locally

With all the core functions in place, it's time to test them locally before deploying to Azure.

Let's test each function locally to ensure everything works as expected.

1. Start the Azure Functions runtime locally:

   ```bash
    func start
   ```

   You should see output indicating your functions are running, typically on `http://localhost:7071`. And you should see the URLs for each function:

   ```bash
   Functions:

    generateReferralCode: [POST] http://localhost:7071/api/generateReferralCode

    getReferralStats: [GET] http://localhost:7071/api/getReferralStats

    processReferral: [POST] http://localhost:7071/api/processReferral
   ```

2. To test the functions, you can use `curl` or a tool like Postman.

   First, create a test user in your database:

   ```sql
   INSERT INTO users (email, name)
   VALUES ('test@example.com', 'Test User')
   RETURNING user_id;
   ```

   Note the returned `user_id` (let's say it's 1) and use it to test the referral code generation:

   ```bash
   curl -X POST http://localhost:7071/api/generateReferralCode \
   -H "Content-Type: application/json" \
   -d '{"userId": 1}'
   ```

   You should receive a response like:

   ```json
   {
     "code": "a1b2c3d4",
     "expiresAt": "2024-12-29T14:30:00.000Z"
   }
   ```

   Check the response for the generated referral code and expiration date.

   Verify in the database:

   ```sql
   SELECT * FROM referral_codes WHERE referrer_id = 1;
   ```

3. With the referral code in hand, test the referral processing function.

   Using the referral code from the previous step:

   ```bash

   curl -X POST http://localhost:7071/api/processReferral \
    -H "Content-Type: application/json" \
    -d '{
        "referralCode": "a1b2c3d4",
        "newUserEmail": "friend@example.com",
        "newUserName": "Referred Friend"
    }'
   ```

   If the referral is successful, you should see:

   ```json
   {
     "message": "Referral processed successfully"
   }
   ```

   Otherwise, check the response for error messages from the function.

   Verify the changes in the database:

   ```sql
   -- Check the new user was created
   SELECT * FROM users WHERE email = 'friend@example.com';

   -- Check the referral was recorded
   SELECT * FROM referrals ORDER BY created_at DESC LIMIT 1;

   -- Verify points were awarded to the referrer
   SELECT points FROM users WHERE user_id = 1;

   -- Check referral code usage was incremented
   SELECT times_used FROM referral_codes WHERE code = 'a1b2c3d4';
   ```

4. Next, test the referral statistics function.

   Using the original user's ID:

   ```bash
   curl "http://localhost:7071/api/getReferralStats?userId=1"
   ```

   You should see something like:

   ```json
   {
     "total_points": 100,
     "total_referrals": 1,
     "successful_referrals": 1,
     "recent_referrals": 1
   }
   ```

5. You can also test error conditions to ensure your functions handle them correctly.

   Test invalid referral code:

   ```bash
   curl -X POST http://localhost:7071/api/processReferral \
    -H "Content-Type: application/json" \
    -d '{
        "referralCode": "invalid",
        "newUserEmail": "another@example.com",
        "newUserName": "Another Friend"
    }'
   ```

   Test duplicate referral by using the same email:

   ```bash
   curl -X POST http://localhost:7071/api/processReferral \
    -H "Content-Type: application/json" \
    -d '{
        "referralCode": "a1b2c3d4",
        "newUserEmail": "friend@example.com",
        "newUserName": "Duplicate User"
    }'
   ```

6. The Azure Functions runtime will output logs to your terminal.

   Watch for any errors or debugging information as you test:

   ```bash
   [2024-11-29T14:30:00.000Z] Executing 'Functions.generateReferralCode'
      (Reason='This function was programmatically called via the host APIs.', Id=...)
   ```

If you encounter any issues, you can check the function logs or query the database directly to understand what's happening. Common issues might include:

If you need to, you can reset the test data in your database state by running the following SQL commands to delete the test data:

```sql
-- Clean up test data
DELETE FROM referrals;
DELETE FROM referral_codes;
DELETE FROM users;
-- Reset sequences
ALTER SEQUENCE users_user_id_seq RESTART WITH 1;
ALTER SEQUENCE referral_codes_code_id_seq RESTART WITH 1;
ALTER SEQUENCE referrals_referral_id_seq RESTART WITH 1;
```

After testing your functions locally, you're ready to deploy them to Azure.

## Deploying to Azure

Deploying your referral system to Azure involves three main steps:

1. Create Azure Resources
2. Configure Application Settings
3. Deploy Your Functions

Let's walk through each step.

### Step 1: Create Azure Resources

First, you need to create the necessary resources in Azure: a resource group, a storage account, and a Function App.

1. Log in to Azure CLI:

   ```bash
   az login
   ```

   Follow the instructions to authenticate with your Azure account and select the appropriate subscription.

2. Create a resource group:

   ```bash
   az group create --name referral-system --location eastus2
   ```

   This command creates a new resource group named `referral-system` in the East US 2 region. You can choose a different region if needed. A resource group is a logical container for your Azure resources and helps you manage them together more efficiently for billing, access control, and monitoring.

3. Create a storage account:

   Azure Functions requires a storage account to store logs and runtime state.

   ```bash
   az storage account create \
     --name <referral-storage-unique-name> \
     --location eastus \
     --resource-group referral-system \
     --sku Standard_LRS
   ```

   Make sure to replace `referral-storage-unique-name` with a unique name for your storage account. Otherwise, the command will fail.

4. Create the Function App:

   Use the following command to create a Function App running Node.js 18 on the Consumption Plan:

   ```bash
   az functionapp create \
     --name referral-system \
     --storage-account <referral-storage-unique-name> \
     --consumption-plan-location eastus2 \
     --resource-group referral-system \
     --runtime node \
     --runtime-version 18 \
     --functions-version 4
   ```

   Replace `referral-system` with your desired Function App name and `referral-storage-unique-name` with your storage account name.

### Step 2: Configure Application Settings

After creating the Function App, you need to configure it to connect to your Neon database by setting environment variables.

1. Set the `NEON_CONNECTION_STRING` using the Azure CLI:

   ```bash
   az functionapp config appsettings set \
     --name referral-system \
     --resource-group referral-system \
     --settings NEON_CONNECTION_STRING="postgres://user:password@ep-xyz.region.azure.neon.tech/neondb?sslmode=require"
   ```

2. Alternative: Configure settings in the Azure Portal:

   - Go to your **Function App** in the Azure Portal.
   - Select **Configuration** under the **Settings** section.
   - Add a new application setting:
     - **Name:** `NEON_CONNECTION_STRING`
     - **Value:** `postgres://user:password@ep-xyz.region.azure.neon.tech/neondb`
   - Save your changes.

### Step 3: Deploy Your Functions

With everything set up, you can now deploy your functions to Azure directly from Visual Studio Code.

- Open **VS Code** and press `F1` to open the command palette.
- Search for and select **Azure sign in** to authenticate with your Azure account.
- Then again press `F1` to open the command palette.
- Search for and select **Azure Functions: Deploy to Function App...**.
- Select your Azure subscription.
- Choose the Function App (`referral-system`) you created earlier.
- Click **Deploy** when prompted.

You'll see a notification in VS Code once the deployment is successful but it may take a few minutes to complete.

### Step 4: Verify the Deployment

After deploying your functions, test them to ensure they're working correctly.

1. Retrieve the function URL:

   Use the Azure CLI to get the URL for your `generateReferralCode` function:

   ```bash
   az functionapp function show \
     --name referral-system \
     --resource-group referral-system \
     --function-name generateReferralCode \
     --query "invokeUrlTemplate"
   ```

2. Test the function:

   Replace `<function-url>` with the URL returned from the previous step:

   ```bash
   curl -X POST <function-url> \
     -H "Content-Type: application/json" \
     -d '{"userId": 1}'
   ```

## Performance Optimization

With your referral system deployed to Azure, you should consider some standard performance optimizations to make sure it scales well under load.

1. Add indexes to frequently queried columns:

   ```sql
   CREATE INDEX idx_referral_codes_code ON referral_codes(code);
   CREATE INDEX idx_referrals_referrer ON referrals(referrer_id);
   CREATE INDEX idx_referrals_status ON referrals(status);
   ```

   You can learn more about indexing in the [Neon documentation](https://neon.tech/postgresql/postgresql-indexes).

2. Implement connection pooling in your database utility:

   ```js
   const pool = new Pool({
     connectionString: process.env.NEON_CONNECTION_STRING,
     ssl: true,
     max: 20, // maximum number of clients
     idleTimeoutMillis: 30000,
     connectionTimeoutMillis: 2000,
   });
   ```

   Alternatively, you can use the [Neon connection pool](https://neon.tech/docs/connect/connection-pooling) feature to manage connections efficiently.

3. On the Azure Functions side, consider enabling [Azure Functions Premium Plan](https://azure.microsoft.com/en-us/pricing/details/functions/), which offers more control over scaling and performance.

### Cleaning Up Azure Resources (Optional)

If you no longer need the Azure resources created for this project, you can delete them to avoid incurring any charges.

1. Deleting the resource group will remove all associated resources, including the Function App, storage account, and any other resources within the group:

   ```bash
   az group delete --name referral-system --yes --no-wait
   ```

   - The `--yes` flag skips the confirmation prompt.
   - The `--no-wait` flag allows the command to run asynchronously.

2. To verify the deletion:
   - Log in to the [Azure Portal](https://portal.azure.com).
   - Navigate to **Resource Groups** and confirm that the `referral-system` group is no longer listed.

## Conclusion

You now have a scalable referral system built with Neon Postgres and Azure Functions. The system handles referral code generation, tracks successful referrals, awards points, and provides referral statistics.

As a next step, you can add more features to your referral system, such as authentication, email notifications, and user dashboards.

## Additional Resources

- [Neon Documentation](/docs)
- [Azure Functions Documentation](https://docs.microsoft.com/en-us/azure/azure-functions/)
- [Node-Postgres Documentation](https://node-postgres.com/)

<NeedHelp />


# Building Azure Static Web Apps with Neon

---
title: Building Azure Static Web Apps with Neon
subtitle: A step-by-step guide to creating and deploying static sites using Azure and Neon Postgres
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2024-12-14T00:00:00.000Z'
updatedOn: '2024-12-14T00:00:00.000Z'
---

If you’re looking for a modern way to deploy web applications without managing traditional server infrastructure, [Azure Static Web Apps](https://azure.microsoft.com/en-us/products/app-service/static) might be just what you need. It’s a service optimized for hosting static assets with global distribution, but its real strength lies in its integration with [Azure Functions](https://azure.microsoft.com/en-us/products/functions) for backend operations.

One of the most compelling features of Azure Static Web Apps is its built-in CI/CD pipeline powered by [GitHub Actions](https://github.com/features/actions). When you connect your repository, Azure configures the required GitHub workflows automatically. You only need to push your code, and GitHub Actions will build, optimize, and deploy your entire application across a global network.

In this guide, you’ll learn how to build a simple to-do application using Azure Static Web Apps. You’ll cover the basics of getting your website online and creating your first API endpoint with Azure Functions. By the end, you’ll understand how to combine static web content with dynamic features to build a fully functional web application.

## Prerequisites

Before you begin, make sure you have:

- [Node.js](https://nodejs.org/) 18.x or later installed
- [Visual Studio Code](https://code.visualstudio.com/)
- An [Azure account](https://azure.microsoft.com/free/) with an active subscription

## Create a Neon Project

Neon is now available in Azure! You can create serverless Postgres databases that run on Azure infrastructure. To learn more about Neon's Azure launch, check out the [announcement post](/blog/neon-is-coming-to-azure).

To create your Neon project on Azure, follow our [Getting Started with Neon on Azure guide](/guides/neon-azure-integration).

Once created, save your database connection string, which you'll need to connect to your Neon Postgres database from Azure Functions.

## Database Schema

For your todo application, you'll need a simple database schema to store todo items. You'll create a `todos` table with the following fields:

- `id`: Auto-incrementing unique identifier (Primary Key)
- `text`: Required text field to store the task description
- `completed`: Boolean field with a default value of false to track task completion status

\*You'll be creating the table via Azure Functions later in the guide.

## Setting up your development environment

To begin building your Azure Static Web App with Neon Postgres, you'll need to set up your development environment. This involves installing the required tools and configuring your project.

### Installing required tools

1. Install the **Azure Static Web Apps** and **Azure Functions** extensions for Visual Studio Code:

   - Open VS Code.
   - Click the Extensions icon or press `Ctrl+Shift+X` or `Cmd+Shift+X`.
   - Search for "Azure Static Web Apps" and "Azure Functions" extensions.
   - Install both of the extensions from Microsoft.

   ![Extensions to Download](/docs/guides/swa-extensions-to-download.png)

2. Install **Azure Functions Core Tools version 4.x**:

   In Visual Studio Code, select `F1` to open the command palette, and then search for and run the command 'Azure Functions: Install or Update Core Tools' and select **Azure Functions v4**.

3. Install the **Azure Static Web Apps CLI**:

   Open a terminal and run the following command to install the Azure Static Web Apps CLI:

   ```bash
   npm install -g @azure/static-web-apps-cli
   ```

   The CLI makes it easy to run your static web app and Azure Functions locally for testing and debugging.

### Project setup

With the Azure tools installed, you can now create your new Azure Static Web App.

Open a terminal and navigate to the directory where you want to create your project:

```bash
mkdir swa-todo
cd swa-todo && code .
```

## Creating the Static Web App

You'll start by creating the frontend of your todo application. The frontend will be a simple HTML, CSS, and JavaScript application that allows you to add, update, and delete todo items. For the backend, you'll use Azure Functions to handle API requests and interact with the Neon Postgres database.

Architecture overview:

- Frontend: A web application built with HTML, CSS, and JavaScript.
- Backend: Serverless API endpoints using Azure Functions.
- Hosting: Azure Static Web Apps for reliable and scalable web hosting.
- Database: Neon serverless Postgres for storing todo data.

Project structure:

```
swa-todo/
├── index.html     # The main HTML file for the todo app
├── styles.css     # The CSS file for styling the app
└── app.js         # The JavaScript file for handling user interactions
└── api/           # Azure Functions backend
```

### Building the Frontend

Create a new file named `index.html` in the root directory of your project and add the following content:

```html
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Simple Todo App</title>
    <link rel="stylesheet" href="styles.css" />
  </head>
  <body>
    <div class="todo-container">
      <h1>Todo List</h1>
      <div class="todo-input">
        <input type="text" id="todoInput" placeholder="Add a new todo" />
        <button onclick="addTodo()">Add</button>
      </div>
      <ul id="todoList" class="todo-list">
        <!-- Todos will be inserted here -->
      </ul>
      <div id="error" class="error"></div>
    </div>
    <script src="app.js"></script>
  </body>
</html>
```

The simple html file contains a form for adding new todos and a list to display existing todos. To display the todos, we'll use JavaScript to fetch the data from the backend API and update the DOM accordingly.

Create a new file named `styles.css` in the root directory of your project and add the following CSS styles:

```css
/* Base styles */
body {
  font-family: Arial, sans-serif;
  max-width: 600px;
  margin: 0 auto;
  padding: 20px;
  background-color: #f5f5f5;
}

/* Container styles */
.todo-container {
  background-color: white;
  padding: 20px;
  border-radius: 8px;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
}

/* Input area styles */
.todo-input {
  display: flex;
  gap: 10px;
  margin-bottom: 20px;
}

input[type='text'] {
  flex-grow: 1;
  padding: 8px;
  border: 1px solid #ddd;
  border-radius: 4px;
}

button {
  padding: 8px 16px;
  background-color: #4caf50;
  color: white;
  border: none;
  border-radius: 4px;
  cursor: pointer;
  transition: background-color 0.2s;
}

button:hover {
  background-color: #45a049;
}

/* Todo list styles */
.todo-list {
  list-style: none;
  padding: 0;
}

.todo-item {
  display: flex;
  align-items: center;
  gap: 10px;
  padding: 10px;
  border-bottom: 1px solid #eee;
}

.todo-item:last-child {
  border-bottom: none;
}

.todo-item.completed span {
  text-decoration: line-through;
  color: #888;
}

/* Delete button styles */
.delete-btn {
  background-color: #f44336;
  margin-left: auto;
}

.delete-btn:hover {
  background-color: #da190b;
}

/* Utility styles */
.loading {
  text-align: center;
  color: #666;
  padding: 20px;
}

.error {
  color: #f44336;
  margin-top: 10px;
}
```

The CSS file contains styles for the todo app, including the layout, input fields, buttons, and todo list items. You can customize the styles to match your design preferences. The example uses a simple design to help you get started.

Create a new file named `app.js` in the root directory of your project and add the following JavaScript code:

```javascript
// State management
let todos = [];

// Initialize app
document.addEventListener('DOMContentLoaded', loadTodos);

// Load todos from API
async function loadTodos() {
  try {
    showLoading();
    const response = await fetch('/api/todos');
    todos = await response.json();
    if (!response.ok) throw new Error('Failed to load todos');
  } catch (error) {
    showError(error.message);
  } finally {
    hideLoading();
    renderTodos();
  }
}

// Add new todo
async function addTodo() {
  const input = document.getElementById('todoInput');
  const text = input.value.trim();

  if (!text) return;

  try {
    const response = await fetch('/api/todos', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ text, completed: false }),
    });

    const todo = await response.json();
    if (!response.ok) throw new Error('Failed to add todo');
    const todoId = todo?.[0]?.id;

    todos.push({ id: todoId, text, completed: false });
    renderTodos();
    input.value = '';
  } catch (error) {
    showError(error.message);
  }
}

// Toggle todo completion
async function toggleTodo(id) {
  try {
    const todo = todos.find((t) => t.id === id);
    if (!todo) throw new Error('Todo not found');

    const response = await fetch('/api/todos', {
      method: 'PUT',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ id, completed: !todo.completed }),
    });

    await response.json();
    if (!response.ok) {
      throw new Error('Failed to update todo');
    }
    todos[todos.indexOf(todo)].completed = !todo.completed;
    renderTodos();
  } catch (error) {
    showError(error.message);
    await loadTodos();
  }
}

// Delete todo
async function deleteTodo(id) {
  try {
    const response = await fetch('/api/todos', {
      method: 'DELETE',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ id }),
    });

    await response.json();
    if (!response.ok) throw new Error('Failed to delete todo');

    todos = todos.filter((todo) => todo.id !== id);
    renderTodos();
  } catch (error) {
    showError(error.message);
    await loadTodos();
  }
}

// Render todos to DOM
function renderTodos() {
  const todoList = document.getElementById('todoList');
  if (!todoList) {
    console.error('Todo list container not found');
    return;
  }

  todoList.innerHTML = '';

  todos.forEach((todo) => {
    const li = document.createElement('li');
    li.className = `todo-item ${todo.completed ? 'completed' : ''}`;

    const checkbox = document.createElement('input');
    checkbox.type = 'checkbox';
    checkbox.checked = todo.completed;
    checkbox.addEventListener('change', () => toggleTodo(todo.id));

    const span = document.createElement('span');
    span.textContent = todo.text;

    const deleteButton = document.createElement('button');
    deleteButton.className = 'delete-btn';
    deleteButton.textContent = 'Delete';
    deleteButton.addEventListener('click', () => deleteTodo(todo.id));

    li.append(checkbox, span, deleteButton);
    todoList.appendChild(li);
  });
}

// Utility functions
function showLoading() {
  document.getElementById('todoList').innerHTML = '<div class="loading">Loading...</div>';
}

function hideLoading() {
  document.getElementById('todoList').innerHTML = '';
}

function showError(message) {
  const errorDiv = document.getElementById('error');
  errorDiv.textContent = message;
  setTimeout(() => (errorDiv.textContent = ''), 3000);
}
```

The JavaScript file contains the core functions for managing todos, namely loading todos from the backend API, adding new todos, toggling completion status, and deleting todos. It also includes utility functions for showing loading indicators and error messages to the user.

Let's break down the key functions and features of the todo app:

Core Functions:

1. `loadTodos()`

   - Fetches existing todos from the Azure Functions API endpoint via `GET` request
   - Handles loading states and error conditions
   - Automatically called when the DOM loads
   - Updates the UI with current todo items

2. `addTodo()`

   - Creates new todo items via `POST` request to the API
   - Validates input to prevent empty submissions
   - Updates local state and UI after successful creation
   - Includes error handling with user feedback

3. `toggleTodo(id)`

   - Updates todo completion status via `PUT` request
   - Includes error handling and state updates

4. `deleteTodo(id)`
   - Removes todos via `DELETE` request to the API
   - Updates local state after successful deletion

UI Management:

1. `renderTodos()`
   - Dynamically generates DOM elements for each todo.
   - Handles todo item styling based on completion status.
   - Creates interactive elements (checkboxes, delete buttons).

Utility Functions:

1. `showLoading()`

   - Displays loading indicator during API operations.
   - Provides visual feedback for better user experience.

2. `hideLoading()`

   - Removes loading indicator after operations complete.
   - Prepares UI for content display.

3. `showError(message)`
   - Displays error messages to users.
   - Implements auto-dismissing notifications (3-second timeout).
   - Provides clear feedback for error conditions.

State Management:

- All todo items are stored in the `todos` array.

Error Handling:

- Comprehensive try-catch blocks around API operations.
- Detailed error messages for debugging.

### Testing the Frontend locally

To test the frontend, in your terminal, run the following command to start a local server:

```bash
swa start
```

This will start a local server on [`http://localhost:4280`](http://localhost:4280) where you can access the todo app. Open the URL in your browser to be greeted with the initial todo list interface:

![Todo App Initial State](/docs/guides/swa-todo-app-intial.png)

\*Note that full functionality will be available once you set up the Azure Functions backend API.

### Creating Azure Functions

Azure Functions lets you write backend code that runs without managing any servers. We'll use it to create API endpoints that handle all our todo operations – creating, reading, updating, and deleting todos from our database. Think of these functions as small pieces of code that wake up when needed and automatically handle user requests.

Create a new directory named `api` in the root of your project:

```bash
mkdir api && cd api
```

Press F1 in VS Code to open the command palette and run the command `Azure Static Web Apps: Create HTTP Function`. Choose `JavaScript` as the language with the Model version `v4`. Name the function `todos` when prompted.

This will create a new Azure Function in the `api` directory, which will serve as the backend API for our todo application with an api endpoint `/api/todos`.

You'll need the [`@neondatabase/serverless`](https://www.npmjs.com/package/@neondatabase/serverless) package to connect to the Neon Postgres database. Install the package by running the following command in the `api` directory:

```bash
npm install @neondatabase/serverless
```

Edit the `api/src/functions/todos.js` file to add the following code:

```javascript
const { app } = require('@azure/functions');
const { neon } = require('@neondatabase/serverless');

const sql = neon(process.env.DATABASE_URL);

// Helper function to get todos and create table if it doesn't exist
const getTodos = async () => {
  try {
    const todos = await sql`SELECT * FROM todos`;
    return todos;
  } catch (error) {
    if (error.code === '42P01') {
      // Table does not exist, so create it
      await sql`
        CREATE TABLE todos (
          id SERIAL PRIMARY KEY,
          text TEXT NOT NULL,
          completed BOOLEAN NOT NULL
        )`;
      return [];
    }
    throw error;
  }
};

app.http('todos', {
  methods: ['GET', 'POST', 'PUT', 'DELETE'],
  authLevel: 'anonymous',
  handler: async (request, context) => {
    const method = request.method.toLowerCase();

    try {
      switch (method) {
        case 'get':
          context.log('Processing GET request for todos');
          const todos = await getTodos();
          return { status: 200, jsonBody: todos };

        case 'post':
          const newTodo = await request.json();
          context.log('Adding new todo:', newTodo);

          if (!newTodo.text) {
            return {
              status: 400,
              jsonBody: { error: 'Todo text is required' },
            };
          }

          const createdTodo = await sql`
            INSERT INTO todos (text, completed)
            VALUES (${newTodo.text}, ${newTodo.completed || false})
            RETURNING *
          `;
          return { status: 201, jsonBody: createdTodo };

        case 'put':
          const updatedTodo = await request.json();
          context.log('Updating todo:', updatedTodo);

          if (!updatedTodo.id) {
            return {
              status: 400,
              jsonBody: { error: 'Todo ID is required' },
            };
          }

          const todo = await sql`
            UPDATE todos
            SET completed = ${updatedTodo.completed}
            WHERE id = ${updatedTodo.id}
            RETURNING *
          `;

          if (todo.length === 0) {
            return {
              status: 404,
              jsonBody: { error: 'Todo not found' },
            };
          }
          return { status: 200, jsonBody: todo };

        case 'delete':
          const { id } = await request.json();
          context.log('Deleting todo:', id);

          if (!id) {
            return {
              status: 400,
              jsonBody: { error: 'Todo ID is required' },
            };
          }

          const deletedTodo = await sql`
            DELETE FROM todos
            WHERE id = ${id}
            RETURNING *
          `;

          if (deletedTodo.length === 0) {
            return {
              status: 404,
              jsonBody: { error: 'Todo not found' },
            };
          }
          return {
            status: 200,
            jsonBody: { message: 'Todo deleted successfully' },
          };

        default:
          return {
            status: 405,
            jsonBody: { error: 'Method not allowed' },
          };
      }
    } catch (error) {
      context.error(`Error processing ${method} request:`, error);
      return {
        status: 500,
        jsonBody: { error: `Failed to process ${method} request` },
      };
    }
  },
});
```

This Azure Function (`todos.js`) serves as our API endpoint and handles all database operations. Here's the detailed breakdown:

Core Components:

1. Database setup

   ```javascript
   const { neon } = require('@neondatabase/serverless');
   const sql = neon(process.env.DATABASE_URL);
   ```

   Establishes connection to our Postgres using the Neon serverless driver.

2. Auto-initialization of the `todos` table

   ```javascript
   const getTodos = async () => {
     // Creates the todos table if it doesn't exist
     // Returns all todos from the table
   };
   ```

   Handles first-time setup and retrieves todos.

3. Main API handler

   ```javascript
   app.http('todos', {
     methods: ['GET', 'POST', 'PUT', 'DELETE'],
     authLevel: 'anonymous',
     handler: async (request, context) => {
       // Request handling logic based on HTTP method
     },
   });
   ```

   Supported operations:

   1. `GET`: Retrieves all todos

      - Automatically creates table on first request.
      - Returns array of todo items.

   2. `POST`: Creates new todo

      - Requires: `text` field.
      - Returns: newly created todo.

   3. `PUT`: Updates todo completion status

      - Requires: `id` and `completed` status.
      - Returns: updated todo or `404` if not found.

   4. `DELETE`: Removes a todo

      - Requires: todo `id`.
      - Returns: success message or `404` if not found.

Error Handling:

- Input validation for required fields.
- Database error handling.
- Proper `HTTP` status codes.
- Detailed error messages for debugging.
- Logging via `context.log` for monitoring.

### Adding Neon Postgres connection string

Start by configuring the `local.settings.json` in your `api` directory with your Neon database connection string:

```json
{
  "Values": {
    ...
    "DATABASE_URL": "postgresql://neondb_owner:<your_password>@<your_host>.neon.tech/neondb?sslmode=require"
  }
}
```

Replace the `DATABASE_URL` value with your Neon Postgres connection string which you saved earlier.

### Testing the Azure Functions locally

To test the Azure Functions locally, navigate to the root directory of your project and run the following command:

```bash
cd ..
swa start --api-location api
```

This will start the Azure Functions backend and serve the static web app locally. You can access the app at [`http://localhost:4280`](http://localhost:4280) and test the functionality. It should be fully functional, allowing you to add, update, and delete todos. It should look like this:

![Todo App Completed](/docs/guides/swa-todo-app-completed.png)

## Deploying your Azure Static Web App

Once you've tested your Azure Static Web App locally and are satisfied with the functionality, you can deploy it to Azure to make it accessible to users worldwide.

### Creating a Static Web App in Azure

To deploy your Azure Static Web App, follow these steps:

1. Open the command palette in Visual Studio Code by pressing `F1`.
2. Search for and run the command `Azure Static Web Apps: Create Static Web App`.
3. Sign in to your Azure account if prompted.
4. When prompted, commit your changes to a Git repository.
5. Enter name for your Static Web App, for example `swa-todo`.
6. Enter repo name for your GitHub repository to push the code.
7. Choose the region to deploy your app, for example `East US 2`.
8. Select `HTML` for the build preset.
9. Enter `/` for the app location and build output path.

Once you connect your repository, Azure automatically sets up a GitHub Actions workflow file in your repository. This workflow handles the build and deployment process whenever you push changes. You can watch your deployment progress in real-time through either the GitHub Actions tab in your repository or the Azure portal.

### Add environment variables

Now that your Azure Static Web App is deployed, you will need to add the Neon Postgres connection string to the Azure Static Web App environment variables. This will allow your Azure Functions to connect to the Neon Postgres database.

1. Go to the Azure Portal and navigate to your Azure Static Web App resource.
2. Click on the `Environment Variables` under `Settings`.
3. Add a new environment variable with the key `DATABASE_URL` and the value as your Neon Postgres connection string.
   ![Environment Variables](/docs/guides/swa-neon-postgres-env-vars.png)
4. Click on `Apply` to save the changes.
5. Now visit the URL of your Azure Static Web App from the Overview tab to see your todo app live. The app should be fully functional allowing you to add, update, and delete todos.

## Summary

In this guide, you've built a simple todo application using Azure Static Web Apps and Neon Postgres. You've covered the following key steps:

1. Setting up your development environment with Azure Static Web Apps and Neon Postgres.
2. Creating the frontend of your todo app with HTML, CSS, and JavaScript.
3. Setting up Azure Functions to handle API requests and interact with the Neon Postgres database.
4. Testing your app locally and deploying it to Azure.

By combining Azure Static Web Apps with Neon Postgres, you can build powerful data-driven applications that are fast, reliable, and scalable. Azure Static Web Apps provides a robust hosting platform for static assets and serverless APIs, while Neon Postgres offers a serverless database solution that scales with your application. Together, they provide a seamless development experience for building modern web applications.

This guide should have helped you get started with Azure Static Web Apps and Neon Postgres. As a next step, you can look at [Neon Authorize](/docs/guides/neon-authorize) to add authentication and authorization to your app, allowing users to securely log in and manage their own todo lists.

## Additional Resources

- [Azure Static Web Apps Documentation](https://docs.microsoft.com/en-us/azure/static-web-apps/)
- [Neon Postgres Documentation](/docs/introduction)
- [Azure Functions Documentation](https://docs.microsoft.com/en-us/azure/azure-functions/)
- [Neon Authorize Guide](/docs/guides/neon-authorize)

<NeedHelp />


# Build a RAG chatbot with Astro, Postgres, and LlamaIndex

---
title: Build a RAG chatbot with Astro, Postgres, and LlamaIndex
subtitle: A step-by-step guide for building a RAG chatbot in an Astro application with LlamaIndex and Postgres
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-06-11T00:00:00.000Z'
updatedOn: '2024-06-11T00:00:00.000Z'
---

## Prerequisites

To follow the steps in this guide, you will need the following:

- [Node.js 18](https://nodejs.org/en) or later
- A [Neon](https://console.neon.tech/signup) account
- An [OpenAI](https://platform.openai.com/api-keys) account
- An [AWS](https://aws.amazon.com/free) account

## Steps

- [Generate the OpenAI API token](#generate-the-openai-api-token)
- [Provisioning a Serverless Postgres](#provisioning-a-serverless-postgres)
- [Create a new Astro application](#create-a-new-astro-application)
  - [Add Tailwind CSS to the application](#add-tailwind-css-to-the-application)
  - [Integrate React in your Astro project](#integrate-react-in-your-astro-project)
  - [Enabling Server Side Rendering in Astro using Node.js Adapter](#enabling-server-side-rendering-in-astro-using-nodejs-adapter)
- [Setting up a Postgres database connection](#setting-up-a-postgres-database-connection)
- [Define the Astro application routes](#define-the-astro-application-routes)
  - [Build Conversation User Interface using Vercel AI SDK](#build-conversation-user-interface-using-vercel-ai-sdk)
  - [Build UI to update Chabot’s Knowledge](#build-ui-to-update-chabots-knowledge)
  - [Build an entrypoint React component](#build-an-entrypoint-react-component)
  - [Initialize Postgres Vector Store in LlamaIndex](#initialize-postgres-vector-store-in-llamaindex)
  - [Build the Chat API Endpoint](#build-the-chat-api-endpoint)
  - [Build the Learn API Endpoint](#build-the-learn-api-endpoint)
- [Dockerize your Astro application](#dockerize-your-astro-application)
- [Deploy your Astro application to Amazon ECS](#deploy-your-astro-application-to-amazon-ecs)
  - [Create Amazon ECR private repository](#create-amazon-ecr-private-repository)
  - [Configure your IAM Roles](#configure-your-iam-roles)
  - [Create an Amazon ECS Task Definition](#create-an-amazon-ecs-task-definition)
  - [Create an Amazon ECS Cluster](#create-an-amazon-ecs-cluster)
  - [Create an Amazon ECS Service](#create-an-amazon-ecs-service)
  - [Create Access Keys for IAM users](#create-access-keys-for-iam-users)
- [Configure GitHub Actions for Continuous Deployment (CD) Workflow](#configure-github-actions-for-continuous-deployment-cd-workflow)

## Generate the OpenAI API token

To create vector embeddings, you will use OpenAI API with LlamaIndex. To set up OpenAI, do the following:

- Log in to your [OpenAI](https://platform.openai.com/) account.
- Navigate to the [API Keys](https://platform.openai.com/api-keys) page.
- Enter a name for your token and click the **Create new secret key** button to generate a new key.
- Copy and securely store this token for later use as the **OPENAI_API_KEY** environment variable.

## Provisioning a Serverless Postgres

Using a serverless Postgres database lets you scale compute resources down to zero, which helps you save on compute costs.

To get started, go to the [Neon Console](https://console.neon.tech/app/projects) and create a project.

You will then be presented with a dialog that provides a connection string of your database. Click on the **Pooled connection** option and the connection string automatically updates.

![](/guides/images/chatbot-astro-postgres-llamaindex/c200c4ed-f62d-469c-9690-c572c482c536.png)

All Neon connection strings have the following format:

```bash
postgres://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>?sslmode=require
```

- `user` is the database user.
- `password` is the database user’s password.
- `endpoint_hostname` is the host with `neon.tech` as the [top-level domain (TLD)](https://www.cloudflare.com/en-gb/learning/dns/top-level-domain/).
- `port` is the Neon port number. The default port number is 5432.
- `dbname` is the name of the database. `neondb` is the default database created with a Neon project if you do not define your own database.
- `?sslmode=require` an optional query parameter that enforces [SSL](https://www.cloudflare.com/en-gb/learning/ssl/what-is-ssl/) mode for better security when connecting to the Postgres instance.

Save the connection string somewhere safe. It will be used to set the **POSTGRES_URL** variable later.

## Create a new Astro application

Let’s get started by creating a new Astro project. Open your terminal and run the following command:

```bash
npm create astro@latest my-app
```

`npm create astro` is the recommended way to scaffold an Astro project quickly.

When prompted, choose:

- `Empty` when asked how to start the new project.
- `Yes` when asked if you plan to write Typescript.
- `Strict` when asked how strict Typescript should be.
- `Yes` when prompted to install dependencies.
- `Yes` when prompted to initialize a git repository.

Once that’s done, change the project directory and start the app:

```bash
cd my-app
npm run dev
```

The app should be running on [localhost:4321](http://localhost:4321/). Let's close the development server for now.

Next, execute the command in your terminal window below to install the necessary libraries and packages for building the application:

```bash
npm install dotenv ai llamaindex@0.3.4
```

The command installs the following packages:

- `dotenv`: A library for handling environment variables.
- `ai`: A library to build AI-powered streaming text and chat UIs.
- `llamaindex`: A data framework for creating LLM applications.

Next, make the following additions in your `astro.config.mjs` file to populate the environment variables and make them accessible via `process.env` object:

```tsx
// File: astro.config.mjs

import 'dotenv/config'; // [!code ++]
import { defineConfig } from 'astro/config';

// https://astro.build/config
export default defineConfig({});
```

Then, add the following code to your `tsconfig.json` file to make relative imports within the project easier:

```json
{
  "extends": "astro/tsconfigs/base",
  "compilerOptions": {
    // [!code ++]
    "baseUrl": ".", // [!code ++]
    "paths": {
      // [!code ++]
      "@/*": ["src/*"] // [!code ++]
    } // [!code ++]
  } // [!code ++]
}
```

Let's move on to integrating Tailwind CSS into the Astro application.

### Add Tailwind CSS to the application

For styling the app, you will be using Tailwind CSS. Install and set up Tailwind at the root of our project's directory by running:

```bash
npx astro add tailwind
```

When prompted, choose:

- `Yes` when prompted to install the Tailwind dependencies.
- `Yes` when prompted to generate a minimal `tailwind.config.mjs` file.
- `Yes` when prompted to make changes to Astro configuration file.

After making the selections outlined above, the command finishes integrating TailwindCSS into your Astro project and installs the following dependencies:

- `tailwindcss`: TailwindCSS as a package to scan your project files and generate corresponding styles.
- `@astrojs/tailwind`: An adapter that brings Tailwind's utility CSS classes to every `.astro` file and framework component in your project.

Let's move on to integrating React into the Astro application.

### Integrate React in your Astro project

To prototype the reactive user interface quickly, you will use React as the library with Astro. In your terminal window, execute the following command:

```bash
npx astro add react
```

`npx` allows us to execute npm package binaries without having to install `npm` globally.

When prompted, choose the following:

- `Yes` to install the React dependencies.
- `Yes` to make changes to Astro configuration file.
- `Yes` to make changes to `tsconfig.json` file.

Let's move on to enabling server-side rendering in the Astro application.

### Enabling Server Side Rendering in Astro using Node.js Adapter

To interact with the chatbot over a server-side API, you will enable server-side rendering in your Astro application. Execute the following command in your terminal:

```bash
npx astro add node
```

When prompted, choose:

- `Yes` to install the Node.js dependencies.
- `Yes` to make changes to Astro configuration file.

After making the selections outlined above, the command finishes integrating the Node.js adapter into your Astro project and installs the following dependency:

- `@astrojs/node`: The adapter that allows your Astro SSR site to deploy to Node targets.

Let's move on to loading the Postgres URL through an environment variable in the Astro application.

## Setting up a Postgres database connection

Create an `.env` file in the root directory of your project with the following environment variable to initiate the setup of a database connection:

```bash
# Neon Postgres Pooled Connection URL

POSTGRES_URL="postgres://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>?sslmode=require"
```

The file, `.env`, should be kept secret and not included in your Git history. Ensure that `.env` is added to the `.gitignore` file in your project.

## Define the Astro application routes

The structure below is what our `src/pages` directory will look like at the end of this section:

```bash
├── index.astro
├── api/
└───── chat.ts
└───── learn.ts
```

- `index.astro` will serve responses with dynamically created HTML to incoming requests at the index route.
- `api/chat.ts` will serve responses as an API Endpoint to incoming requests at `/api/chat`.
- `api/learn.ts` will serve responses as an API Endpoint to incoming requests at `/api/learn`.

### Build Conversation User Interface using Vercel AI SDK

Inside the `src` directory, create a `Chat.jsx` file with the following code:

```tsx
// File: src/Chat.jsx

import { useChat } from 'ai/react';

export default function () {
  const { messages, handleSubmit, input, handleInputChange } = useChat();
  return (
    <form onSubmit={handleSubmit} className="flex w-full max-w-[300px] flex-col">
      <span className="text-2xl font-semibold">Chat</span>
      <input
        id="input"
        name="prompt"
        value={input}
        onChange={handleInputChange}
        placeholder="What's your next question?"
        className="mt-3 rounded border px-2 py-1 outline-none focus:border-black"
      />
      <button
        type="submit"
        className="mt-3 max-w-max rounded border px-5 py-1 outline-none hover:bg-black hover:text-white"
      >
        Ask &rarr;
      </button>
      {messages.map((message, i) => (
        <div className="mt-3 border-t pt-3" key={i}>
          {message.content}
        </div>
      ))}
    </form>
  );
}
```

The code above does the following:

- Imports the `useChat` hook by `ai` SDK to manage the conversation between the user and the chatbot. It simplifies the management of the conversation between the user and the chatbot. By default, it posts to the `/api/chat` endpoint to obtain responses from the chatbot.
- Exports a React component that returns a form containing an `<input>` element to allow users to enter their query.
- Creates a conversation UI looping over the set of messages (managed by the AI SDK).

Now, let’s create a component that will allow the user to add text to the chatbot's knowledge.

### Build UI to update Chabot’s Knowledge

Inside the `src` directory, create a `Learn.jsx` file with the following code:

```jsx
// File: src/Learn.jsx

import { useState } from 'react';

export default function () {
  const [message, setMessage] = useState();
  return (
    <form
      className="flex w-full max-w-[300px] flex-col"
      onSubmit={(e) => {
        e.preventDefault();
        if (message) {
          fetch('/api/learn', {
            method: 'POST',
            body: JSON.stringify({ message }),
            headers: { 'Content-Type': 'application/json' },
          });
        }
      }}
    >
      <span className="text-2xl font-semibold">Learn</span>
      <textarea
        value={message}
        placeholder="A text to learn."
        onChange={(e) => setMessage(e.target.value)}
        className="mt-3 min-h-[100px] rounded border px-2 py-1 outline-none focus:border-black"
      />
      <button
        type="submit"
        className="mt-3 max-w-max rounded border px-5 py-1 outline-none hover:bg-black hover:text-white"
      >
        Learn &rarr;
      </button>
    </form>
  );
}
```

The code above does the following:

- Imports `useState` hook by React.
- Exports a React component that returns a form containing a `<textarea>` element to accept a string.
- Upon form submission, it posts the message string input by the user as JSON to the `/api/learn` endpoint.

### Build an entrypoint React component

Inside the `src` directory, create a `App.jsx` file with the following code:

```tsx
// File: src/App.jsx

import Chat from './Chat';
import Learn from './Learn';

export default function () {
  return (
    <>
      <Chat />
      <Learn />
    </>
  );
}
```

The code above imports and renders both the Chat and the Learn component created earlier. Finally, update the `index.astro` file to import the `App` component:

```astro
---
// File: src/pages/index.astro // [!code ++]

import App from "../App"; // [!code ++]
---

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" type="image/svg+xml" href="/favicon.svg" />
    <meta name="viewport" content="width=device-width" />
    <meta name="generator" content={Astro.generator} />
    <title>Astro</title>
  </head>
  <body
   class="flex w-screen flex-col md:flex-row items-center md:items-start md:justify-center md:gap-x-3 md:mt-12" // [!code ++]
  >
    <h1>Astro</h1> // [!code --]
    <App client:load /> // [!code ++]
  </body>
</html>
```

The changes above import the App component. Additionally, using Astro's [`client:load` directive](https://docs.astro.build/en/reference/directives-reference/#clientload) the code makes sure that the React application is hydrated immediately on the page.

Let's move on to using Postgres as the vector store for your chatbot.

### Initialize Postgres Vector Store in LlamaIndex

To query and add documents to the Postgres vector store, you are going to use `PGVectorStore` class to communicate. Inside the `src` directory, create `vectorStore.ts` with the following code:

```tsx
// File: src/vectorStore.ts

import 'dotenv/config';
import { PGVectorStore } from 'llamaindex';

export default new PGVectorStore({
  connectionString: process.env.POSTGRES_URL,
});
```

The code above begins with importing the `dotenv/config`, loading all the environment variables into the scope. Additionally, it exports an instance of `PGVectorStore` initialized using the Postgres pooled connection URL obtained earlier.

Let's move on to building the chat API endpoint.

### Build the Chat API Endpoint

The Vercel AI SDK uses `/api/chat` by default to obtain the chatbot responses. Create a file `src/pages/api/chat.ts` with the following code:

```tsx
// File: src/pages/api/chat.ts

import type { APIContext } from 'astro';

import vectorStore from '@/vectorStore';
import { VectorStoreIndex } from 'llamaindex';

export async function POST({ request }: APIContext) {
  const { messages = [] } = await request.json();
  const userMessages = messages.filter((i: { role: string }) => i.role === 'user');
  const encoder = new TextEncoder();
  const index = await VectorStoreIndex.fromVectorStore(vectorStore);
  const queryEngine = index.asQueryEngine();
  const query = userMessages[userMessages.length - 1].content;
  const stream = await queryEngine.query({ query, stream: true });
  const customReadable = new ReadableStream({
    async start(controller) {
      for await (const chunk of stream) {
        controller.enqueue(encoder.encode(chunk.response));
      }
      controller.close();
    },
  });
  return new Response(customReadable, {
    headers: {
      Connection: 'keep-alive',
      'Content-Encoding': 'none',
      'Cache-Control': 'no-cache, no-transform',
      'Content-Type': 'text/event-stream; charset=utf-8',
    },
  });
}
```

The code above does the following:

- Imports the vector store instance that is using Postgres.
- Imports the VectorStoreIndex helper by llamaindex.
- Exports a POST HTTP Handler which responds to incoming POST requests on `/api/chat`.
- Destructs messages array from the request body.
- Creates the LlamaIndex's query engine using Postgres as the vector store.
- Creates a stream handler that streams the response from LlamaIndex's query engine.
- Returns the stream handler as a standard Web Response.

Let's move on to building the endpoint to update chatbot's knowledge.

### Build the Learn API Endpoint

As you saw earlier, with LlamaIndex you do not need to manually create context and pass it to an external API. The vector store is searched for similar vector embeddings based on the user query, internally. To keep the knowledge of the chatbot up-to-date, create a file `src/pages/api/learn.ts` with the following code:

```tsx
// File: src/pages/api/learn.ts

import type { APIContext } from 'astro';

import {
  Document,
  Settings,
  OpenAIEmbedding,
  VectorStoreIndex,
  storageContextFromDefaults,
} from 'llamaindex';
import vectorStore from '@/vectorStore';

export async function POST({ request }: APIContext) {
  Settings.embedModel = new OpenAIEmbedding();
  const { text } = await request.json();
  if (!text) return new Response(null, { status: 400 });
  const storageContext = await storageContextFromDefaults({ vectorStore });
  const document = new Document({ text });
  await VectorStoreIndex.fromDocuments([document], { storageContext });
}
```

The code above does the following:

- Imports the vector store instance that is using Postgres.
- Imports the VectorStoreIndex and storageContextFromDefaults helpers by llamaindex.
- Exports a POST HTTP Handler which responds to incoming POST requests on `/api/learn`.
- Destructs the text message from the request body.
- Creates a LlamaIndex document with text as its sole data.
- Pushes the vector embeddings generated for the text along with the document metadata to the Postgres database.

Let's move on to dockerizing the Astro application.

## Dockerize your Astro application

To dockerize your Astro application, you are going to create two files at the root of your Astro project:

- `.dockerignore`: The set of files that would not be included in your Docker image.
- `Dockerfile`: The set of instructions that would be executed while your Docker image builds.

Create the `.dockerignore` file at the root of your Astro project with the following code:

```
# build output
dist/

# generated types
.astro/

# dependencies
node_modules/

# logs
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*

# macOS-specific files
.DS_Store
```

Create the `Dockerfile` file at the root of your Astro project with the following code:

```bash
ARG NODE_VERSION=20.11.0
FROM node:${NODE_VERSION}-slim as base

WORKDIR /app

# Set production environment
ENV NODE_ENV="production"

# Throw-away build stage to reduce size of final image
FROM base as build

# Install packages needed to build node modules
RUN apt-get update -qq && \
    apt-get install --no-install-recommends -y build-essential node-gyp pkg-config python-is-python3

# Install node modules
COPY --link package-lock.json package.json ./
RUN npm install

# Copy application code
COPY --link . .

# Build application
RUN npm run build

# Final stage for app image
FROM base

# Copy built application
COPY --from=build /app/node_modules /app/node_modules
COPY --from=build /app/dist /app/dist

ENV PORT=80
ENV HOST=0.0.0.0

# Start the server by default, this can be overwritten at runtime
EXPOSE 80
CMD [ "node", "./dist/server/entry.mjs" ]
```

The Dockerfile above defines the following set of actions:

- Sets up Node.js 20.11.0.
- Sets the environment to `production` with `NODE_ENV` environment variable.
- Instasll the dependencies of your Astro project.
- Builds the application with `astro build`.
- Sets the `PORT` environment variable to `80` (default port on Amazon ECS).
- Sets the `HOST` environment variable to `0.0.0.0` to listen to all incoming requests on the host.
- Runs the production server with `node ./dist/server/entry.mjs` command.

Now, let's create a file that creates Amazon ECS Task definition during the deployment via GitHub Actions. This is useful as it protects the secrets stored in the GitHub repo. Create an `env.mjs` file at the root of your Astro application with the following code:

```tsx
// File: env.mjs

import 'dotenv/config';
import { join } from 'node:path';
import { writeFileSync } from 'node:fs';

if (!process.env.AWS_ACCOUNT_ID || !process.env.POSTGRES_URL || !process.env.OPENAI_API_KEY) {
  console.error(`AWS_ACCOUNT_ID, POSTGRES_URL or OPENAI_API_KEY environment variable not found.`);
  process.exit();
}

writeFileSync(
  join(process.cwd(), 'task-definition.json'),
  JSON.stringify({
    containerDefinitions: [
      {
        cpu: 256,
        memory: 512,
        portMappings: [
          {
            containerPort: 80,
            hostPort: 80,
            protocol: 'tcp',
          },
        ],
        essential: true,
        name: 'astro-app',
        image: `${process.env.AWS_ACCOUNT_ID}.dkr.ecr.ap-south-1.amazonaws.com/${process.env.AWS_ECR_REPOSITORY_NAME}`,
        environment: [
          {
            name: 'POSTGRES_URL',
            value: process.env.POSTGRES_URL,
          },
          {
            name: 'OPENAI_API_KEY',
            value: process.env.OPENAI_API_KEY,
          },
        ],
      },
    ],
    cpu: '1024',
    memory: '3072',
    family: 'astro-task-definitions',
    networkMode: 'awsvpc',
    taskRoleArn: `arn:aws:iam::${process.env.AWS_ACCOUNT_ID}:role/ecsTaskRole`,
    executionRoleArn: `arn:aws:iam::${process.env.AWS_ACCOUNT_ID}:role/ecsTaskExecutionRole`,
  }),
  'utf8'
);
```

The code above does the following:

- Imports `dotenv/config` to load all the environment variables into the scope and make them accessible via the `process.env` object.
- Validates the presense of `AWS_ACCOUNT_ID`, `POSTGRES_URL`, and `OPENAI_API_KEY` environment variables.
- Writes a `task-definition.json` file at the root of your Astro application following the format used earlier, and adds the `OPENAI_API_KEY` and `POSTGRES_URL` environment variables.

## Deploy your Astro application to Amazon ECS

In this section, you will learn how to create an Amazon ECR repository for your Docker-based deployments, spin up Amazon ECS Cluster and an ECS service, create AWS ECS Task Definition(s) and grant ECS Full Access to your AWS IAM user.

### Create Amazon ECR private repository

- Open the [Amazon ECR console](https://console.aws.amazon.com/ecr/repositories), and click **Get started**.

![](/guides/images/chatbot-astro-postgres-llamaindex/bf86c622-e7b2-4d43-80d0-140757512e7d.png)

- Enter a repository name, say `astro-repo`, for example. Scroll down and choose **Create repository**.

![](/guides/images/chatbot-astro-postgres-llamaindex/9c566960-540a-448e-880e-97ec16ba4f0f.png)

You are now done setting up an Amazon ECR repository. Let’s move on to configuring IAM roles for your account.

### Configure your IAM Roles

- Open the [IAM console](https://console.aws.amazon.com/iam/), and click **Create role**.

![](/guides/images/chatbot-astro-postgres-llamaindex/7fdeea5f-f2f6-4115-9f55-0b904b510944.png)

- Select **AWS Service** and choose **Elastic Container Service** as the Service or use case.

![](/guides/images/chatbot-astro-postgres-llamaindex/fd0ce7ed-21f7-492c-ac01-9c4cff96d590.png)

- Filter the large set of permissions policies, select **AmazonECS_FullAccess** only, and click **Next**.

![](/guides/images/chatbot-astro-postgres-llamaindex/202bcbe5-5614-404a-ac45-562ef580ede6.png)

- Enter `ecsTaskRole` as the **Role name**.

![](/guides/images/chatbot-astro-postgres-llamaindex/6f33848c-8d6a-42eb-922a-2a4916a430c3.png)

- Go back to the IAM Console, and click **Create role**.

![](/guides/images/chatbot-astro-postgres-llamaindex/25510e84-f354-47fc-8bdd-0380e0480477.png)

- Select **AWS Service** and choose **Elastic Container Service Task** as the Service or use case.

![](/guides/images/chatbot-astro-postgres-llamaindex/325c52ce-dbc4-4906-97e0-5f29ba510560.png)

- Filter the large set of permissions policies, select **AmazonECSTaskExecutionRolePolicy** only, and click **Next**.

![](/guides/images/chatbot-astro-postgres-llamaindex/6fbb4748-9380-4264-9d81-d36f2d6f83aa.png)

- Enter `ecsTaskExecutionRole` as the **Role name**.

![](/guides/images/chatbot-astro-postgres-llamaindex/c6f45124-7441-4562-81d4-d45e93f9a567.png)

You are now done setting up IAM Roles for your account. Let’s move on to creating an Amazon ECS task definition.

### Create an Amazon ECS Task Definition

- Open the [Amazon ECS Console](https://console.aws.amazon.com/ecs/v2) and choose **Task Definitions**. Further, select **Create new task definition with JSON**.

![](/guides/images/chatbot-astro-postgres-llamaindex/afdc546d-2214-45b0-916d-bd0868fbe2dc.png)

- Copy the following JSON in the field, and click **Create**.

```json
{
  "containerDefinitions": [
    {
      "cpu": 256,
      "memory": 512,
      "portMappings": [
        {
          "containerPort": 80,
          "hostPort": 80,
          "protocol": "tcp"
        }
      ],
      "essential": true,
      "name": "astro-app",
      "image": "2*.dkr.ecr.ap-south-1.amazonaws.com/astro-repo"
    }
  ],
  "cpu": "1024",
  "memory": "3072",
  "family": "astro-task-definitions",
  "networkMode": "awsvpc",
  "taskRoleArn": "arn:aws:iam::2*:role/ecsTaskRole",
  "executionRoleArn": "arn:aws:iam::2*:role/ecsTaskExecutionRole"
}
```

![](/guides/images/chatbot-astro-postgres-llamaindex/b812ef9a-df98-4d46-aa0a-71711a967eeb.png)

You are now done setting up an Amazon ECS task definition for your service. Let's move on to creating an Amazon ECS Cluster.

### Create an Amazon ECS Cluster

- Open [Amazon ECS Console](https://console.aws.amazon.com/ecs/v2) and click **Create cluster**.

![](/guides/images/chatbot-astro-postgres-llamaindex/fe2342b6-fd6e-4f6f-8e0d-39aff8a87060.png)

- Enter a name for your cluster, say `astro-cluster`, and choose **Create**.

![](/guides/images/chatbot-astro-postgres-llamaindex/cb8e6d74-5745-4c2a-88c7-2598ac85e286.png)

You are now done setting up an Amazon ECS Cluster for your service.

![](/guides/images/chatbot-astro-postgres-llamaindex/9a88a385-3ff9-496c-8ec0-7389fc4c69b0.png)

Let's move on to creating an Amazon ECS Service.

### Create an Amazon ECS Service

- Click on the Cluster created in the section earlier, and click on **Create** in the Services section.

![](/guides/images/chatbot-astro-postgres-llamaindex/9a88a385-3ff9-496c-8ec0-7389fc4c69b0.png)

- Enter a name for your service, say `astro-service`, and expand the **Networking** section.

![](/guides/images/chatbot-astro-postgres-llamaindex/eccf2a66-c1e8-43e4-95dc-4e8550488a8d.png)

- Select the **VPC** created earlier (or the default one). Select **Create a new security group** option and enter the following details for it:
  - Security group name: `astro-sg`
  - Security group description: `astro sg`
  - Inbound rules for security groups:
    - Type: `HTTP`
    - Source: `Anywhere`

![](/guides/images/chatbot-astro-postgres-llamaindex/7dc7a5ea-e593-40a0-adb3-f9519536bdea.png)

You are now done creating an ECS Service in your ECS Cluster. Let's move on to creating access keys for IAM users for your account.

### Create Access Keys for IAM users

- In the navigation bar in your AWS account, choose your user name, and then choose **Security credentials**.

![](/guides/images/chatbot-astro-postgres-llamaindex/75769ba6-07ee-48d3-9421-4f7005c9127d.png)

- Scroll down to **Access keys** and click on **Create access key**.

![](/guides/images/chatbot-astro-postgres-llamaindex/780854a9-c705-4a86-9c16-f4a54fec02fa.png)

- Again, click on **Create access key**.

![](/guides/images/chatbot-astro-postgres-llamaindex/68d86de4-ab60-4a94-bd10-a9492a458f0e.png)

- Copy the **Access key** and **Secret access key** generated to be used as `AWS_ACCESS_KEY_ID` and `AWS_ACCESS_KEY_SECRET` respectively.

![](/guides/images/chatbot-astro-postgres-llamaindex/8586ea3d-d5b2-43dc-b81c-38c1a991b593.png)

Let's move on to configuring GitHub Workflows for continuous deployments.

## Configure GitHub Actions for Continuous Deployment (CD) Workflow

First, let's add all the required environment variables obtained in the AWS steps above to your GitHub repo as repository Secrets. Go to your GitHub repository's Settings, and click on **Secrets and Variables**. Then, click on **New repository secret**.

![](/guides/images/chatbot-astro-postgres-llamaindex/c95dacdf-987c-4a78-97e3-68668f0e1ad5.png)

- Enter **AWS_ACCOUNT_ID** as the value obtained earlier.

![](/guides/images/chatbot-astro-postgres-llamaindex/357f312c-af9d-4f11-991c-71b7df20a285.png)

- Enter **AWS_ACCESS_KEY_ID** as the value obtained earlier.

![](/guides/images/chatbot-astro-postgres-llamaindex/816e7d04-5723-4e11-bde1-5debcd788e1a.png)

- Enter **AWS_ACCESS_KEY_SECRET** as the value obtained earlier.

![](/guides/images/chatbot-astro-postgres-llamaindex/bd5e7d9a-ea81-464a-8949-1f65d292c10b.png)

- Enter **POSTGRES_URL** as the value obtained earlier.

![](/guides/images/chatbot-astro-postgres-llamaindex/80e9eb62-3ffa-4ab9-a6cd-ed0190692ae6.png)

- Enter **OPENAI_API_KEY** as the value obtained earlier.

![](/guides/images/chatbot-astro-postgres-llamaindex/6848ea49-3eb5-4201-927c-183fdf9cb0d7.png)

Next, to automate deployments of your Astro application, you are going to use [GitHub Actions](https://github.com/features/actions). Create a `.github/workflows/deploy.yml` with the following code:

```yml
name: Deploy Astro to Amazon ECS on AWS Fargate

on:
  push:
    branches:
      - master
  workflow_dispatch:

env:
  # AWS ECS
  AWS_CONTAINER_NAME_NAME: astro-app
  AWS_ECS_CLUSTER_NAME: astro-cluster
  AWS_ECS_SERVICE_NAME: astro-service
  AWS_ECS_TASK_DEFINITION: ./task-definition.json

  # AWS ECR
  AWS_ECR_REPOSITORY_NAME: astro-repo

  # AWS Account
  AWS_REGION: us-west-1
  AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

jobs:
  deploy:
    name: Deploy
    runs-on: ubuntu-latest
    environment: production

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ env.AWS_REGION }}
          aws-access-key-id: ${{ env.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1

      - name: Load Environment Variables
        id: environment-variables
        env:
          AWS_ACCOUNT_ID: ${{ env.AWS_ACCOUNT_ID }}
          POSTGRES_URL: ${{ secrets.POSTGRES_URL }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          npm install
          node ./env.mjs

      - name: Build, tag, and push image to Amazon ECR
        id: build-image
        env:
          IMAGE_TAG: ${{ github.sha }}
          AWS_ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        run: |
          # Build a docker container and
          # push it to ECR so that it can
          # be deployed to ECS.
          docker build -t $AWS_ECR_REGISTRY/$AWS_ECR_REPOSITORY_NAME:$IMAGE_TAG .
          docker push $AWS_ECR_REGISTRY/$AWS_ECR_REPOSITORY_NAME:$IMAGE_TAG
          echo "image=$AWS_ECR_REGISTRY/$AWS_ECR_REPOSITORY_NAME:$IMAGE_TAG" >> $GITHUB_OUTPUT

      - name: Fill in the new image ID in the Amazon ECS task definition
        id: task-def
        uses: aws-actions/amazon-ecs-render-task-definition@v1
        with:
          container-name: ${{ env.AWS_CONTAINER_NAME_NAME }}
          image: ${{ steps.build-image.outputs.image }}
          task-definition: ${{ env.AWS_ECS_TASK_DEFINITION }}

      - name: Deploy Amazon ECS task definition
        uses: aws-actions/amazon-ecs-deploy-task-definition@v1
        with:
          service: ${{ env.AWS_ECS_SERVICE_NAME }}
          cluster: ${{ env.AWS_ECS_CLUSTER_NAME }}
          wait-for-service-stability: true
          task-definition: ${{ steps.task-def.outputs.task-definition }}
```

The workflow above does the following:

- Allows itself to be triggered manually or when a git push is done to the master branch.
- Sets global environment variables as per your AWS setup variables (we obtained earlier during the setup).
- Loads the environment variables added to the GitHub repo as secrets into the scope.
- Writes the task definition including the environment variables.
- Builds and pushes the Docker image to Amazon ECR.
- Loads the updated (if) task definition to Amazon ECS.

Now, push the added GitHub workflow file to your GitHub repo. Follow the steps below to trigger the deployment:

- Go to your GitHub repository's **Actions** tab.

- Select **Deploy to Amazon ECS** workflow.

- Click **Run workflow**.

- Once the action is completed, open [ECS Console](https://console.aws.amazon.com/ecs/v2) and select your service.

![image](/guides/images/chatbot-astro-postgres-llamaindex/932be3bf-f3b1-43d9-b465-828b72a8cd4c.png)

- Click on the **Tasks** tab.

![image (1)](/guides/images/chatbot-astro-postgres-llamaindex/b1904935-d5e3-411f-9e57-922d450ed26a.png)

- Click on the completed **Task**.

![image (2)](/guides/images/chatbot-astro-postgres-llamaindex/3b6f3c70-ee7f-4d80-b1b8-c73cb7d649a0.png)

- Click **open address** to open your deployment.

![image (3)](/guides/images/chatbot-astro-postgres-llamaindex/d4c77bfb-83ab-46e1-a70c-02f25e6c6314.png)

## Summary

In this guide, you learned how to build a RAG Chatbot using LlamaIndex, Astro, and Neon Postgres. Additionally, you learned how to automate deployments of your Astro application using GitHub Actions to Amazon ECS on Amazon Fargate.

<NeedHelp />


# Using DBeaver with a Hosted Postgres

---
title: Using DBeaver with a Hosted Postgres
subtitle: A comprehensive guide on how to manage your Postgres database using DBeaver.
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-12-21T00:00:00.000Z'
updatedOn: '2024-12-21T00:00:00.000Z'
---

DBeaver is a versatile database management tool that allows you to interact with a wide range of databases, including PostgreSQL. This guide will walk you through the steps to set up and use DBeaver with a hosted Postgres database, enabling you to perform various database operations efficiently.

## Table of Contents

- [Setting Up DBeaver](#setting-up-dbeaver)
- [Connecting to Your Hosted Postgres Database](#connecting-to-your-hosted-postgres-database)
- [Basic Operations in DBeaver](#basic-operations-in-dbeaver)

## Setting Up DBeaver

1. **Download and Install DBeaver**: If you haven't already, download DBeaver from the [official website](https://dbeaver.io/download/). Choose the version suitable for your operating system and follow the installation instructions.

2. **Launch DBeaver**: Open DBeaver from your applications menu and ensure it is running.

## Provisioning a Serverless Postgres

To get started, go to the [Neon console](https://console.neon.tech/app/projects) and create a new project by entering a project name of your choice.

![](/guides/images/pg-notify/index.png)

All Neon connection strings have the following format:

```bash
postgres://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>
```

- `user` is the database user.
- `password` is the database user’s password.
- `endpoint_hostname` is the host with neon.tech as the [TLD](https://www.cloudflare.com/en-gb/learning/dns/top-level-domain/).
- `port` is the Neon port number. The default port number is 5432.
- `dbname` is the name of the database. “neondb” is the default database created with each Neon project.
- `?sslmode=require` is an optional query parameter that enforces the [SSL](https://www.cloudflare.com/en-gb/learning/ssl/what-is-ssl/) mode while connecting to the Postgres instance for better security.

You will be using these connection string components in the following steps to connect DBeaver to your Postgres database.

## Connecting to Your Hosted Postgres Database

1. **Open DBeaver**: Ensure DBeaver is running. You will see the main dashboard.

2. **Create a New Database Connection**:

   - Click on the "New Database Connection" button (usually a plug icon or from the "Database" menu).
   - In the "Connect to Database" wizard, select "PostgreSQL" from the list of database types and click "Next".

3. **Enter Connection Details**:

   ![](/guides/images/dbeaver/conn-1.png)

   - Fill in the required fields based on your Neon connection string:
     - **Host**: The endpoint of your hosted Postgres database (e.g., `ep-...us-east-2.aws.neon.tech`).
     - **Port**: The port number (default is `5432`).
     - **Database**: The database name (e.g., `neondb`).
     - **Username**: Your database username.
     - **Password**: Your database password.
   - Enable "Show all databases" to ensure all databases in your Neon project are listed.

   ![](/guides/images/dbeaver/conn-2.png)

   - Click "Edit Driver Settings" if needed to ensure SSL is enabled. Under the "Driver Properties" tab, set `sslmode` to `require`.

4. **Test the Connection**:

   - Click the "Test Connection" button to verify the connection details.
   - If successful, click "Finish" to save the connection. Your new database connection will appear in the left sidebar.

## Basic Operations in DBeaver

### 1. Running SQL Queries

- Right-click on your database connection in the left sidebar and select "SQL Editor" > "New SQL Script".
- Enter your SQL queries in the editor and click the "Execute" button (play icon) to run them.
- View the results in the results pane below the editor.

### 2. Managing Tables

- Expand your database connection in the left sidebar, then navigate to "Databases" > "neondb" > "Schemas" > "public" > "Tables".
- Right-click on "Tables" to create a new table or manage existing ones (e.g., view, edit, or drop tables).

### 3. Importing and Exporting Data

- To import data:
  - Right-click on a table and select "Import Data".
  - Choose the source file (e.g., CSV) and follow the prompts to map the columns.
- To export data:
  - Right-click on a table and select "Export Data".
  - Choose the format (e.g., CSV, JSON) and follow the prompts to save the file.

## Conclusion

DBeaver is a powerful tool for managing your hosted Postgres database. With its intuitive interface and robust features, you can easily perform tasks such as creating tables, running queries, and visualizing data. By following this guide, you should be well-equipped to utilize DBeaver effectively for your database management needs.

<NeedHelp />


# Using Directus CMS with Neon Postgres and Astro to build a blog

---
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-12-22T00:00:00.000Z'
updatedOn: '2024-12-22T00:00:00.000Z'
title: Using Directus CMS with Neon Postgres and Astro to build a blog
subtitle: A step-by-step guide for building your own blog in an Astro application with Directus CMS and Postgres powered by Neon
---

In this guide, you will learn how to set up a serverless Postgres database with Neon, configure Directus CMS with Postgres, define a blog schema, and author content using Directus CMS. The guide also covers configuring API read permissions and building a dynamic frontend with Astro to display blog pages fetched from the Directus CMS instance.

## Prerequisites

To follow the steps in this guide, you will need the following:

- [Node.js 18](https://nodejs.org/en) or later
- A [Neon](https://console.neon.tech/signup) account
- [Docker Desktop](https://www.docker.com/products/docker-desktop/) set up locally

## Provisioning a serverless Postgres database

Using a serverless Postgres database powered by Neon lets you scale compute resources down to zero, which helps you save on compute costs.

To get started, go to the [Neon console](https://console.neon.tech/app/projects) and create a project. You will then be presented with a dialog that provides a connection string for your database. You will be using the connection string to connect the Directus CMS instance to your Postgres database.

## Setting up Directus locally with Postgres

Let's begin with creating a Directus CMS backend to serve the content for blog posts. Open your terminal and run the following command:

```bash
mkdir directus-cms
cd directus-cms
```

and create a `docker-compose.yml` with the following code:

```yml
services:
  directus:
    image: directus/directus:11.3.5
    ports:
      - 8055:8055
    volumes:
      - ./database:/directus/database
      - ./uploads:/directus/uploads
      - ./extensions:/directus/extensions
    environment:
      SECRET: 'replace-with-secure-random-value'
      ADMIN_EMAIL: 'admin@example.com'
      ADMIN_PASSWORD: 'd1r3ctu5'
      DB_CLIENT: 'pg'
      DB_CONNECTION_STRING: 'postgresql://neondb_owner:...@ep-...us-east-1.aws.neon.tech/neondb?sslmode=require'
      DB_SSL__REJECT_UNAUTHORIZED: 'true'
      WEBSOCKETS_ENABLED: 'true'
      CORS_ENABLED: 'true'
      CORS_ORIGIN: '*'
```

Now, set the `DB_CONNECTION_STRING` value to the connection string you obtained previously. Finally, run the following command to start the local Directus CMS instance:

```shell
docker-compose up -d
```

Once the migrations are run successfully, the Directus CMS instance will be accessible at [localhost:8055/admin](http://localhost:8055/admin/). Sign in with the credentials set in the `docker-compose.yml` file.

## Configure the authors schema in Directus CMS

Navigate to the [Data Model](http://localhost:8055/admin/settings/data-model) view, and click the `+` icon to create a new data model.

Set the **Name** field to **authors**, press the next arrow icon, and save by clicking the tick icon.

![](/guides/images/directus-cms/author-1.png)

Once that's done, click on `Create Field` to add a field to the `authors` schema.

![](/guides/images/directus-cms/author-2.png)

Select `Input` as the field type and set the `Key` as **name** to indicate that the field represents the author's name.

![](/guides/images/directus-cms/author-3.png)

Finally, click `Save` to finish adding the `name` field to the `authors` schema.

![](/guides/images/directus-cms/author-4.png)

Now, let's move on to creating the `post` schema in Directus CMS.

## Configure the posts schema in Directus CMS

Navigate to the [Data Model](http://localhost:8055/admin/settings/data-model) view, and click the `+` icon to start creating a new data model.

Enter **posts** in the **Name** field, press the next arrow icon, and save by clicking the `+` icon.

Then, follow the same process as earlier to add the following fields to the `posts` schema:

- An **Input** field called **title**.
- A **WYSIWYG** field called **content**.
- An **Image** field called **image**.
- A **Datetime** field called **published_date**.
- A **Many to One** field called **author** with the **Related Collection** set to **authors**.

## Configure API Read Permissions in Directus CMS

To be able to fetch the data authored in your local Directus CMS instance, you will need to configure what is readable and writeable using APIs. Navigate to [Settings > Access Policies](http://localhost:8055/admin/settings/policies), click on **Public**, and add the permissions for the `authors`, `posts` and `directus_files` to be read publicly.

![](/guides/images/directus-cms/public-api.png)

Now, let's move on to creating an Astro application to create dynamic blog pages based on blog data that's accessible via your locally hosted instance of Directus CMS.

## Create a new Astro application

Let’s get started by creating a new Astro project. Open your terminal and run the following command:

```bash
npm create astro@latest blog-ui
```

`npm create astro` is the recommended way to scaffold an Astro project quickly.

When prompted, choose:

- `Empty` when prompted on how to start the new project.
- `Yes` when prompted if plan to write Typescript.
- `Strict` when prompted how strict Typescript should be.
- `Yes` when prompted to install dependencies.
- `Yes` when prompted to initialize a git repository.

Once that’s done, change to the project directory and start the app:

```bash
cd blog-ui
npm run dev
```

The app should be running on [localhost:4321](http://localhost:4321/). Let's close the development server for now.

Next, execute the following command to install the necessary libraries and packages for building the application:

```bash
npm install @directus/sdk
npm install -D typescript
```

The commands above install the packages, with the `-D` flag specifying the libraries intended for development purposes only.

The libraries installed include:

- [@directus/sdk](https://npmjs.com/package/@directus/sdk): Typescript SDK to query from your Directus CMS instance.

The development-specific libraries include:

- [typescript](https://npmjs.com/package/typescript): TypeScript is a language for application-scale JavaScript.

Then, add the following lines to your `tsconfig.json` file to make relative imports within the project easier:

```diff
{
  "extends": "astro/tsconfigs/base",
  "include": [".astro/types.d.ts", "**/*"],
  "exclude": ["dist"],
  "compilerOptions": { // [!code ++]
    "baseUrl": ".", // [!code ++]
    "paths": { // [!code ++]
      "@/*": ["src/*"] // [!code ++]
    } // [!code ++]
  } // [!code ++]
}
```

Now, create a `.env` file. You are going to add the API token obtained earlier.

The `.env` file should contain the following keys:

```bash
# .env

DIRECTUS_URL="http://localhost:8055"
```

## Create dynamic blog routes in Astro

To programmatically create pages as you keep authoring more content in your locally hosted Directus CMS, you are going to use [dynamic routes](https://docs.astro.build/en/guides/routing/#dynamic-routes) in Astro. With dynamic routes, you create a single file with a name like `[slug].astro`, where slug represents a [unique and dynamic variable](https://docs.astro.build/en/reference/api-reference/#contextparams) for each blog. Using [getStaticPaths](https://docs.astro.build/en/reference/api-reference/#getstaticpaths), you can programmatically create multiple blog pages with custom data using Directus CMS as your data source. Let's see this in action. Create a file named `[slug].astro` in the `src/pages` directory with the following code:

```astro
---
// File: src/pages/[slug].astro

import Layout from "@/layouts/Layout.astro";
import directus from "@/lib/directus";
import { readItems } from "@directus/sdk";

export async function getStaticPaths() {
  const posts = await directus.request(
    readItems("posts", {
      fields: ['*'],
    })
  );
  return posts.map((post) => ({ params: { slug: post.slug }, props: post }));
}
const post = Astro.props;
---

<Layout title={post.title}>
  <main>
    <img src={`${import.meta.env.DIRECTUS_URL}/assets/${post.image}`} />
    <h1>{post.title}</h1>
    <div set:html={post.content} />
  </main>
</Layout>
```

Let's understand the code above in two parts:

- Inside `getStaticPaths` function, a fetch call is made to the locally hosted Directus CMS API to get all the blogs with their **title**, **image** and **content** values. Looping over each blog item, an array is created that passes all the data obtained as the [props](https://docs.astro.build/en/reference/api-reference/#contextprops), and its **slug** as the unique variable to be associated with each blog.

- The HTML section represents the content of a particular blog page. The blog data attributes such as Title, Image URL, and the blog content in HTML are obtained from `Astro.props` (passed in `getStaticPaths` as props).

## Build and Test your Astro application locally

To test the Astro application in action, prepare a build and run the preview server using the following command:

```bash
npm run build && npm run preview
```

## Summary

In this guide, you learned how to build a blog in an Astro application using Directus CMS and a serverless Postgres database (powered by Neon). Additionally, you learned how to create content collections in Directus CMS and dynamic blog routes in an Astro application.

<NeedHelp />


# Building an API with Django, Django REST Framework, and Neon Postgres

---
title: Building an API with Django, Django REST Framework, and Neon Postgres
subtitle: Learn how to create a robust RESTful API for an AI Model Marketplace using Django, Django REST Framework, and Neon's serverless Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-09-15T00:00:00.000Z'
updatedOn: '2024-09-15T00:00:00.000Z'
---

Django is one of the most popular Python web frameworks for building web applications and APIs. Django REST Framework extends Django to provide powerful tools for building RESTful APIs quickly and efficiently based on your Django models with minimal code.

In this guide, we will go over how to build a RESTful API for a fictional AI Model Marketplace using Django and Django REST Framework with Neon's serverless Postgres as the database backend.

## Prerequisites

To follow this guide, you'll need:

- Python 3.8 or higher installed on your machine
- A [Neon account](https://console.neon.tech/signup) with a project created
- Basic familiarity with Django and RESTful API concepts

## Setting up the project

### Create a virtual environment

First, let's set up a new Python virtual environment for our project:

```bash shouldWrap
python -m venv neon-django-ai-marketplace
source neon-django-ai-marketplace/bin/activate  # On Windows, use `neon-django-ai-marketplace\Scripts\activate`
```

This creates a new virtual environment named `neon-django-ai-marketplace` and activates it, ensuring our project dependencies are isolated.

After activating the virtual environment, you should see `(neon-django-ai-marketplace)` in your terminal prompt.

### Install required packages

Now, let's install the necessary packages:

```bash
pip install django djangorestframework psycopg2-binary python-dotenv
```

This command installs Django, Django REST Framework, the PostgreSQL adapter for Python, and a package to manage environment variables. We'll use Django for the web framework, DRF for building the API, and psycopg2-binary to connect to the Neon Postgres database.

### Create a new Django project

With the dependencies installed, create a new Django project named `ai_marketplace`:

```bash
django-admin startproject ai_marketplace
```

Once the project is created, navigate to the project directory:

```bash
cd ai_marketplace
```

### Configure the database connection

To connect to Neon's serverless Postgres database, we need to set up the database connection in the Django project.

Open the `settings.py` file to configure the database connection. By default, Django uses SQLite as the database backend. Replace the `DATABASES` section with the following:

```python
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'your_database_name',
        'USER': 'your_username',
        'PASSWORD': 'your_password',
        'HOST': 'your_neon_hostname',
        'PORT': '5432',
    }
}
```

Replace the placeholders with your Neon database details. You can find these details in the Neon Console under **Connection Details**.

To verify the connection, run the Django development server:

```bash
python manage.py runserver
```

If the server starts without errors, you've successfully connected to the Neon database.

## Creating the API

Now that we have the Django project set up and connected to the Neon database, let's create the simple API for our AI Model Marketplace.

### Define the models

In Django, models are Python classes that represent database tables. Create a new Django app for our AI Model Marketplace:

```bash
python manage.py startapp models_api
```

Add the new app to `INSTALLED_APPS` in `settings.py`, this essentially registers the app with the Django project:

```python
INSTALLED_APPS = [
    # ... (existing apps)
    'rest_framework',
    'models_api',
]
```

Now, let's define our models in `models_api/models.py`:

```python
from django.db import models
from django.core.validators import MinValueValidator, MaxValueValidator

class ModelAuthor(models.Model):
    name = models.CharField(max_length=200)
    bio = models.TextField()
    contact_info = models.EmailField()
    rating = models.FloatField(validators=[MinValueValidator(0), MaxValueValidator(5)])

    def __str__(self):
        return self.name

class AIModel(models.Model):
    MODEL_TYPES = [
        ('NLP', 'Natural Language Processing'),
        ('CV', 'Computer Vision'),
        ('RL', 'Reinforcement Learning'),
        ('OTHER', 'Other'),
    ]
    FRAMEWORKS = [
        ('PT', 'PyTorch'),
        ('TF', 'TensorFlow'),
        ('KRS', 'Keras'),
        ('OTHER', 'Other'),
    ]
    name = models.CharField(max_length=200)
    model_type = models.CharField(max_length=5, choices=MODEL_TYPES)
    description = models.TextField()
    framework = models.CharField(max_length=5, choices=FRAMEWORKS)
    version = models.CharField(max_length=50)
    download_url = models.URLField()
    price = models.DecimalField(max_digits=10, decimal_places=2)
    tags = models.JSONField()
    author = models.ForeignKey(ModelAuthor, on_delete=models.CASCADE, related_name='models_uploaded')

    def __str__(self):
        return f"{self.name} - {self.version}"

class ModelPurchase(models.Model):
    user = models.CharField(max_length=200)  # Simplified for this example
    ai_model = models.ForeignKey(AIModel, on_delete=models.CASCADE)
    purchase_date = models.DateTimeField(auto_now_add=True)
    price_paid = models.DecimalField(max_digits=10, decimal_places=2)
    license_key = models.CharField(max_length=100)
    download_link = models.URLField()

    def __str__(self):
        return f"{self.user} - {self.ai_model.name}"

class UsageScenario(models.Model):
    ai_model = models.ForeignKey(AIModel, on_delete=models.CASCADE, related_name='usage_scenarios')
    title = models.CharField(max_length=200)
    description = models.TextField()
    code_snippet = models.TextField()
    usage_frequency = models.IntegerField(default=0)

    def __str__(self):
        return f"{self.ai_model.name} - {self.title}"

class ModelBenchmark(models.Model):
    ai_model = models.ForeignKey(AIModel, on_delete=models.CASCADE, related_name='benchmarks')
    metric_name = models.CharField(max_length=100)
    value = models.FloatField()
    benchmark_date = models.DateTimeField(auto_now_add=True)
    hardware_used = models.CharField(max_length=200)

    def __str__(self):
        return f"{self.ai_model.name} - {self.metric_name}: {self.value}"
```

Here we define five models:

- `ModelAuthor`: Represents the creator of AI models. It includes fields for name, bio, contact info, and rating.
- `AIModel`: Represents individual AI models with their details. It includes fields for name, model type, description, framework, version, download URL, price, tags, and author.
- `ModelPurchase`: Tracks purchases and downloads of AI models. It includes fields for the user, AI model, purchase date, price paid, license key, and download link.
- `UsageScenario`: Represents suggested use cases for each AI model. It includes fields for the AI model, title, description, code snippet, and usage frequency.
- `ModelBenchmark`: Stores performance benchmarks for AI models. It includes fields for the AI model, metric name, value, benchmark date, and hardware used.

The models are related to each other using foreign keys and related names to establish relationships between entities.

Django's ORM will automatically create the corresponding database migrations for tables based on these models. You can customize the models further by adding fields, methods, or meta options as needed.

### Create and apply migrations

Unlike other web frameworks like Laravel where you need to manually create database migrations which are separate from your models, Django allows you to define your models and then generate migrations automatically based on those models.

With the models defined in the previous step, all that's left is to create and apply the migrations to create the corresponding tables in the database. To generate the migrations, run:

```bash
python manage.py makemigrations
```

You should see output similar to:

```bash
Migrations for 'models_api':
  models_api/migrations/0001_initial.py
    + Create model ModelAuthor
    + Create model AIModel
    + Create model ModelBenchmark
    + Create model ModelPurchase
    + Create model UsageScenario
```

You can review the generated migration files in the `models_api/migrations` directory to see the actual migration operations that will be applied to the database based on your models.

Apply the migrations to create the corresponding tables in the Neon database:

```bash
python manage.py migrate
```

This command will create the tables for the models defined in the `models_api` app in the Neon database. The output should indicate that the migrations were applied successfully:

```
Operations to perform:
  Apply all migrations: admin, auth, contenttypes, models_api, sessions
Running migrations:
  Applying models_api.0001_initial... OK
```

You can verify that the tables were created in the Neon Console or by connecting to the database using a PostgreSQL client like `psql`.

### Implement serializers

With our models defined, we need to create serializers to convert model instances to JSON and vice versa. Serializers are a key component of Django REST Framework and are used to handle the conversion between complex data types (like Django model instances) and Python datatypes that can be easily rendered into JSON, XML, or other content types.

Start by creating a new file `models_api/serializers.py`:

```python
from rest_framework import serializers
from .models import ModelAuthor, AIModel, ModelPurchase, UsageScenario, ModelBenchmark

class ModelAuthorSerializer(serializers.ModelSerializer):
    class Meta:
        model = ModelAuthor
        fields = ['id', 'name', 'bio', 'contact_info', 'rating']

class AIModelSerializer(serializers.ModelSerializer):
    author = ModelAuthorSerializer(read_only=True)
    author_id = serializers.PrimaryKeyRelatedField(
        queryset=ModelAuthor.objects.all(), source='author', write_only=True
    )

    class Meta:
        model = AIModel
        fields = ['id', 'name', 'model_type', 'description', 'framework', 'version',
                  'download_url', 'price', 'tags', 'author', 'author_id']

class ModelPurchaseSerializer(serializers.ModelSerializer):
    class Meta:
        model = ModelPurchase
        fields = ['id', 'user', 'ai_model', 'purchase_date', 'price_paid', 'license_key', 'download_link']

class UsageScenarioSerializer(serializers.ModelSerializer):
    class Meta:
        model = UsageScenario
        fields = ['id', 'ai_model', 'title', 'description', 'code_snippet', 'usage_frequency']

class ModelBenchmarkSerializer(serializers.ModelSerializer):
    class Meta:
        model = ModelBenchmark
        fields = ['id', 'ai_model', 'metric_name', 'value', 'benchmark_date', 'hardware_used']
```

Let's break down each serializer to better understand their purpose:

1. `ModelAuthorSerializer`:

   - This serializer is used for the `ModelAuthor` model, it basically represents the author details.
   - It includes all fields of the model (`id`, `name`, `bio`, `contact_info`, `rating`).
   - By using `ModelSerializer`, we automatically get create and update functionality that matches the model fields.

2. `AIModelSerializer`:

   - This serializer is more complex due to its relationship with `ModelAuthor`.
   - We include a nested `author` field using `ModelAuthorSerializer(read_only=True)`. This means when serializing an `AIModel`, it will include all the author's details, but this field can't be used for writing (creating or updating).
   - We also include an `author_id` field, which is write-only. This allows clients to specify an author when creating or updating an `AIModel` by just providing the author's ID.
   - The `source='author'` in the `author_id` field tells DRF to use this field to set the `author` attribute of the `AIModel`.

3. `ModelPurchaseSerializer`:

   - This serializer includes all fields from the `ModelPurchase` model.
   - It will handle the serialization of purchase records, including details like the user, the AI model purchased, purchase date, and license information.

4. `UsageScenarioSerializer`:

   - This serializer corresponds to the `UsageScenario` model.
   - It includes all fields, allowing for the representation of different use cases or scenarios for AI models.

5. `ModelBenchmarkSerializer`:
   - This serializer is for the `ModelBenchmark` model.
   - It includes all fields, enabling the representation of performance benchmarks for AI models.

These serializers provide a powerful abstraction layer between your Python objects and the JSON representations of your API. They handle both serialization (Python to JSON) and deserialization (JSON to Python), including validation of incoming data.

By using `ModelSerializer`, we get a lot of functionality out of the box, such as automatically generated fields based on the model fields, default implementations of `create()` and `update()` methods, and validation based on model field types.

This approach reduces the amount of code we need to write while still providing flexibility where needed (like in the `AIModelSerializer` where we customize the author-related fields).

### Create API views

Now, let's create views to handle API requests. We'll use ViewSets for a clean, RESTful API structure.

Start by opening the `models_api/views.py` file and defining the views:

```python
from rest_framework import viewsets
from rest_framework.decorators import action
from rest_framework.response import Response
from .models import ModelAuthor, AIModel, ModelPurchase, UsageScenario, ModelBenchmark
from .serializers import (ModelAuthorSerializer, AIModelSerializer, ModelPurchaseSerializer,
                          UsageScenarioSerializer, ModelBenchmarkSerializer)

class ModelAuthorViewSet(viewsets.ModelViewSet):
    queryset = ModelAuthor.objects.all()
    serializer_class = ModelAuthorSerializer

    @action(detail=True, methods=['get'])
    def models(self, request, pk=None):
        author = self.get_object()
        models = author.models_uploaded.all()
        serializer = AIModelSerializer(models, many=True)
        return Response(serializer.data)

class AIModelViewSet(viewsets.ModelViewSet):
    queryset = AIModel.objects.all()
    serializer_class = AIModelSerializer

    @action(detail=True, methods=['get'])
    def usage_scenarios(self, request, pk=None):
        model = self.get_object()
        scenarios = model.usage_scenarios.all()
        serializer = UsageScenarioSerializer(scenarios, many=True)
        return Response(serializer.data)

    @action(detail=True, methods=['get'])
    def benchmarks(self, request, pk=None):
        model = self.get_object()
        benchmarks = model.benchmarks.all()
        serializer = ModelBenchmarkSerializer(benchmarks, many=True)
        return Response(serializer.data)

class ModelPurchaseViewSet(viewsets.ModelViewSet):
    queryset = ModelPurchase.objects.all()
    serializer_class = ModelPurchaseSerializer

class UsageScenarioViewSet(viewsets.ModelViewSet):
    queryset = UsageScenario.objects.all()
    serializer_class = UsageScenarioSerializer

class ModelBenchmarkViewSet(viewsets.ModelViewSet):
    queryset = ModelBenchmark.objects.all()
    serializer_class = ModelBenchmarkSerializer
```

This code defines ViewSets for each model, providing CRUD operations for all entities in our AI Model Marketplace. The `ModelAuthorViewSet` and `AIModelViewSet` include custom actions to retrieve related data (uploaded models for authors, usage scenarios and benchmarks for AI models).

### Configure URL routing

Create a new file `models_api/urls.py` to define the URL patterns for our API:

```python
from django.urls import path, include
from rest_framework.routers import DefaultRouter
from .views import (ModelAuthorViewSet, AIModelViewSet, ModelPurchaseViewSet,
                    UsageScenarioViewSet, ModelBenchmarkViewSet)

router = DefaultRouter()
router.register(r'authors', ModelAuthorViewSet)
router.register(r'models', AIModelViewSet)
router.register(r'purchases', ModelPurchaseViewSet)
router.register(r'usage-scenarios', UsageScenarioViewSet)
router.register(r'benchmarks', ModelBenchmarkViewSet)

urlpatterns = [
    path('', include(router.urls)),
]
```

This sets up the URL routing for our API views using DRF's `DefaultRouter`.

Now, update the project's main `urls.py` file to include the app's URLs:

```python
from django.contrib import admin
from django.urls import path, include

urlpatterns = [
    path('admin/', admin.site.urls),

    # Include the API URLs:
    path('api/', include('models_api.urls')),
]
```

This configuration makes our API accessible under the `/api/` path.

## Testing the API

With our API views and URL routing configured, we can now test the API by running the Django development server:

```bash
python manage.py runserver
```

If you were to now visit `http://localhost:8000/api/` in your browser, you would see a list of available API endpoints. This is the default behavior of DRF's `DefaultRouter`. If you were to visit `http://localhost:8000/api/authors/`, you would see a list of authors (which is currently empty), and so on for other endpoints. The web interface provided by DRF allows you to interact with the API endpoints directly from the browser like a simple API client, you can view, create, update, and delete records by interacting with the API endpoints directly.

Alternatively, you can use tools like `curl` or Postman to interact with the API programmatically. Here are some example `curl` commands to test the API:

1. Create a new model author:

   ```bash shouldWrap
   curl -X POST http://localhost:8000/api/authors/ -H "Content-Type: application/json" -d '{"name":"AI Innovations Inc.", "bio":"Leading AI research company", "contact_info":"contact@aiinnovations.com", "rating":4.8}'
   ```

   This will create a new model author with the specified details.

2. Create a new AI model:

   ```bash shouldWrap
   curl -X POST http://localhost:8000/api/models/ -H "Content-Type: application/json" -d '{"name":"AdvancedNLP", "model_type":"NLP", "description":"State-of-the-art NLP model", "framework":"PT", "version":"1.0", "download_url":"https://example.com/model", "price":"99.99", "tags":["NLP", "transformer"], "author_id":1}'
   ```

   This will create a new AI model associated with the author created in the previous step.

3. Get all AI models:

   ```bash
   curl http://localhost:8000/api/models/
   ```

4. Get usage scenarios for a specific AI model:

   ```bash
   curl http://localhost:8000/api/models/1/usage_scenarios/
   ```

5. Add a benchmark for an AI model:
   ```bash shouldWrap
   curl -X POST http://localhost:8000/api/benchmarks/ -H "Content-Type: application/json" -d '{"ai_model":1, "metric_name":"Accuracy", "value":0.95, "hardware_used":"NVIDIA A100 GPU"}'
   ```

## Conclusion

In this guide, we've built a RESTful API for a simple AI Model Marketplace using Django, Django REST Framework, and Neon's serverless Postgres. We covered setting up the project, defining models for AI models, authors, purchases, usage scenarios, and benchmarks, creating serializers and views, and configuring URL routing.

This API provides a solid foundation for an AI Model Marketplace platform. You can extend it with features like user authentication, advanced search and filtering, model versioning, and integration with payment systems. The combination of Django's powerful ORM, DRF's flexibility, and Neon's scalable Postgres database makes it easy to build and deploy robust, performant APIs for complex applications like AI model distribution platforms.

## Additional Resources

- [Django REST Framework Documentation](https://www.django-rest-framework.org/)
- [Django Documentation](https://docs.djangoproject.com/en/stable/)
- [Neon Documentation](/docs/)

<NeedHelp />


# Document Store using JSONB in Postgres

---
title: Document Store using JSONB in Postgres
subtitle: A step-by-step guide describing how to use Postgres as a document store using JSONB
author: vkarpov15
enableTableOfContents: true
createdAt: '2024-12-17T13:24:36.612Z'
updatedOn: '2024-12-17T13:24:36.612Z'
---

The JSONB type enables you to store and query nested JSON-like data in Postgres.
With JSONB, you can to store arbitrarily complex objects and arrays in your Postgres tables, as well as query based on properties in those objects and arrays.
You can even use GIN indexes to index nested properties within JSONB objects.

## Steps

- Set up a table with a JSONB column
- Insert and retrieve JSONB data
- Query based on JSONB fields
- Document store using Sequelize ORM
- Query arrays and objects in JSONB
- Type casting in JSONB queries
- Update and modify JSONB data
- Index JSONB fields using GIN indexes

## Set up a table with a JSONB column

To use Postgres as a document store, you can create a table with two columns: an `id` and a `data` property that is of type `JSONB`.
You can run the following `CREATE TABLE` statement in the Neon SQL Editor or from a client such as `psql` that is connected to Neon.

```sql
CREATE TABLE documents (
  id SERIAL PRIMARY KEY,
  data JSONB
);
```

JSONB columns can store any JSON object, including objects, arrays, and even nested objects.

## Insert and retrieve JSONB data

Run the following SQL to insert two new rows into the `documents` table. These rows will have `data` columns with slightly different properties: the first row has a `steps` property, and the second row has a nested object property, `author`.

```sql
INSERT INTO documents (data)
VALUES (
  '{
    "title": "Neon and JSONB",
    "body": "Using JSONB to store flexible data structures in Postgres.",
    "tags": ["Postgres", "Neon", "JSONB"],
    "steps": ["Set up a table with a JSONB column", "Insert and retrieve JSONB data"]
  }'
),
(
  '{
    "title": "Scaling Neon with Postgres",
    "body": "Learn how to scale your Neon instances with PostgreSQL features.",
    "tags": ["Neon", "Postgres", "scaling"],
    "author": { "name": "John Smith", "age": 30 }
  }'
);
```

You can then load rows from the `documents` collection by `id`. For example, you can load the "Neon and JSONB" row using the following query.

```sql
SELECT * FROM documents WHERE id = 1
```

## Query based on JSONB fields

You can also query based on properties in the JSONB column using the `->>` operator, which extracts values from the JSONB column.
For example, you can load all documents with a given `title` property using the following query.
Note the quotes around `title` in the `WHERE` clause.

```sql
SELECT * FROM documents WHERE data->>'title' = 'Neon and JSONB'
```

You can also query based on nested properties.
For example, the following query returns all documents whose `author` property is an object with a `name` property equal to 'John Smith'.

```sql
SELECT * FROM documents WHERE data->'author'->>'name' = 'John Smith'
```

## Document store using Sequelize ORM

Many developers use Postgres through an ORM, like [Sequelize](https://sequelize.org/) in Node.js.
ORMs often provide neat syntactic shortcuts for working with JSONB.
For example, the following Node.js code shows how you can connect to the existing `documents` table from previous examples using Sequelize.

```javascript
import Sequelize from 'sequelize';

const sequelize = new Sequelize(process.env.POSTGRES_CONNECTION_STRING);

const Document = sequelize.define(
  'Document',
  {
    data: {
      type: Sequelize.DataTypes.JSONB,
      allowNull: false,
    },
  },
  { tableName: 'documents', timestamps: false }
);
```

You can then create rows in the `documents` collection using the following:

```javascript
await Document.bulkCreate([
  {
    data: {
      title: 'Neon and JSONB',
      body: 'Using JSONB to store flexible data structures in Postgres.',
      tags: ['Postgres', 'Neon', 'JSONB'],
      author: { name: 'John Smith', age: 30 },
    },
  },
]);
```

Finally, you can find documents by the author's name using the following query.
Note that Sequelize takes care of converting `data.author.name` to `data->'author'->>'name'` under the hood.

```javascript
const documents = await Document.findAll({
  where: {
    'data.author.name': 'John Smith',
  },
});
```

You can read more about working with JSONB in [Sequelize](https://sequelize.org/docs/v7/querying/json/) [Prisma](https://www.prisma.io/docs/orm/prisma-client/special-fields-and-types/working-with-json-fields), and [Objection.js](https://vincit.github.io/objection.js/recipes/json-queries.html) on their respective documentation sites.

## Query arrays and objects in JSONB

Postgres has several operators that are useful for working with JSONB, including the `@>` operator, which checks if a given property contains the given value.
With arrays, `@>` can check whether the array contains a given value.
For example, the following query returns all documents whose `tags` property contains the string "JSONB".

```sql
SELECT * FROM documents WHERE data->'tags' @> '["JSONB"]'
```

Note that the right-hand side of `@>` is a JSON string.

With objects, `@>` can check whether the document contains one or more properties.
For example, the following query returns all documents whose `author` property has `author` equal to 'John Smith' and `age` equal to 30.

```sql
SELECT * FROM documents WHERE data->'author' @> '{"name":"John Smith","age":30}'
```

The query above is equivalent to this query:

```sql shouldWrap
SELECT * FROM documents WHERE data->'author'->>'name' = 'John Smith' AND data->'author'->>'age' = '30'
```

## Type casting in JSONB queries

Operators like `=` and `@>` are fairly easy to work with: they don't throw any errors if the JSONB property has the wrong type.
However, things get a bit more tricky if you want to find all documents whose `author`'s `age` property is greater than 25.
For example, this query throws an "operator does not exist" error:

```sql
SELECT *
FROM documents
WHERE (data -> 'author' ->> 'age') > 29;
```

You need to explicitly cast `age` to an `int` type for the above query to run, as shown here:

```sql
SELECT *
FROM documents
WHERE (data -> 'author' ->> 'age')::int > 29;
```

Depending on your data, you may need to add extra checks to avoid throwing an error if a document has an `age` property that can't be casted to an int.
The following query explicitly checks if `age` is a numeric string before attempting to cast to an `int`.

```sql
SELECT *
FROM documents
WHERE (data -> 'author' ->> 'age') ~ '^\d+$'
  AND (data -> 'author' ->> 'age')::int > 29;
```

## Update and modify JSONB data

You can also update individual properties within your JSONB document without overwriting the entire document using the [`jsonb_set()` function](https://neon.tech/postgresql/postgresql-json-functions/postgresql-jsonb_set).
For example, the following code updates the `author.age` property to 35 for all documents whose `author.name` property is "John Smith".

```sql
UPDATE documents
SET data = jsonb_set(data, '{author,age}', '35'::jsonb)
WHERE data->'author'->>'name' = 'John Smith';
```

Note that `jsonb_set()` expects the nested property name separated by commas (`,`), not dots (`.`).

## Index JSONB fields using GIN indexes

[GIN indexes](https://www.postgresql.org/docs/current/gin-intro.html) allow you to index JSONB properties, which can make your queries faster as your data grows. This query shows how you can create a GIN index on the `data` property:

```sql
CREATE INDEX content_idx ON documents USING GIN (data);
```

To test out the GIN index, let's first insert 100 documents, 1 of which has `author.name` set to "John Smith", and 99 that do not. Sometimes Postgres decides to skip using indexes and use a sequential scan instead when a query matches most of the table.

```sql
DO $$
BEGIN
  FOR i IN 1..100 LOOP
    IF i = 1 THEN
      -- Insert the special document with author name 'John Smith'
      INSERT INTO documents (data)
      VALUES (
        '{
          "title": "Scaling Neon with Postgres",
          "body": "Learn how to scale your Neon instances with PostgreSQL features.",
          "tags": ["Neon", "Postgres", "scaling"],
          "author": { "name": "John Smith", "age": 30 }
        }'::jsonb
      );
    ELSE
      -- Insert general documents for other iterations
      INSERT INTO documents (data)
      VALUES (
        '{
          "title": "Neon and JSONB",
          "body": "Using JSONB to store flexible data structures in Postgres.",
          "tags": ["Postgres", "Neon", "JSONB"],
          "steps": ["Set up a table with a JSONB column", "Insert and retrieve JSONB data"]
        }'::jsonb
      );
    END IF;
  END LOOP;
END $$;
```

Next, you can run an `EXPLAIN ANALYZE` query (or just click the "Explain" button in the Neon SQL Editor) to confirm that Postgres is using your GIN index.

```sql
EXPLAIN ANALYZE
SELECT *
FROM documents
WHERE data @> '{"author": {"name": "John Smith"}}'::jsonb;
```

Note that the query above uses the containment operator `@>`, **not** `WHERE data->'author'->>'name' = 'John Smith'`. [GIN indexes only support certain operators with JSONB data](https://www.postgresql.org/docs/current/datatype-json.html#JSON-INDEXING), including `@>`.

The `EXPLAIN ANALYZE` query should produce output that resembles the following. The Bitmap Index Scan means that Postgres is using a GIN index rather than a sequential scan to answer the query.

```
Bitmap Heap Scan on documents  (cost=8.52..12.54 rows=1 width=245) (actual time=0.014..0.016 rows=3 loops=1)
  Recheck Cond: (data @> '{"author": {"name": "John Smith"}}'::jsonb)
  Heap Blocks: exact=1
  ->  Bitmap Index Scan on idx_documents_data  (cost=0.00..8.52 rows=1 width=0) (actual time=0.007..0.007 rows=3 loops=1)
        Index Cond: (data @> '{"author": {"name": "John Smith"}}'::jsonb)
Planning Time: 0.066 ms
Execution Time: 0.096 ms
```


# Building ASP.NET Core Applications with Neon and Entity Framework Core

---
title: Building ASP.NET Core Applications with Neon and Entity Framework Core
subtitle: Learn how to build a .NET application with Neon's serverless Postgres and Entity Framework Core
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-11-02T00:00:00.000Z'
updatedOn: '2024-11-02T00:00:00.000Z'
---

When building .NET applications, choosing the right database solution is an important step to good performance and scalability. Neon's serverless Postgres is a great choice for .NET developers, thanks to features like automatic scaling, branching, and connection pooling that integrate well with .NET's ecosystem.

In this guide, we'll walk through setting up a Neon database with a .NET application and explore best practices for connecting and interacting with Neon Postgres and structuring your application using Entity Framework Core.

## Prerequisites

- .NET 8.0 or later installed
- A [Neon account](https://console.neon.tech/signup)
- Basic familiarity with Entity Framework Core

## Setting Up Your Neon Database

1. Create a new Neon project from the [Neon Console](https://console.neon.tech)
2. Note your connection string from the connection details page

Your connection string will look similar to this:

```shell
postgres://[user]:[password]@[neon_hostname]/[dbname]?sslmode=require
```

## Creating a .NET Project with Neon Integration

With your Neon database set up, let's create a sample inventory management system to demonstrate Neon integration.

1. Create a new .NET Web API project:

   ```bash
   dotnet new webapi -n NeonInventoryApi
   ```

   This command creates a new Web API project with a basic structure including:

   - `Program.cs`: The entry point of your application
   - `appsettings.json`: Configuration files
   - `Properties/launchSettings.json`: Debug and launch configuration

   Then navigate to the project directory:

   ```bash
   cd NeonInventoryApi
   ```

2. Install the required NuGet packages:

   ```bash
   dotnet add package Npgsql.EntityFrameworkCore.PostgreSQL
   dotnet add package Microsoft.EntityFrameworkCore.Tools
   dotnet add package Microsoft.EntityFrameworkCore.Design
   ```

   These packages provide us with the following:

   - `Npgsql.EntityFrameworkCore.PostgreSQL`: The Postgres database provider for Entity Framework Core
   - `Microsoft.EntityFrameworkCore.Tools`: Command-line tools for migrations
   - `Microsoft.EntityFrameworkCore.Design`: Design-time tools for EF Core

3. Create a Models directory and add your entity models:

   ```bash
   mkdir Models
   ```

   Create a new file `Models/Product.cs`:

   ```csharp
   using System;
   using System.ComponentModel.DataAnnotations;

   namespace NeonInventoryApi.Models
   {
       public class Product
       {
           [Key]
           public int? Id { get; set; }

           [Required]
           [MaxLength(100)]
           public string? Name { get; set; }

           [Required]
           [MaxLength(20)]
           public string? SKU { get; set; }

           [Range(0, 999999.99)]
           public decimal? Price { get; set; }

           [Range(0, int.MaxValue)]
           public int? StockLevel { get; set; }

           public DateTime LastRestocked { get; set; }

           // Additional validation attributes
           [Timestamp]
           public byte[]? Version { get; set; }

       }
   }
   ```

   Our `Product` model includes the following:

   - Data annotations for validation
   - A unique identifier (`Id`)
   - Basic product information fields
   - Optimistic concurrency control (`Version`)
   - Comments indicating where to add relationships

4. Create a Data directory for your database context, this is where you will define your `DbContext` which represents your database schema:

   ```bash
   mkdir Data
   ```

   Create a new file `Data/InventoryContext.cs` with the following content:

   ```csharp
   using Microsoft.EntityFrameworkCore;
   using NeonInventoryApi.Models;
   using System.Reflection;

   namespace NeonInventoryApi.Data
   {
       public class InventoryContext : DbContext
       {
           public InventoryContext(DbContextOptions<InventoryContext> options)
               : base(options)
           { }

           public DbSet<Product> Products { get; set; }

           protected override void OnModelCreating(ModelBuilder modelBuilder)
           {
               // Configure the Product entity
               modelBuilder.Entity<Product>(entity =>
               {
                   // Create a unique index on SKU
                   entity.HasIndex(p => p.SKU)
                         .IsUnique();

                   // Configure the Name property
                   entity.Property(p => p.Name)
                         .IsRequired()
                         .HasMaxLength(100);

                   // Configure the Price property
                   entity.Property(p => p.Price)
                         .HasPrecision(10, 2);

                   // Add a default value for LastRestocked
                   entity.Property(p => p.LastRestocked)
                         .HasDefaultValueSql("CURRENT_TIMESTAMP");
               });

               modelBuilder.ApplyConfigurationsFromAssembly(Assembly.GetExecutingAssembly());

               base.OnModelCreating(modelBuilder);
           }
       }
   }
   ```

   The `DbContext` includes:

   - Entity configuration using Fluent API
   - Precision settings for decimal values
   - Default value configurations
   - Index definitions
   - Support for separate configuration classes

We've set up the basic structure for our application. Next, we'll configure the database connection and implement the repository pattern for database operations.

## Configuring Database Connection

With the database context in place, we need to configure the connection to our Neon database. Let's set this up securely.

### Basic Configuration

Update `Program.cs` to include the database context:

```csharp
using NeonInventoryApi.Data;

var connectionString = builder.Configuration.GetConnectionString("NeonConnection");

builder.Services.AddDbContext<InventoryContext>(options =>
    options.UseNpgsql(builder.Configuration.GetConnectionString("NeonConnection")));
```

### Managing Connection Strings Securely

There are two main approaches to storing your connection string securely:

1. **Development**: Use `appsettings.Development.json` for local development:

```json
{
  "ConnectionStrings": {
    "NeonConnection": "Server=your-neon-hostname;Database=neondb;User Id=your-username;Password=your-password;SSL Mode=Require;Trust Server Certificate=true"
  }
}
```

2. **Production**: Use environment variables:

```csharp
// Program.cs
var connectionString = Environment.GetEnvironmentVariable("NEON_CONNECTION_STRING")
    ?? builder.Configuration.GetConnectionString("NeonConnection");
```

That way, you can set the `NEON_CONNECTION_STRING` environment variable in your production environment to securely store your connection string.

As an alternative, you can use the `Azure Key Vault` to store your connection string securely. To learn more about this approach, check out the [Azure Key Vault documentation](https://learn.microsoft.com/en-us/azure/key-vault/general/basic-concepts).

### Testing the Configuration

To verify everything is working correctly, start your application:

```bash
dotnet run
```

If everything is set up correctly, you should see a message indicating that the application is running and listening on a specific port.

## Implementing Repository Pattern

The repository pattern acts as an abstraction layer between your application logic and data access code. This pattern helps maintain clean separation of concerns and makes your code more testable and maintainable.

In our inventory system, we'll implement this pattern to handle all database operations related to products.

First, let's define the interface that specifies what operations our repository can perform. Create a new file `Repositories/IProductRepository.cs` with the following content:

```csharp
using NeonInventoryApi.Models;

namespace NeonInventoryApi.Repositories
{
    public interface IProductRepository
    {
        Task<Product> GetByIdAsync(int id);
        Task<IEnumerable<Product>> GetAllAsync();
        Task<Product> CreateAsync(Product product);
        Task UpdateAsync(Product product);
        Task DeleteAsync(int id);
    }
}
```

This interface defines the contract for our repository. That way all implementations will provide these basic CRUD (Create, Read, Update, Delete) operations. Using an interface allows us to easily swap implementations or create mock versions for testing.

Next, let's implement the repository, starting with the `ProductRepository` class in `Repositories/ProductRepository.cs`:

```csharp
using System.Collections.Generic;
using System.Threading.Tasks;
using Microsoft.EntityFrameworkCore;
using NeonInventoryApi.Data;
using NeonInventoryApi.Models;

namespace NeonInventoryApi.Repositories{
    public class ProductRepository : IProductRepository
    {
        private readonly InventoryContext _context;

        public ProductRepository(InventoryContext context)
        {
            _context = context;
        }

        public async Task<Product> GetByIdAsync(int id)
        {
            return await _context.Products
                .FirstOrDefaultAsync(p => p.Id == id);
        }

        public async Task<IEnumerable<Product>> GetAllAsync()
        {
            return await _context.Products.ToListAsync();
        }

        public async Task<Product> CreateAsync(Product product)
        {
            product.LastRestocked = product.LastRestocked.ToUniversalTime();
            _context.Products.Add(product);
            await _context.SaveChangesAsync();
            return product;
        }

        public async Task UpdateAsync(Product product)
        {
            product.LastRestocked = product.LastRestocked.ToUniversalTime();
            _context.Products.Update(product);
            _context.Entry(product).State = EntityState.Modified;
            await _context.SaveChangesAsync();
        }

        public async Task DeleteAsync(int id)
        {
            var product = await GetByIdAsync(id);
            if (product != null)
            {
                _context.Products.Remove(product);
                await _context.SaveChangesAsync();
            }
        }
    }
}
```

Each method in the repository serves a specific purpose:

- `GetByIdAsync`: Retrieves a single product by its ID using asynchronous operations
- `GetAllAsync`: Returns all products from the database as an enumerable collection
- `CreateAsync`: Adds a new product to the database and returns the created product
- `UpdateAsync`: Modifies an existing product's information in the database
- `DeleteAsync`: Removes a product from the database by its ID

To use this repository in your application, register it with the dependency injection container in `Program.cs`:

```csharp
builder.Services.AddScoped<IProductRepository, ProductRepository>();
```

This registration makes the repository available throughout your application.

Then you can inject it into your controllers or services, create a new controller `Controllers/ProductsController.cs`:

```csharp
using Microsoft.AspNetCore.Mvc;
using NeonInventoryApi.Models;
using NeonInventoryApi.Repositories;
using System.Collections.Generic;
using System.Threading.Tasks;

namespace NeonInventoryApi.Controllers
{
    [ApiController]
    [Route("api/[controller]")]
    public class ProductsController : ControllerBase
    {
        private readonly IProductRepository _repository;

        public ProductsController(IProductRepository repository)
        {
            _repository = repository;
        }

        [HttpGet]
        public async Task<ActionResult<IEnumerable<Product>>> GetAll()
        {
            return Ok(await _repository.GetAllAsync());
        }

        [HttpPost]
        public async Task<ActionResult<Product>> Create([FromBody] Product product)
        {
            var createdProduct = await _repository.CreateAsync(product);
            return CreatedAtAction(nameof(GetById), new { id = createdProduct.Id }, createdProduct);
        }

        [HttpGet("{id}")]
        public async Task<ActionResult<Product>> GetById(int id)
        {
            var product = await _repository.GetByIdAsync(id);
            if (product == null)
                return NotFound();

            return Ok(product);
        }

        [HttpPut("{id}")]
        public async Task<ActionResult> Update(int id, [FromBody] Product product)
        {
            if (id != product.Id)
                return BadRequest();

            await _repository.UpdateAsync(product);
            return NoContent();
        }

        [HttpDelete("{id}")]
        public async Task<ActionResult> Delete(int id)
        {
            await _repository.DeleteAsync(id);
            return NoContent();
        }
    }
}
```

The controller uses the repository to interact with the database. Each action corresponds to a CRUD operation and returns appropriate HTTP status codes based on the operation's success.

The repository pattern is particularly useful when working with Postgres as it provides a single place to implement database-specific optimizations and connection handling strategies.

After implementing the repository pattern, make sure to register the controllers in `Program.cs`:

```csharp
app.MapControllers();
```

With the repository pattern in place, we are nearly ready to start using our Neon database. Before that, let's add some sample data to the database using migrations.

## Migrations

Similar to other web frameworks, Entity Framework Core uses migrations to keep track of changes to your database schema. With migrations, you can incrementally apply schema updates as your data models evolve over time.

### Installing `dotnet-ef`

Before creating migrations, make sure you have the `dotnet-ef` tool is installed. This tool provides command-line capabilities for managing Entity Framework migrations and database updates.

Install `dotnet-ef` globally by running:

```bash
dotnet tool install --global dotnet-ef
```

> **Note**: If you encounter any issues after installation, make sure your environment's `PATH` includes the directory where .NET global tools are installed. It is usually `~/.dotnet/tools` on macOS and Linux. You can add this to your `PATH` temporarily with the following command, or add it to your shell configuration file (like `.bashrc` or `.zshrc`) to make it permanent:

```bash
export PATH="$PATH:$HOME/.dotnet/tools"
```

### Creating and Applying Migrations

Once `dotnet-ef` is installed, you can create a migration to define your database schema based on your `DbContext` and entity classes.

1. The following command generates the initial migration files, representing the schema of your database:

   ```bash
   dotnet ef migrations add InitialCreate
   ```

   This command creates a new folder called `Migrations` (if it doesn't already exist) and generates files that contain code to create your database tables. It examines your `DbContext` and entity classes to determine the schema, so that you don't have to write SQL manually.

2. Now, apply the migration to your actual database:

   ```bash
   dotnet ef database update
   ```

   This command executes the generated migration code against the database, creating the necessary tables and constraints based on your model definitions.

After these steps, your database will be fully synchronized with your data model, and you're ready to start using it in your application.

Now if you run your application and navigate to `http://localhost:5221/api/products`, you should see an empty array `[]` as we haven't added any products yet.

## Testing CRUD Operations

With our API and database set up, we’re ready to test the CRUD operations. We’ll use simple HTTP requests to add, retrieve, update, and delete products from our Neon database.

### 1. Adding a New Product

To add a new product, send a `POST` request to `http://localhost:5221/api/products`. Here’s an example using `curl`:

```bash
curl -X POST http://localhost:5221/api/products \
     -H "Content-Type: application/json" \
     -d '{
           "name": "Sample Product",
           "sku": "SP123",
           "price": 29.99,
           "stockLevel": 100,
           "lastRestocked": "2024-01-01T00:00:00"
         }'
```

> Alternatively, you can use Postman by setting up a `POST` request to the same URL, setting the header `Content-Type` to `application/json`, and adding the JSON body.

If the request is successful, the API will return the newly created product with an auto-generated `Id`.

### 2. Retrieving All Products

To retrieve all products, send a `GET` request to `http://localhost:5221/api/products`:

```bash
curl -X GET http://localhost:5221/api/products
```

This should return a list of all products in the database. If you just added one product, you’ll see an array with that single product.

### 3. Retrieving a Product by ID

To retrieve a specific product, send a `GET` request to `http://localhost:5221/api/products/{id}`, replacing `{id}` with the actual product ID. For example:

```bash
curl -X GET http://localhost:5221/api/products/1
```

This request will return the product with the specified `Id`. If no product is found, it may return a `404 Not Found`.

### 4. Updating a Product

To update an existing product, send a `PUT` request to `http://localhost:5221/api/products/{id}`. Include the updated information in the request body. Here’s an example:

```bash
curl -X PUT http://localhost:5221/api/products/1 \
     -H "Content-Type: application/json" \
     -d '{
           "id": 1,
           "name": "Updated Product",
           "sku": "SP123",
           "price": 24.99,
           "stockLevel": 120,
           "lastRestocked": "2024-01-02T00:00:00"
         }'
```

This request updates the product with `Id = 1`. The API will return the updated product information if the operation succeeds.

### 5. Deleting a Product

To delete a product, send a `DELETE` request to `http://localhost:5221/api/products/{id}`, replacing `{id}` with the product’s actual ID:

```bash
curl -X DELETE http://localhost:5221/api/products/1
```

If successful, the API will delete the product from the database and return a status code indicating success.

## Conclusion

Connecting .NET applications to Neon gives you a strong and scalable database setup. By following these steps and using features like connection pooling and automatic scaling, you can create applications that perform well even as they grow.

As next steps, consider adding more features to your application, such as authentication and authorization. You can also explore advanced Neon features like branching and data replication to enhance your application's performance and reliability.

For more details, check out:

- [Neon Documentation](/docs)
- [Entity Framework Core Documentation](https://docs.microsoft.com/en-us/ef/core/)
- [Npgsql Documentation](https://www.npgsql.org/doc/index.html)

<NeedHelp />


# Connecting .NET Applications to Neon Database

---
title: Connecting .NET Applications to Neon Database
subtitle: Learn how to connect your .NET applications to Neon's serverless Postgres database
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-11-02T00:00:00.000Z'
updatedOn: '2024-11-02T00:00:00.000Z'
---

In this guide, we'll walk through the process of connecting a .NET application to Neon Postgres, exploring best practices for connection management and basic performance optimization.

## Prerequisites

Before we begin, make sure you have:

- .NET 8.0 or later installed
- A [Neon account](https://console.neon.tech/signup)
- Basic familiarity with .NET development

## Setting Up Your Neon Database

First, let's create a Neon database that we'll connect to from our .NET application.

1. Log in to the [Neon Console](https://console.neon.tech)
2. Click "New Project" and follow the creation wizard
3. Once created, you'll see your connection details. Your connection string will look like this:

```
postgres://[user]:[password]@[neon_hostname]/[dbname]?sslmode=require
```

Save these details - you'll need them when setting up your .NET application.

## Creating a Basic .NET Project

Let's create a simple .NET project and add the necessary dependencies to connect to your Neon database.

Open your terminal and run:

```bash
dotnet new console -n NeonDemo
cd NeonDemo
```

This creates a new console application named "NeonDemo" and navigates to the project directory.

The directory will contain a `NeonDemo.csproj` file, which is the project file for your application, similar to a `package.json` file in Node.js if you are coming from a JavaScript background.

Next, we need to add the required package for Postgres connectivity. We can do this using the `dotnet add package` command:

```bash
dotnet add package Npgsql.EntityFrameworkCore.PostgreSQL
```

This package provides the Postgres database provider for Entity Framework Core, Microsoft's object-database mapper for .NET applications and is a popular choice for working with Postgres databases.

## Configuring Database Connection

There are several ways to manage your database connection string. Let's look at the most common approaches for .NET applications and how to handle sensitive information securely.

### Basic Connection Setup

For local development, start by creating an `appsettings.json` file in your project root. This file will store your connection string:

```json
{
  "ConnectionStrings": {
    "NeonConnection": "Host=your-neon-hostname;Database=neondb;Username=your-username;Password=your-password;SSL Mode=Require;Trust Server Certificate=true"
  }
}
```

This approach works well for development but isn't recommended for production use since it stores sensitive information in a file that might be committed to source control or exposed in other ways.

### Using Environment Variables

For production environments, it's better to use environment variables. Here's how to implement this:

```csharp
var connectionString = Environment.GetEnvironmentVariable("NEON_CONNECTION_STRING")
    ?? builder.Configuration.GetConnectionString("NeonConnection");
```

This code first checks for an environment variable, falling back to the configuration file if not found. This gives you flexibility in different environments while keeping sensitive data secure.

You can set the `NEON_CONNECTION_STRING` environment variable in your production environment, or use a tool like [Azure Key Vault](https://azure.microsoft.com/en-us/services/key-vault/) to manage secrets.

## Understanding Connection Pooling

Connection pooling helps improve performance by maintaining and reusing database connections. In many cases, too many connections can lead to performance issues, so it's important to manage them effectively.

There are two levels of connection pooling available when working with Neon: [Neon's built-in connection pooling service](/docs/connect/connection-pooling) (PgBouncer) and application-side pooling through Npgsql.

### Neon's Connection Pooling

Neon uses PgBouncer to provide connection pooling at the infrastructure level, supporting up to 10,000 concurrent connections. To use Neon's pooled connections, select the "Pooled connection" option in your project's connection settings. Your connection string will look like this:

```
postgres://[user]:[password]@[pooled-hostname].pool.[region].neon.tech/[dbname]?sslmode=require
```

However, using a pooled connection string for database migrations can be prone to errors. For this reason, it is recommended to use a direct (non-pooled) connection when performing database migrations. For more information about direct and pooled connections, see [Connection pooling](/docs/connect/connection-pooling).

### Application-Side Pooling

While Neon handles connection pooling at the infrastructure level, you can also configure Npgsql's built-in connection pooling for additional control:

```csharp
using Npgsql;

var connectionStringBuilder = new NpgsqlConnectionStringBuilder(
    builder.Configuration.GetConnectionString("NeonConnection"))
{
    MaxPoolSize = 50,               // Maximum number of connections in the pool
    MinPoolSize = 5,                // Minimum number of connections to maintain
    ConnectionIdleLifetime = 300,   // How long in seconds an idle connection is kept
    ConnectionPruningInterval = 60, // How often to check for idle connections
    Pooling = true,                 // Enable connection pooling
    Timeout = 30,                   // Connection timeout in seconds
    CommandTimeout = 30             // Command timeout in seconds
};

services.AddDbContext<InventoryContext>(options =>
    options.UseNpgsql(connectionString));
```

This code snippet configures the connection pooling settings for Npgsql. You can adjust the `MaxPoolSize`, `MinPoolSize`, `ConnectionIdleLifetime`, and other parameters to suit your application's needs. In most cases, you can rely on Neon's built-in connection pooling service for optimal performance.

### PgBouncer vs Application-Side Pooling

Both pooling methods have their advantages, this is also valid for other frameworks and languages:

- **Neon's PgBouncer**: Handles connection pooling at the infrastructure level, reducing overhead and managing connections efficiently across multiple application instances.
- **Npgsql Pooling**: Provides fine-grained control at the application level and can be useful for specific application requirements.

For most applications, using Neon's connection pooling service is sufficient. You can consider configuring application-side pooling if you have specific requirements or need additional control over connection management.

As mentioned earlier, when performing database migrations, it's recommended to use a direct connection to avoid potential issues with pooled connections.

## Basic Performance Optimization

Besides connection pooling, let's look at some simple ways to optimize your database operations. Most of these optimizations apply to all types of frameworks, but we'll focus on Entity Framework Core, a popular ORM for .NET applications.

### Using Async Operations

Always use async methods for database operations. This improves application responsiveness by not blocking threads while waiting for database operations to complete:

```csharp
// Example of a basic repository method using async/await
public async Task<List<User>> GetUsersAsync()
{
    using var context = new YourDbContext();
    // ToListAsync() is non-blocking
    return await context.Users.ToListAsync();
}
```

This approach is more efficient than synchronous operations, especially in high-traffic applications. The `ToListAsync` method is provided by Entity Framework Core and returns a `Task<List<T>>`.

### Batch Operations

When working with multiple records, use batch operations to reduce database round trips.

For example, instead of saving one record at a time like this:

```csharp
foreach (var item in items)
{
    context.Items.Add(item);
    await context.SaveChangesAsync();
}
```

Which results in multiple database calls, causing performance overhead. You can batch the operations like this:

```csharp
context.Items.AddRange(items);
await context.SaveChangesAsync();
```

This approach is much more efficient as it reduces the number of database calls. Entity Framework Core will automatically generate a single `INSERT` statement for all the records.

This is especially useful when dealing with large datasets and large numbers of records.

### Use NoTracking for Read-Only Operations

The tracking feature in Entity Framework Core keeps track of changes to entities, which can be useful for update and delete operations.

However, for read-only operations where you don't need to track changes, you can disable tracking to improve performance:

```csharp
public async Task<List<User>> GetUsersAsync()
{
    using var context = new YourDbContext();
    return await context.Users.AsNoTracking().ToListAsync();
}
```

The `AsNoTracking` method tells Entity Framework Core not to track changes to entities, which can improve performance for read-only operations.

### Avoid N+1 Queries

Avoid N+1 queries, a common performance issue in ORMs where multiple queries are executed for each record in a collection. This can lead to a large number of database calls and performance degradation.

This also applies to Entity Framework Core, especially when using lazy loading. To avoid N+1 queries, use eager loading or explicit loading:

```csharp
// Eager loading
var users = context.Users.Include(u => u.Orders).ToList();

// Explicit loading
var users = context.Users.ToList();
foreach (var user in users)
{
    context.Entry(user).Collection(u => u.Orders).Load();
}
```

Eager loading fetches related entities in a single query, while explicit loading allows you to load related entities on demand.

## Best Practices

When working with Neon in your .NET applications, and any database in general, it's important to carefully consider best practices for connection management, error handling, and security.

### Proper Connection Management

Use `using` statements when working with database connections to make sure they're properly disposed of. This prevents connection leaks and helps maintain optimal pool performance:

```csharp
public async Task<User> GetUserByIdAsync(int id)
{
    // The using statement ensures the context is disposed properly
    using var context = new YourDbContext();
    return await context.Users.FindAsync(id);
}
```

For dependency injection scenarios, you can implement a scoped context:

```csharp
builder.Services.AddScoped<YourDbContext>();
```

For more information, consider the [Working with DbContext](https://docs.microsoft.com/en-us/ef/core/dbcontext-configuration/) documentation.

### Error Handling and Retry Logic

For production applications, it is important to handle errors gracefully and provide appropriate logging. For example, you can implement retry logic to handle failures that can occur in a distributed system:

```csharp
builder.Services.AddDbContext<YourDbContext>(options =>
{
    options.UseNpgsql(connectionString, npgsqlOptions =>
    {
        npgsqlOptions.EnableRetryOnFailure(
            maxRetryCount: 3,
            maxRetryDelay: TimeSpan.FromSeconds(30),
            errorCodesToAdd: null);
    });
});
```

In addition to retry logic, you can also consider adding logging and exception handling to your database operations:

```csharp
public async Task<User> GetUserByIdAsync(int id)
{
    try
    {
        using var context = new YourDbContext();
        return await context.Users.FindAsync(id);
    }
    catch (PostgresException ex)
    {
        _logger.LogError(ex, "Error retrieving user {UserId}", id);
        throw;
    }
}
```

## Monitoring Database Performance

Neon provides built-in monitoring capabilities, which you can complement with application-side monitoring in your .NET application.

### Using Neon's Monitoring Dashboard

The Neon Console includes a [monitoring dashboard](/docs/introduction/monitoring) that provides real-time insights into your database's performance. You can access it from the sidebar in the Neon Console and view some key metrics like CPU usage, memory, IOPS, and more.

Your Neon plan determines the range of metrics and historical data available. The monitoring dashboard makes it easy to identify performance trends and potential issues before they impact your application. Regular monitoring of these metrics helps you make informed decisions about scaling and optimization.

### The pg_stat_statements extension

In addition to the monitoring dashboard, you can use the `pg_stat_statements` extension to track query performance and identify slow queries. This extension provides detailed statistics about query execution, including the number of times a query is executed, the total execution time, and more.

You can check out the [pg_stat_statements documentation](/docs/extensions/pg_stat_statements) for more information on how to enable and use this extension.

This is very useful for identifying performance bottlenecks and optimizing your database queries. For example, once you identify slow queries, you can use tools like `EXPLAIN` to analyze query plans and then consider adding indexes or rewriting queries to improve performance. For more information, read the [Performance tips for Neon Postgres](/blog/performance-tips-for-neon-postgres) blog post.

### Application-Side Monitoring

Beyond Neon's monitoring capabilities, you can also implement application-side monitoring in your .NET application to track database operations and performance.

You can use the built-in health checks feature in ASP.NET Core to monitor database connectivity and performance. Here's an example of adding health checks for a Neon database:

```csharp
// Add health checks for database monitoring
builder.Services.AddHealthChecks()
    .AddNpgsql(
        connectionString,
        name: "neon-db",
        tags: new[] { "db", "postgres", "neon" },
        timeout: TimeSpan.FromSeconds(3));

// Add a health check endpoint
app.MapHealthChecks("/health/database");
```

Then, configure appropriate logging levels to track database operations:

```json
{
  "Logging": {
    "LogLevel": {
      "Default": "Information",
      "Microsoft.EntityFrameworkCore": "Warning",
      "Npgsql": "Information"
    },
    "Console": {
      "TimestampFormat": "yyyy-MM-dd HH:mm:ss"
    }
  }
}
```

And after that, add basic operation logging in your data access layer:

```csharp
public class DatabaseService
{
    private readonly ILogger<DatabaseService> _logger;

    public async Task ExecuteOperation()
    {
        try
        {
            // Your database operation
            _logger.LogInformation("Database operation completed successfully");
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Database operation failed");
            throw;
        }
    }
}
```

For more information on logging and monitoring in .NET applications, check out the [Logging in .NET Core](https://docs.microsoft.com/en-us/aspnet/core/fundamentals/logging/) documentation.

## Conclusion

You now have the foundational knowledge needed to connect your .NET application to Neon Postgres. We've covered the basics of setting up connections, implementing pooling, and following best practices for performance and security.

As a next step, consider checking out the [Building ASP.NET Core Applications with Neon and Entity Framework Core](/guides/dotnet-neon-entity-framework) guide for a more detailed example of integrating Neon with Entity Framework Core.

For more information, check out:

- [Neon Documentation](/docs)
- [Npgsql Documentation](https://www.npgsql.org/doc/index.html)
- [Entity Framework Core Documentation](https://docs.microsoft.com/en-us/ef/core/)

<NeedHelp />


# Drizzle with Local and Serverless Postgres

---
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-12-16T00:00:00.000Z'
updatedOn: '2024-12-16T00:00:00.000Z'
title: Drizzle with Local and Serverless Postgres
subtitle: A step-by-step guide to configure Drizzle ORM for local and serverless Postgres.
---

Drizzle is an ORM that simplifies database interactions in JavaScript applications. This guide will walk you through the steps to set up Drizzle to work with both local and hosted Postgres databases, and run schema migrations against them.

## Prerequisites

- **Install Docker Desktop**: To set up a local Postgres database, ensure you have [Docker Desktop](https://www.docker.com/products/docker-desktop/) installed on your machine.
- A [Neon](https://console.neon.tech) account to set up a hosted Postgres.

## Create a new Next.js application

Let’s get started by creating a new Next.js project with the following command:

```shell shouldWrap
npx create-next-app@latest my-app
```

When prompted, choose:

- `Yes` when prompted to use TypeScript.
- `No` when prompted to use ESLint.
- `Yes` when prompted to use Tailwind CSS.
- `No` when prompted to use `src/` directory.
- `Yes` when prompted to use App Router.
- `No` when prompted to customize the default import alias (`@/*`).

Once that is done, move into the project directory, and start the application in development mode with the following command:

```shell shouldWrap
cd my-app
npm run dev
```

## Setting Up a Local Postgres

You will use Docker to run your instance of local Postgres. First, create a `docker-compose.yml` file in the root directory with the following code:

```yaml
services:
  postgres:
    image: 'postgres:latest'
    environment:
    POSTGRES_USER: postgres
    POSTGRES_PASSWORD: postgres
    POSTGRES_DB: postgres
    ports:
      - '5432:5432'
  pg_proxy:
    image: ghcr.io/neondatabase/wsproxy:latest
    environment:
    APPEND_PORT: 'postgres:5432'
    ALLOW_ADDR_REGEX: '.*'
    LOG_TRAFFIC: 'true'
    ports:
      - '5433:80'
    depends_on:
      - postgres
```

In the YAML configuration file above, you have set up two services using Docker: a PostgreSQL database and a WebSocket proxy for Neon. The `postgres` service uses the latest PostgreSQL image and configures the necessary environment variables for the database user, password, and database name. It exposes port 5432 for database connections. The `pg_proxy` service uses a WebSocket proxy image, allowing connections to the PostgreSQL service through port `5433`.

Next, spin up the services in Docker via the following command:

```shell shouldWrap
docker-compose up -d
```

Use the connection string (`postgres://postgres:postgres@localhost:5432/postgres`) of the Postgres instance created as an environment variable, designated as `LOCAL_POSTGRES_URL` in the `.env` file.

## Setting Up a Serverless Postgres

To set up Neon serverless Postgres, go to the [Neon console](https://console.neon.tech/app/projects) and create a new project. Once your project is created, you will receive a connection string that you can use to connect to your Neon database. The connection string will look like this:

```bash
postgresql://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>?sslmode=require
```

Replace `<user>`, `<password>`, `<endpoint_hostname>`, `<port>`, and `<dbname>` with your specific details.

Use this connection string as an environment variable designated as `POSTGRES_URL` in the `.env` file.

## Integrate Drizzle with Next.js

To use Drizzle with Next.js and Neon, install the necessary packages via the following command:

```bash
npm install ws postgres drizzle-orm @neondatabase/serverless
npm install -D @types/ws drizzle-kit
```

Note that the installation of the `postgres` package is important, as in local environments, Drizzle will automatically use that to apply the schema migrations to the Postgres. In production, Drizzle will use Neon’s serverless driver to apply schema migrations to Neon’s hosted Postgres instance.

Then, create a file named `drizzle.server.ts` with the following code:

```typescript
// File: drizzle.server.ts

import { neonConfig, Pool } from '@neondatabase/serverless';
import { drizzle } from 'drizzle-orm/neon-serverless';
import { WebSocket } from 'ws';

const connectionString =
  process.env.NODE_ENV === 'production' ? process.env.POSTGRES_URL : process.env.LOCAL_POSTGRES_URL;

if (process.env.NODE_ENV === 'production') {
  neonConfig.webSocketConstructor = WebSocket;
  neonConfig.poolQueryViaFetch = true;
} else {
  neonConfig.wsProxy = (host) => `${host}:5433/v1`;
  neonConfig.useSecureWebSocket = false;
  neonConfig.pipelineTLS = false;
  neonConfig.pipelineConnect = false;
}

const pool = new Pool({ connectionString });

export default drizzle(pool);
```

The code above determines the connection string based on the environment variable (production or local). In production, it configures WebSocket settings for Neon, while in local development, it sets up a WebSocket proxy. Finally, it creates a connection pool and exports a Drizzle instance for database interactions.

Next, create a file named `drizzle.config.ts` with the following code:

```typescript
// File: drizzle.config.ts

import { defineConfig } from 'drizzle-kit';

const url =
  process.env.NODE_ENV === 'production' ? process.env.POSTGRES_URL : process.env.LOCAL_POSTGRES_URL;
if (!url)
  throw new Error(
    `Connection string to ${process.env.NODE_ENV ? 'Neon' : 'local'} Postgres not found.`
  );

export default defineConfig({
  dialect: 'postgresql',
  dbCredentials: { url },
  schema: './lib/schema.ts',
});
```

The code above determines the Postgres connection string to be used based on the environment (production or local) for database operations, such as running schema migrations.

## Running Schema Migrations

Now, you can manage both the local and production environments and select the respective (local or production) Postgres to run the Drizzle migrations via the following commands:

```bash
npx drizzle-kit generate
npx drizzle-kit migrate
```

## Source code

You can find the source code for the application described in this guide on GitHub.

<DetailIconCards>

<a href="https://github.com/neondatabase/examples/tree/main/with-nextjs-drizzle-local-vercel" description="Drizzle with Local and Serverless Postgres" icon="github">Drizzle with Local and Serverless Postgres</a>

</DetailIconCards>

<NeedHelp />


# Building an Async Product Management API with FastAPI, Pydantic, and PostgreSQL

---
title: Building an Async Product Management API with FastAPI, Pydantic, and PostgreSQL
subtitle: Learn how to create an asynchronous API for managing products using FastAPI, Pydantic for data validation, and PostgreSQL with connection pooling
author: sam-harri
enableTableOfContents: true
createdAt: '2024-10-08T00:00:00.000Z'
updatedOn: '2024-10-08T00:00:00.000Z'
---

Following this guide, you’ll build an asynchronous product management API and leverage FastAPI's async capabilities and connection pools to efficiently manage database connections, ensuring your API can scale and handle high traffic with ease. Whether you’re aiming to improve performance or simply learn the best practices for building async APIs, this guide has you covered.

## Prerequisites

Before starting, ensure you have the following tools and services ready:

- pip : Required for installing and managing Python packages, including [uv](https://docs.astral.sh/uv/) for creating virtual environments. You can check if `pip` is installed by running the following command:
  ```bash
  pip --version
  ```
- Neon serverless Postgres : you will need a Neon account for provisioning and scaling your `PostgreSQL` database. If you don't have an account yet, [sign up here](https://console.neon.tech/signup)

## Setting up the Project

Follow these steps to set up your project and virtual environment:

1.  Create a `uv` project

    If you don't already have uv installed, you can install it with:

    ```bash
    pip install uv
    ```

    Once `uv` is installed, create a new project:

    ```bash
    uv init async_postgres
    ```

    This will create a new project directory called `async_postgres`. Open this directory in your code editor of your choice.

2.  Set Up the Virtual Environment

        You will now create and activate a virtual environment in which your project's dependencies will be installed.

        <CodeTabs labels={["Linux/macOS", "Windows"]}>

            ```bash
            uv venv
            source .venv/bin/activate
            ```

            ```bash
            uv venv
            .venv\Scripts\activate
            ```

        </CodeTabs>

        You should see `(async_postgres)` in your terminal now, this means that your virtual environment is activated.

3.  Install Dependencies

    Next, add all the necessary dependencies for your project:

    ```bash
    uv add python-dotenv asyncpg loguru fastapi uvicorn requests
    ```

    Where each package does the following :

    - `FastAPI` : A Web / API framework
    - `AsyncPG` : An asynchronous PostgreSQL client
    - `Uvicorn` : An ASGI server for our app
    - `Loguru` : A logging library
    - `Python-dotenv` : To load environment variables from a .env file

4.  Create the project structure

    Now, create the following directory structure to organize your project files:

    ```md
    async_postgres
    ├── src/
    │ ├── database/
    │ │ └── postgres.py
    │ ├── models/
    │ │ └── product_models.py
    │ ├── routes/
    │ │ └── product_routes.py
    │ └── main.py
    ├── .env  
    ├── .python-version
    ├── README.md  
    ├── pyproject.toml  
    └── uv.lock
    ```

## Setting up your Database

In this section, you will set up the connection pool, ensure your database schema is in place, and manage database connections effectively. To connect to your `PostgreSQL` database, you will use the `asyncpg` library for asynchronous database connections.

First, create a `.env` file in the root of your project to store the database connection URL. This file will hold environment-specific variables, such as the connection string to your Neon PostgreSQL database.

```bash
DATABASE_URL=postgres://user:password@your-neon-hostname.neon.tech/neondb?sslmode=require
```

Make sure to replace the placeholders (user, password, your-neon-hostname, etc.) with your actual Neon database credentials which are available in the console.

In your project, the `database.py` file manages the connection to `PostgreSQL` using `asyncpg` and its connection pool, which is a mechanism for managing and reusing database connections efficiently. With this, you can use asynchronous queries, allowing the application to handle multiple requests concurrently.

```python
import os
import asyncpg
import dotenv
from loguru import logger
from typing import Optional

dotenv.load_dotenv()

conn_pool: Optional[asyncpg.Pool] = None

async def init_postgres() -> None:
    """
    Initialize the PostgreSQL connection pool and create the products table if it doesn't exist.

    This function is meant to be called at the startup of the FastAPI app to
    initialize a connection pool to PostgreSQL and ensure that the required
    database schema is in place.
    """
    global conn_pool
    try:
        logger.info("Initializing PostgreSQL connection pool...")

        conn_pool = await asyncpg.create_pool(
            dsn=os.getenv("DATABASE_URL"), min_size=1, max_size=10
        )
        logger.info("PostgreSQL connection pool created successfully.")

    except Exception as e:
        logger.error(f"Error initializing PostgreSQL connection pool: {e}")
        raise
    try:
        async with conn_pool.acquire() as conn:
            create_table_query = """
            CREATE TABLE IF NOT EXISTS products (
                id SERIAL PRIMARY KEY,
                name VARCHAR(100) NOT NULL,
                price NUMERIC(10, 2) NOT NULL CHECK (price >= 0),
                quantity INT NOT NULL CHECK (quantity >= 0),
                description VARCHAR(255)
            );
            """
            async with conn.transaction():
                await conn.execute(create_table_query)
            logger.info("Products table ensured to exist.")

    except Exception as e:
        logger.error(f"Error creating the products table: {e}")
        raise


async def get_postgres() -> asyncpg.Pool:
    """
    Return the PostgreSQL connection pool.

    This function returns the connection pool object, from which individual
    connections can be acquired as needed for database operations. The caller
    is responsible for acquiring and releasing connections from the pool.

    Returns
    -------
    asyncpg.Pool
        The connection pool object to the PostgreSQL database.

    Raises
    ------
    ConnectionError
        Raised if the connection pool is not initialized.
    """
    global conn_pool
    if conn_pool is None:
        logger.error("Connection pool is not initialized.")
        raise ConnectionError("PostgreSQL connection pool is not initialized.")
    try:
        return conn_pool
    except Exception as e:
        logger.error(f"Failed to return PostgreSQL connection pool: {e}")
        raise



async def close_postgres() -> None:
    """
    Close the PostgreSQL connection pool.

    This function should be called during the shutdown of the FastAPI app
    to properly close all connections in the pool and release resources.
    """
    global conn_pool
    if conn_pool is not None:
        try:
            logger.info("Closing PostgreSQL connection pool...")
            await conn_pool.close()
            logger.info("PostgreSQL connection pool closed successfully.")
        except Exception as e:
            logger.error(f"Error closing PostgreSQL connection pool: {e}")
            raise
    else:
        logger.warning("PostgreSQL connection pool was not initialized.")
```

`init_postgres` is responsible for opening the connection pool to the `PostgreSQL` database and setting up the required database schema. Specifically, it ensures that the necessary database tables (such as the `products` table) are created if they don’t already exist, preparing the application to start accepting requests.

To properly manage the lifecycle of the database, you need a function to close the connection pool when the API spins down `close_postgres` is responsible for gracefully closing all connections in the pool when the `FastAPI` app shuts down.

Throughout your API you will also need access to the pool to get connection instances and run queries. `get_postgres` returns the active connection pool. If the pool is not initialized, an error is raised. The term for passing this in is Dependency Injection.

## Defining the Pydantic Models

`Pydantic` is a data validation library in Python that ensures data entering or leaving your API is valid by enforcing constraints and data types.

In your application, you will define several models using Pydantic to represent the data for products. These models will be used to create, update, and manage products in the database, as well as handle validation when clients interact with our API.

```python
from pydantic import BaseModel, Field, ConfigDict
from typing import Optional


class Product(BaseModel):
    """
    Represents the product table in the database.
    """
    model_config = ConfigDict(from_attributes=True)
    id: int
    name: str
    price: float
    quantity: int
    description: Optional[str]


class ProductCreate(BaseModel):
    """
    Represents the required fields to create a new product.
    """
    name: str
    price: float = Field(..., ge=0)
    quantity: int = Field(..., ge=0)
    description: Optional[str] = Field(None, max_length=255)


class ProductUpdate(BaseModel):
    """
    Represents optional fields to update an existing product.
    Allows partial updates.
    """
    name: Optional[str] = None
    price: Optional[float] = Field(None, ge=0)
    quantity: Optional[int] = Field(None, ge=0)
    description: Optional[str] = Field(None, max_length=255)


class ProductStockUpdate(BaseModel):
    """
    Represents the stock update for a product's quantity.
    """
    quantity: int = Field(..., ge=0)
```

## Creating the API Endpoints

In this section, you will create the API endpoints that allow you to manage products in your `PostgreSQL` database. These endpoints will allow you to create, retrieve, update, delete, and manage product stock. You will leverage asynchronous database connections using `asyncpg`.

Each endpoint follows a similar flow for interacting with the database. You will first get a connection from the connection pool, execute the desired query, and release the connection back to the pool. Since the connection pool is used as a context manager, the connection will automatically be returned to the pool after each operation.

The common database flow goes as follows :

1. Getting the Connection Pool:
   - You inject the connection pool using FastAPI's `Depends()` function, which allows you to easily retrieve a connection from the pool.
2. Acquiring a Connection:
   - Using the connection pool, you acquire a connection by calling `async with db_pool.acquire() as conn:`. This ensures you obtain a database connection to run the query.
3. Running the Query:
   - Once the connection is acquired, you run the query using methods such as `fetchrow()` (for single rows) or `fetch()` (for multiple rows) depending on the operation.
4. Returning the Connection to the Pool:
   - Once the query is complete, the connection is automatically returned to the pool because the async with context manager handles the lifecycle of the connection.

```python
from fastapi import HTTPException, Query, Path, Body, APIRouter, Depends
from models.product_models import Product, ProductCreate, ProductUpdate, ProductStockUpdate
from database.postgres import get_postgres
from typing import List
import asyncpg
from loguru import logger

product_router = APIRouter()


@product_router.post("/products", response_model=Product)
async def create_product(
    product: ProductCreate = Body(...),
    db_pool: asyncpg.Pool = Depends(get_postgres),
) -> Product:
    """
    Create a new product.

    Parameters
    ----------
    product : ProductCreate
        The product details to create.
    db_pool : asyncpg.Pool
        Database connection pool injected by dependency.

    Returns
    -------
    Product
        The newly created product.
    """
    query = """
    INSERT INTO products (name, price, quantity, description)
    VALUES ($1, $2, $3, $4)
    RETURNING id, name, price, quantity, description
    """
    try:
        async with db_pool.acquire() as conn:
            result = await conn.fetchrow(
                query,
                product.name,
                product.price,
                product.quantity,
                product.description,
            )

        if result:
            return Product(**dict(result))
        else:
            logger.error("Failed to create product")
            raise HTTPException(status_code=500, detail="Failed to create product")
    except Exception as e:
        logger.error(f"Error during product creation: {e}")
        raise HTTPException(
            status_code=500, detail="Internal server error during product creation"
        )


@product_router.get("/products", response_model=List[Product])
async def get_all_products(
    db_pool: asyncpg.Pool = Depends(get_postgres),
) -> List[Product]:
    """
    Get a list of all products.

    Parameters
    ----------
    db_pool : asyncpg.Pool, optional
        Database connection pool injected by dependency.

    Returns
    -------
    List[Product]
        A list of all products in the inventory.
    """
    query = "SELECT id, name, price, quantity, description FROM products"

    try:
        async with db_pool.acquire() as conn:
            results = await conn.fetch(query)
            return [Product(**dict(result)) for result in results]
    except Exception as e:
        logger.error(f"Error fetching products: {e}")
        raise HTTPException(status_code=500, detail="Failed to retrieve products")


@product_router.get("/products/{id}", response_model=Product)
async def get_product_by_id(
    id: int = Path(..., ge=1),
    db_pool: asyncpg.Pool = Depends(get_postgres),
) -> Product:
    """
    Get a product by its ID.

    Parameters
    ----------
    id : int
        The ID of the product.
    db_pool : asyncpg.Pool, optional
        Database connection pool injected by dependency.

    Returns
    -------
    Product
        The product details for the given ID.
    """
    query = "SELECT id, name, price, quantity, description FROM products WHERE id = $1"

    try:
        async with db_pool.acquire() as conn:
            result = await conn.fetchrow(query, id)
            if result:
                return Product(**dict(result))
            else:
                logger.warning(f"Product with ID {id} not found")
                raise HTTPException(status_code=404, detail="Product not found")
    except Exception as e:
        logger.error(f"Error fetching product by ID: {e}")
        raise HTTPException(
            status_code=500, detail="Internal server error during product retrieval"
        )


@product_router.put("/products/{id}", response_model=Product)
async def update_product(
    id: int = Path(..., ge=1),
    product: ProductUpdate = Body(...),
    db_pool: asyncpg.Pool = Depends(get_postgres),
) -> Product:
    """
    Update a product by its ID.

    Parameters
    ----------
    id : int
        The ID of the product to update.
    product : ProductUpdate
        The fields to update (partial updates allowed).
    db_pool : asyncpg.Pool, optional
        Database connection pool injected by dependency.

    Returns
    -------
    Product
        The updated product details.
    """
    query = """
    UPDATE products
    SET name = COALESCE($1, name),
        price = COALESCE($2, price),
        quantity = COALESCE($3, quantity),
        description = COALESCE($4, description)
    WHERE id = $5
    RETURNING id, name, price, quantity, description
    """

    try:
        async with db_pool.acquire() as conn:
            result = await conn.fetchrow(
                query,
                product.name,
                product.price,
                product.quantity,
                product.description,
                id,
            )
            if result:
                return Product(**dict(result))
            else:
                logger.warning(f"Product with ID {id} not found for update")
                raise HTTPException(status_code=404, detail="Product not found")
    except Exception as e:
        logger.error(f"Error updating product: {e}")
        raise HTTPException(
            status_code=500, detail="Internal server error during product update"
        )


@product_router.delete("/products/{id}")
async def delete_product(
    id: int = Path(..., ge=1),
    db_pool: asyncpg.Pool = Depends(get_postgres)
) -> dict:
    """
    Delete a product by its ID.

    Parameters
    ----------
    id : int
        The ID of the product to delete.
    db_pool : asyncpg.Pool, optional
        Database connection pool injected by dependency.

    Returns
    -------
    dict
        A message indicating the product was deleted.
    """
    query = "DELETE FROM products WHERE id = $1 RETURNING id"

    try:
        async with db_pool.acquire() as conn:
            result = await conn.fetchrow(query, id)
            if result:
                return {"message": "Product deleted successfully"}
            else:
                logger.warning(f"Product with ID {id} not found for deletion")
                raise HTTPException(status_code=404, detail="Product not found")
    except Exception as e:
        logger.error(f"Error deleting product: {e}")
        raise HTTPException(
            status_code=500, detail="Internal server error during product deletion"
        )


@product_router.patch("/products/{id}/stock", response_model=Product)
async def update_product_stock(
    id: int = Path(..., ge=1),
    stock: ProductStockUpdate = Body(...),
    db_pool: asyncpg.Pool = Depends(get_postgres),
) -> Product:
    """
    Update the stock (quantity) of a product by its ID.

    Parameters
    ----------
    id : int
        The ID of the product to update.
    stock : ProductStockUpdate
        The new quantity for the product.
    db_pool : asyncpg.Pool, optional
        Database connection pool injected by dependency.

    Returns
    -------
    Product
        The updated product with new stock quantity.
    """
    query = """
    UPDATE products
    SET quantity = $1
    WHERE id = $2
    RETURNING id, name, price, quantity, description
    """
    try:
        async with db_pool.acquire() as conn:
            result = await conn.fetchrow(query, stock.quantity, id)
            if result:
                return Product(**dict(result))
            else:
                raise HTTPException(status_code=404, detail="Product not found")
    except Exception as e:
        logger.error(f"Error updating product stock: {e}")
        raise HTTPException(
            status_code=500, detail="Internal server error during product stock update"
        )


@product_router.get("/products/filter/price", response_model=List[Product])
async def filter_products_by_price(
    min_price: float = Query(...),
    max_price: float = Query(...),
    db_pool: asyncpg.Pool = Depends(get_postgres),
) -> List[Product]:
    """
    Get products within a specific price range.

    Parameters
    ----------
    min_price : float
        The minimum price for filtering.
    max_price : float
        The maximum price for filtering.
    db_pool : asyncpg.Pool, optional
        Database connection pool injected by dependency.

    Returns
    -------
    List[Product]
        A list of products within the specified price range.
    """
    query = """
    SELECT id, name, price, quantity, description
    FROM products
    WHERE price BETWEEN $1 AND $2
    """
    try:
        async with db_pool.acquire() as conn:
            results = await conn.fetch(query, min_price, max_price)
            return [Product(**dict(result)) for result in results]
    except Exception as e:
        logger.error(f"Error filtering products by price: {e}")
        raise HTTPException(
            status_code=500, detail="Internal server error during price filtering"
        )
```

The code defines endpoints for :

- `POST /products`: Creates a new product. It receives the product data (name, price, quantity, and description) and inserts it into the database. The newly created product is returned.

- `GET /products`: Retrieves all products from the database. The response is a list of products, each containing its ID, name, price, quantity, and description.

- `GET /products/{id}`: Retrieves a product by its unique ID. If the product exists, its details are returned; otherwise, a 404 error is raised.

- `PUT /products/{id}`: Updates an existing product by its ID. The update can be partial, as it uses `COALESCE` to only update the fields provided. The updated product is returned.

- `DELETE /products/{id}`: Deletes a product by its ID. If the product is successfully deleted, a success message is returned.

- `PATCH /products/{id}/stock`: Updates the stock (quantity) of a specific product by its ID. The updated product, with the new quantity, is returned.

- `GET /products/filter/price`: Retrieves products within a specific price range. You pass min_price and max_price as query parameters, and the endpoint returns a list of products that fall within that range.

## Running the Application

After setting up the database, models, and API routes, the next step is to run the `FastAPI` application. The `main.py` file is the entry point for the application, and `Uvicorn` starts and serves it.

The `main.py` file defines the `FastAPI` application, manages lifecycle events like starting and closing the `PostgreSQL` connection pool, and includes the product-related routes. Here, you will use the `@asynccontextmanager` decorator to manage the database connection pool lifecycle.

```python
from fastapi import FastAPI
from contextlib import asynccontextmanager
from database.postgres import init_postgres, close_postgres
from routes.product_routes import product_router
import uvicorn


@asynccontextmanager
async def lifespan(app: FastAPI):
    await init_postgres()
    yield
    await close_postgres()


app: FastAPI = FastAPI(lifespan=lifespan, title="Async FastAPI PostgreSQL Inventory Manager")
app.include_router(product_router)

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8080, reload=True)

```

To run the application, use the following command:

```bash
uv run src/main.py
```

Once the server is running, you can access the API documentation and test the endpoints directly in your browser:

- Interactive API Docs (Swagger UI):  
  Visit `http://127.0.0.1:8080/docs` to access the automatically generated API documentation where you can test the endpoints.
- Alternative Docs (ReDoc):  
  Visit `http://127.0.0.1:8080/redoc` for another style of API documentation.

## Testing the API

You can also use tools like `httpie`, `curl`, and `Postman` to test the API.

Below are examples of how to interact with the API using `httpie`, a command-line HTTP client.

1. Create a Product

   Start by creating a new product:

   ```
   http POST http://127.0.0.1:8080/products name="Test Product" price:=9.99 quantity:=100 description="A test product"
   ```

   You should see a response with the created product data:

   ```
   {
       "id": 1,
       "name": "Test Product",
       "price": 9.99,
       "quantity": 100,
       "description": "A test product"
   }
   ```

2. Retrieve All Products

   Next, retrieve all products from the database:

   ```
   http GET http://127.0.0.1:8080/products
   ```

   This will return a list of all products in the database:

   ```
   [
       {
           "id": 1,
           "name": "Test Product",
           "price": 9.99,
           "quantity": 100,
           "description": "A test product"
       }
   ]
   ```

3. Retrieve a Specific Product by ID

   You can also retrieve a specific product by its ID:

   ```
   http GET http://127.0.0.1:8080/products/1
   ```

   This will return the product details for the product with ID `1`:

   ```
   {
       "id": 1,
       "name": "Test Product",
       "price": 9.99,
       "quantity": 100,
       "description": "A test product"
   }
   ```

4. Update a Product

   To update an existing product, use the following command:

   ```
   http PUT http://127.0.0.1:8080/products/1 name="Updated Product" price:=12.99 quantity:=150 description="An updated product description"
   ```

   This will return the updated product data:

   ```
   {
       "id": 1,
       "name": "Updated Product",
       "price": 12.99,
       "quantity": 150,
       "description": "An updated product description"
   }
   ```

5. Update Product Stock

   You can also update just the stock (quantity) of a product:

   ```
   http PATCH http://127.0.0.1:8080/products/1/stock quantity:=200
   ```

   This will return the updated product with the new quantity:

   ```
   {
       "id": 1,
       "name": "Updated Product",
       "price": 12.99,
       "quantity": 200,
       "description": "An updated product description"
   }
   ```

6. Filter Products by Price Range

   To filter products by a specific price range, use this command:

   ```
   http GET http://127.0.0.1:8080/products/filter/price min_price==5.00 max_price==15.00
   ```

   This will return products that fall within the specified price range:

   ```
   [
       {
           "id": 1,
           "name": "Updated Product",
           "price": 12.99,
           "quantity": 200,
           "description": "An updated product description"
       }
   ]
   ```

7. Delete a Product

   To delete a product by its ID, use the following command:

   ```
   http DELETE http://127.0.0.1:8080/products/1
   ```

   If successful, you will receive a confirmation message:

   ```
   {
       "message": "Product deleted successfully"
   }
   ```

## Conclusion

Using this guide, you have built a fully functional API for managing products using `FastAPI`, `Pydantic`, and `PostgreSQL` with `asyncpg`.

This stack provides a solid foundation for building high-performance and scalable web services. `FastAPI`'s asynchronous support, combined with `Pydantic`'s robust data validation and `asyncpg`'s efficient database interactions, allows for fast and reliable API development.

As a next step, you can look at deploying this application in the cloud using scalable technologies like `Docker` and `Kubernetes`, or implementing automated test, build, and deployment workflows using `GitHub CI`


# Implementing Secure User Authentication in FastAPI using JWT Tokens and Neon Postgres

---
title: Implementing Secure User Authentication in FastAPI using JWT Tokens and Neon Postgres
subtitle: Learn how to build a secure user authentication system in FastAPI using JSON Web Tokens (JWT) and Neon Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-08-17T00:00:00.000Z'
updatedOn: '2024-08-17T00:00:00.000Z'
---

In this guide, we'll walk through the process of implementing secure user authentication in a FastAPI application using JSON Web Tokens (JWT) and Neon Postgres.

We'll cover user registration, login, and protecting routes with authentication, using PyJWT for handling JWT operations.

By the end of this guide, you'll have a FastAPI application with an authentication system that uses JWT tokens for secure user management.

## Prerequisites

Before we begin, make sure you have the following:

- Python 3.9 or later installed on your system
- [pip](https://pip.pypa.io/en/stable/installation/) for managing Python packages
- A [Neon](https://console.neon.tech/signup) account for serverless Postgres
- Basic knowledge of [FastAPI, SQLAlchemy, and Pydantic](/guides/fastapi-overview)

## How JWT Works

Before we dive into the building our API, let's understand how JWT works. If you're already familiar with JWT, feel free to skip ahead to the next section.

JSON Web Tokens or JWT for short provide a secure way to authenticate and authorize users in web applications.

A JWT consists of three parts, each separated by a dot (`.`):

```
Header.Payload.Signature
```

Each part is Base64Url encoded, resulting in a structure like this:

```
eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c
```

Let's break down each part of the JWT:

### JWT Header

The header typically consists of two parts:

- The type of token (JWT)
- The hashing algorithm being used (e.g., HMAC SHA256 or RSA)

Example:

```json
{
  "alg": "HS256",
  "typ": "JWT"
}
```

This JSON is then Base64Url encoded to form the first part of the JWT.

### Payload

The payload contains claims. Claims are statements about the user and additional metadata. There are three types of claims:

- Registered claims: Predefined claims such as `iss` (issuer), `exp` (expiration time), `sub` (subject), `aud` (audience)
- Public claims: Can be defined at will by those using JWTs
- Private claims: Custom claims to share additional information between the client and server

Example:

```json
{
  "sub": "1234567890",
  "name": "John Doe",
  "admin": true,
  "iat": 1516239022
}
```

This JSON is then Base64Url encoded to form the second part of the JWT.

### Signature

The signature is used to verify that the sender of the JWT is who it says it is and to ensure that the message wasn't changed along the way. To create the signature part, you have to take the encoded header, the encoded payload, a secret, and the algorithm specified in the header, and sign that.

For example, if you want to use the HMAC SHA256 algorithm, the signature will be created in the following way:

```
HMACSHA256(
  base64UrlEncode(header) + "." +
  base64UrlEncode(payload),
  secret)
```

This signature is then Base64Url encoded to form the third part of the JWT.

### The Process of Using JWTs

The overall process of using JWTs for authentication and authorization typically involves the following steps:

1. **User Authentication**:

   - The process begins when a user logs in with their credentials (e.g., username and password).
   - The server verifies these credentials against the stored user information.

2. **JWT Creation**:

   - Upon successful authentication, the server creates a JWT.
   - It generates the header and payload, encoding the necessary information.
   - Using a secret key (kept secure on the server), it creates the signature.
   - The three parts (header, payload, signature) are combined to form the complete JWT.

3. **Sending the Token**:

   - The server sends this token back to the client in the response.
   - The client stores this token, often in local storage or a secure cookie.

4. **Subsequent Requests**:

   - For any subsequent requests to protected routes or resources, the client includes this token in the Authorization header.
   - The format is: `Authorization: Bearer <token>`

5. **Server-side Token Validation**:

   - When the server receives a request with a JWT, it first splits the token into its three parts.
   - It base64 decodes the header and payload.
   - The server then recreates the signature using the header, payload, and its secret key.
   - If this newly created signature matches the signature in the token, the server knows the token is valid and hasn't been tampered with.

6. **Accessing Protected Resources**:

   - If the token is valid, the server can use the information in the payload without needing to query the database.
   - This allows the server to authenticate the user and know their permissions for each request without needing to store session data.

7. **Token Expiration**:
   - JWTs typically have an expiration time specified in the payload.
   - The server checks this expiration time with each request.
   - If the token has expired, the server will reject the request, requiring the client to authenticate again.

## Setting up the Project

With the theory out of the way, let's start by creating a new project directory and setting up a virtual environment:

1. Create a new directory and navigate to it:

   ```bash
   mkdir fastapi-auth-demo
   cd fastapi-auth-demo
   ```

2. Create a virtual environment:

   ```bash
   python -m venv venv
   ```

3. Activate the virtual environment:
   - On Windows:
     ```bash
     venv\Scripts\activate
     ```
   - On macOS and Linux:
     ```bash
     source venv/bin/activate
     ```

Now, let's install the necessary packages for our project:

```bash
pip install "fastapi[all]" sqlalchemy psycopg2-binary pyjwt "passlib[bcrypt]" python-dotenv
```

This command installs:

- FastAPI: Our web framework
- SQLAlchemy: An ORM for database interactions
- psycopg2-binary: PostgreSQL adapter for Python
- PyJWT: For working with JWT tokens instead of handling them manually
- passlib: For password hashing
- python-dotenv: To load environment variables from a .env file

You can also create a `requirements.txt` file to manage your dependencies using the following:

```bash
pip freeze > requirements.txt
```

This file can be used to install the dependencies in another environment using `pip install -r requirements.txt`.

## Connecting to Neon Postgres

Next, let's set up a connection to Neon Postgres for storing user data.

Create a `.env` file in your project root and add the following configuration:

```env
DATABASE_URL=postgres://user:password@your-neon-hostname.neon.tech/dbname?sslmode=require
```

Replace the placeholders with your actual Neon database credentials.

While editing the `.env` file, add the following configuration for JWT token signing:

```env
SECRET_KEY=your-secret-key
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30
```

Choose a secure secret key for signing the JWT tokens. The `ALGORITHM` specifies the hashing algorithm to use, and `ACCESS_TOKEN_EXPIRE_MINUTES` sets the token expiration time.

Now, create a `database.py` file to manage the database connection:

```python
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from dotenv import load_dotenv
import os

load_dotenv()

SQLALCHEMY_DATABASE_URL = os.getenv("DATABASE_URL")

engine = create_engine(SQLALCHEMY_DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

Base = declarative_base()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```

This script sets up the database connection using SQLAlchemy and provides a `get_db` function to manage database sessions.

The `DATABASE_URL` is read from the `.env` file for security, and the Neon Postgres connection string is used to connect to the database.

The `SessionLocal` object is a factory for creating new database sessions, and the `get_db` function ensures that sessions are properly closed after use.

## User Model and Schema

We will be using SQLAlchemy for database interactions and Pydantic for data validation. SQLAlchemy provides an ORM for working with databases, while Pydantic is used for defining data models. These models will be used to interact with the database and validate user input.

Start by creating a `models.py` file for the SQLAlchemy User model:

```python
from sqlalchemy import Column, Integer, String
from database import Base

class User(Base):
    __tablename__ = "users"

    id = Column(Integer, primary_key=True, index=True)
    username = Column(String, unique=True, index=True)
    email = Column(String, unique=True, index=True)
    hashed_password = Column(String)
```

This defines a `User` model with fields for `id`, `username`, `email`, and `hashed_password`. The `unique=True` constraint ensures that usernames and emails are unique across all users. The `index=True` constraint creates an index on these fields for faster lookups.

The `Base` object is imported from the `database` module and is used to create the database schema.

Next, create a `schemas.py` file for Pydantic models:

```python
from pydantic import BaseModel, EmailStr

class UserCreate(BaseModel):
    username: str
    email: EmailStr
    password: str

class User(BaseModel):
    id: int
    username: str
    email: EmailStr

    class Config:
        orm_mode = True

class Token(BaseModel):
    access_token: str
    token_type: str
```

These Pydantic models define the structure for user creation, user representation, and JWT tokens. The `EmailStr` type ensures that the email is in a valid format.

One of the benefits of using Pydantic models is that they can be used for data validation and serialization. The `orm_mode = True` configuration allows Pydantic to work with SQLAlchemy models directly.

## Authentication Utilities

Now that we have the database models and schemas in place, let's add some utility functions for authentication.

Create a file called `auth.py` where we will define functions for password hashing, verification, and JWT token creation:

```python
from passlib.context import CryptContext
import jwt
from datetime import datetime, timedelta
from dotenv import load_dotenv
import os

# Load environment variables
load_dotenv()

SECRET_KEY = os.getenv("SECRET_KEY")
ALGORITHM = os.getenv("ALGORITHM")
ACCESS_TOKEN_EXPIRE_MINUTES = int(os.getenv("ACCESS_TOKEN_EXPIRE_MINUTES"))

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

def verify_password(plain_password, hashed_password):
    return pwd_context.verify(plain_password, hashed_password)

def get_password_hash(password):
    return pwd_context.hash(password)

def create_access_token(data: dict):
    to_encode = data.copy()
    expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt

def verify_token(token: str):
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        username: str = payload.get("sub")
        if username is None:
            return None
        return username
    except jwt.PyJWTError:
        return None
```

This file includes functions for:

- Verifying and hashing passwords using bcrypt
- Creating JWT access tokens
- Verifying JWT tokens

The `CryptContext` from passlib is used for secure password hashing, while `PyJWT` is used for JWT token creation and verification. PyJWT provides a simpler and more focused API for JWT operations compared to `python-jose`.

## API Endpoints

With all the necessary components in place, we can now create the API endpoints for user registration, login, and protected routes.

To do this, create a `main.py` file with the following content:

```python
from fastapi import FastAPI, Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm
from sqlalchemy.orm import Session
from database import engine, get_db
import models, schemas, auth

# Run the database migrations
models.Base.metadata.create_all(bind=engine)

# Initialize the FastAPI app
app = FastAPI()

# Define the OAuth2 scheme for token-based authentication
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")

@app.post("/register", response_model=schemas.User)
def register_user(user: schemas.UserCreate, db: Session = Depends(get_db)):
    db_user = db.query(models.User).filter(models.User.username == user.username).first()
    if db_user:
        raise HTTPException(status_code=400, detail="Username already registered")
    hashed_password = auth.get_password_hash(user.password)
    new_user = models.User(username=user.username, email=user.email, hashed_password=hashed_password)
    db.add(new_user)
    db.commit()
    db.refresh(new_user)
    return new_user

@app.post("/token", response_model=schemas.Token)
def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends(), db: Session = Depends(get_db)):
    user = db.query(models.User).filter(models.User.username == form_data.username).first()
    if not user or not auth.verify_password(form_data.password, user.hashed_password):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password",
            headers={"WWW-Authenticate": "Bearer"},
        )
    access_token = auth.create_access_token(data={"sub": user.username})
    return {"access_token": access_token, "token_type": "bearer"}

async def get_current_user(token: str = Depends(oauth2_scheme), db: Session = Depends(get_db)):
    username = auth.verify_token(token)
    if username is None:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid authentication credentials",
            headers={"WWW-Authenticate": "Bearer"},
        )
    user = db.query(models.User).filter(models.User.username == username).first()
    if user is None:
        raise HTTPException(status_code=404, detail="User not found")
    return user

@app.get("/users/me", response_model=schemas.User)
async def read_users_me(current_user: models.User = Depends(get_current_user)):
    return current_user
```

Let's break down the key components of this file:

1. The `/register` endpoint allows new users to create an account. It checks if the username is already taken, hashes the password, and stores the new user in the database.

2. The `/token` endpoint handles user login. It verifies the username and password, and if correct, issues a JWT access token.

3. The `get_current_user` function is a dependency that verifies the JWT token and retrieves the current user. This is used to protect routes that require authentication.

4. The `/users/me` endpoint is an example of a protected route. It returns the current user's information, but only if a valid JWT token is provided.

The tables will be created in your Neon database when the application starts, thanks to the `Base.metadata.create_all(bind=engine)` line in the `main.py` file.

## Running the API

To run the API, use the following command:

```bash
python -m uvicorn main:app --reload
```

This starts the Uvicorn server with hot-reloading enabled for development. This means that the server will automatically restart when you make changes to the code thanks to the `--reload` flag.

## Testing the Authentication System

You can test the authentication system using tools like `curl`, `httpie`, or the FastAPI Swagger UI at `http://127.0.0.1:8000/docs`.

Here are some example requests using the `httpie` command-line HTTP client to go through the registration, login, and protected route access flow as we discussed earlier.

1. Start by registering a new user:

   ```bash
   http POST http://127.0.0.1:8000/register username=testuser email=test@example.com password=securepassword
   ```

   You should receive a response with the new user's details in JSON format if the registration is successful.

2. Login and get an access token using the registered user's credentials:

   ```bash
   http --form POST http://127.0.0.1:8000/token username=testuser password=securepassword
   ```

   This request should return a JSON response with an access token.

   If you were to copy the token and decode it at [jwt.io](https://jwt.io/), you would see the payload containing the username and expiration time.
   As we discussed earlier, in some cases the token might contain additional claims like `iss` (issuer), `aud` (audience), etc. These can be used for additional security checks.

   The token will be valid for the duration specified in the `.env` file.

3. Access the protected `/users/me` route using the access token:

   ```bash
   http GET http://127.0.0.1:8000/users/me "Authorization: Bearer <your_access_token>"
   ```

   This request should return the user's details if the token is valid.

Replace `<your_access_token>` with the token received from the login request.

You would see a `401 Unauthorized` response if the token is invalid or has expired. This is because the `get_current_user` dependency checks the token validity before allowing access to the protected route.

## Dockerizing the Application

In many cases, you may want to containerize your FastAPI application for deployment. You can use Docker to create a container image for your FastAPI application.

Let's create a Dockerfile to package the application into a Docker container:

```Dockerfile
FROM python:3.12-slim

WORKDIR /app

COPY requirements.txt .

RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

This Dockerfile uses the official Python image as the base image, installs the project dependencies, and copies the project files into the container. The `CMD` instruction specifies the command to run when the container starts.

To build the Docker image, run the following command:

```bash
docker build -t fastapi-auth-demo .
```

This command builds the Docker image with the tag `fastapi-auth-demo` based on the `Dockerfile` in the current directory.

Make sure that you don't include the `.env` file in the Docker image to keep your secrets secure. You can pass environment variables to the container using the `--env-file` flag when running the container.

To run the Docker container, use the following command:

```bash
docker run -d -p 8000:8000 --env-file .env fastapi-auth-demo
```

This command starts the container in detached mode, maps port 8000 on the host to port 8000 in the container, and loads environment variables from the `.env` file.

## Conclusion

In this guide, we've implemented a secure user authentication system in FastAPI using JWT tokens (with PyJWT) and Neon Postgres. This provides a good start for building secure web applications with user accounts and protected routes which can be integrated with other microservices or front-end applications.

## Additional Resources

- [FastAPI Security](https://fastapi.tiangolo.com/tutorial/security/)
- [JSON Web Tokens](https://jwt.io/introduction)
- [PyJWT Documentation](https://pyjwt.readthedocs.io/)
- [SQLAlchemy Documentation](https://docs.sqlalchemy.org/)
- [Neon Documentation](/docs)


# Building a High-Performance API with FastAPI, Pydantic, and Neon Postgres

---
title: Building a High-Performance API with FastAPI, Pydantic, and Neon Postgres
subtitle: Learn how to create an API for managing a tech conference system using FastAPI, Pydantic for data validation, and Neon's serverless Postgres for data storage
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-08-17T00:00:00.000Z'
updatedOn: '2024-08-17T00:00:00.000Z'
---

FastAPI is a high-performance Python web framework for building APIs quickly and efficiently.

When combined with Pydantic for data validation and Neon's serverless Postgres for data storage, you can create a powerful and efficient API with minimal effort.

In this guide, we'll walk through the process of building an API for managing a tech conference system, focusing on best practices and performance optimizations.

## Prerequisites

Before we begin, make sure you have the following:

- Python 3.9 or later installed on your system
- [pip](https://pip.pypa.io/en/stable/installation/) for managing Python packages
- A [Neon](https://console.neon.tech/signup) account for serverless Postgres

## Setting up the Project

Let's start by creating a new project directory and setting up a virtual environment:

1. Create a new directory and navigate to it:

   ```bash
   mkdir fastapi-neon-conference-api
   cd fastapi-neon-conference-api
   ```

2. Create a virtual environment:

   ```bash
   python -m venv venv
   ```

   Creating a virtual environment isolates your project dependencies from other Python installations on your system.

3. Activate the virtual environment:

   - On Windows:
     ```bash
     venv\Scripts\activate
     ```
   - On macOS and Linux:
     ```bash
     source venv/bin/activate
     ```

   You should see `(venv)` in your terminal prompt, indicating that the virtual environment is active.

Now, let's install the necessary packages for our project using `pip`:

```bash
pip install fastapi uvicorn sqlalchemy psycopg2-binary "pydantic[email]" python-dotenv
```

This command installs:

- FastAPI: Our web framework
- Uvicorn: An ASGI server to run our FastAPI application
- SQLAlchemy: An ORM for database interactions
- psycopg2-binary: PostgreSQL adapter for Python
- Pydantic: For data validation and settings management
- python-dotenv: To load environment variables from a .env file

With the dependencies installed, we can start building our API.

You can also create a `requirements.txt` file to manage your dependencies by running:

```bash
pip freeze > requirements.txt
```

This will create a `requirements.txt` file with all the installed packages in your virtual environment and their versions. This is useful for sharing your project with others or deploying it to a server.

## Connecting to Neon Postgres

First, let's set up our database connection. Create a `.env` file in your project root:

```env
DATABASE_URL=postgres://user:password@your-neon-hostname.neon.tech/neondb?sslmode=require
```

Replace the placeholders with your actual Neon database credentials.

Now, create a `database.py` file to manage the database connection:

```python
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from dotenv import load_dotenv
import os

load_dotenv()

SQLALCHEMY_DATABASE_URL = os.getenv("DATABASE_URL")

engine = create_engine(SQLALCHEMY_DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

Base = declarative_base()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```

This script does the following:

1. Loads the environment variables from `.env` using `load_dotenv`
2. Creates a SQLAlchemy engine using the database URL from the environment variables
3. Creates a SessionLocal class for database sessions
4. Defines a Base class for declarative models which will be used to create database tables
5. Provides a `get_db` function to manage database connections

We're now ready to define our database models and API endpoints.

## Defining Models and Schemas

Let's start by creating an API for managing a tech conference system. We'll need database models and Pydantic schemas for talks and speakers.

Create a `models.py` file for SQLAlchemy models and relationships:

```python
from sqlalchemy import Column, Integer, String, ForeignKey, DateTime
from sqlalchemy.orm import relationship
from database import Base

class Speaker(Base):
    __tablename__ = "speakers"

    id = Column(Integer, primary_key=True, index=True)
    name = Column(String, index=True)
    bio = Column(String)
    company = Column(String)

    talks = relationship("Talk", back_populates="speaker")

class Talk(Base):
    __tablename__ = "talks"

    id = Column(Integer, primary_key=True, index=True)
    title = Column(String, index=True)
    description = Column(String)
    speaker_id = Column(Integer, ForeignKey("speakers.id"))
    start_time = Column(DateTime)
    end_time = Column(DateTime)

    speaker = relationship("Speaker", back_populates="talks")
```

This defines `Speaker` and `Talk` models with their respective fields and relationships. The `speaker_id` field in the `Talk` model establishes a foreign key relationship with the `Speaker` model using SQLAlchemy's `ForeignKey` construct.

Now, create a `schemas.py` file for Pydantic models with data validation:

```python
from pydantic import BaseModel
from datetime import datetime
from typing import List, Optional

class TalkBase(BaseModel):
    title: str
    description: str
    start_time: datetime
    end_time: datetime

class TalkCreate(TalkBase):
    speaker_id: int

class Talk(TalkBase):
    id: int
    speaker_id: int

    class Config:
        orm_mode = True

class SpeakerBase(BaseModel):
    name: str
    bio: str
    company: str

class SpeakerCreate(SpeakerBase):
    pass

class Speaker(SpeakerBase):
    id: int
    talks: List[Talk] = []

    class Config:
        orm_mode = True

class SpeakerWithTalks(Speaker):
    talks: List[Talk]
```

Here we define Pydantic models for creating and returning speaker and talk data.

The `TalkCreate` model includes a `speaker_id` field for associating talks with speakers. The `SpeakerWithTalks` model extends the `Speaker` model to include a list of talks. The `orm_mode = True` configuration enables automatic data conversion between SQLAlchemy models and Pydantic models.

These models will be used to validate incoming data and serialize outgoing data in our API endpoints instead of manually handling data conversion.

## Creating API Endpoints

Now, let's create our FastAPI application with CRUD operations for speakers and talks. Create a `main.py` file:

```python
from fastapi import FastAPI, Depends, HTTPException
from sqlalchemy.orm import Session
from typing import List

import models, schemas
from database import engine, get_db

models.Base.metadata.create_all(bind=engine)

app = FastAPI()

@app.post("/speakers/", response_model=schemas.Speaker)
def create_speaker(speaker: schemas.SpeakerCreate, db: Session = Depends(get_db)):
    db_speaker = models.Speaker(**speaker.dict())
    db.add(db_speaker)
    db.commit()
    db.refresh(db_speaker)
    return db_speaker

@app.get("/speakers/", response_model=List[schemas.Speaker])
def read_speakers(skip: int = 0, limit: int = 100, db: Session = Depends(get_db)):
    speakers = db.query(models.Speaker).offset(skip).limit(limit).all()
    return speakers

@app.get("/speakers/{speaker_id}", response_model=schemas.SpeakerWithTalks)
def read_speaker(speaker_id: int, db: Session = Depends(get_db)):
    db_speaker = db.query(models.Speaker).filter(models.Speaker.id == speaker_id).first()
    if db_speaker is None:
        raise HTTPException(status_code=404, detail="Speaker not found")
    return db_speaker

@app.post("/talks/", response_model=schemas.Talk)
def create_talk(talk: schemas.TalkCreate, db: Session = Depends(get_db)):
    db_talk = models.Talk(**talk.dict())
    db.add(db_talk)
    db.commit()
    db.refresh(db_talk)
    return db_talk

@app.get("/talks/", response_model=List[schemas.Talk])
def read_talks(skip: int = 0, limit: int = 100, db: Session = Depends(get_db)):
    talks = db.query(models.Talk).offset(skip).limit(limit).all()
    return talks

@app.get("/talks/{talk_id}", response_model=schemas.Talk)
def read_talk(talk_id: int, db: Session = Depends(get_db)):
    db_talk = db.query(models.Talk).filter(models.Talk.id == talk_id).first()
    if db_talk is None:
        raise HTTPException(status_code=404, detail="Talk not found")
    return db_talk
```

This code defines endpoints for:

- Creating and retrieving speakers at `/speakers/`
- Retrieving a specific speaker with their talks at `/speakers/{speaker_id}`
- Creating and retrieving talks at `/talks/`
- Retrieving a specific talk at `/talks/{talk_id}`

Each endpoint uses dependency injection to get a database session, ensuring efficient connection management.

Pagination is supported for the `read_speakers` and `read_talks` endpoints using the `skip` and `limit` parameters to avoid loading large datasets at once.

## Running the API

To run the API, use the following command:

```bash
python -m uvicorn main:app --reload
```

This starts the Uvicorn server with hot-reloading enabled for development.

By default, the API will be available on port 8000. You can access the API documentation at `http://127.0.0.1:8000/docs` or `http://127.0.0.1:8000/redoc`.

Your database tables will be created automatically when you run the API for the first time thanks to the `models.Base.metadata.create_all(bind=engine)` line in `main.py`. You can check your database to see the tables using the Neon console.

## Testing the API

You can test the API using tools like `curl`, `Postman`, or even via the `/docs` endpoint provided by FastAPI directly via your browser.

Let's test this out using `httpie`, a command-line HTTP client:

1. Start by creating a speaker:

   ```bash
   http POST http://127.0.0.1:8000/speakers/ name="John Doe" bio="Software Engineer" company="Tech Inc."
   ```

   You should see a response with the created speaker data.

2. Next, create a talk associated with the speaker:

   ```bash
   http POST http://127.0.0.1:8000/talks/ title="Introduction to FastAPI" description="Learn how to build APIs with FastAPI" start_time="2024-08-18T09:00:00" end_time="2024-08-18T10:00:00" speaker_id=1
   ```

   This will create a talk associated with the speaker created earlier and return the talk data.

3. Retrieve all speakers:

   ```bash
   http http://127.0.0.1:8000/speakers/
   ```

   This endpoint will return a list of all speakers in the database.

4. Retrieve a specific speaker with talks:

   ```bash
   http http://127.0.0.1:8000/speakers/1
   ```

   This will return the speaker data along with any talks associated with them.

5. Retrieve all talks:

   ```bash
   http http://127.0.0.1:8000/talks/
   ```

6. Retrieve a specific talk:
   ```bash
   http http://127.0.0.1:8000/talks/1
   ```

You can modify these requests to test other endpoints and functionalities.

## Dockerizing the Application

In many cases, you may want to containerize your FastAPI application for deployment. Here's how you can create a Dockerfile for your project:

```Dockerfile
FROM python:3.12-slim

WORKDIR /app

COPY requirements.txt .

RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

This Dockerfile uses a slim Python image, installs dependencies from a `requirements.txt` file, copies the project files, and runs the Uvicorn server.

To build a new Docker image, run the following command in your project directory:

```bash
docker build -t fastapi-neon-conference-api .
```

You can then run the Docker container with:

```bash
docker run -d -p 8000:8000 fastapi-neon-conference-api
```

This will start the FastAPI application in a Docker container, accessible on port 8000 of your host machine.

You need to make sure that your `.env` file is not included in the Docker image. Instead, you can pass the environment variables as arguments when running the container.

## Performance Considerations

1. **Database Indexing**: We've added indexes to frequently queried fields (`id`, `name`, `title`) in our models. This improves query performance. To learn more about indexing, refer to the [Neon documentation](/docs/postgres/indexes).

2. **Pagination**: The `read_speakers` and `read_talks` endpoints include `skip` and `limit` parameters for pagination, preventing the retrieval of unnecessarily large datasets.

3. **Dependency Injection**: By using `Depends(get_db)`, we make sure that database connections are properly managed and closed after each request. This prevents connection leaks and improves performance.

4. **Pydantic Models**: Using Pydantic for request and response models provides automatic data validation and serialization, reducing the need for manual checks.

5. **Relationships**: We've used SQLAlchemy relationships to efficiently load related data (speakers and their talks) in a single query.

## Conclusion

In this guide, we've built a simple API for managing a tech conference system using FastAPI, Pydantic, and Neon Postgres.

This combination provides a very good foundation for building scalable and efficient web services. FastAPI's speed and ease of use, combined with Pydantic's powerful data validation and Neon's serverless Postgres, make for a formidable tech stack.

As a next step, you can extend the API with more features like authentication, authorization, and advanced query capabilities. You can check out the [Implementing Secure User Authentication in FastAPI using JWT Tokens and Neon Postgres](/guides/fastapi-jwt) guide for adding JWT-based authentication to your API.

## Additional Resources

- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Pydantic Documentation](https://docs.pydantic.dev/latest/)
- [SQLAlchemy Documentation](https://docs.sqlalchemy.org/)
- [Neon Documentation](/docs)

<NeedHelp />


# Add feature flags in SvelteKit apps with Neon Postgres

---
title: Add feature flags in SvelteKit apps with Neon Postgres
subtitle: A step-by-step guide to integrating feature flags in SvelteKit apps with Postgres
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-05-24T13:24:36.612Z'
updatedOn: '2024-05-24T13:24:36.612Z'
---

This guide covers the step-by-step process of integrating feature flags in SvelteKit apps with Postgres (powered by Neon). Feature flags provide a way to control the behavior of your application without deploying new code, allowing you to test and roll out new features dynamically. Upon completing the guide, you will understand how to manage and roll out new features using dynamic feature flag integration.

## Prerequisites

To follow the steps in this guide, you will need the following:

- [Node.js 18](https://nodejs.org/en/blog/announcements/v18-release-announce) or later
- A [Neon](https://console.neon.tech/signup) account – The feature flags will be defined (or mutated) in a Postgres database

## Steps

- [Provisioning a Postgres database powered by Neon](#provisioning-a-postgres-database-powered-by-neon)
- [Creating a new SvelteKit application](#creating-a-new-sveltekit-application)
- [(Optional) Adding Tailwind CSS to the application](#optional-adding-tailwind-css-to-the-application)
- [Managing Feature Flags in Serverless Postgres](#managing-feature-flags-in-serverless-postgres)
- [Dynamic Feature Flag Integration for Testing Fast Payment Methods](#dynamic-feature-flag-integration-for-testing-fast-payment-methods)

## Provisioning a Postgres database powered by Neon

Using Serverless Postgres database powered by Neon helps you scale down to zero. With Neon, you only have to pay for what you use.

To get started, go to the [Neon console](https://console.neon.tech/app/projects) and enter the name of your choice as the project name.

You will then be presented with a dialog that provides a connecting string of your database. Click on **Pooled connection** on the top right of the dialog and the connecting string automatically updates in the box below it.

![](/guides/images/feature-flags-sveltekit/index.png)

All Neon connection strings have the following format:

```bash
postgres://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>
```

- `user` is the database user.
- `password` is the database user’s password.
- `endpoint_hostname` is the host with neon.tech as the [TLD](https://www.cloudflare.com/en-gb/learning/dns/top-level-domain/).
- `port` is the Neon port number. The default port number is 5432.
- `dbname` is the name of the database. “neondb” is the default database created with each Neon project.
- `?sslmode=require` an optional query parameter that enforces the [SSL](https://www.cloudflare.com/en-gb/learning/ssl/what-is-ssl/) mode while connecting to the Postgres instance for better security.

Save this connecting string somewhere safe to be used as the `DATABASE_URL` further in the guide. Proceed further in this guide to create a SvelteKit application.

## Creating a new SvelteKit application

To start building the application, create a new SvelteKit project. Open your terminal and run the following command:

```bash
npm create svelte@latest my-app
```

When prompted, choose:

- `Skeleton project` for **Which Svelte app template?**
- `Yes, using Typescript syntax` for **Add type checking with Typescript?**

Press **Enter** to proceed. Now, follow the instructions to install the dependencies and start the development server:

```bash
npm run dev
```

The app now should be running on [localhost:5173](http://localhost:5173).

> Note: According to an [advanced SvelteKit guide](https://kit.svelte.dev/docs/server-only-modules), using `.server` in the filename allows you to mark the code to be executed on server only.

Next, run the commands below to install the necessary libraries and packages for building the application:

<CodeTabs labels={["Neon serverless driver", "node-postgres"]}>

```bash
npm install @neondatabase/serverless
npm install -D dotenv tsx
```

```bash
npm install pg
npm install -D dotenv tsx @types/pg
```

</CodeTabs>

The commands install the required libraries and packages, with the `-D` flag specifying the libraries intended for development purposes only.

The libraries installed include:

<CodeTabs labels={["Neon serverless driver", "node-postgres"]}>

```markdown
- `@neondatabase/serverless`: Neon's serverless Postgres driver for JavaScript and TypeScript.
```

```markdown
- `pg`: A Postgres client for Node.js.
```

</CodeTabs>

The development-specific libraries include:

<CodeTabs labels={["Neon serverless driver", "node-postgres"]}>

```markdown
- `tsx`: A library for executing and rebuilding TypeScript efficiently.
- `dotenv`: A library for handling environment variables.
```

```markdown
- `@types/pg`: Type definitions for pg.
- `tsx`: A library for executing and rebuilding TypeScript efficiently.
- `dotenv`: A library for handling environment variables.
```

</CodeTabs>

## (Optional) Adding Tailwind CSS to the application

For styling the app, we will use Tailwind CSS. Install and set up Tailwind at the root of our project's directory by running:

```bash
npm install -D tailwindcss postcss autoprefixer
npx tailwindcss init -p
```

Create an `app.css` file in the `src` directory, and add the snippet below:

```css
@tailwind base;
@tailwind components;
@tailwind utilities;
```

Next, add the paths to all of your template files in your `tailwind.config.js` file:

```js
/** @type {import('tailwindcss').Config} */
export default {
  content: [], // [!code --]
  content: ['./src/**/*.{html,js,svelte,ts}'], // [!code ++]
  theme: {
    extend: {},
  },
  plugins: [],
};
```

Finally, add an import to `app.css` in your `+page.svelte` file:

```svelte
<script lang="ts"> // [!code ++]
  import '../app.css' // [!code ++]
</script> // [!code ++]

<!-- +page.svelte's HTML -->
```

## Managing Feature Flags in Serverless Postgres

Feature flags offer a powerful way to control the behavior of your application without deploying new code. In a Serverless Postgres environment, you can easily create, read, and update feature flags using the following steps:

### Create a serverless Postgres client

To create a client that interacts with your serverless postgres, create a `postgres.server.ts` file inside the `src/lib` directory with the following content:

<CodeTabs labels={["Neon serverless driver", "node-postgres"]}>

```tsx
// File: src/lib/postgres.server.ts

// Load Environment Variables
import 'dotenv/config';

// Load the postgres module
import { neon } from '@neondatabase/serverless';

// Create a connection string to the Neon Postgres instance
const connectionString: string = process.env.DATABASE_URL as string;

// Create a in-memory query function
export default neon(connectionString);
```

```tsx
// File: src/lib/postgres.server.ts

// Load Environment Variables
import 'dotenv/config';

// Load the postgres module
import pg from 'pg';

// Create a connection string to the Neon Postgres instance
const connectionString: string = process.env.DATABASE_URL as string;

// Create a in-memory pool so that it's cached for multiple calls
export default new pg.Pool({ connectionString });
```

</CodeTabs>

The code above starts with importing the Postgres client. It further imports the config module by `dotenv` that makes sure that all environment variables are populated in application environment. It then creates a new instance of a Postgres connection pool.

To create, read or update the feature flags from your SvelteKit application, you can use re-usable helper functions. Let's create a new directory by executing the following command in your terminal window to start creating those functions:

```bash
mkdir src/lib/feature_flags
```

### Create & Populate Feature Flags Database

In the `feature_flags` directory, create a file named `setup.server.ts` with the following code which will allow you to create and populate a database table for feature flags.

<CodeTabs labels={["Neon serverless driver", "node-postgres"]}>

```tsx
// File: src/lib/feature_flags/setup.server.ts

import sql from '../postgres.server';

async function populateFeatureFlags() {
  await sql`CREATE TABLE IF NOT EXISTS feature_flags (flagName text PRIMARY KEY, enabled boolean)`;
  console.log('✅ Setup database for feature flag');
  await sql`INSERT INTO feature_flags (flagName, enabled) VALUES ('fast_payments', true)`;
  console.log('✅ Setup an enabled feature flag to accept fast payment methods.');
}

populateFeatureFlags();
```

```tsx
// File: src/lib/feature_flags/setup.server.ts

import pool from '../postgres.server';

async function populateFeatureFlags() {
  await pool.query(
    'CREATE TABLE IF NOT EXISTS feature_flags (flagName text PRIMARY KEY, enabled boolean)'
  );
  console.log('✅ Setup database for feature flag');
  await pool.query({
    text: 'INSERT INTO feature_flags (flagName, enabled) VALUES ($1, $2)',
    values: ['fast_payments', true],
  });
  console.log('✅ Setup an enabled feature flag to accept fast payment methods.');
}

populateFeatureFlags();
```

</CodeTabs>

The code snippet above first ensures the existence of a table named `feature_flags` in the Postgres database. Then, it inserts a feature flag named `fast_payments` with the value `true`, indicating that fast payment methods are enabled.

### Read and update the feature flags

In the `feature_flags` directory, create a file named `get.server.ts` with the following code which will allow you to read the feature flag value in the database.

<CodeTabs labels={["Neon serverless driver", "node-postgres"]}>

```tsx
// File: src/lib/feature_flags/get.server.ts

import sql from '../postgres.server';

export const isEnabled = async (flagName: string): Promise<boolean> => {
  const rows = await sql`SELECT enabled FROM feature_flags WHERE flagName = ${flagName}`;
  return rows[0].enabled;
};
```

```tsx
// File: src/lib/feature_flags/get.server.ts

import pool from '../postgres.server';

export const isEnabled = async (flagName: string): Promise<boolean> => {
  const { rows } = await pool.query({
    text: 'SELECT enabled FROM feature_flags WHERE flagName = $1',
    values: [flagName],
  });
  return rows[0].enabled;
};
```

</CodeTabs>

The `isEnabled` function queries the database to check whether a specific feature flag is enabled or not. In this example, you will use it to check if `fast_payments` feature flag is enabled or not.

In the `feature_flags` directory, create a file named `set.server.ts` with the following code which will allow you to update the feature flag value in the database.

<CodeTabs labels={["Neon serverless driver", "node-postgres"]}>

```tsx
// File: src/lib/feature_flags/set.server.ts

import sql from '../postgres.server';

export const setEnabled = async (flagName: string, flagValue: boolean) => {
  await sql`UPDATE feature_flags SET enabled = ${flagValue} WHERE flagName = ${flagName}`;
};
```

```tsx
// File: src/lib/feature_flags/set.server.ts

import pool from '../postgres.server';

export const setEnabled = async (flagName: string, flagValue: boolean) => {
  await pool.query({
    text: 'UPDATE feature_flags SET enabled = $2 WHERE flagName = $1',
    values: [flagName, flagValue],
  });
};
```

</CodeTabs>

The `setEnabled` function updates the value of a feature flag in the database. In this example, you will update the `fast_payments` feature dynamically per request to get a taste of how feature flags are used in production.

Great! You can use these helper functions in your application to manage and control feature flags dynamically.

## Dynamic feature flag integration for testing fast payment methods

In this section, you will get an example of how a feature flag helps test and roll out new features, dynamically. For example, you are a payment processing company. You have just added a payment method named `PayGM` that helps users pay faster. But you want to test it out on a random basis for each cart that you process. Let's walk through the hypothetical code to understand the usage of a feature flag in this case.

### Computing the user destination

In a SvelteKit route, the data from the server to the user interface is passed via `+page.server.ts` file to `+page.svelte`. For the sake of this example, you will load the feature flag dynamically and check if it's enabled to determine the user's destination experience. To do that, add the following snippet to `+page.server.ts` file:

```tsx
// File: src/routes/+page.server.ts

import { isEnabled } from '$lib/feature_flags/get.server';

/** @type {import('./$types').LayoutServerLoad} */
export async function load({ cookies }) {
  const bucket = cookies.get('destination_bucket');
  if (!bucket) {
    const tmp = await isEnabled('fast_payments');
    // If the feature is enabled, try bucketing users randomly
    if (tmp) cookies.set('destination_bucket', Math.random() > 0.5 ? '1' : '0', { path: '/' });
    // If the feature is disabled, do not bucket and preserve the current experience
    else cookies.set('destination_bucket', '0', { path: '/' });
  }
  const fast_payments = Boolean(Number(cookies.get('destination_bucket')));
  return {
    fast_payments,
  };
}
```

The code above first looks for the bucket assigned to the user. If no such bucket is found, it looks for the value of the feature flag in the database, randomly assigns a boolean whenever the `/` route is visited, and sets the value in the cookie. Finally, it reads the cookie as the source to determine the user experience and check if the fast payment methods are enabled or not.

### Creating a conditional user experience

Now, let's look at how the feature flag value can be used in the user interface to conditionally render UI elements. This will allow you to accept payments via PayGM if the `fast_payments` feature flag is enabled. To do that, use the following code in `+page.svelte` file:

```svelte
<script lang="ts">
  // File: src/routes/+page.svelte

  import '../app.css'

  import type { PageData } from './$types'

  export let data: PageData
</script>

<div class="w-screen h-screen flex flex-col items-center justify-center">
  {#if data.fast_payments}
    <div class="mb-6 w-full flex flex-col max-w-[300px]">
      <span class="font-semibold">Fast Payment Methods</span>
      <button class="mt-3 flex flex-col items-center border rounded w-full px-3 py-1">Pay via PayGM</button>
    </div>
  {/if}
  <form action="/" method="post" class="w-full flex flex-col max-w-[300px]">
    <span class="font-semibold">Pay with card</span>
    <input class="mt-3 w-full border px-2 py-1 rounded" type="text" placeholder="Full name on card" />
    <input class="mt-3 w-full border px-2 py-1 rounded" type="text" placeholder="1234 1234 1234 1234" />
    <div class="flex flex-row gap-x-2">
      <input class="w-1/2 border px-2 py-1 rounded" type="text" placeholder="MM/YY" />
      <input class="w-1/2 border px-2 py-1 rounded" type="text" placeholder="CVV" />
    </div>
  </form>
</div>
```

In the code above, UI elements related to fast payment methods are conditionally rendered based on the value of the `fast_payments` feature flag. If `fast_payments` feature flag is enabled, the UI will display options for paying via PayGM; otherwise, it will display options for paying with a card.

## Summary

In this guide, you learned how to add feature flags in your SvelteKit apps using Serverless Postgres powered by Neon. By dynamically updating and utilizing feature flags, you can effectively test and roll out new features like fast payment methods, providing a controlled and iterative approach to your deployments.

## Source code

You can find the source code for the application described in this guide on GitHub.

<DetailIconCards>
<a href="https://github.com/neondatabase/examples/tree/main/with-sveltekit-feature-flags" description="Feature flags with SvelteKit and Neon" icon="github">Feature flags with SvelteKit and Neon</a>
</DetailIconCards>

<NeedHelp />


# Managing database migrations and schema changes with Flask and Neon Postgres

---
title: Managing database migrations and schema changes with Flask and Neon Postgres
subtitle: Learn how to handle database migrations and schema changes in a Flask application using Flask-Migrate and Neon Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-09-14T00:00:00.000Z'
updatedOn: '2024-09-14T00:00:00.000Z'
---

Flask is a lightweight and flexible web framework for Python that makes it easy to build web applications. When working with databases in [Flask](https://flask.palletsprojects.com/), [SQLAlchemy](https://www.sqlalchemy.org/) is a popular choice for an ORM.

As your Flask application grows, so does your database schema and its complexity. Managing these changes effectively is important for maintaining data integrity and smooth deployments.

This guide will walk you through the process of handling database migrations and schema changes in a Flask application using Flask-Migrate and Neon Postgres.

## Prerequisites

Before we begin, make sure you have:

- Python 3.7 or later installed
- A [Neon](https://console.neon.tech/signup) account for Postgres hosting
- Basic familiarity with Flask and SQLAlchemy

## Setting up the Project

1. Create a new directory for your project and navigate into it:

   ```bash
   mkdir flask-migrations-demo
   cd flask-migrations-demo
   ```

2. Create a virtual environment and activate it:

   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
   ```

   This will isolate your project dependencies from other Python projects, so you can manage them independently.

3. Install the required packages:

   ```bash
   pip install flask flask-sqlalchemy flask-migrate psycopg2-binary python-dotenv
   ```

   We are installing Flask, Flask-SQLAlchemy, Flask-Migrate, psycopg2-binary (Postgres driver), and python-dotenv (for managing environment variables).

   An additional thing that you might want to do is to create a `requirements.txt` file with the installed packages:

   ```bash
   pip freeze > requirements.txt
   ```

   This will allow you to easily install the required packages on another machine by running:

   ```bash
   pip install -r requirements.txt
   ```

4. Create a `.env` file in your project root and add your Neon Postgres connection string:

   ```
   DATABASE_URL=postgresql://user:password@your-neon-host:5432/your-database
   ```

   Replace the placeholders with your actual Neon database credentials.

   Note that you should never commit your `.env` file to version control. Add it to your `.gitignore` file to prevent accidental commits.

   Instead, you can have a `.env.example` file with the required variables and commit that to your repository. Then, each developer can create their own `.env` file based on the `.env.example` template including their own credentials.

## Creating the Flask Application

With the project set up, let's create the Flask application and set up database migrations.

Create a new file called `app.py` with the following content:

```python
# Import necessary modules
from flask import Flask
from flask_sqlalchemy import SQLAlchemy
from flask_migrate import Migrate
import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Initialize Flask application
app = Flask(__name__)

# Configure the database URI using the environment variable
app.config['SQLALCHEMY_DATABASE_URI'] = os.getenv('DATABASE_URL')

# Disable SQLAlchemy modification tracking for better performance
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False

# Initialize SQLAlchemy with the Flask app
db = SQLAlchemy(app)

# Initialize Flask-Migrate with the Flask app and SQLAlchemy instance
migrate = Migrate(app, db)

# Define the User model
class User(db.Model):
    # Primary key for the User table
    id = db.Column(db.Integer, primary_key=True)

    # User's name (required field, maximum 100 characters)
    name = db.Column(db.String(100), nullable=False)

    # User's email (required field, unique, maximum 120 characters)
    email = db.Column(db.String(120), unique=True, nullable=False)

    # String representation of the User object
    def __repr__(self):
        return f'<User {self.name}>'

# Run the Flask application in debug mode if this file is executed directly
if __name__ == '__main__':
    app.run(debug=True)
```

This sets up a basic Flask application with SQLAlchemy and Flask-Migrate, and defines a simple User model. Here's a breakdown of what each part does:

1. **Imports**: We import necessary modules including Flask, SQLAlchemy, Flask-Migrate, os (for environment variables), and dotenv (for loading .env files).

2. **Environment Variables**: We use `load_dotenv()` to load environment variables from a .env file, which will include our database URL.

3. **Flask App Initialization**: We create a Flask application instance.

4. **Database Configuration**: We configure the SQLAlchemy database URI using the `DATABASE_URL` environment variable.

5. **SQLAlchemy and Flask-Migrate Setup**: We initialize SQLAlchemy and Flask-Migrate with our Flask app. This sets up our ORM and migration capabilities.

6. **User Model**: We define a `User` model that represents the structure of our `user` table in the database. It includes:

   - An `id` field as the primary key
   - A `name` field that's required and has a maximum length of 100 characters
   - An `email` field that's required, unique, and has a maximum length of 120 characters
   - A `__repr__` method that provides a string representation of the User object

7. **Application Run**: Finally, we include a conditional to run the application in debug mode if the script is executed directly.

This setup provides a foundation for building a Flask application with database integration and migration capabilities. The `User` model can be expanded or additional models can be added as the application grows.

## Initializing Migrations

To start using Flask-Migrate, you need to initialize it in your project. Run the following command in your terminal:

```bash
flask db init
```

This creates a `migrations` directory in your project that will store migration scripts. It also generates a `migrations/alembic.ini` file that contains the configuration for Alembic, the migration engine used by Flask-Migrate.

Make sure to add the `migrations` directory to your Git repository so that you can track changes to your database schema over time.

## Creating the Initial Migration

Now, let's create our first migration to set up the initial database schema. Run the following command:

```bash
flask db migrate -m "Initial migration"
```

This command generates a new migration script in the `migrations/versions` directory and the `-m` flag allows you to provide a message describing the migration. The message is useful for tracking changes and understanding the purpose of each migration.

Open the generated file and review the changes. It should contain the SQL to create the `user` table based on our `User` model. This is possible because Flask-Migrate uses SQLAlchemy's reflection capabilities to generate the migration script based on the model definitions instead of writing raw SQL or manually creating the schema.

It is a good practice to review the generated migration script before applying it to your database. This way, you can ensure that the changes are correct and will not cause any issues as in some cases Alembic might not generate the migration script as expected and you might need to modify it manually, so remember to always review the generated migration scripts.

## Applying the Migration

After reviewing the migration script, to apply the migration and create the table in your Neon Postgres database, run:

```bash
flask db upgrade
```

This command executes the migration script and creates the `user` table in your database.

The output that you should see after running the `flask db upgrade` command should look something like this:

```
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running upgrade  -> 1e48b6167844, Initial migration
```

This indicates that the migration was successful and the `user` table was created in your database.

## Making Schema Changes

Now that the initial migration is complete, let's make some changes to the schema. We'll add a new field to the `User` model and create a new migration to apply the changes.

Let's modify our `User` model to add a new field. Update the `User` class in `app.py`:

```python {5}
class User(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String(100), nullable=False)
    email = db.Column(db.String(120), unique=True, nullable=False)
    age = db.Column(db.Integer)

    def __repr__(self):
        return f'<User {self.name}>'
```

With the new `age` field added to the `User` model, we need to create a new migration to apply this change.

```bash
flask db migrate -m "Add age to User model"
```

Alembic will generate a new migration script that includes the necessary SQL to add the `age` column to the `user` table.

Review the generated migration script, then apply it:

```bash
flask db upgrade
```

The `age` column should now be added to the `user` table in your database. If you want to revert the changes, you can run `flask db downgrade` to roll back the last migration.

If you were to check the `user` table in your database, you should see that the `age` column has been added:

```sql
SELECT * FROM user;
```

This will return the data from the `user` table including the new `age` column.

## Renaming Columns

As your application evolves, you may need to rename columns in your database schema.

Such changes need to be handled carefully to avoid data loss or corruption. You also need to make sure that the application code is updated to reflect the new column names otherwise it might not be backwards compatible leading to issues.

To rename a column, you'll need to use SQLAlchemy's `alter_column` operation. Let's rename the `age` column to `years_old`:

1. Create a new migration:

```bash
flask db migrate -m "Rename age to years_old"
```

2. As the `alter_column` operation is not directly supported by Flask-Migrate, you'll need to modify the generated migration script manually.

Open the generated migration file and modify it:

```python
from alembic import op
import sqlalchemy as sa

def upgrade():
    op.alter_column('user', 'age', new_column_name='years_old')

def downgrade():
    op.alter_column('user', 'years_old', new_column_name='age')
```

3. Apply the migration:

```bash
flask db upgrade
```

The `age` column should now be renamed to `years_old` in your database. Remember to be cautious when renaming columns, as it can have implications on your application code and queries.

## Working with Indexes

Adding indexes can improve query performance. To learn more about indexing, refer to the [Neon documentation](/docs/postgres/indexes).

Let's add an index to the `email` column.

Open the `User` model in `app.py` and add the `index=True` parameter to the `email` column:

```python {6}
class User(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String(100), nullable=False)
    email = db.Column(db.String(120), unique=True, nullable=False)
    age = db.Column(db.Integer)
    email = db.Column(db.String(120), unique=True, nullable=False, index=True)

    def __repr__(self):
        return f'<User {self.name}>'
```

Create a new migration with a descriptive migration message:

```bash
flask db migrate -m "Add index to email column"
```

Review the generated migration script, it should contain an `op.create_index` operation for the `email` column:

```python {4}
def upgrade():
    with op.batch_alter_table('user', schema=None) as batch_op:
        batch_op.drop_constraint('user_email_key', type_='unique')
        batch_op.create_index(batch_op.f('ix_user_email'), ['email'], unique=True)
```

Finally, apply the newly created migration:

```bash
flask db upgrade
```

The `email` column should now have an index in your database, which can improve query performance when searching by email.

## Migrations in CI/CD Pipeline

Automating database migrations in your Continuous Integration pipeline can help with catching potential issues early.

By using Neon's branching feature, you can test your migrations safely without affecting your production database while ensuring that your application code and database schema changes are always in sync.

Here's an example of how you can automate migration testing using GitHub Actions and Neon branches:

```yaml
name: Test Migrations

on:
  pull_request:
    branches: [main]

jobs:
  test-migrations:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create Neon Branch
        uses: neondatabase/create-branch-action@v5
        id: create-branch
        with:
          project_id: ${{ secrets.NEON_PROJECT_ID }}
          branch_name: migrate-${{ github.sha }}
          api_key: ${{ secrets.NEON_API_KEY }}

      - name: Run migrations on Neon branch
        env:
          DATABASE_URL: ${{ steps.create-branch.outputs.db_url }}
        run: flask db upgrade

      - name: Run tests
        env:
          DATABASE_URL: ${{ steps.create-branch.outputs.db_url }}
        run: pytest

      - name: Clean up Neon branch
        if: always()
        uses: neondatabase/delete-branch-action@v3
        with:
          project_id: ${{ secrets.NEON_PROJECT_ID }}
          branch: ${{ steps.create-branch.outputs.branch_id }}
          api_key: ${{ secrets.NEON_API_KEY }}
```

This workflow does the following:

1. Fetches the latest code from the pull request.

2. Installs the specified Python version.

3. Installs the required Python packages listed in `requirements.txt`.

4. Uses the [official Neon GitHub action](/docs/guides/branching-github-actions) to create a new branch in your Neon project. This allows you to test migrations in isolation.

5. Applies any pending database migrations to the newly created Neon branch.

6. Executes your test suite against the updated database schema in the Neon branch.

7. Deletes the temporary Neon branch after the workflow completes, regardless of success or failure to make sure that no resources are left behind.

Using Neon's branching feature in your CI pipeline offers several advantages:

- You can test your migrations and schema changes in a separate branch without affecting your production or staging databases.
- Catch migration issues before they reach your main branch or production environment.
- Ensures that your database schema changes are always tested alongside your application code changes.
- Allows you to run your full test suite against the updated schema without risk to existing data.

## Conclusion

Managing database migrations is an important part of maintaining and evolving your Flask application. With Flask-Migrate and Neon Postgres, you have powerful tools at your disposal to handle schema changes efficiently and safely. Remember to always test your migrations thoroughly and have a solid backup strategy in place.

One thing that you should get in the habit of doing is to always review the generated migration scripts before applying them to your database. This way you can ensure that the changes that are about to be applied are correct and that they will not cause any issues. As well as that, you should use meaningful names for your migrations so that you can easily identify what each migration does.

## Additional Resources

- [Flask-Migrate Documentation](https://flask-migrate.readthedocs.io/en/latest/)
- [SQLAlchemy Migrations](https://docs.sqlalchemy.org/en/20/)
- [Alembic Documentation](https://alembic.sqlalchemy.org/en/latest/)
- [Neon Documentation](/docs)


# Developing a Scalable Flask Application with Neon Postgres

---
title: Developing a Scalable Flask Application with Neon Postgres
subtitle: Learn how to build a scalable Flask application with Neon Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-09-14T00:00:00.000Z'
updatedOn: '2024-09-14T00:00:00.000Z'
---

Building scalable web applications requires careful planning and the right tools. Flask is a Python web framework well-suited for building small to large web applications. It provides flexibility and extensibility, making it a popular choice for developers.

In this guide, we'll walk through developing a Flask application that uses Neon Postgres. We'll cover setting up the project structure, defining models, creating routes, and handling database migrations. We'll also explore frontend development using Tailwind CSS for responsive styling.

## Prerequisites

- Python 3.7 or later installed
- [Node.js 18](https://nodejs.org/en) or later
- A [Neon](https://console.neon.tech/signup) account for Postgres hosting
- Basic familiarity with Flask and SQLAlchemy

## Project Setup

1. Create a new directory and set up a virtual environment:

   ```bash
   mkdir flask-neon-app
   cd flask-neon-app
   python -m venv venv
   source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
   ```

2. Install required packages:

   ```bash
   pip install flask flask-sqlalchemy psycopg2-binary python-dotenv flask-migrate
   ```

   Create a `requirements.txt` file for package management:

   ```bash
   pip freeze > requirements.txt
   ```

   This file will help you manage dependencies and ensure consistent environments across development and deployment.

3. Create a `.env` file for environment variables:

   ```
   DATABASE_URL=postgresql://user:password@your-neon-host:5432/your-database
   ```

   Replace `user`, `password`, `your-neon-host`, and `your-database` with your Neon Postgres credentials.

## Application Structure

For small applications, you can keep all code in a single file. However, as your application grows, it's best to organize your code into separate modules. It is a good practice to separate models, routes, services, and utilities into different directories.

A typical Flask application structure might look like this:

```
flask-neon-app/
├── app/
│   ├── __init__.py
│   ├── models/
│   ├── routes/
│   ├── services/
│   ├── static/
│   │   └── css/
│   ├── templates/
│   └── utils/
├── config.py
├── requirements.txt
├── run.py
└── tailwind.config.js
```

This structure separates concerns and makes it easier to maintain and scale your application. The `app` directory contains the core application logic, while other files and directories handle configuration, dependencies, and frontend assets.

## Database Configuration

You can think of the `app/__init__.py` file as the entry point for your Flask application. It initializes the Flask app, sets up the database connection, and registers blueprints for routing.

By adding the `__init__.py` file, you transform the `app` directory into a Python package. This allows you to import modules from the `app` package in other files.

The `app` directory name is a common convention for Flask applications, but you can choose a different name if you prefer based on your project needs.

With that in mind, let's set up the database connection and initialize the Flask app. In `app/__init__.py`, add the following code:

```python
from flask import Flask
from flask_sqlalchemy import SQLAlchemy
from flask_migrate import Migrate
from dotenv import load_dotenv
import os

load_dotenv()

db = SQLAlchemy()
migrate = Migrate()

def create_app():
    app = Flask(__name__)
    app.config['SQLALCHEMY_DATABASE_URI'] = os.getenv('DATABASE_URL')
    app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False

    db.init_app(app)
    migrate.init_app(app, db)

    # Register blueprints here
    from app.routes import user_routes
    app.register_blueprint(user_routes.user_bp)

    return app
```

This setup initializes Flask, SQLAlchemy, and Flask-Migrate. It loads the database URL from the environment variables and disables SQLAlchemy's modification tracking for better performance.

Blueprints are a way to organize related routes and views in Flask applications. We will cover blueprints and routes in the next sections.

To learn more about Flask-Migrate, check out the [Managing database migrations and schema changes with Flask and Neon Postgres](/guides/flask-database-migrations) guide.

## Model Definition

In web applications, models represent the data structure and relationships in your database. In the context of Flask and SQLAlchemy, models are Python classes that map to database tables and also allow you to define the schema and relationships between tables.

As an example of a typical model definition, let's create a `User` model in `app/models/user.py`:

```python
from app import db
from datetime import datetime

class User(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(80), unique=True, nullable=False)
    email = db.Column(db.String(120), unique=True, nullable=False)
    created_at = db.Column(db.DateTime, default=datetime.utcnow)

    def __repr__(self):
        return f'<User {self.username}>'

    def to_dict(self):
        return {
            'id': self.id,
            'username': self.username,
            'email': self.email,
            'created_at': self.created_at.isoformat()
        }
```

This model defines a User with `username`, `email`, and `created_at` fields. The `__repr__` method provides a string representation of the model instance for debugging purposes and logging. The `to_dict` method allows easy serialization of the model to JSON which is useful when returning data from API endpoints.

## Route Creation with Blueprints

In Flask, blueprints are a good way to organize your application into components. They allow you to group related routes, view functions, templates, and static files. Blueprints help in structuring large applications allowing you to separate different functional areas of your project.

If you're familiar with Laravel, blueprints in Flask serve a similar purpose to Laravel's controllers and route groups, allowing you to logically organize your routes and associated functionality.

Some of the main benefits of using blueprints in Flask include grouping related routes together, organizing your application into modular components, and avoiding naming conflicts between different parts of your application.

Here's an expanded example of how to use blueprints in a Flask application, in a file named `app/routes/user_routes.py` for user-related routes:

```python
from flask import Blueprint, jsonify, request, render_template, redirect, url_for
from app.models.user import User
from app import db

# Create a blueprint named 'user' with a URL prefix '/user'
user_bp = Blueprint('user', __name__, url_prefix='/user')

# Route for creating a new user (HTML form submission)
@user_bp.route('/create', methods=['POST'])
def create_user():
    data = request.form
    new_user = User(username=data['username'], email=data['email'])
    db.session.add(new_user)
    db.session.commit()
    return redirect(url_for('user.list_users'))

# Route for displaying all users (HTML)
@user_bp.route('/list', methods=['GET'])
def list_users():
    users = User.query.all()
    return render_template('users.html', users=users)

# API route for retrieving all users (JSON)
@user_bp.route('/api/list', methods=['GET'])
def get_users_api():
    users = User.query.all()
    return jsonify([user.to_dict() for user in users])

# Route for displaying a single user (HTML)
@user_bp.route('/<int:user_id>', methods=['GET'])
def get_user(user_id):
    user = User.query.get_or_404(user_id)
    return render_template('user_detail.html', user=user)

# API route for retrieving a single user (JSON)
@user_bp.route('/api/<int:user_id>', methods=['GET'])
def get_user_api(user_id):
    user = User.query.get_or_404(user_id)
    return jsonify(user.to_dict())
```

Let's break down this code:

1. We import necessary modules and create a blueprint named 'user' with a URL prefix '/user'. This prefix will be prepended to all routes defined within this blueprint which helps in organizing routes into logical groups. If you have multiple blueprints, each can have its own URL prefix.

2. `create_user()`: This route handles POST requests to create a new user. It reads form data, creates a new `User` instance, adds it to the database session, and commits the transaction. It then redirects to the `list_users()` route.

3. `list_users()`: This route displays all users in HTML format. It queries all users from the database and renders a template with the user data.

4. `get_users_api()`: This API route returns all users in JSON format. It queries all users from the database, converts them to dictionaries using the `to_dict()` method, and returns a JSON response.

5. `get_user()`: This route displays details of a single user in HTML format. It retrieves a user by ID or returns a 404 error if the user doesn't exist.

6. `get_user_api()`: This API route returns details of a single user in JSON format. It retrieves a user by ID or returns a 404 error if the user doesn't exist.

To use this blueprint in your main Flask application, you would register it like this in `app/__init__.py` which we've seen earlier:

```python
from flask import Flask
from app.routes.user_routes import user_bp

app = Flask(__name__)
app.register_blueprint(user_bp)
```

This structure allows you to organize related routes together, making your application more modular and easier to maintain as it grows.

## Frontend with Tailwind CSS and Templates

Tailwind CSS is a utility-first CSS framework that allows you to rapidly build custom user interfaces. It provides low-level utility classes that let you build completely custom designs without ever leaving your HTML.

To integrate Tailwind CSS with Flask templates, you can follow these steps:

1. Install Tailwind CSS:

   ```bash
   npm init -y
   npm install tailwindcss
   npx tailwindcss init
   ```

   This initializes a new Node.js project, installs Tailwind CSS, and creates a basic Tailwind configuration file.

2. Update the Tailwind CSS configuration file `tailwind.config.js` to include your HTML templates:

   ```javascript
   module.exports = {
     content: ['./app/templates/**/*.html'],
     theme: {
       extend: {},
     },
     plugins: [],
   };
   ```

   This configuration tells Tailwind to scan your HTML templates for classes to include in the final CSS output. The `extend` key allows you to customize Tailwind's default theme.

3. Create `app/static/css/main.css`:

   ```css
   @tailwind base;
   @tailwind components;
   @tailwind utilities;
   ```

   This file imports Tailwind's base styles, component classes, and utility classes. It serves as the entry point for Tailwind to inject its styles.

4. Compile Tailwind CSS:

   ```bash
   npx tailwindcss -i ./app/static/css/main.css -o ./app/static/css/output.css --watch
   ```

   This command compiles your Tailwind CSS file and watches for changes. This allows you to keep your CSS file as small as possible by only including the styles you use in your templates, which is important for performance as your application grows, and unnecessary styles can slow down your site.

   The `--watch` flag ensures that the CSS is recompiled whenever you make changes to your HTML files. This is useful for rapid development and live reloading.

5. Create a base template in `app/templates/base.html`:

   ```html
   <!doctype html>
   <html lang="en">
     <head>
       <meta charset="UTF-8" />
       <meta name="viewport" content="width=device-width, initial-scale=1.0" />
       <title>{% block title %}Flask Neon App{% endblock %}</title>
       <link rel="stylesheet" href="{{ url_for('static', filename='css/output.css') }}" />
     </head>
     <body class="bg-gray-100">
       <nav class="bg-blue-500 p-4 text-white">
         <div class="container mx-auto">
           <a href="/" class="text-2xl font-bold">Flask Neon App</a>
         </div>
       </nav>
       <main class="container mx-auto mt-8">{% block content %}{% endblock %}</main>
     </body>
   </html>
   ```

   This base template sets up the basic structure of your HTML pages. It includes:

   - A title block that can be overridden in child templates
   - A link to the compiled Tailwind CSS file
   - A simple navigation bar with Tailwind classes for styling
   - A main content area with a block that child templates can fill thanks to Jinja2 template inheritance

6. Create `app/templates/users.html`:

   ```html
   {% extends "base.html" %} {% block title %}Users{% endblock %} {% block content %}
   <h1 class="mb-4 text-3xl font-bold">Users</h1>
   <div class="rounded bg-white p-4 shadow-md">
     <form action="{{ url_for('user.create_user') }}" method="post" class="mb-4">
       <input type="text" name="username" placeholder="Username" required class="mr-2 border p-2" />
       <input type="email" name="email" placeholder="Email" required class="mr-2 border p-2" />
       <button type="submit" class="bg-blue-500 rounded px-4 py-2 text-white">Add User</button>
     </form>
     <ul>
       {% for user in users %}
       <li class="mb-2">{{ user.username }} ({{ user.email }})</li>
       {% endfor %}
     </ul>
   </div>
   {% endblock %}
   ```

   This template extends the base template using `{% extends "base.html" %}` and provides specific content for the users page. It includes:

   - A form for adding new users, styled with Tailwind classes
   - A list of existing users, also styled with Tailwind
   - Jinja2 template syntax for dynamic content (e.g., `{% for user in users %}`)

Tailwind CSS and Jinja2 templates give you the flexibility to create your frontend design while keeping your codebase organized and maintainable. This approach allows you to build responsive and visually appealing web applications with ease.

## Database Migrations

Database migrations are an important part of managing your application's database schema over time. They allow you to evolve your database structure incrementally, keeping it in sync with your application's models. We will be using Flask-Migrate, which is an extension for Flask that handles SQLAlchemy database migrations using Alembic, makes this process straightforward.

Here's a quick guide to setting up and using Flask-Migrate for managing database migrations:

1. Initialize the migration repository:

   ```bash
   flask db init
   ```

   This command creates a new migration repository. It sets up a `migrations` directory with the necessary files for managing your migrations.

2. Create a migration:

   ```bash
   flask db migrate -m "Initial migration"
   ```

   This command creates a new migration script based on the changes detected in your models. Unlike some other migration tools, Flask-Migrate automatically detects changes to your models and generates the migration script for you having to write it manually. However, it's always a good idea to review the generated migration script to ensure it reflects the intended changes.

   The `-m` flag allows you to provide a brief description of the migration, which is helpful for tracking changes over time.

3. Apply the migration:

   ```bash
   flask db upgrade
   ```

   This command applies the migration to your database, making the necessary schema changes.

It's important to review the generated migration scripts before applying them, especially in production environments. While Flask-Migrate is generally good at detecting changes, complex modifications might require manual adjustments to the migration scripts.

For more advanced usage, Flask-Migrate provides additional commands:

- `flask db downgrade`: Reverts the last migration
- `flask db current`: Displays the current revision of the database
- `flask db history`: Shows the migration history

You should commit your migration files to version control so that all developers and deployment environments can maintain consistent database schemas.

To learn more about managing database migrations with Flask and Neon Postgres, check out the [Managing database migrations and schema changes with Flask and Neon Postgres](/guides/flask-database-migrations) guide.

## Scalability Considerations

Besides the above steps, as your Flask application grows, you can consider a few strategies to improve performance and scalability. Here are some best practices to keep in mind:

1. Connection pooling is a technique used to manage database connections efficiently. Instead of opening and closing a new connection for each database operation, a pool of reusable connections is maintained.

   Neon Postgres supports connection pooling, which can significantly improve your application's performance by reducing the overhead of creating new connections.

   To use connection pooling with Neon:

   ```python
   # Update your DATABASE_URL to use the pooled connection string
   app.config['SQLALCHEMY_DATABASE_URI'] = 'postgresql://user:password@pooler.address:5432/database'
   ```

   Refer to the [Neon documentation on connection pooling](https://neon.tech/docs/connect/connection-pooling) for detailed instructions.

2. For performance optimization, consider caching frequently accessed data. Caching reduces the load on your database and speeds up response times for users.

   Implement caching for frequently accessed data using Flask-Caching:

   ```python
   from flask_caching import Cache

   cache = Cache()

   def create_app():
       # ... previous code ...
       cache.init_app(app, config={'CACHE_TYPE': 'simple'})

       return app

   @user_bp.route('/api/users', methods=['GET'])
   @cache.cached(timeout=60)  # Cache for 60 seconds
   def get_users_api():
       users = User.query.all()
       return jsonify([user.to_dict() for user in users])
   ```

   This example uses in-memory caching ('simple'). For production, you can switch to a caching backend like Redis.

3. Asynchronous processing allows your application to handle time-consuming tasks without blocking the main request-response cycle. You can use Celery, a distributed task queue, to run background tasks asynchronously.

   To integrate [Celery with Flask](https://flask.palletsprojects.com/en/2.3.x/patterns/celery/), you need to install the `celery` package and configure it in your Flask application:

   ```bash
   pip install celery
   ```

   ```python
   from celery import Celery

   celery = Celery(__name__)

   def create_app():
       # ... previous code ...
       celery.conf.update(app.config)

       return app

   @celery.task
   def send_email(user_id):
       user = User.query.get(user_id)
       # Send email to user

   # To call the task
   send_email.delay(user_id)
   ```

   This example defines a Celery task to send an email to a user asynchronously. You can run Celery workers to process these tasks in the background.

4. Rate limiting helps prevent abuse of your API and ensures fair usage among clients. It's an important security measure for public APIs.

   Implement rate limiting using Flask-Limiter:

   ```python
   from flask_limiter import Limiter
   from flask_limiter.util import get_remote_address

   limiter = Limiter(key_func=get_remote_address)

   def create_app():
       # ... previous code ...
       limiter.init_app(app)

       return app

   @user_bp.route('/api/users', methods=['GET'])
   @limiter.limit("5 per minute")
   def get_users_api():
       users = User.query.all()
       return jsonify([user.to_dict() for user in users])
   ```

   This example limits the `/api/users` endpoint to 5 requests per minute per IP address. You can adjust the limit based on your application's needs and resources.

## Conclusion

By following these practices, you've set up a scalable Flask application with Neon Postgres, including a responsive frontend using Tailwind CSS. This structure allows for easy expansion and maintenance as your project grows.

As a next step, consider adding authentication, authorization, and error handling to your application. These features are essential for securing your application and providing a good user experience.

You should also consider testing your application to ensure its reliability and performance. Unit tests, integration tests, and end-to-end tests can help you catch bugs early and maintain code quality. Testing your application with Neon's branching feature can help you test new features in isolation before deploying them to production.

## Additional Resources

- [Flask-Migrate Documentation](https://flask-migrate.readthedocs.io/en/latest/)
- [SQLAlchemy Migrations](https://docs.sqlalchemy.org/en/20/)
- [Alembic Documentation](https://alembic.sqlalchemy.org/en/latest/)
- [Neon Documentation](/docs)


# Testing Flask Applications with Neon's Database Branching

---
title: Testing Flask Applications with Neon's Database Branching
subtitle: Leveraging Realistic Production Data for Robust Testing with Flask and Neon Branching
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-09-15T00:00:00.000Z'
updatedOn: '2024-09-15T00:00:00.000Z'
---

[Flask](https://flask.palletsprojects.com/) is a popular Python micro-framework widely used for building web applications. It includes powerful tools for automated testing, with [pytest](https://docs.pytest.org/) being a preferred option due to its simplicity and effectiveness.

Testing with realistic data is crucial as it helps ensure that your application performs well under real-world conditions. Neon's database branching feature offers a unique solution by allowing you to test with actual production data without affecting your live database, thus maintaining data integrity and security.

## Understanding Flask Testing Approaches

In Flask applications, you would commonly use an in-memory SQLite database for testing. This method is favored because it allows for starting with a clean state for each test run by applying all database migrations and seeders. This setup is also great for parallel testing, as tests run quickly and do not interfere with each other.

However, testing with SQLite can differ significantly from your production environment, which might use a different database system, such as PostgreSQL. These differences can affect your application's behavior and lead to unexpected issues in production. This is one of the reasons why testing with real data can provide a more accurate finding of how your application will perform in its live environment.

## Neon Branching

Neon offers a database [branching feature](/docs/introduction/branching) that allows you to create isolated branches of your database for development, testing, and more.

A branch in Neon is a copy-on-write clone of your data that can be made from the current database state or any past state. This means you can have an exact copy of your production data at a specific point in time to use for testing.

Neon's branching is particularly useful in continuous integration and delivery pipelines, helping you be more productive by reducing the setup time needed for test environments.

This allows you to test with realistic data scenarios without the overhead of maintaining multiple separate databases. For more information on how to use Neon branching, refer to the [Neon documentation](/docs/introduction/branching).

Certainly! I'll rewrite this section with more in-depth explanations and remove the #### headings. Here's an improved version:

## Setting Up Your Testing Environment

Now that we've covered the benefits of testing Flask applications with Neon's database branching, let's walk through setting up a Flask project with a PostgreSQL database and writing tests using pytest.

### Prerequisites

Before you begin, ensure you have the following:

- Python 3.8 or higher installed on your machine
- A [Neon account](https://console.neon.tech/signup) with a project created
- Basic familiarity with Flask and SQLAlchemy

### Installation and Configuration

To set up your testing environment with Neon and Flask, follow these steps:

1. Configure Database Connection:

   After creating your Neon account and a new database branch, obtain the connection details from the Neon dashboard. Create a `.env` file with the Neon database connection parameters:

   ```env
   DATABASE_URL=postgresql://user:password@your-neon-hostname.neon.tech:5432/dbname
   ```

   Replace `user`, `password`, `your-neon-hostname`, and `dbname` with your Neon database details.

2. Install Required Packages:

   Install Flask, SQLAlchemy, pytest, and other necessary packages:

   ```bash
   pip install flask flask-sqlalchemy psycopg2-binary python-dotenv pytest
   ```

   Freeze the requirements for easy replication:

   ```bash
   pip freeze > requirements.txt
   ```

### Creating a Migration and Model

As we briefly mentioned earlier, you can use SQLAlchemy for database operations in Flask applications. Along with Flask-Migrate, you can manage database migrations effectively.

1. Set Up Flask-Migrate:

   Install and initialize Flask-Migrate:

   ```bash
   pip install Flask-Migrate
   ```

   In your main application file, initialize Flask-Migrate with your Flask app and database instance:

   ```python
   from flask_migrate import Migrate

   migrate = Migrate(app, db)
   ```

   This setup allows you to manage database migrations using Flask-Migrate.

2. Create a Model:

   In `models.py`, define a `Question` model:

   ```python
   from flask_sqlalchemy import SQLAlchemy

   db = SQLAlchemy()

   class Question(db.Model):
       id = db.Column(db.Integer, primary_key=True)
       title = db.Column(db.String(100), nullable=False)
       description = db.Column(db.Text, nullable=False)
   ```

3. Generate and Run Migrations:

   Create and apply the initial migration:

   ```bash
   flask db init
   flask db migrate -m "Initial migration"
   flask db upgrade
   ```

### Creating a Questions Route

In your main Flask application file, add a route to handle fetching questions from the database:

```python
from flask import jsonify
from models import Question

@app.route('/questions')
def get_questions():
    questions = Question.query.all()
    return jsonify([{
        'id': q.id,
        'title': q.title,
        'description': q.description
    } for q in questions])
```

This route fetches all questions from the database and returns them as JSON. You can expand this route to include additional functionality as needed.

If you don't have any questions in your database yet, you can add some manually or create a seed script to populate the database with test data.

To verify the setup, run the Flask development server:

```bash
flask run
```

If everything is set up correctly, you should be able to access the `/questions` route and see the questions returned as JSON.

### Writing a pytest Test for the Questions Route

The standard convention for naming test files is to prefix them with `test_`. This allows pytest to automatically discover and run the tests.

In this case, if your Flask application is in a file named `app.py`, create a file named `test_app.py` in the same directory:

```python
import pytest
from app import app, db
from models import Question

@pytest.fixture
def client():
    app.config['TESTING'] = True
    with app.test_client() as client:
        with app.app_context():
            db.create_all()
            yield client
            db.session.remove()
            db.drop_all()

def test_get_questions(client):
    # Add a test question
    question = Question(title='Test Question', description='This is a test')
    db.session.add(question)
    db.session.commit()

    response = client.get('/questions')
    assert response.status_code == 200
    data = response.get_json()
    assert len(data) == 1
    assert data[0]['title'] == 'Test Question'
```

Here we define a test fixture to set up and tear down the test environment. The `test_get_questions` function tests the `/questions` route by adding a test question to the database, making a request to the route, and asserting the response. This simple test verifies that the route returns the expected data.

### Running the Tests

With the simple test in place, you can now run the tests using pytest:

```bash
pytest
```

This setup provides a foundation for testing Flask applications with Neon Postgres, which you can expand upon for more complex applications and comprehensive test suites.

## Using Neon Branching with Flask

You should never run tests against your production database, as it can lead to data corruption and security risks. This is where Neon branching comes in handy.

Neon's branching feature enables you to create isolated database environments, which is ideal for testing changes without impacting the production database.

This can be particularly useful when testing complex features or changes that require realistic data scenarios. Especially when there are schema changes or data migrations involved, Neon branching provides a safe and efficient way to validate your application's behavior on a copy of your production data.

### Creating a Neon Branch

1. **Log In to Neon Dashboard:**

   - Access your Neon dashboard by logging in at [Neon's official website](https://neon.tech).

2. **Select Your Database:**

   - Navigate to the database project that you are using for your production environment.

3. **Create a New Branch:**
   - Click on "Branches" in the sidebar menu.
   - Click on "Create Branch."
   - Name your new branch (e.g., "testing-branch") and specify if it should be created from the current state of the database or from a specific point in time. This creates a copy-on-write clone of your database.
   - Wait for the branch to be fully provisioned, which usually takes just a few seconds.

### Integrating Neon Branching with Flask Testing

Go back to your Flask project and integrate the Neon branch into your testing setup:

1. **Update Environment Configuration:**

   - Once your branch is created, obtain the get details (hostname, database name, username, and password) from the Neon dashboard.
   - Create a new environment file for testing, such as `.env.test`, and configure it to use the Neon testing branch:

     ```env
     DATABASE_URL=postgresql://user:password@your-neon-testing-hostname.neon.tech:5432/dbname
     ```

2. **Update Test Configuration:**

   - Modify your `test_app.py` file to use the testing environment:

     ```python
     import os
     from dotenv import load_dotenv

     # Load test environment variables
     load_dotenv('.env.test')

     # Use the DATABASE_URL from the test environment
     app.config['SQLALCHEMY_DATABASE_URI'] = os.getenv('DATABASE_URL')
     ```

3. **Run Tests:**

   - With the testing branch configured, you can run your tests against the isolated database environment:

     ```bash
     pytest
     ```

   - Examine the output from pytest to ensure your application behaves as expected against the testing branch. This approach allows you to test changes in a controlled environment that mirrors your production setup instead of using an in-memory SQLite database.

In addition to running tests locally, you can automate the testing process by integrating Neon branching with your CI/CD pipeline. Neon provides a GitHub Actions workflow that simplifies the process of creating and managing database branches for testing. For more information, refer to the [Neon Branching GitHub Actions Guide](/docs/guides/branching-github-actions).

## Managing Neon Branches with `neonctl` CLI

With the `neonctl` CLI tool, managing your Neon database branches becomes more efficient and straightforward. You can create, list, obtain connection strings, and delete branches using simple commands.

### Installing `neonctl`

Before you can start using `neonctl`, you need to install it on your local machine. Follow the installation instructions provided in the [Neon CLI documentation](/docs/reference/cli-install) to set up `neonctl` on your system.

### Using `neonctl` to Manage Branches

Once `neonctl` is installed, you can use it to interact with your Neon database branches. Here are the basic commands for managing branches:

#### 1. [Creating a Branch](/docs/reference/cli-branches#create)

To create a new branch, use the `neonctl branches create` command:

```bash
neonctl branches create --project-id PROJECT_ID --parent PARENT_BRANCH_ID --name BRANCH_NAME
```

Replace `PROJECT_ID`, `PARENT_BRANCH_ID`, and `BRANCH_NAME` with the appropriate values for your Neon project. This command will create a new branch based on the specified parent branch.

#### 2. [Listing Branches](/docs/reference/cli-branches#list)

To list all branches in your Neon project, use the `neonctl branches list` command:

```bash
neonctl branches list --project-id PROJECT_ID
```

Replace `PROJECT_ID` with your Neon project ID. This command will display a list of all branches along with their IDs, names, and other relevant information.

#### 3. [Obtaining Connection String](/docs/reference/cli-connection-string)

Once you've created a branch, you'll need to obtain the connection string to configure your Laravel application. Use the `neonctl connection-string` command:

```bash
neonctl connection-string BRANCH_ID
```

Replace `BRANCH_ID` with the ID of the branch you want to connect to. This command will output the connection string that you can use to configure your Laravel `.env` file.

#### 4. [Deleting a Branch](/docs/reference/cli-branches#delete)

After you've finished testing with a branch, you can delete it using the `neonctl branches delete` command:

```bash
neonctl branches delete BRANCH_ID
```

Replace `BRANCH_ID` with the ID of the branch you want to delete. This command will remove the branch from your Neon project, ensuring that resources are not left unused.

## Conclusion

Testing Flask applications with Neon's database branching offers a solution that lets you test changes with realistic production data without affecting your live database.

By using realistic production data in a controlled testing environment, you can confidently validate your changes without risking your live application's integrity.

Neon's branching feature provides isolation, efficiency, flexibility, and simplicity, making it a valuable tool for streamlining the testing process.

## Additional Resources

- [Flask Documentation](https://flask.palletsprojects.com/)
- [pytest Documentation](https://docs.pytest.org/)
- [SQLAlchemy Documentation](https://docs.sqlalchemy.org/)
- [Neon Branching GitHub Actions Guide](/docs/guides/branching-github-actions)

<NeedHelp />


# Full Text Search using tsvector with Neon Postgres

---
title: Full Text Search using tsvector with Neon Postgres
subtitle: A step-by-step guide describing how to implement full text search with tsvector in Postgres
author: vkarpov15
enableTableOfContents: true
createdAt: '2024-09-17T13:24:36.612Z'
updatedOn: '2024-09-17T13:24:36.612Z'
---

The `tsvector` type enables you to use full text search on your text content in Postgres. Full text search allows you to search text content in a more flexible way than using `LIKE`. Full text search also supports features like _stemming_, which means searching for the word "run" will match variations like "ran" and "running".

## Steps

- Set up a table with a `tsvector` column
- Execute your first full text search
- Search for multiple words
- Rank the results
- Create a GIN index

## Set up a table with a `tsvector` column

To set up full text search, you need to create a column of type `tsvector` that will enable full text search. You can run the following `CREATE TABLE` statement in the [Neon SQL Editor](/docs/get-started-with-neon/query-with-neon-sql-editor) or from a client such as [psql](/docs/connect/query-with-psql-editor) that is connected to Neon. This statement will create a table with a column `searchable` of type `tsvector`.

```sql
CREATE TABLE documents (
  id SERIAL PRIMARY KEY,
  title TEXT,
  body TEXT,
  searchable tsvector
);
```

Next, insert two new rows into the `documents` table. The [to_tsvector()] (https://www.postgresql.org/docs/current/textsearch-controls.html) function takes in a language and the text content to tokenize. In the following example, the text content is the `title` and `body` columns concatenated together.

```sql
INSERT INTO documents (title, body, searchable)
  VALUES (
    'PostgreSQL Full-Text Search',
    'This is an introduction to full-text search in PostgreSQL.',
    to_tsvector('english', 'PostgreSQL Full-Text Search This is an introduction to full-text search in PostgreSQL.')
  );

INSERT INTO documents (title, body, searchable)
  VALUES (
    'My Mashed Potatoes Recipe',
    'These amazing homemade mashed potatoes are perfectly rich and creamy, full of great flavor, easy to make, and always a crowd fave!',
    to_tsvector('english', 'My Mashed Potatoes Recipe These amazing homemade mashed potatoes are perfectly rich and creamy, full of great flavor, easy to make, and always a crowd fave!')
  );
```

Once you have inserted the new rows, try running `SELECT * FROM documents;`. You will see that the data stored in the `searchable` column looks like the following.

```
'full':3,12 'full-text':2,11 'introduct':9 'postgresql':1,16 'search':5,14 'text':4,13
```

Internally, `to_tsvector()` uses a parser to break the text content into tokens for easier searching.

## Execute your first full text search

You can execute a full text search query using a `WHERE` clause with `@@ to_tsquery('english', 'content here')` as shown below. The following query returns the "mashed potatoes" row because, although the word "flavorful" does not appear in that row's text content, the word "flavor" does. And "flavor" matches "flavorful".

```sql
SELECT
    *
  FROM documents
  WHERE searchable @@ to_tsquery('english', 'flavorful');
```

Similarly, the following query returns the "PostgeSQL" row, even though the word "searching" does not appear in that row's text content, but "search" does.

```sql
SELECT
    *
  FROM documents
  WHERE searchable @@ to_tsquery('english', 'searching');
```

The `@@` operator is a special operator which compares the `tsvector` value stored in the `searchable` column with the `tsquery` value provided in the query. The `tsquery` type is different from the `tsvector` type. For example, if you run the following command, Postgres will return the string "searching".

```sql
SELECT to_tsquery('english', 'searching');
```

## Search for multiple words

If you try using `to_tsquery()` with multiple words, like `to_tsquery('english', 'searching text');`, Postgres will throw the following error.

```
ERROR: syntax error in tsquery: "searching text" (SQLSTATE 42601)
```

That's because the input to `to_tsquery()` must be tokens separated by `tsquery` operators like `&`. The correct way to search for "searching" and "text" would be `to_tsquery('english', 'searching & text');`. To make full text search easier to work with, Postgres also has a `phraseto_tsquery()` function that converts text into a `tsquery` with no need for operators. The following query will successfully return the "PostgreSQL" row.

```sql
SELECT
    *
  FROM documents
  WHERE searchable @@ phraseto_tsquery('english', 'searching text');
```

`tsquery` also supports negations. For example, the following query will search for rows whose text content matches "searching" and does **not** match "text".

```sql
SELECT
    *
  FROM documents
  WHERE searchable @@ to_tsquery('english', 'searching & !text');
```

Postgres also supports a `websearch_to_tsquery()` function, which uses an alternative syntax that doesn't require putting operators between all tokens. `websearch_to_tsquery()` supports negations by prefixing a token with `-`. The following query also searches for rows whose text content matches "searching" and does **not** match "text".

```sql
SELECT
    *
  FROM documents
  WHERE searchable @@ websearch_to_tsquery('english', 'searching -text');
```

## Rank the results

Postgres provides two functions for ranking the results, allowing you to sort by which results are the best match. The following statement sorts rows that match "searching text" using the `ts_rank()` function, which counts the number of tokens that match.

```sql
SELECT
    id,
    title,
    ts_rank(searchable, websearch_to_tsquery('english', 'searching text')) AS rank
  FROM documents
  WHERE searchable @@ websearch_to_tsquery('english', 'searching text')
  ORDER BY rank DESC;
```

To see how sorting works in practice, insert two more rows as follows. The first row contains 6 tokens that match "search" and "text", so that row should show up first.

```sql
INSERT INTO documents (title, body, searchable)
  VALUES (
    'PostgreSQL Text Search',
    'A comprehensive, searchable guide in plain text format. Covers full text search in PostgreSQL',
    to_tsvector('english', 'PostgreSQL Text Search A comprehensive, searchable guide in plain text format. Covers full text search in PostgreSQL')
  );
```

Running the `ts_rank()` `SELECT` statement with these two new rows outputs rows in the following order. "PostgreSQL Text Search" appears first because it has the most occurrences of tokens that match "search" and "text".

```sql
2	PostgreSQL Text Search	0.34941113
1	PostgreSQL Full-Text Search	0.3054688
```

Postgres also has a `ts_rank_cd()` function which uses an alternative ranking algorithm based on _cover density_. `ts_rank_cd()` also takes proximity of matching tokens into consideration, so the "PostgreSQL Text Search" row will rank slightly lower with `ts_rank_cd()` because there's more words between the matching tokens.

```sql
SELECT
    id,
    title,
    ts_rank_cd(searchable, websearch_to_tsquery('english', 'searching text')) AS rank
  FROM documents
  WHERE searchable @@ websearch_to_tsquery('english', 'searching text')
  ORDER BY rank DESC;
```

```
1	PostgreSQL Full-Text Search	0.21666667
2	PostgreSQL Text Search	0.21428572
```

## Create a GIN index

[GIN indexes](https://www.postgresql.org/docs/current/gin-intro.html) allow you to index your `tsvector` properties, which can make your full text search queries faster as your data grows. Just be careful, [GIN indexes can slow down your updates](https://pganalyze.com/blog/gin-index). Below is how you can create a GIN index on the `searchable` column.

```sql
CREATE INDEX searchable_idx ON documents USING GIN(searchable);
```

To test out the GIN index, let's first insert 100 copies of the "mashed potatoes" document. Sometimes Postgres decides to skip using indexes and use a sequential scan instead when a query matches most of the table.

```sql
DO $$
BEGIN
  FOR i IN 1..100 LOOP
    INSERT INTO documents (title, body, searchable)
    VALUES (
      'My Mashed Potatoes Recipe',
      'These amazing homemade mashed potatoes are perfectly rich and creamy, full of great flavor, easy to make, and always a crowd fave!',
      to_tsvector('english', 'My Mashed Potatoes Recipe These amazing homemade mashed potatoes are perfectly rich and creamy, full of great flavor, easy to make, and always a crowd fave!')
    );
  END LOOP;
END $$;
```

Next, you can run an `EXPLAIN ANALYZE` query (or just click the **Explain** button in the Neon SQL Editor) to confirm that Postgres is using your GIN index.

```sql
EXPLAIN ANALYZE
SELECT
    id, title
  FROM documents
WHERE searchable @@ to_tsquery('english', 'search');
```

The `EXPLAIN ANALYZE` query should produce output that resembles the following. The `Bitmap Index Scan on searchable_idx` means that Postgres is using a GIN index rather than a sequential scan to answer the query.

```
Bitmap Heap Scan on documents  (cost=8.54..13.10 rows=2 width=29) (actual time=0.021..0.022 rows=2 loops=1)
  Recheck Cond: (searchable @@ '''search'''::tsquery)
  Heap Blocks: exact=1
  ->  Bitmap Index Scan on searchable_idx  (cost=0.00..8.54 rows=2 width=0) (actual time=0.009..0.009 rows=2 loops=1)
        Index Cond: (searchable @@ '''search'''::tsquery)
Planning Time: 0.095 ms
Execution Time: 0.105 ms
```


# Setting up GitHub Codespaces with Neon Database Branching for Pull Requests

---
title: Setting up GitHub Codespaces with Neon Database Branching for Pull Requests
subtitle: Learn how to create separate development environments for each pull request using GitHub Codespaces and Neon's Postgres branching
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-08-18T00:00:00.000Z'
updatedOn: '2024-08-18T00:00:00.000Z'
---

When working on a team project, it's useful to have separate environments for each new feature or bug fix. This helps prevent conflicts and makes it easier to test changes. In this guide, we'll show you how to set up a process that creates a new development environment for each pull request. We'll use GitHub Codespaces for the coding environment and Neon's Postgres branching for the database.

By the end of this guide, you'll have a setup that automatically creates a new Codespace and a new database branch for each pull request. This means each change can be tested separately, making it easier to find and fix problems.

## What you'll need

Before we start, make sure you have:

- A GitHub account that can use Codespaces
- A [Neon](https://console.neon.tech/signup) account and project
- A Neon API key (you can learn how to get one [here](/docs/manage/api-keys#create-an-api-key))
- Basic knowledge of Git, GitHub Actions, and CI/CD

## Creating a new project

This process will work with any language or framework, but for this guide, we'll use [Laravel](/guides/laravel-overview).

Let's start by making a new Laravel project and putting it on GitHub.

1. First, create a new Laravel project:

```bash
composer create-project laravel/laravel codespaces-neon-demo
cd codespaces-neon-demo
```

This command creates a new Laravel project in a folder called `codespaces-neon-demo` then uses `cd` to access the new project.

2. Next, set up Git for this project:

```bash
git init
git add .
git commit -m "First commit: New Laravel project"
```

The above commands will initialize a new Git repository, add all the project files to it, and create the first commit.

3. Now, create a new repository on GitHub and upload your code:

```bash
git remote add origin https://github.com/<yourusername>/codespaces-neon-demo.git
git branch -M main
git push -u origin main
```

Replace `yourusername` with your actual GitHub username. These commands connect your local repository to GitHub and upload your code.

## Setting up GitHub Codespaces

Now we'll set up GitHub Codespaces. This will define the development environment that will be created for each pull request.

1. Make a new folder in your project for the Codespaces configuration:

```bash
mkdir .devcontainer
```

2. In this new folder, create a file called `devcontainer.json`:

```json
{
  "name": "Laravel Codespaces",
  "image": "mcr.microsoft.com/devcontainers/php:8.2",
  "customizations": {
    "vscode": {
      "extensions": [
        "felixfbecker.php-debug",
        "bmewburn.vscode-intelephense-client",
        "mikestead.dotenv",
        "amiralizadeh9480.laravel-extra-intellisense"
      ]
    }
  },
  "forwardPorts": [8000],
  "postCreateCommand": "cp .env.example .env && composer install && php artisan key:generate && .devcontainer/setup-db.sh",
  "features": {
    "ghcr.io/devcontainers/features/node:1": {}
  }
}
```

This file tells GitHub Codespaces how to set up the development environment. Here's what each part does:

- `"name"`: This is just a name for the Codespace.
- `"image"`: This specifies the base Docker image to use. We're using a pre-built image with PHP 8.2.
- `"customizations"`: This section lists VS Code extensions to install.
- `"forwardPorts"`: This tells Codespaces which ports to make available.
- `"postCreateCommand"`: This runs commands after the Codespace is created. It installs PHP dependencies, generates an application key, and runs a setup script for the database.
- `"features"`: This adds Node.js to the environment.

## Setting up Neon Postgres

Now let's connect our project to a Neon Postgres database.

1. Go to the [Neon Console](https://console.neon.tech) and create a new project.

2. After creating the project, you'll see a connection string. Copy the details as you'll need them later.

3. Open the `.env` file in your Laravel project and update the database settings:

```env
DB_CONNECTION=pgsql
DB_HOST=your-neon-hostname.neon.tech
DB_PORT=5432
DB_DATABASE=your_database_name
DB_USERNAME=your_username
DB_PASSWORD=your_password
```

Replace the placeholders with the details from your Neon connection string.

4. Run the database migrations:

```bash
php artisan migrate
```

This command creates the necessary tables in your Neon database.

## Setting up GitHub Actions for Neon Branching

Now we'll set up GitHub Actions to create and delete Neon database branches automatically. First, we need to add your Neon API key to your GitHub repository:

1. In your GitHub repository, go to "Settings", then "Secrets and variables", then "Actions".
2. Click "New repository secret".
3. Name it `NEON_API_KEY` and paste your [Neon API key](/docs/manage/api-keys#create-an-api-key) as the value.
4. Click "Add secret".

Next, we'll create two GitHub Actions workflows: one to create a new Neon branch when a pull request is opened, and another to delete the branch when the pull request is closed.

### Workflow to Create a Branch

If you don't already have a `.github/workflows` directory, create one:

```bash
mkdir -p .github/workflows
```

Then create a file in this directory called `create-neon-branch.yml` with the following content:

```yaml
name: Create Neon Branch

on:
  pull_request:
    types: [opened, reopened]

jobs:
  create-branch:
    runs-on: ubuntu-latest
    steps:
      - uses: neondatabase/create-branch-action@v5
        with:
          project_id: your-neon-project-id
          branch_name: pr-${{ github.event.pull_request.number }}
          username: your-database-username
          api_key: ${{ secrets.NEON_API_KEY }}
        id: create-branch
      - run: echo ${{ steps.create-branch.outputs.db_url }}
      - run: echo ${{ steps.create-branch.outputs.branch_id }}
```

Replace `your-neon-project-id` and `your-database-username` with your actual Neon project ID and database username.

This workflow does the following:

- It runs when a pull request is opened or reopened thanks to the `on` section.
- It uses Neon's official action to create a new database branch.
- The branch name is based on the pull request number.
- It outputs the new branch's database URL and ID.

### Workflow to Delete a Branch

With the workflow to create a branch set up, let's create another one to delete the branch when the pull request is closed.

Create another file at `.github/workflows/delete-neon-branch.yml`:

```yaml
name: Delete Neon Branch

on:
  pull_request:
    types: [closed]

jobs:
  delete-branch:
    runs-on: ubuntu-latest
    steps:
      - uses: neondatabase/delete-branch-action@v3
        with:
          project_id: your-neon-project-id
          branch: pr-${{ github.event.pull_request.number }}
          api_key: ${{ secrets.NEON_API_KEY }}
```

Again, replace `your-neon-project-id` with your actual Neon project ID.

This workflow:

- Runs when a pull request is closed.
- Uses Neon's action to delete the database branch associated with the pull request.

## Configuring Codespaces to Use Neon Branches

Now we need to tell Codespaces how to connect to the right database branch. Create a file called `setup-db.sh` in the `.devcontainer` directory:

```bash
#!/bin/bash

PR_NUMBER=$(echo $GITHUB_REF | sed 's/refs\/pull\/\([0-9]*\).*/\1/')

if [ -n "$PR_NUMBER" ]; then
    BRANCH_NAME="pr-$PR_NUMBER"

    # Use GitHub CLI to get the branch details
    BRANCH_DETAILS=$(gh api /repos/$GITHUB_REPOSITORY/actions/runs/$GITHUB_RUN_ID/jobs -H "Accept: application/vnd.github.v3+json" | jq -r '.jobs[] | select(.name == "create-branch") | .steps[] | select(.name == "Create Neon Branch") | .outputs.db_url')

    if [ -n "$BRANCH_DETAILS" ]; then
        # Parse the connection string
        DB_HOST=$(echo $BRANCH_DETAILS | sed -n 's/.*@\(.*\):.*/\1/p')
        DB_NAME=$(echo $BRANCH_DETAILS | sed -n 's/.*\/\(.*\)?.*/\1/p')
        DB_USER=$(echo $BRANCH_DETAILS | sed -n 's/.*:\/\/\(.*\):.*/\1/p')
        DB_PASSWORD=$(echo $BRANCH_DETAILS | sed -n 's/.*:\/\/.*:\(.*\)@.*/\1/p')

        # Update the .env file
        sed -i "s/DB_HOST=.*/DB_HOST=$DB_HOST/" .env
        sed -i "s/DB_DATABASE=.*/DB_DATABASE=$DB_NAME/" .env
        sed -i "s/DB_USERNAME=.*/DB_USERNAME=$DB_USER/" .env
        sed -i "s/DB_PASSWORD=.*/DB_PASSWORD=$DB_PASSWORD/" .env

        echo "Updated .env file with PR-specific database details"
    else
        echo "No branch details found for PR $PR_NUMBER"
    fi
else
    echo "This is not a PR environment, using default database settings"
fi

# Run database migrations
php artisan migrate --force
```

The script does the following:

- It checks if we're in a pull request environment.
- If we are, it gets the details of the newly created database branch.
- It updates the `.env` file with these details.
- Finally, it runs database migrations.

Make sure to make this script executable:

```bash
chmod +x .devcontainer/setup-db.sh
```

After setting up these files, commit and push your changes to GitHub:

```bash
git add .
git commit -m "Add GitHub Actions and Codespaces configuration"
git push origin main
```

With everything set up, you can now create a new branch in your project, open a pull request, and see the new Codespace and database branch in action.

## How to Use This Setup

With everything set up, here's how you would use this in your development process:

1. Create a new branch in your project and make your changes:

   - Create and switch to a new branch: `git checkout -b feature-branch-name`
   - Make your changes to the code
   - Commit your changes: `git add .` and `git commit -m "Description of changes"`
   - Push your branch to GitHub: `git push -u origin feature-branch-name`

2. Open a pull request with your changes:

   - Go to your repository on GitHub
   - Click on "Pull requests" then "New pull request"
   - Select your feature branch as the compare branch
   - Click "Create pull request"
   - Fill in the title and description, then click "Create pull request"

3. GitHub Actions will automatically create a new Neon database branch for your pull request:

   - This happens automatically when the pull request is opened
   - You can check the "Actions" tab in your GitHub repository to see the progress
   - Once complete, you'll see a new branch in your Neon console named `pr-[number]`

4. Open a Codespace for this pull request:

   - On the pull request page, click the "Code" dropdown
   - Select "Open with Codespaces"
   - Click "New codespace"
   - Wait for the Codespace to build and start

5. Test your changes in the isolated environment:

   - The Codespace is now connected to your PR-specific database branch
   - Run your application: `php artisan serve`
   - Run tests: `php artisan test`
   - Make additional changes if needed, commit, and push

6. Review and merge the pull request:

   - Once you're satisfied with the changes, request a review if required
   - Reviewers can open their own Codespaces to test the changes
   - When ready, merge the pull request on GitHub

7. Automatic cleanup:
   - When the pull request is closed (either merged or declined), GitHub Actions will automatically delete the associated Neon database branch
   - You can verify this in your Neon console

## Keeping Things Secure

It's important to keep your project and its data safe:

1. Never share your Neon API key. Always use GitHub Secrets to store it.
2. Be careful about what information you put in public repositories.
3. Regularly change your API keys and check who has access to what.

## Conclusion

By setting up GitHub Codespaces with Neon database branching, you've created a system that gives each pull request its own complete development environment. This can help your team work more effectively by making it easier to test changes and avoid conflicts.

This workflow can be adapted to work with other languages and frameworks. You can also add more steps to the GitHub Actions workflows to suit your specific needs like running tests, deploying to staging environments, or sending notifications.

## Where to Learn More

- [GitHub Codespaces Documentation](https://docs.github.com/en/codespaces)
- [Neon Documentation](/docs)
- [GitHub Actions Documentation](https://docs.github.com/en/actions)

<NeedHelp />


# Implementing Fine-Grained Authorization in Laravel with Neon Postgres

---
title: Implementing Fine-Grained Authorization in Laravel with Neon Postgres
subtitle: Learn how to set up and utilize Laravel's powerful authorization features to create a secure and flexible application using Neon's high-performance database.
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-07-14T00:00:00.000Z'
updatedOn: '2024-07-14T00:00:00.000Z'
---

Laravel provides an authorization system that allows developers to implement fine-grained access control in their applications. While Laravel's built-in features are powerful, some projects require even more advanced role-based access control (RBAC). This is where third-party packages like Spatie's Laravel Permission come into play.

In this guide, we'll walk through the process of setting up fine-grained authorization in a Laravel application using Neon Postgres. We'll start with Laravel's native authorization features, including Gates and Policies, and then expand our implementation to incorporate Spatie's Laravel Permission package for more sophisticated RBAC capabilities.

By the end of this tutorial, you'll have a good understanding of how to create a flexible and secure authorization system that can scale with your application's needs.

## Prerequisites

Before we begin, make sure you have the following:

- PHP 8.1 or higher installed on your system
- [Composer](https://getcomposer.org/) for managing PHP dependencies
- A [Neon](https://console.neon.tech/signup) account for database hosting
- Basic knowledge of Laravel and its authentication system

## Setting up the Project

Let's start by creating a new Laravel project and configuring it to use Neon Postgres.

### Creating a New Laravel Project

Open your terminal and run the following command to create a new Laravel project:

```bash
composer create-project laravel/laravel laravel-auth-demo
cd laravel-auth-demo
```

This will create a new Laravel project in a directory named `laravel-auth-demo` and navigate you into the project directory.

### Connecting to Neon Database

Update your `.env` file with your Neon database credentials:

```env
DB_CONNECTION=pgsql
DB_HOST=your-neon-hostname.neon.tech
DB_PORT=5432
DB_DATABASE=your_database_name
DB_USERNAME=your_username
DB_PASSWORD=your_password
```

Make sure to replace the placeholders with your actual Neon database details.

## Understanding Laravel's Authorization System

Laravel's authorization system is built on two main concepts: [Gates](https://laravel.com/docs/11.x/authorization#gates) and [Policies](https://laravel.com/docs/11.x/authorization#creating-policies).

- **Gates** are Closure-based approaches to authorization. They provide a simple, Closure-based method of authorizing actions. Gates are ideal for simple checks that don't necessarily relate to a specific model or resource. They can be thought of as general-purpose authorization checks that can be used throughout your application.

- **Policies** are classes that organize authorization logic around a particular model or resource. They encapsulate the logic for authorizing actions on a specific type of model. Policies are particularly useful when you have multiple authorization checks related to a single model, as they help keep your authorization logic organized and maintainable.

For fine-grained authorization, we'll primarily focus on Policies, as they provide a more structured and scalable approach for complex applications. Policies allow you to group related authorization logic together, making it easier to manage and understand the permissions associated with each model in your application.

That said, Gates can still play an important role in your authorization strategy. They're great for defining broader, application-wide permissions that aren't tied to a specific model. You might use Gates for actions like "access admin dashboard" or "manage site settings".

It's worth noting that Laravel's authorization system is deeply integrated with its authentication system. This means you can easily check a user's permissions within your controllers, views, and even database queries. Whether you're using Gates or Policies, you'll find that Laravel provides a consistent and intuitive API for performing authorization checks throughout your application.

## Implementing Fine-Grained Authorization

Let's implement a fine-grained authorization system for a blog application where users can create, read, update, and delete posts.

### Setting up the Post Model and Migration

For the purpose of this guide, we'll create a `Post` model with basic fields like `title`, `content`, and `is_published`. We'll also associate each post with a user.

First, let's create a `Post` model with its migration:

```bash
php artisan make:model Post -m
```

Update the migration file in `database/migrations` to define the structure of our `posts` table:

```php
public function up()
{
    Schema::create('posts', function (Blueprint $table) {
        $table->id();
        $table->foreignId('user_id')->constrained()->onDelete('cascade');
        $table->string('title');
        $table->text('content');
        $table->boolean('is_published')->default(false);
        $table->timestamps();
    });
}
```

Run the migration:

```bash
php artisan migrate
```

This will create a `posts` table in your Neon Postgres database.

### Creating a Policy for Posts

Now, let's create a policy for the `Post` model:

```bash
php artisan make:policy PostPolicy --model=Post
```

This command creates a new policy class in `app/Policies/PostPolicy.php`. Let's update it with our authorization logic:

```php
<?php

namespace App\Policies;

use App\Models\Post;
use App\Models\User;
use Illuminate\Auth\Access\HandlesAuthorization;

class PostPolicy
{
    use HandlesAuthorization;

    public function viewAny(User $user)
    {
        return true; // Allow all users to view the list of posts
    }

    public function view(User $user, Post $post)
    {
        return true; // Allow all users to view individual posts
    }

    public function create(User $user)
    {
        return true; // Allow all authenticated users to create posts
    }

    public function update(User $user, Post $post)
    {
        return $user->id === $post->user_id; // Allow only the author to update the post
    }

    public function delete(User $user, Post $post)
    {
        return $user->id === $post->user_id; // Allow only the author to delete the post
    }

    public function publish(User $user, Post $post)
    {
        // Allow only the author or an admin to publish the post
        return $user->id === $post->user_id || $user->is_admin;
    }
}
```

Rundown of the `PostPolicy` class:

1. `viewAny` method: This allows all users to view the list of posts. This could be useful for a public blog where anyone can see the list of available posts.

2. `view` method: Similar to `viewAny`, this allows all users to view individual posts. Again, this is suitable for a public blog where post content is accessible to everyone.

3. `create` method: This permits all authenticated users to create posts. It assumes that any logged-in user should be able to write a post.

4. `update` method: This method only allows the author of the post to update it. It compares the ID of the current user with the user ID associated with the post.

5. `delete` method: Similar to `update`, this method restricts deletion to the author of the post. This ensures that users can only delete their own posts.

6. `publish` method: This introduces a more complex authorization rule. It allows either the author of the post or an admin user to publish the post. This is useful for blogs where posts might need approval before being made public.

Each method in this policy corresponds to a specific action that can be performed on a Post model. The methods return boolean values: `true` if the action is allowed, and `false` if it's not.

This policy provides a fine-grained control over post-related actions, ensuring that users can only perform actions they're authorized to do. It's a good example of how policies can encapsulate complex authorization logic in a clean, readable way.

### Registering the Policy

By default, Laravel 11.x and later versions automatically discover policies. However, if you're using an older version, you might need to manually register the policy in the `AuthServiceProvider`.

To automatically discover policies, your policies should be in the `app/Policies` directory and follow the naming convention of `ModelNamePolicy`. Laravel will automatically associate the policy with the corresponding model.

To manually register the policy, add the following line to the `boot` method of your `AuthServiceProvider`:

```php
use App\Models\Post;
use App\Policies\PostPolicy;

public function boot(): void
{
    Gate::policy(Post::class, PostPolicy::class);
}
```

This line tells Laravel to use the `PostPolicy` class for authorization checks related to the `Post` model.

### Implementing Role-Based Access Control

To support our `is_admin` flag and implement basic role-based access control, let's update our `users` table.

To do that, create a new migration to add the `is_admin` column:

```bash
php artisan make:migration add_is_admin_to_users_table
```

Update the migration file:

```php
public function up()
{
    Schema::table('users', function (Blueprint $table) {
        $table->boolean('is_admin')->default(false);
    });
}

public function down()
{
    Schema::table('users', function (Blueprint $table) {
        $table->dropColumn('is_admin');
    });
}
```

Run the migration to add the `is_admin` column to the `users` table:

```bash
php artisan migrate
```

This column will allow us to differentiate between regular users and administrators.

### Using Authorization in Controllers

Now that we have our policy set up, let's use it in a controller. Create a new `PostController`:

```bash
php artisan make:controller PostController --resource
```

Update the `PostController` with authorization checks:

```php
<?php

namespace App\Http\Controllers;

use App\Models\Post;
use Illuminate\Http\Request;

class PostController extends Controller
{
    public function __construct()
    {
        $this->authorizeResource(Post::class, 'post');
    }

    public function index()
    {
        $posts = Post::all();
        return view('posts.index', compact('posts'));
    }

    public function show(Post $post)
    {
        return view('posts.show', compact('post'));
    }

    public function create()
    {
        return view('posts.create');
    }

    public function store(Request $request)
    {
        $validatedData = $request->validate([
            'title' => 'required|max:255',
            'content' => 'required',
        ]);

        $post = auth()->user()->posts()->create($validatedData);

        return redirect()->route('posts.show', $post);
    }

    public function edit(Post $post)
    {
        return view('posts.edit', compact('post'));
    }

    public function update(Request $request, Post $post)
    {
        $validatedData = $request->validate([
            'title' => 'required|max:255',
            'content' => 'required',
        ]);

        $post->update($validatedData);

        return redirect()->route('posts.show', $post);
    }

    public function destroy(Post $post)
    {
        $post->delete();

        return redirect()->route('posts.index');
    }

    public function publish(Post $post)
    {
        $this->authorize('publish', $post);

        $post->update(['is_published' => true]);

        return redirect()->route('posts.show', $post);
    }
}
```

The `__construct` method uses the `authorizeResource` method to automatically authorize resource controller methods. We've also added a `publish` method with a manual authorization check.

Alternatively, you can use the `authorize` method within each controller method to perform authorization checks manually. This method takes the name of the policy method to call and the model to authorize against:

```php
$this->authorize('publish', $post);
```

This line checks if the current user is authorized to publish the post. If not, Laravel will throw an `AuthorizationException` preventing the action from being executed.

### Using Authorization in Views

In your Blade views, you can use the `@can` directive to conditionally show or hide elements based on the user's permissions. For example, in `resources/views/posts/show.blade.php`:

```html
<h1>{{ $post->title }}</h1>
<p>{{ $post->content }}</p>

@can('update', $post)
<a href="{{ route('posts.edit', $post) }}">Edit Post</a>
@endcan @can('delete', $post)
<form action="{{ route('posts.destroy', $post) }}" method="POST">
  @csrf @method('DELETE')
  <button type="submit">Delete Post</button>
</form>
@endcan @can('publish', $post) @if(!$post->is_published)
<form action="{{ route('posts.publish', $post) }}" method="POST">
  @csrf
  <button type="submit">Publish Post</button>
</form>
@endif @endcan
```

Rundown of the Blade view:

1. The view starts by displaying the post's title and content, which are accessible to all users as per our policy.

2. `@can('update', $post)` directive: This checks if the current user is authorized to update the post. If true, it displays an "Edit Post" link. This corresponds to the `update` method in our `PostPolicy`.

3. `@can('delete', $post)` directive: Similar to the update check, this verifies if the user can delete the post. If authorized, it shows a delete form with a submit button. This uses the `delete` method from our policy.

4. `@can('publish', $post)` directive: This checks if the user can publish the post, corresponding to the `publish` method in our policy.

5. Inside the publish check, there's an additional `@if(!$post->is_published)` condition. This ensures the publish button only appears if the post isn't already published.

6. Each form includes a `@csrf` directive for CSRF protection, which is a security feature in Laravel to prevent cross-site request forgery attacks.

7. The delete form also includes `@method('DELETE')`, which is Laravel's way of spoofing HTTP methods that aren't supported by HTML forms (like DELETE, PUT, PATCH).

By using these directives, you can conditionally display elements based on the user's permissions, providing a tailored experience for each user based on their role and authorization level.

## Integrating Spatie's Laravel Permission for Advanced RBAC

While Laravel's built-in authorization system provides you with a good foundation for managing permissions and policies, you might require more complex role and permission structures.

Spatie's Laravel Permission package provides a solution for implementing advanced RBAC (Role-Based Access Control) in your Laravel application.

### Installing Spatie Laravel Permission

First, let's install the package using Composer:

```bash
composer require spatie/laravel-permission
```

After installation, publish the package's configuration and migration files:

```bash
php artisan vendor:publish --provider="Spatie\Permission\PermissionServiceProvider"
```

This command will create a `config/permission.php` file and a migration file in your `database/migrations` directory.

Run the migrations to create the necessary tables in your Neon Postgres database:

```bash
php artisan migrate
```

This will create the permissions, roles, and model_has_roles tables in your database.

### Configuring the Package

The package's configuration file is located at `config/permission.php`. For most use cases, the default configuration works well. However, you can customize it based on your needs. For example, you can change the table names or add a cache expiration time.

### Setting Up Roles and Permissions

Let's create some roles and permissions for our blog application. We'll do this in a seeder for easy setup and testing.

Create a new seeder:

```bash
php artisan make:seeder RolesAndPermissionsSeeder
```

Update the seeder file (`database/seeders/RolesAndPermissionsSeeder.php`):

```php
<?php

namespace Database\Seeders;

use Illuminate\Database\Seeder;
use Spatie\Permission\Models\Role;
use Spatie\Permission\Models\Permission;

class RolesAndPermissionsSeeder extends Seeder
{
    public function run()
    {
        // Reset cached roles and permissions
        app()[\Spatie\Permission\PermissionRegistrar::class]->forgetCachedPermissions();

        // Create permissions
        Permission::create(['name' => 'view posts']);
        Permission::create(['name' => 'create posts']);
        Permission::create(['name' => 'edit posts']);
        Permission::create(['name' => 'delete posts']);
        Permission::create(['name' => 'publish posts']);
        Permission::create(['name' => 'unpublish posts']);

        // Create roles and assign permissions
        $role = Role::create(['name' => 'writer']);
        $role->givePermissionTo(['view posts', 'create posts', 'edit posts', 'delete posts']);

        $role = Role::create(['name' => 'editor']);
        $role->givePermissionTo(['view posts', 'create posts', 'edit posts', 'delete posts', 'publish posts', 'unpublish posts']);

        $role = Role::create(['name' => 'admin']);
        $role->givePermissionTo(Permission::all());
    }
}
```

Update your `DatabaseSeeder.php` to include this new seeder:

```php
public function run()
{
    $this->call([
        RolesAndPermissionsSeeder::class,
    ]);
}
```

Run the seeder:

```bash
php artisan db:seed
```

This will create roles for `writer`, `editor`, and `admin`, along with permissions for viewing, creating, editing, deleting, publishing, and unpublishing posts.

### Updating the User Model

To use the package, your `User` model should use the `HasRoles` trait. Update your `app/Models/User.php`:

```php {1-1}
use Spatie\Permission\Traits\HasRoles;

class User extends Authenticatable
{
    use HasFactory, Notifiable, HasRoles;

    // ...
}
```

This trait provides methods for assigning and checking roles and permissions for users in your application.

### Implementing RBAC in Controllers

Now, let's update our `PostController` to use the new permissions:

```php {10-17}
<?php

namespace App\Http\Controllers;

use App\Models\Post;
use Illuminate\Http\Request;

class PostController extends Controller
{
    public function __construct()
    {
        $this->middleware('permission:view posts')->only('index', 'show');
        $this->middleware('permission:create posts')->only('create', 'store');
        $this->middleware('permission:edit posts')->only('edit', 'update');
        $this->middleware('permission:delete posts')->only('destroy');
        $this->middleware('permission:publish posts')->only('publish');
    }

    // ... other methods remain the same

    public function publish(Post $post)
    {
        $post->update(['is_published' => true]);
        return redirect()->route('posts.show', $post);
    }
}
```

The `__construct` method now uses middleware to check permissions for each controller action. This ensures that only users with the appropriate permissions can access the corresponding methods.

### Using RBAC in Blade Templates

You can use the package's directives in your Blade templates to show or hide elements based on the user's roles and permissions:

```html
@can('edit posts')
<a href="{{ route('posts.edit', $post) }}">Edit Post</a>
@endcan @role('admin')
<a href="{{ route('admin.dashboard') }}">Admin Dashboard</a>
@endrole @hasanyrole('writer|editor')
<a href="{{ route('posts.create') }}">Create New Post</a>
@endhasanyrole
```

You can also use the `@canany` directive to check if the user has any of the specified permissions:

```html
@canany(['edit posts', 'delete posts'])
<form action="{{ route('posts.destroy', $post) }}" method="POST">
  @csrf @method('DELETE')
  <button type="submit">Delete Post</button>
</form>
@endcanany
```

### Dynamic Role and Permission Assignment

Besides seeding roles and permissions, you can also assign roles and permissions dynamically based on user actions. For example, you might assign the `writer` role to users who have published a certain number of posts.

Here's an example of how you can assign roles and permissions dynamically in your controllers:

```php
public function assignRole(User $user, Request $request)
{
    $validatedData = $request->validate([
        'role' => 'required|exists:roles,name',
    ]);

    $user->assignRole($validatedData['role']);

    return back()->with('success', 'Role assigned successfully');
}

public function revokeRole(User $user, Request $request)
{
    $validatedData = $request->validate([
        'role' => 'required|exists:roles,name',
    ]);

    $user->removeRole($validatedData['role']);

    return back()->with('success', 'Role revoked successfully');
}
```

Besides the `removeRole` and `assignRole` methods, the package provides other methods for managing roles and permissions, such as `syncRoles`, `givePermissionTo`, and `revokePermissionTo` for more advanced use cases.

### Optimizing RBAC Performance with Neon Postgres

When working with RBAC, especially in larger applications, you might encounter performance issues due to the increased number of database queries.

Here are some tips to optimize performance when using Spatie Laravel Permission with Neon Postgres:

1. **Caching**: Enable caching in the package's configuration to reduce database queries:

   ```php
   // In config/permission.php
   'cache' => [
       'expiration_time' => \DateInterval::createFromDateString('24 hours'),
       'key' => 'spatie.permission.cache',
       'model_key' => 'name',
       'store' => 'default',
   ],
   ```

2. **Eager Loading**: When fetching users with their roles and permissions, use eager loading:

   ```php
   $users = User::with('roles', 'permissions')->get();
   ```

3. **Indexing**: Ensure that the `model_id` and `model_type` columns in the `model_has_roles` and `model_has_permissions` tables are properly indexed. For more information on indexing, refer to the [Neon Postgres documentation](https://neon.tech/docs/postgres/indexes).

4. **Minimize Permission Checks**: Instead of checking individual permissions, consider using roles or permission groups to reduce the number of checks you do on each request.

5. **Use Database-Level Permissions**: For very large-scale applications, consider implementing some permissions at the database level using [Neon Postgres's role-based access control features](https://neon.tech/blog/the-non-obviousness-of-postgres-roles).

## Conclusion

In this guide, we've implemented a fine-grained authorization system in Laravel using Policies and Gates. We've covered creating and registering policies, implementing role-based access control, using authorization in controllers and views, and testing our authorization rules.

This implementation provides a solid foundation for a secure application, but there are always ways to enhance and expand its functionality.

For more complex applications, Spatie's Laravel Permission package provides a flexible way to implement advanced RBAC in your Laravel application.

## Additional Resources

- [Laravel Authorization Documentation](https://laravel.com/docs/11.x/authorization)
- [Laravel Policies](https://laravel.com/docs/11.x/authorization#creating-policies)
- [Laravel Gates](https://laravel.com/docs/11.x/authorization#gates)
- [Spatie Laravel Permission Documentation](https://spatie.be/docs/laravel-permission)
- [Neon Postgres Documentation](/docs)


# Building a CRUD API with Laravel and Sanctum

---
title: Building a CRUD API with Laravel and Sanctum
subtitle: Learn how to create a robust, secure CRUD API using Laravel and Laravel Sanctum for authentication
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-07-01T00:00:00.000Z'
updatedOn: '2024-07-01T00:00:00.000Z'
---

Laravel is a powerful PHP framework that allows developers to easily build web applications and APIs.

In this guide, we'll walk through the process of creating a CRUD (Create, Read, Update, Delete) API using Laravel, and we'll implement authentication using [Laravel Sanctum](https://laravel.com/docs/11.x/sanctum).

By the end of this tutorial, you'll have a fully functional API that allows authenticated users to perform CRUD operations on a resource. We'll use a 'Task' model as our example resource and implement the necessary endpoints to manage tasks.

## Prerequisites

Before we begin, ensure you have the following:

- PHP 8.1 or higher installed on your system
- [Composer](https://getcomposer.org/) for managing PHP dependencies
- A [Neon](https://console.neon.tech/signup) account for database hosting
- Basic knowledge of Laravel and RESTful API principles

## Setting up the Project

Let's start by creating a new Laravel project and setting up the necessary components.

### Creating a New Laravel Project

Open your terminal and run the following command to create a new Laravel project:

```bash
composer create-project laravel/laravel laravel-crud-api
cd laravel-crud-api
```

This will create a new Laravel project in a directory named `laravel-crud-api` and install all the necessary dependencies.

### Setting up the Database

Update your `.env` file with your Neon database credentials:

```env
DB_CONNECTION=pgsql
DB_HOST=your-neon-hostname.neon.tech
DB_PORT=5432
DB_DATABASE=your_database_name
DB_USERNAME=your_username
DB_PASSWORD=your_password
```

Make sure to replace `your-neon-hostname`, `your_database_name`, `your_username`, and `your_password` with your actual database credentials.

### Installing Laravel Sanctum

Laravel Sanctum provides a featherweight authentication system for SPAs and simple APIs. To install it, all you need to do is use the following command:

```bash
php artisan install:api
```

If you get asked to run all pending migrations, type `yes` and press Enter. Else, run the migrations to create the necessary tables:

```bash
php artisan migrate
```

This will create the necessary tables for Sanctum to work.

## Creating the Task Model and Migration

As mentioned earlier, for our CRUD API, we'll use a 'Task' model as our example resource. This model will have fields such as title, description, status, due date, and priority.

Let's create it along with its migration file:

```bash
php artisan make:model Task -m
```

Once created, open the newly created migration file in `database/migrations` and update it to include the necessary columns for the 'tasks' table:

```php
public function up()
{
    Schema::create('tasks', function (Blueprint $table) {
        $table->id();
        $table->foreignId('user_id')->constrained()->onDelete('cascade');
        $table->string('title');
        $table->text('description')->nullable();
        $table->enum('status', ['pending', 'in_progress', 'completed'])->default('pending');
        $table->date('due_date')->nullable();
        $table->integer('priority')->default(1);
        $table->timestamps();
    });
}
```

We've defined a foreign key `user_id` to associate each task with a user. This allows us to implement user-specific tasks and ensure that each task belongs to a specific user. The `constrained()` method creates a foreign key constraint that references the `id` column of the `users` table. The `onDelete('cascade')` method ensures that when a user is deleted, all associated tasks are also deleted.

Run the migration to create the 'tasks' table:

```bash
php artisan migrate
```

After that, update the `app/Models/Task.php` model file to include the necessary fields in the `$fillable` array:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class Task extends Model
{
    use HasFactory;

    protected $fillable = ['title', 'description', 'status', 'due_date', 'priority'];

    public function user()
    {
        return $this->belongsTo(User::class);
    }
}
```

As a reference, the `fillable` property specifies which fields can be mass-assigned when creating or updating a model. This helps protect against mass assignment vulnerabilities and ensures that only the specified fields can be modified.

## Implementing API Authentication

Before we create our CRUD endpoints, let's set up authentication using Laravel Sanctum. This will allow users to register, log in, and access protected routes in our API.

### Creating the User Registration and Login Controllers

By default, Laravel comes with a few route groups like `web`, `api`, and `auth`. We'll use the `api` group for our API routes which will be protected by Sanctum.

Start by creating a controller called `AuthController` to handle user registration and login using the artisan command:

```bash
php artisan make:controller Api/AuthController
```

Then, update the `app/Http/Controllers/Api/AuthController.php` file and add the necessary methods:

```php
<?php

namespace App\Http\Controllers\Api;

use App\Http\Controllers\Controller;
use App\Models\User;
use Illuminate\Http\Request;
use Illuminate\Support\Facades\Hash;
use Illuminate\Validation\ValidationException;

class AuthController extends Controller
{
    public function register(Request $request)
    {
        $request->validate([
            'name' => 'required|string|max:255',
            'email' => 'required|string|email|max:255|unique:users',
            'password' => 'required|string|min:8|confirmed',
        ]);

        $user = User::create([
            'name' => $request->name,
            'email' => $request->email,
            'password' => Hash::make($request->password),
        ]);

        $token = $user->createToken('auth_token')->plainTextToken;

        return response()->json([
            'access_token' => $token,
            'token_type' => 'Bearer',
        ]);
    }

    public function login(Request $request)
    {
        $request->validate([
            'email' => 'required|email',
            'password' => 'required',
        ]);

        $user = User::where('email', $request->email)->first();

        if (! $user || ! Hash::check($request->password, $user->password)) {
            throw ValidationException::withMessages([
                'email' => ['The provided credentials are incorrect.'],
            ]);
        }

        $token = $user->createToken('auth_token')->plainTextToken;

        return response()->json([
            'access_token' => $token,
            'token_type' => 'Bearer',
        ]);
    }

    public function logout(Request $request)
    {
        $request->user()->currentAccessToken()->delete();

        return response()->json(['message' => 'Logged out successfully']);
    }
}
```

Rundown of the methods in the `AuthController`:

- `register`: Handles user registration. Validates the request data, creates a new user, and returns an access token.
- `login`: Handles user login. Validates the request data, checks the user credentials, and returns an access token.
- `logout`: Logs out the authenticated user by deleting the current access token.

### Setting up Authentication Routes

Update `routes/api.php` to include the authentication routes within the `api` route group:

```php
<?php

use App\Http\Controllers\Api\AuthController;
use Illuminate\Support\Facades\Route;

Route::post('/register', [AuthController::class, 'register']);
Route::post('/login', [AuthController::class, 'login']);

Route::middleware('auth:sanctum')->group(function () {
    Route::post('/logout', [AuthController::class, 'logout']);
    // We'll add our task routes here later
});
```

The `auth:sanctum` middleware protects the routes by requiring a valid access token. This ensures that only authenticated users can access the protected routes.

The `/register` and `/login` routes are public and do not require authentication. Users can register and log in to obtain an access token.

### Issuing API Tokens

To issue API tokens, we need to update the `User` model to use the `HasApiTokens` trait. Laravel ships with a default `User` model located at `app/Models/User.php`.

Let's update the `app/Models/User.php` file and add the `HasApiTokens` trait:

```php
<?php

// Existing imports
use Laravel\Sanctum\HasApiTokens;

class User extends Authenticatable
{
    // Add the HasApiTokens trait here
    use HasApiTokens, HasFactory, Notifiable;

    // Existing code
}
```

After adding the `HasApiTokens` trait, you can use the `createToken` method to generate an access token for a user. We've used this method in the `AuthController` to issue tokens during registration and login.

While we're here, let's also update the `User` model to include a relationship with the `Task` model:

```php
public function tasks()
{
    return $this->hasMany(Task::class);
}
```

This relationship allows us to retrieve all tasks associated with a user and create new tasks for a user, simplifying the task management process.

### Testing the Authentication Endpoints

To test the authentication endpoints, you can use a tool like [Postman](https://www.postman.com/) or [Insomnia](https://insomnia.rest/).

For the sake of simplicity, you can use the `curl` command in your terminal.

Let's start by testing the registration route and try to register a new user:

```bash
curl -X POST http://laravel-crud-api.test/api/register \
    -H 'Content-Type: application/json' \
    -d '{
        "name": "John Doe",
        "email": "john@example.com",
        "password": "password",
        "password_confirmation": "password"
    }'
```

> Note: Replace `laravel-crud-api.test` with your Laravel project URL.

The response should include an access token like this:

```json
{
  "access_token": "1|eyJ...your_access_token_here",
  "token_type": "Bearer"
}
```

To log in with the registered user:

```bash
curl -X POST http://laravel-crud-api.test/api/login \
    -H 'Content-Type: application/json' \
    -d '{
        "email": "john@example.com",
        "password": "password"
    }'
```

This will return another access token:

```json
{
  "access_token": "1|eyJ...your_new_access_token_here",
  "token_type": "Bearer"
}
```

With the access token, you can now access the protected routes. To log out, use the `/logout` route:

```bash
curl -X POST http://laravel-crud-api.test/api/logout \
    -H 'Authorization: Bearer <your_access_token_here>'
```

Replace `<your_access_token_here>` with the access token you received during login. This will log out the user and delete the access token and you will get a response like:

```json
{
  "message": "Logged out successfully"
}
```

## Implementing CRUD Operations

Now that we have authentication set up, let's create our CRUD operations for the Task model.

### Creating the TaskController

Generate a new controller for handling task operations:

```bash
php artisan make:controller Api/TaskController --api
```

The `--api` flag generates a controller with the necessary methods for a RESTful API. This will create a new controller file at `app/Http/Controllers/Api/TaskController.php` and will include methods like `index`, `store`, `show`, `update`, and `destroy`.

After that, update `app/Http/Controllers/Api/TaskController.php` and populate those methods with the necessary logic:

```php
<?php

namespace App\Http\Controllers\Api;

use App\Http\Controllers\Controller;
use App\Models\Task;
use Illuminate\Http\Request;

class TaskController extends Controller
{
    public function index()
    {
        $tasks = Task::all();
        return response()->json($tasks);
    }

    public function store(Request $request)
    {
        $request->validate([
            'title' => 'required|string|max:255',
            'description' => 'nullable|string',
            'status' => 'required|in:pending,in_progress,completed',
            'due_date' => 'nullable|date',
            'priority' => 'required|integer|min:1|max:5',
        ]);

        $task = $request->user()->tasks()->create($request->all());
        return new TaskResource($task);
    }

    public function show(Task $task)
    {
        return response()->json($task);
    }

    public function update(Request $request, Task $task)
    {
        $this->authorize('update', $task);

        $validated = $request->validate([
            'title' => 'sometimes|required|string|max:255',
            'description' => 'nullable|string',
            'status' => 'sometimes|required|in:pending,in_progress,completed',
            'due_date' => 'nullable|date',
            'priority' => 'sometimes|required|integer|min:1|max:5',
        ]);

        $task->update($validated);

        return new TaskResource($task);
    }

    public function destroy(Task $task)
    {
        $task->delete();
        return response()->json(null, 204);
    }
}
```

> Note: We'll create the `TaskResource` class later to transform the task model into a JSON response.

Rundown of the methods in the `TaskController`:

- `index`: Fetches all tasks. Returns a JSON response with all tasks. We'll update this method to use API Resources later.
- `store`: Creates a new task. Validates the request data, creates a new task, and returns the task as JSON.
- `show`: Fetches a single task. Returns a JSON response with the specified task.
- `update`: Updates a task. Validates the request data, updates the task, and returns the updated task as JSON.
- `destroy`: Deletes a task. Deletes the specified task and returns a 204 No Content response.

### Adding Task Routes

Once we have the `TaskController` set up, let's add the task routes to `routes/api.php` to include the task routes within the authenticated group:

```php
// Include the TaskController at the top:
use App\Http\Controllers\Api\TaskController;

Route::middleware('auth:sanctum')->group(function () {
    Route::post('/logout', [AuthController::class, 'logout']);

    // Add the task routes here:
    Route::apiResource('tasks', TaskController::class);
});
```

The `Route::apiResource` method automatically generates the necessary routes for a RESTful resource. This will create routes for `tasks` with the appropriate HTTP verbs and route names.

You can use the `php artisan route:list` command to see a list of all registered routes.

## Implementing API Resources

To provide a consistent and customizable way of transforming our models into JSON responses, let's use Laravel's API Resources.

A resource class represents a single model that needs to be transformed into a JSON structure. It allows you to customize the data that is returned when a model is converted to JSON rather than returning the entire model instance.

Generate a new resource for the Task model:

```bash
php artisan make:resource TaskResource
```

Open the `app/Http/Resources/TaskResource.php` file and update it as follows:

```php
<?php

namespace App\Http\Resources;

use Illuminate\Http\Request;
use Illuminate\Http\Resources\Json\JsonResource;

class TaskResource extends JsonResource
{
    public function toArray(Request $request): array
    {
        return [
            'id' => $this->id,
            'title' => $this->title,
            'description' => $this->description,
            'status' => $this->status,
            'due_date' => $this->due_date,
            'priority' => $this->priority,
            'created_at' => $this->created_at,
            'updated_at' => $this->updated_at,
        ];
    }
}
```

Here, we've defined the fields that should be included in the JSON response for a task and how they should be formatted. For more complex transformations, you can customize the `toArray` method as needed.

Now, update the `TaskController` to use this resource when returning task data instead of returning the raw model:

```php
<?php

namespace App\Http\Controllers\Api;

use App\Http\Resources\TaskResource;
use App\Http\Controllers\Controller;
use App\Models\Task;
use Illuminate\Http\Request;

class TaskController extends Controller
{
    public function index()
    {
        $tasks = Task::all();
        // Change this line to use the TaskResource:
        return TaskResource::collection($tasks);
    }

    public function store(Request $request)
    {
        $validated = $request->validate([
            'title' => 'required|string|max:255',
            'description' => 'nullable|string',
            'status' => 'required|in:pending,in_progress,completed',
            'due_date' => 'nullable|date',
            'priority' => 'required|integer|min:1|max:5',
        ]);

        $task = $request->user()->tasks()->create($validated);

        return new TaskResource($task);
    }

    public function show(Task $task)
    {
        return new TaskResource($task);
    }

    public function update(Request $request, Task $task)
    {
        $request->validate([
            'title' => 'required|string|max:255',
            'description' => 'nullable|string',
            'status' => 'required|in:pending,in_progress,completed',
            'due_date' => 'nullable|date',
            'priority' => 'required|integer|min:1|max:5',
        ]);

        $task->update($request->all());
        return new TaskResource($task);
    }

    public function destroy(Task $task)
    {
        $task->delete();
        return response()->json(null, 204);
    }
}
```

Now, when you fetch tasks, create a new task, or update an existing task, the response will be formatted according to the `TaskResource` class.

## Testing the API Endpoints

To test the new Task API, you can again use `curl` or a tool like Postman or Insomnia.

Let's first create a new task:

```bash
curl -X POST http://laravel-crud-api.test/api/tasks \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer <your_access_token_here>' \
    -d '{
        "title": "New Task",
        "description": "Task description",
        "status": "pending",
        "due_date": "2024-12-31",
        "priority": 2
    }'
```

As a response, you should see the newly created task:

```json
{
  "id": 1,
  "title": "New Task",
  "description": "Task description",
  "status": "pending",
  "due_date": "2024-12-31",
  "priority": 2,
  "created_at": "2024-07-01T00:00:00.000000Z",
  "updated_at": "2024-07-01T00:00:00.000000Z"
}
```

You can then fetch all tasks:

```bash
curl -X GET http://laravel-crud-api.test/api/tasks \
    -H 'Authorization: Bearer <your_access_token_here>'
```

This will return a list of tasks in JSON format.

## Adding Pagination

To improve performance and reduce payload size, we can add pagination to the task list. Laravel provides a simple way to paginate query results using the `paginate` method when fetching data. That way the response will include only a subset of tasks per page instead of the entire collection which can be large depending on the number of entries in the database.

To do that, update the `index` method in `TaskController` and change the `all` method to `paginate` followed by the number of items per page:

```php
public function index()
{
    $tasks = Task::paginate(15);
    return TaskResource::collection($tasks);
}
```

This pagination method will limit the number of tasks returned to 15 per page, significantly reducing the payload size for large datasets.

The response will now include additional pagination metadata such as the total number of items, the number of pages, and links to the next and previous pages, allowing for easy navigation through the entire collection of tasks.

## Implementing Request Classes

Request classes allow you to encapsulate request validation logic within dedicated classes. This helps keep your controller clean and improves reusability.

To keep our controller clean and improve reusability, let's create dedicated request classes for validation for creating and updating tasks:

```bash
php artisan make:request StoreTaskRequest
php artisan make:request UpdateTaskRequest
```

Update `app/Http/Requests/StoreTaskRequest.php` to include the validation rules:

```php
<?php

namespace App\Http\Requests;

use Illuminate\Foundation\Http\FormRequest;

class StoreTaskRequest extends FormRequest
{
    public function authorize()
    {
        return true;
    }

    public function rules()
    {
        return [
            'title' => 'required|string|max:255',
            'description' => 'nullable|string',
            'status' => 'required|in:pending,in_progress,completed',
            'due_date' => 'nullable|date',
            'priority' => 'required|integer|min:1|max:5',
        ];
    }
}
```

Update `app/Http/Requests/UpdateTaskRequest.php` the same way:

```php
<?php

namespace App\Http\Requests;

use Illuminate\Foundation\Http\FormRequest;

class UpdateTaskRequest extends FormRequest
{
    public function authorize()
    {
        return true;
    }

    public function rules()
    {
        return [
            'title' => 'required|string|max:255',
            'description' => 'nullable|string',
            'status' => 'required|in:pending,in_progress,completed',
            'due_date' => 'nullable|date',
            'priority' => 'required|integer|min:1|max:5',
        ];
    }
}
```

Now, we are ready to update the `TaskController` to use these request classes instead of validating the request directly in the controller methods:

```php
// Include the request classes at the top:
use App\Http\Requests\StoreTaskRequest;
use App\Http\Requests\UpdateTaskRequest;

class TaskController extends Controller
{
    // ... other methods ...

    public function store(StoreTaskRequest $request)
    {
        $task = $request->user()->tasks()->create($request->validated());
        return new TaskResource($task);
    }

    public function update(UpdateTaskRequest $request, Task $task)
    {
        $task->update($request->validated());
        return new TaskResource($task);
    }

    // ... other methods ...
}
```

The end-user will still receive the same JSON response, but the validation logic is now encapsulated within the request classes. This makes the controller cleaner and easier to maintain.

## Testing the API

To ensure our API works as expected, Laravel provides a powerful testing suite out of the box.

To learn more about testing in Laravel along with Neon branding, check out the [Testing Laravel Applications with Neon's Database Branching](https://neon.tech/guides/laravel-test-on-branch).

## Adding API Documentation

For better developer experience, it's important to have good API documentation.

You can use a third-party package called [Scribe](https://scribe.knuckles.wtf/) to generate your API documentation.

To install Scribe, run the following command:

```bash
composer require --dev knuckleswtf/scribe
```

Publish the configuration file:

```bash
php artisan vendor:publish --tag=scribe-config
```

Then update the `config/scribe.php` file to configure the documentation settings according to your preferences.

Generate the documentation:

```bash
php artisan scribe:generate
```

This will create a `public/docs` directory with the generated API documentation. You can access it by visiting `http://laravel-crud-api.test/docs`. The generated format will also include Postman collections and OpenAPI specifications.

## Implementing API Versioning

As your API evolves, you might need to introduce breaking changes. API versioning allows you to do this without affecting existing clients.

To implement a simple versioning strategy, you can prefix your API routes with a version number. This way, you can maintain backward compatibility while introducing new features in future versions.

To do that, create a new directory for v1 of your API:

```bash
mkdir app/Http/Controllers/Api/V1
```

Move your `TaskController.php` to this new directory and update its namespace:

```php
namespace App\Http\Controllers\Api\V1;
```

Then update your `routes/api.php` file to include versioning and the new namespace:

```php
// Update the TaskController import at the top:
use App\Http\Controllers\Api\V1\TaskController;

// Update the routes to include the version prefix:
Route::prefix('v1')->group(function () {
    Route::post('/register', [AuthController::class, 'register']);
    Route::post('/login', [AuthController::class, 'login']);

    Route::middleware('auth:sanctum')->group(function () {
        Route::post('/logout', [AuthController::class, 'logout']);
        Route::apiResource('tasks', \App\Http\Controllers\Api\V1\TaskController::class);
    });
});
```

Now, your API endpoints will be prefixed with `/api/v1`. This allows you to introduce breaking changes in future versions without affecting existing clients.

Later on, you can create a new version (e.g., `v2`) and update the routes accordingly to maintain backward compatibility.

## Implementing Caching

To improve performance, especially for frequently accessed and rarely changing data, it is a good idea to implement caching for our task list.

This can significantly reduce the response time and server load and reduce the number of database queries putting less pressure on the database.

As an example, let's implement that for the `index` method in `TaskController`:

```php
// Include the Cache facade at the top:
use Illuminate\Support\Facades\Cache;

public function index()
{
    // Use the Cache facade to store the tasks for one hour
    $tasks = Cache::remember('tasks', 3600, function () {
        return Task::paginate(15);
    });

    return TaskResource::collection($tasks);
}
```

This caches the task list for one hour. Remember to clear the cache when tasks are updated, created, or deleted.

## Conclusion

In this guide, we've walked through the process of building a simple CRUD API with Laravel, secured with Laravel Sanctum for authentication.

We've covered setting up the project, configuring the database with Neon, and implementing CRUD operations for a Task model. We also added essential features such as API versioning, API documentation, and caching to improve the API's performance, security, and maintainability.

By following these steps, you now have a fully functional API that allows authenticated users to manage tasks effectively. This can be used as the foundation for more complex applications and extended with additional features as needed.

As next steps you can think about adding more features to the API, such as search, filtering, sorting, and more advanced authentication and authorization mechanisms.

Additionally, it is a good idea to implement throttling to protect your API from abuse and to ensure fair usage.

## Additional Resources

- [Laravel Documentation](https://laravel.com/docs)
- [Laravel Sanctum Documentation](https://laravel.com/docs/sanctum)
- [Scribe Documentation](https://scribe.knuckles.wtf/laravel)
- [Neon Documentation](/docs)


# Getting Started with Laravel Events and Listeners

---
title: Getting Started with Laravel Events and Listeners
subtitle: Learn how to implement and utilize Laravel's event system with Neon
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-06-30T00:00:00.000Z'
updatedOn: '2024-06-30T00:00:00.000Z'
---

Laravel's event system provides a simple observer implementation, allowing you to subscribe and listen for various events that occur in your application.

This can be particularly useful for decoupling various parts of your application's logic and not blocking the main request flow. Queued listeners can also be used to handle time-consuming tasks asynchronously, improving the performance of your application.

In this guide, we'll walk through the process of setting up and using Laravel Events and Listeners, with a focus on database operations using Neon Postgres.

## Prerequisites

Before we begin, ensure you have the following:

- PHP 8.1 or higher installed on your system
- [Composer](https://getcomposer.org/) for managing PHP dependencies
- A [Neon](https://console.neon.tech/signup) account for database hosting
- Basic knowledge of Laravel and database operations

## Setting up the Project

Let's start by creating a new Laravel project and setting up the necessary components. If you already have a Laravel project set up, you can skip this section.

### Creating a New Laravel Project

Open your terminal and run the following command to create a new Laravel project:

```bash
composer create-project laravel/laravel laravel-events
cd laravel-events
```

This will create a new Laravel project in a directory named `laravel-events` with all the necessary dependencies installed.

### Setting up the Database

Once you have your Laravel project set up, you'll need to configure your Neon database connection. If you don't have a Neon account, you can sign up [here](https://console.neon.tech/signup).

Update your `.env` file with your Neon database credentials:

```env
DB_CONNECTION=pgsql
DB_HOST=your-neon-hostname.neon.tech
DB_PORT=5432
DB_DATABASE=your_database_name
DB_USERNAME=your_username
DB_PASSWORD=your_password
```

Make sure to replace `your-neon-hostname`, `your_database_name`, `your_username`, and `your_password` with your actual Neon database credentials.

## Creating a Model and Migration

For this tutorial, let's create a simple `Order` model that we'll use to demonstrate events and listeners.

To create the model and migration, run the following command:

```bash
php artisan make:model Order -m
```

This command creates both the `Order` model and a migration file for the `orders` table.

Open the newly created migration file in `database/migrations` and update the `up` method with the following content:

```php
public function up()
{
    Schema::create('orders', function (Blueprint $table) {
        $table->id();
        $table->string('customer_name');
        $table->decimal('total', 8, 2);
        $table->enum('status', ['pending', 'processing', 'completed', 'cancelled'])->default('pending');
        $table->timestamps();
    });
}
```

Run the migration to create the 'orders' table in your Neon database:

```bash
php artisan migrate
```

Next update the `Order` model in `app/Models/Order.php` to include the `customer_name`, `total`, and `status` to the `$fillable` property:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class Order extends Model
{
    use HasFactory;

    protected $fillable = ['customer_name', 'total', 'status'];
}
```

As a reference, the `$fillable` property specifies which attributes are mass-assignable, meaning they can be set using the `create` method on the model. This helps protect against mass assignment vulnerabilities in your application.

## Creating an Event

An event in Laravel is a simple class that represents something that has happened in your application. Events can be used to trigger actions or notify other parts of your application that something has occurred.

Now, let's create an event that will be triggered when an order is placed. Run the following command:

```bash
php artisan make:event OrderPlaced
```

This creates a new event class in `app/Events/OrderPlaced.php`. Update it with the following content:

```php
<?php

namespace App\Events;

use App\Models\Order;
use Illuminate\Broadcasting\Channel;
use Illuminate\Broadcasting\InteractsWithSockets;
use Illuminate\Broadcasting\PresenceChannel;
use Illuminate\Broadcasting\PrivateChannel;
use Illuminate\Contracts\Broadcasting\ShouldBroadcast;
use Illuminate\Foundation\Events\Dispatchable;
use Illuminate\Queue\SerializesModels;

class OrderPlaced
{
    use Dispatchable, InteractsWithSockets, SerializesModels;

    public $order;

    public function __construct(Order $order)
    {
        $this->order = $order;
    }
}
```

This event will carry the `Order` model instance, allowing listeners to access the order details. In a real application, you would perform additional actions, such as sending an email or updating other records in the database, when this event is triggered.

## Creating a Listener

A listener in Laravel is a class that listens for a specific event and performs actions in response to that event.

Now that we have an event, let's create a listener that will respond to this event.

Run the following command to create a new listener:

```bash
php artisan make:listener SendOrderConfirmation --event=OrderPlaced
```

This creates a new listener in `app/Listeners/SendOrderConfirmation.php`. Update it with the following content:

```php
<?php

namespace App\Listeners;

use App\Events\OrderPlaced;
use Illuminate\Contracts\Queue\ShouldQueue;
use Illuminate\Queue\InteractsWithQueue;
use Illuminate\Support\Facades\Log;

class SendOrderConfirmation implements ShouldQueue
{
    use InteractsWithQueue;

    public function handle(OrderPlaced $event)
    {
        // In a real application, you would send an email here
        Log::info('Order confirmation sent for Order #' . $event->order->id);

        // Additional actions related to order confirmation
        Log::info('Slack notification sent for Order #' . $event->order->id);
        Log::info('SMS notification sent for Order #' . $event->order->id);
        Log::info('Update inventory for Order #' . $event->order->id);
    }
}
```

A very important thing to note here is that we are implementing the `ShouldQueue` interface.

Here our listener implements the `ShouldQueue` interface, meaning it will be handled by [Laravel's queue system](https://laravel.com/docs/11.x/queues), which is beneficial for performance when dealing with time-consuming tasks.

If we didn't implement the `ShouldQueue` interface, the listener would be executed synchronously, which could slow down the response time of your application but could be useful for certain use cases where you need to perform actions synchronously rather than asynchronously.

Here, we're simply logging a message to the Laravel log file, but in a real application, you would send an email along with other actions related to order confirmation, which could be time-consuming.

## Registering the Event and Listener

Laravel 11.x and later versions automatically discover events and listeners, so you don't need to manually register them. If you are using an older version of Laravel, you can register your events and listeners in the `EventServiceProvider`.

For more information on registering events and listeners, refer to the [Laravel documentation](https://laravel.com/docs/events).

## Dispatching the Event

Now that we have set up our event and listener, let's create a simple controller to simulate an order placement. This controller will handle order creation and dispatch our event.

Run the following command to create a new controller:

```bash
php artisan make:controller OrderController
```

Open `app/Http/Controllers/OrderController.php` and add the following content:

```php
<?php

namespace App\Http\Controllers;

use App\Models\Order;
use App\Events\OrderPlaced;
use Illuminate\Http\Request;

class OrderController extends Controller
{
    public function store(Request $request)
    {
        $order = Order::create([
            'customer_name' => $request->customer_name,
            'total' => $request->total,
        ]);

        event(new OrderPlaced($order));

        return response()->json(['message' => 'Order placed successfully', 'order' => $order]);
    }
}
```

This controller creates a new order in the database and then dispatches the `OrderPlaced` event.

The `event` helper function is used to dispatch the event, passing the order instance to the event constructor.

This allows the listener to access the order details when the event is triggered rather than blocking the main request flow to perform the additional actions directly in the controller method itself.

We will use this controller to create a new order and trigger the event.

## Adding a Route

To use our new controller, let's add a route. Open `routes/api.php` and add the following line:

```php
Route::post('/orders', [App\Http\Controllers\OrderController::class, 'store']);
```

If you don't have a route file for API routes, you can create one by running the following command:

```bash
php artisan install:api
```

This will create a new `api.php` file in the `routes` directory.

## Testing the Event System

Now, let's test our event system. You can use a tool like Postman or `curl` to send a POST request to your `/api/orders` endpoint.

Using `curl`:

```bash
curl -X POST http://laravel-events.test/api/orders \
     -H "Content-Type: application/json" \
     -d '{"customer_name":"John Doe","total":99.99}'
```

Replace `laravel-events.test` with your actual application URL.

If everything is set up correctly, you should see a new order in your Neon database, but you won't see any log messages in your Laravel log file yet because the listener is queued and we haven't run the queue worker to process the queued jobs.

## Running Queued Jobs

As we mentioned earlier, the `SendOrderConfirmation` listener implements the `ShouldQueue` interface, meaning it will be handled by Laravel's queue system.

If you were to check your logs immediately after placing an order, you might not see the log messages from the listener. Instead, you can run the queue worker to process the queued jobs:

```bash
php artisan queue:work
```

This command starts the queue worker, which will process any queued jobs, including the order confirmation listener.

Once the queue worker is running, you should see the log messages from the listener and the following output:

```
$ php artisan queue:work

   INFO  Processing jobs from the [default] queue.

   App\Listeners\SendOrderConfirmation ....... RUNNING
   App\Listeners\SendOrderConfirmation ....... 1s DONE
```

In a different terminal window, you can place a new order using `curl` or Postman to see the listener in action.

If you were to remove the `ShouldQueue` interface from the listener, the actions would be executed synchronously, and you would see the log messages immediately after placing an order, but thanks to the queue system, the response time of your application is not affected.

The default `QUEUE_CONNECTION` in Laravel is `database`, which uses the database to manage the queue. This means that the queued jobs are stored in your Neon database and processed by the queue worker. You can change the queue connection in your `.env` file if you prefer a different queue driver like `redis` for example. To see all available queue drivers, refer to the [Laravel documentation](https://laravel.com/docs/11.x/queues#driver-prerequisites) or review the `config/queue.php` file within your Laravel project where you can configure the queue connection.

The jobs are queued in the `jobs` table in your database, which is usually created by default with new Laravel installations, or you can run the migration to create the table:

```bash
php artisan queue:table
php artisan migrate
```

## Using Database Transactions with Events

When working with database operations and events, it's important to understand how Laravel handles queued event listeners within database transactions. That way you can make sure that your data remains consistent and that your listeners are triggered at the right time.

Let's update our `OrderController` and `SendOrderConfirmation` listener to handle this correctly:

```php
<?php

namespace App\Http\Controllers;

use App\Models\Order;
use App\Events\OrderPlaced;
use Illuminate\Http\Request;
use Illuminate\Support\Facades\DB;

class OrderController extends Controller
{
    public function store(Request $request)
    {
        $order = DB::transaction(function () use ($request) {
            $order = Order::create([
                'customer_name' => $request->customer_name,
                'total' => $request->total,
            ]);

            event(new OrderPlaced($order));

            return $order;
        });

        return response()->json(['message' => 'Order placed successfully', 'order' => $order]);
    }
}
```

Now, let's update our `SendOrderConfirmation` listener to ensure it handles the event after the database transaction has been committed:

```php
<?php

namespace App\Listeners;

use App\Events\OrderPlaced;
use Illuminate\Contracts\Queue\ShouldQueue;
use Illuminate\Contracts\Events\ShouldHandleEventsAfterCommit;
use Illuminate\Queue\InteractsWithQueue;
use Illuminate\Support\Facades\Log;

class SendOrderConfirmation implements ShouldQueue, ShouldHandleEventsAfterCommit
{
    use InteractsWithQueue;

    public function handle(OrderPlaced $event)
    {
        // In a real application, you would send an email here
        Log::info('Order confirmation sent for Order #' . $event->order->id);
    }
}
```

By implementing the `ShouldHandleEventsAfterCommit` interface, we're telling Laravel to only process this listener after all open database transactions have been committed. This is crucial when your listener depends on database changes made within the transaction.

This approach ensures that:

1. The order is created in the database.
2. The `OrderPlaced` event is dispatched within the transaction.
3. The transaction is committed, saving the order to the Neon database.
4. Only after the transaction is successfully committed, the `SendOrderConfirmation` listener is processed.

This prevents potential issues where the listener might try to access data that hasn't been committed to the database yet, ensuring data consistency between your event processing and your Neon database state.

If your queue connection's `after_commit` configuration option is set to `true` in your `config/queue.php` file, all of your queued listeners will automatically wait for open database transactions to commit before they are processed, and you won't need to use the `ShouldHandleEventsAfterCommit` interface.

## Conclusion

In this guide, we've explored how to implement and utilize Laravel's event system, focusing on database operations with Neon as our database provider. We've covered creating and dispatching events, creating and registering listeners, and how to use database transactions with events.

Events and listeners provide a powerful way to decouple various aspects of your application, making your code more maintainable and scalable without blocking the main request flow for time-consuming tasks.

As a next step, you might want to look into implementing [Supervisor](https://laravel.com/docs/11.x/queues#supervisor-configuration) to manage your queue workers in a production environment and [Laravel Horizon](https://laravel.com/docs/11.x/horizon) for monitoring and managing your queues rather than using the `queue:work` command directly.

## Additional Resources

- [Laravel Events Documentation](https://laravel.com/docs/events)
- [Laravel Queues Documentation](https://laravel.com/docs/queues)
- [Neon Documentation](/docs)


# Building a Blog with Laravel, Livewire, and Laravel Breeze

---
title: Building a Blog with Laravel, Livewire, and Laravel Breeze
subtitle: Learn how to create a dynamic blog application using Laravel, Livewire, and Laravel Breeze for authentication and Neon.
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-06-30T00:00:00.000Z'
updatedOn: '2024-06-30T00:00:00.000Z'
---

Laravel is a powerful PHP framework that makes it easy to build web applications. When combined with Livewire, a full-stack framework for Laravel, you can create dynamic, reactive interfaces with minimal JavaScript. In this guide, we'll build a blog application using Laravel and Livewire, and we'll use Laravel Breeze to handle authentication, along with Neon Postgres.

By the end of this tutorial, you'll have a fully functional blog where users can create, read, update, and delete posts. We'll also implement comments and a simple tagging system.

## Prerequisites

Before we start, make sure you have the following:

- PHP 8.1 or higher installed on your system
- [Composer](https://getcomposer.org/) for managing PHP dependencies
- [Node.js](https://nodejs.org/) and [npm](https://www.npmjs.com/) for managing front-end assets
- A [Neon](https://console.neon.tech/signup) account for database hosting
- Basic knowledge of Laravel, Livewire, and Tailwind CSS

## Setting up the Project

Let's start by creating a new Laravel project and setting up the necessary components. We'll use Laravel Breeze for authentication, Livewire for building interactive components, and Tailwind CSS for styling.

### Creating a New Laravel Project

Open your terminal and run the following command to create a new Laravel project:

```bash
composer create-project laravel/laravel laravel-livewire-blog
cd laravel-livewire-blog
```

This command creates a new Laravel project in a directory named `laravel-livewire-blog` and installs all the necessary dependencies.

### Installing Laravel Breeze

[Laravel Breeze](https://laravel.com/docs/11.x/starter-kits) provides a minimal and simple starting point for building a Laravel application with authentication.

An alternative to Laravel Breeze is Laravel Jetstream, which provides more features out of the box, such as team management and two-factor authentication. However, for this tutorial, we'll use Laravel Breeze for its simplicity.

Let's install Laravel Breeze with the Blade views:

```bash
composer require laravel/breeze --dev
php artisan breeze:install blade
```

This command installs Breeze and sets up the necessary views and routes for authentication.

While in the terminal, also install the Livewire package:

```bash
composer require livewire/livewire
```

### Setting up the Database

Update your `.env` file with your Neon database credentials:

```env
DB_CONNECTION=pgsql
DB_HOST=your-neon-hostname.neon.tech
DB_PORT=5432
DB_DATABASE=your_database_name
DB_USERNAME=your_username
DB_PASSWORD=your_password
```

Make sure to replace `your-neon-hostname`, `your_database_name`, `your_username`, and `your_password` with your actual database details and save the file.

### Compiling Assets

Laravel Breeze uses Tailwind CSS for styling, so we need to compile the assets to generate the CSS file.

To compile the assets, run:

```bash
npm install
npm run dev
```

Keep the Vite development server running in the background as you continue with the next steps. This will automatically compile the assets when changes are made so you can see the updates in real-time.

## Creating the Blog Structure

Now that we have our basic setup, we are ready to create the structure for our blog, including models, migrations, and Livewire components, routes, policies, and views.

### Creating the Post Model and Migration

Models in Laravel are used to interact with the database using the Eloquent ORM. We'll create models for posts, comments, and tags, along with their respective migrations.

Run the following command to create a `Post` model with its migration:

```bash
php artisan make:model Post -m
```

Open the migration file in `database/migrations` and update it:

```php
public function up()
{
    Schema::create('posts', function (Blueprint $table) {
        $table->id();
        $table->foreignId('user_id')->constrained()->onDelete('cascade');
        $table->string('title');
        $table->string('slug')->unique();
        $table->text('content');
        $table->boolean('is_published')->default(false);
        $table->timestamp('published_at')->nullable();
        $table->timestamps();
    });
}
```

This migration creates a `posts` table with columns for the post title, content, publication status, and publication date. It also includes a foreign key to the `users` table for the post author. The `slug` column will be used to generate SEO-friendly URLs.

### Creating the Comment Model and Migration

Now, let's create a `Comment` model and its migration:

```bash
php artisan make:model Comment -m
```

The `comments` table will store the comments for each post, along with the user who made the comment, the post ID, and the comment content.

With that in mind, let's update the migration file:

```php
public function up()
{
    Schema::create('comments', function (Blueprint $table) {
        $table->id();
        $table->foreignId('user_id')->constrained()->onDelete('cascade');
        $table->foreignId('post_id')->constrained()->onDelete('cascade');
        $table->text('content');
        $table->timestamps();
    });
}
```

### Creating the Tag Model and Migration

To take this a step further, we can add a tagging system to our blog. This will allow us to categorize posts based on different topics.

```bash
php artisan make:model Tag -m
```

The `tags` table will store the tags that can be associated with posts. Update the migration file as follows:

```php
public function up()
{
    Schema::create('tags', function (Blueprint $table) {
        $table->id();
        $table->string('name')->unique();
        $table->timestamps();
    });
```

We'll also create a pivot table to manage the many-to-many relationship between posts and tags. The convention for naming this table is to combine the singular form of the related models in alphabetical order. In this case, the models are `Post` and `Tag`, so the pivot table will be named `post_tag`.

```bash
php artisan make:migration create_post_tag_table
```

Update the migration file as follows:

```php
    Schema::create('post_tag', function (Blueprint $table) {
        $table->id();
        $table->foreignId('post_id')->constrained()->onDelete('cascade');
        $table->foreignId('tag_id')->constrained()->onDelete('cascade');
        $table->unique(['post_id', 'tag_id']);
    });
}
```

We don't need to create a model for the pivot table, as it will be managed by Laravel's Eloquent ORM.

Now, run the migrations to create all the tables in the Neon database:

```bash
php artisan migrate
```

This command will create the `posts`, `comments`, `tags`, and `post_tag` tables in your database and keep track of the migrations that have been run. If you need to rollback the migrations, you can run `php artisan migrate:rollback` or if you were to add a new migration, you can run `php artisan migrate` and it will only run the new migrations.

### Updating the Models

Let's update our models to define the relationships. What we want to achieve is:

- A post **belongs** to a user
- A post **has many** comments
- A post can **have many** tags
- A comment **belongs to** a user
- A comment **belongs to** a post
- A tag can be associated with **many** posts

We already have that structure in our database, but we need to define these relationships in our models so we can access them easily in our application.

In `app/Models/Post.php` we define the relationships to the `User`, `Comment`, and `Tag` models:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class Post extends Model
{
    use HasFactory;

    protected $fillable = ['title', 'slug', 'content', 'is_published', 'published_at'];

    public function user()
    {
        // Using the `belongsTo` relationship to get the user who created the post
        return $this->belongsTo(User::class);
    }

    public function comments()
    {
        // Using the `hasMany` relationship to get all comments for a post
        return $this->hasMany(Comment::class);
    }

    public function tags()
    {
        // Using the `belongsToMany` relationship to get all tags associated with a post
        return $this->belongsToMany(Tag::class);
    }
}
```

In `app/Models/Comment.php` we define the relationships to the `User` and `Post` models so we can get the user and post associated with a comment:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class Comment extends Model
{
    use HasFactory;

    protected $fillable = ['content', 'user_id'];

    public function user()
    {
        return $this->belongsTo(User::class);
    }

    public function post()
    {
        return $this->belongsTo(Post::class);
    }
}
```

In `app/Models/Tag.php` we define the relationship to the `Post` model, this will allow us to get all posts associated with a tag:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class Tag extends Model
{
    use HasFactory;

    protected $fillable = ['name'];

    public function posts()
    {
        return $this->belongsToMany(Post::class);
    }
}
```

Finally, update `app/Models/User.php` to include the relationship with posts and comments, where a user can have many posts and many comments:

```php
public function posts()
{
    return $this->hasMany(Post::class);
}

public function comments()
{
    return $this->hasMany(Comment::class);
}
```

With these relationships defined, we can now easily access the related models and data using Eloquent.

### Seeding the Database

To populate the database with some sample data, let's create seeders for `Tag` models so we can associate tags with posts.

Create a seeder for the `Tag` model:

```bash
php artisan make:seeder TagSeeder
```

Update the seeder file in `database/seeders/TagSeeder.php`:

```php
<?php

namespace Database\Seeders;

use Illuminate\Database\Seeder;
use App\Models\Tag;

class TagSeeder extends Seeder
{
    public function run()
    {
        $tags = [
            'Postgres',
            'Neon',
            'Web Development',
            'Laravel',
            'PHP',
            'JavaScript',
            'Database',
            'Design',
            'UI/UX',
            'AI',
            'Machine Learning',
            'Cloud Computing',
            'DevOps',
            'Security',
        ];

        foreach ($tags as $tagName) {
            Tag::create(['name' => $tagName]);
        }
    }
}
```

Now, update the main `DatabaseSeeder` in `database/seeders/DatabaseSeeder.php` to include these new seeder:

```php
<?php

namespace Database\Seeders;

use Illuminate\Database\Seeder;

class DatabaseSeeder extends Seeder
{
    public function run()
    {
        $this->call([
            TagSeeder::class,
        ]);
    }
}
```

Finally, to seed your database with this sample data, run:

```bash
php artisan db:seed
```

This command will run the `TagSeeder` and populate the `tags` table with the sample tags which we can associate with posts later on when users create new posts.

## Implementing the Blog Functionality

Now that we have our models and migrations set up, we can go ahead and implement the blog functionality using Livewire components.

We will start by creating two Livewire components:

- `PostList` to display a list of blog posts
- `PostForm` to create and edit posts

### Creating the Post List Component

First, let's create a Livewire component to display the list of blog posts:

```bash
php artisan make:livewire PostList
```

This command creates a new Livewire component in the `app/Livewire` directory, along with a view file in `resources/views/livewire`.

Update `app/Livewire/PostList.php` to fetch the posts and handle search functionality:

```php
<?php

namespace App\Livewire;

use App\Models\Post;
use Livewire\Component;
use Livewire\WithPagination;

class PostList extends Component
{
    use WithPagination;

    public $search = '';

    public function updatingSearch()
    {
        $this->resetPage();
    }

    public function render()
    {
        $posts = Post::where('is_published', true)
            ->where(function ($query) {
                $query->where('title', 'ilike', '%' . $this->search . '%')
                    ->orWhere('content', 'ilike', '%' . $this->search . '%');
            })
            ->with('user', 'tags')
            ->latest('published_at')
            ->paginate(10);

        return view('livewire.post-list', [
            'posts' => $posts,
        ]);
    }
}
```

In the `render` method, we fetch the posts that are published and match the search query.

An important thing to note here is that we also eager load the `user` and `tags` relationships to avoid additional queries when accessing these relationships in the view.

To learn more about how to implement search functionality in Livewire, check out the [Building a Simple Real-Time Search with Laravel, Livewire, and Neon guide](/guides/laravel-livewire-simple-search).

Now, update the view in `resources/views/livewire/post-list.blade.php` to display the list of posts:

```html
<div>
  <div class="mb-4">
    <input
      wire:model.live.debounce.300ms="search"
      type="text"
      placeholder="Search posts..."
      class="focus:ring-blue-500 w-full rounded-lg border px-4 py-2 focus:outline-none focus:ring-2"
    />
  </div>

  <div class="space-y-4">
    @foreach($posts as $post)
    <div class="rounded-lg bg-white p-6 shadow-md">
      <h2 class="mb-2 text-2xl font-bold">
        <a href="{{ route('posts.show', $post) }}" class="text-blue-600 hover:text-blue-800"
          >{{ $post->title }}</a
        >
      </h2>
      <p class="text-gray-600 mb-2">By {{ $post->user->name }} on {{ $post->published_at }}</p>
      <p class="text-gray-700 mb-4">{{ Str::limit($post->content, 200) }}</p>
      <div class="flex flex-wrap gap-2">
        @foreach($post->tags as $tag)
        <span class="bg-blue-100 text-blue-800 rounded px-2.5 py-0.5 text-xs font-semibold"
          >{{ $tag->name }}</span
        >
        @endforeach
      </div>
    </div>
    @endforeach
  </div>

  <div class="mt-4">{{ $posts->links() }}</div>
</div>
```

This view displays the list of posts along with the post title, author, publication date, content, and tags. It also includes a search input field to filter the posts based on the search query.

### Creating the Post Form Component

Now, let's create a Livewire component for creating and editing posts.

```bash
php artisan make:livewire PostForm
```

Update `app/Livewire/PostForm.php` to handle post creation and editing:

```php
<?php

namespace App\Livewire;

use App\Models\Post;
use App\Models\Tag;
use Illuminate\Support\Str;
use Livewire\Component;

class PostForm extends Component
{
    public $post;
    public $title;
    public $content;
    public $tags;
    public $selectedTags = [];

    protected $rules = [
        'title' => 'required|min:5',
        'content' => 'required|min:10',
        'selectedTags' => 'array',
    ];

    public function mount($post = null)
    {
        if ($post) {
            $this->post = $post;
            $this->title = $post->title;
            $this->content = $post->content;
            $this->selectedTags = $post->tags->pluck('id')->toArray();
        }
    }

    public function save()
    {
        $this->validate();

        $isNew = !$this->post;

        if ($isNew) {
            $this->post = new Post();
            $this->post->user_id = auth()->id();
        }

        $this->post->title = $this->title;
        $this->post->slug = Str::slug($this->title);
        $this->post->content = $this->content;
        $this->post->is_published = true;
        $this->post->published_at = now();
        $this->post->save();

        $this->post->tags()->sync($this->selectedTags);

        session()->flash('message', $isNew ? 'Post created successfully.' : 'Post updated successfully.');

        return redirect()->route('posts.show', $this->post);
    }

    public function render()
    {
        $allTags = Tag::all();
        return view('livewire.post-form', [
            'allTags' => $allTags,
        ]);
    }
}
```

Rundown of the methods in the `PostForm` component:

- The `mount` method is used to set the initial values for the form fields when editing a post. The post data is passed to the component as a parameter.
- The `save` method is called when the form is submitted. It validates the form fields, creates a new post or updates an existing one, and redirects to the post detail page.
- The `render` method fetches all tags from the database and passes them to the view.
- In the `rules` property, we define the validation rules for the form fields.

After that, update the `resources/views/livewire/post-form.blade.php` view to display the post form:

```html
<div>
  <form wire:submit.prevent="save">
    <div class="mb-4">
      <label for="title" class="text-gray-700 mb-2 block font-bold">Title</label>
      <input
        wire:model="title"
        type="text"
        id="title"
        class="text-gray-700 w-full rounded-lg border px-3 py-2 focus:outline-none"
        required
      />
      @error('title') <span class="text-red-500">{{ $message }}</span> @enderror
    </div>

    <div class="mb-4">
      <label for="content" class="text-gray-700 mb-2 block font-bold">Content</label>
      <textarea
        wire:model="content"
        id="content"
        rows="6"
        class="text-gray-700 w-full rounded-lg border px-3 py-2 focus:outline-none"
        required
      ></textarea>
      @error('content') <span class="text-red-500">{{ $message }}</span> @enderror
    </div>

    <div class="mb-4">
      <label class="text-gray-700 mb-2 block font-bold">Tags</label>
      <div class="flex flex-wrap gap-2">
        @foreach($allTags as $tag)
        <label class="inline-flex items-center">
          <input
            type="checkbox"
            wire:model="selectedTags"
            value="{{ $tag->id }}"
            class="form-checkbox text-blue-600 h-5 w-5"
          />
          <span class="text-gray-700 ml-2">{{ $tag->name }}</span>
        </label>
        @endforeach
      </div>
    </div>

    <div>
      <button
        type="submit"
        class="bg-blue-500 hover:bg-blue-700 rounded px-4 py-2 font-bold text-white"
      >
        {{ $post ? 'Update Post' : 'Create Post' }}
      </button>
    </div>
  </form>
</div>
```

This view includes form fields for the post title, content, and tags. The tags are displayed as checkboxes, allowing the user to select multiple tags for the post when creating or editing it.

### Creating Routes and Controllers

Now that we have our Livewire components ready, let's create the necessary routes and controllers to handle the blog functionality.

Routes are defined in the `routes/web.php` file, and controllers are used to handle the logic for each route.

```php
<?php

use App\Http\Controllers\PostController;
use Illuminate\Support\Facades\Route;

Route::middleware(['auth'])->group(function () {
    // After the existing Breeze routes add the following routes:
    Route::get('/posts/create', [PostController::class, 'create'])->name('posts.create');
    Route::get('/posts/{post}/edit', [PostController::class, 'edit'])->name('posts.edit');
});

// Outside the middleware group, add a route to display posts publicly:
Route::get('/posts', [PostController::class, 'index'])->name('posts.index');
Route::get('/posts/{post}', [PostController::class, 'show'])->name('posts.show');
```

Next, create a controller which will handle the blog functionality for the above routes that we just defined:

```bash
php artisan make:controller PostController
```

The above command creates a new controller in the `app/Http/Controllers` directory.

Update `app/Http/Controllers/PostController.php` to include the necessary methods:

```php
<?php

namespace App\Http\Controllers;

use App\Models\Post;
use Illuminate\Support\Facades\Gate;

use Illuminate\Http\Request;

class PostController extends Controller
{
    public function index()
    {
        return view('posts.index');
    }

    public function show(Post $post)
    {
        return view('posts.show', compact('post'));
    }

    public function create()
    {
        return view('posts.create');
    }

    public function edit(Post $post)
    {
        if (Gate::denies('update', $post)) {
            abort(403);
        }
        return view('posts.edit', compact('post'));
    }
}
```

For all the methods, we return the corresponding views. The `edit` method also includes an authorization gate to check if the current user is authorized to edit the post which we will define later.

### Creating the Views

With the routes and controllers in place, let's create the views for the blog functionality. The views will include the layout, navigation, and content for the blog posts.

Let's start by creating a `resources/views/posts/index.blade.php` view to display the list of blog posts:

```html
<x-app-layout>
  <x-slot name="header">
    <h2 class="text-gray-800 text-xl font-semibold leading-tight">{{ __('Blog Posts') }}</h2>
  </x-slot>

  <div class="py-12">
    <div class="mx-auto max-w-7xl lg:px-8 sm:px-6">
      <div class="overflow-hidden bg-white shadow-sm sm:rounded-lg">
        <div class="border-gray-200 border-b bg-white p-6">@livewire('post-list')</div>
      </div>
    </div>
  </div>
</x-app-layout>
```

This view includes the `PostList` Livewire component to display the list of blog posts.

Next, create the `resources/views/posts/show.blade.php` view to display a single blog post:

```html
<x-app-layout>
  <x-slot name="header">
    <h2 class="text-gray-800 text-xl font-semibold leading-tight">{{ $post->title }}</h2>
  </x-slot>

  <div class="py-12">
    <div class="mx-auto max-w-7xl lg:px-8 sm:px-6">
      <div class="overflow-hidden bg-white shadow-sm sm:rounded-lg">
        <div class="border-gray-200 border-b bg-white p-6">
          <h1 class="mb-4 text-3xl font-bold">{{ $post->title }}</h1>
          <p class="text-gray-600 mb-4">By {{ $post->user->name }} on {{ $post->published_at }}</p>
          <div class="prose mb-6 max-w-none">{!! nl2br(e($post->content)) !!}</div>
          <div class="mb-6 flex flex-wrap gap-2">
            @foreach($post->tags as $tag)
            <span class="bg-blue-100 text-blue-800 rounded px-2.5 py-0.5 text-xs font-semibold"
              >{{ $tag->name }}</span
            >
            @endforeach
          </div>
          @can('update', $post)
          <a
            href="{{ route('posts.edit', $post) }}"
            class="bg-blue-500 hover:bg-blue-700 rounded px-4 py-2 font-bold text-white"
            >Edit Post</a
          >
          @endcan
        </div>
      </div>
    </div>
  </div>
</x-app-layout>
```

This view displays the post title, author, publication date, content, and tags. It also includes a link to edit the post if the current user is authorized to do so.

After that, create the `resources/views/posts/create.blade.php` and `resources/views/posts/edit.blade.php` views for creating and editing posts, respectively. These views will include the `PostForm` Livewire component, which we created earlier, and handle the form submission.

Create the `resources/views/posts/create.blade.php` view with the following content:

```html
<x-app-layout>
  <x-slot name="header">
    <h2 class="text-gray-800 text-xl font-semibold leading-tight">{{ __('Create New Post') }}</h2>
  </x-slot>

  <div class="py-12">
    <div class="mx-auto max-w-7xl lg:px-8 sm:px-6">
      <div class="overflow-hidden bg-white shadow-sm sm:rounded-lg">
        <div class="border-gray-200 border-b bg-white p-6">@livewire('post-form')</div>
      </div>
    </div>
  </div>
</x-app-layout>
```

Using the `@livewire` directive, we include the `PostForm` component to create a new post.

With the same structure, create the `resources/views/posts/edit.blade.php` view:

```html
<x-app-layout>
  <x-slot name="header">
    <h2 class="text-gray-800 text-xl font-semibold leading-tight">{{ __('Edit Post') }}</h2>
  </x-slot>

  <div class="py-12">
    <div class="mx-auto max-w-7xl lg:px-8 sm:px-6">
      <div class="overflow-hidden bg-white shadow-sm sm:rounded-lg">
        <div class="border-gray-200 border-b bg-white p-6">
          @livewire('post-form', ['post' => $post])
        </div>
      </div>
    </div>
  </div>
</x-app-layout>
```

This view includes the `PostForm` component with the post data passed as a parameter to edit the post. The form fields will be pre-filled with the existing post data when users edit one of their posts.

### Adding Authorization

As this will be a multi-user blog, we need to implement authorization to ensure that users can only edit their own posts. Our goal is to allow users to edit posts only if they are the authors of those posts.

Laravel provides a simple way to define authorization policies using policies and gates. Policies are classes that define the authorization logic for a particular model, while gates are more general-purpose authorization checks.

Let's create a policy for the `Post` model:

```bash
php artisan make:policy PostPolicy --model=Post
```

Update `app/Policies/PostPolicy.php` to define the authorization logic for updating and deleting posts:

```php
<?php

namespace App\Policies;

use App\Models\Post;
use App\Models\User;
use Illuminate\Auth\Access\HandlesAuthorization;

class PostPolicy
{
    use HandlesAuthorization;

    public function update(User $user, Post $post)
    {
        return $user->id === $post->user_id;
    }

    public function delete(User $user, Post $post)
    {
        return $user->id === $post->user_id;
    }
}
```

In the `PostPolicy` class, we define the `update` and `delete` methods to check if the current user is the author of the post. If the user is the author, the method returns `true`, allowing the user to update or delete the post. Otherwise, it returns `false` to deny access and prevent unauthorized actions.

### Implementing Comments

By now we have the basic functionality of our blog in place. If you were to visit the blog, you would see a list of posts, be able to view individual posts, and create new posts. However, a blog wouldn't be complete without the ability to add comments to posts!

Let's add the comment system to our blog posts! First, create a new Livewire component:

```bash
php artisan make:livewire CommentSection
```

Update `app/Livewire/CommentSection.php` to handle adding comments to a post:

```php
<?php

namespace App\Livewire;

use App\Models\Comment;
use Livewire\Component;

class CommentSection extends Component
{
    public $post;
    public $newComment;

    protected $rules = [
        'newComment' => 'required|min:3',
    ];

    public function mount($post)
    {
        $this->post = $post;
    }

    public function addComment()
    {
        $this->validate();

        $this->post->comments()->create([
            'user_id' => auth()->id(),
            'content' => $this->newComment,
        ]);

        $this->newComment = '';
        $this->post = $this->post->fresh(['comments.user']);
    }

    public function deleteComment($commentId)
    {
        $comment = Comment::find($commentId);

        if ($comment->user_id === auth()->id()) {
            $comment->delete();
            $this->post = $this->post->fresh(['comments.user']);
        }
    }

    public function render()
    {
        return view('livewire.comment-section');
    }
}
```

Here, we have the `CommentSection` Livewire component with methods to add and delete comments. The `addComment` method creates a new comment for the post, while the `deleteComment` method deletes a comment if the current user is the author of the comment. You can also see the `rules` property defining the validation rules for the comment content and create a policy for the `Comment` model to handle authorization instead of checking it in the component itself.

Next, update the view in `resources/views/livewire/comment-section.blade.php` to display comments and allow users to add new comments:

```html
<div>
  <h3 class="mb-4 text-2xl font-bold">Comments</h3>

  @foreach($post->comments as $comment)
  <div class="bg-gray-100 mb-4 rounded-lg p-4">
    <p class="text-gray-800">{{ $comment->content }}</p>
    <p class="text-gray-600 mt-2 text-sm">
      By {{ $comment->user->name }} on {{ $comment->created_at }} @if($comment->user_id ===
      auth()->id()) |
      <button wire:click="deleteComment({{ $comment->id }})" class="text-red-500 hover:underline">
        Delete
      </button>
      @endif
    </p>
  </div>
  @endforeach @auth
  <form wire:submit.prevent="addComment" class="mt-6">
    <div class="mb-4">
      <label for="newComment" class="text-gray-700 mb-2 block font-bold">Add a comment</label>
      <textarea
        wire:model="newComment"
        id="newComment"
        rows="3"
        class="text-gray-700 w-full rounded-lg border px-3 py-2 focus:outline-none"
        required
      ></textarea>
      @error('newComment') <span class="text-red-500">{{ $message }}</span> @enderror
    </div>
    <button
      type="submit"
      class="bg-blue-500 hover:bg-blue-700 rounded px-4 py-2 font-bold text-white"
    >
      Post Comment
    </button>
  </form>
  @else
  <p class="text-gray-600 mt-6">
    Please <a href="{{ route('login') }}" class="text-blue-500 hover:underline">log in</a> to leave
    a comment.
  </p>
  @endauth
</div>
```

After that, go back to the `resources/views/posts/show.blade.php` view and update it to include the comment section:

```html
<!-- Add this after the post content -->
<div class="mt-8">@livewire('comment-section', ['post' => $post])</div>
```

### Adding Navigation Links

Laravel Breeze provides a simple layout with a navigation menu that includes links for logging in and registering. Let's add links for creating new posts and logging out.

Update the existing `resources/views/layouts/navigation.blade.php` view to include links for creating new posts:

```html
<!-- Add this inside the navigation menu -->
<x-nav-link :href="route('posts.create')" :active="request()->routeIs('posts.create')">
  {{ __('Create Post') }}
</x-nav-link>
<x-nav-link :href="route('posts.index')" :active="request()->routeIs('posts.index')">
  {{ __('Blog') }}
</x-nav-link>
```

## Testing

To ensure our blog functionality works as expected, it's important to test the application.

To learn more about testing in Laravel along Neon, check out the [Testing Laravel Applications with Neon's Database Branching guide](/guides/laravel-test-on-branch).

## Conclusion

In this tutorial, we've built a fully functional blog application using Laravel, Livewire, and Laravel Breeze. We've implemented features such as user authentication, creating and editing blog posts, adding comments, and basic authorization.

This implementation provides a solid foundation for a blog, but there are always ways to improve and expand its functionality:

- Implement a more advanced authorization system with roles and permissions
- Add a rich text editor for post content
- Implement a more robust tagging system with the ability to create new tags
- Add a search functionality for posts
- Implement social sharing features
- Add an admin panel for managing posts, users, and comments

By combining the power of Laravel, the simplicity of Livewire, and the authentication scaffolding provided by Laravel Breeze, you can quickly create dynamic and interactive web applications that meet your users' needs.

## Additional Resources

- [Laravel Documentation](https://laravel.com/docs)
- [Livewire Documentation](https://laravel-livewire.com/docs)
- [Laravel Breeze Documentation](https://laravel.com/docs/8.x/starter-kits#laravel-breeze)
- [Tailwind CSS Documentation](https://tailwindcss.com/docs)
- [Neon Documentation](/docs)


# Building Dynamic Charts with Laravel, Livewire, and Neon Postgres

---
title: Building Dynamic Charts with Laravel, Livewire, and Neon Postgres
subtitle: Learn how to build dynamic charts with Laravel, Livewire, and Neon Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-10-20T00:00:00.000Z'
updatedOn: '2024-10-20T00:00:00.000Z'
---

Laravel is an amazing PHP framework for building web applications, while Livewire provides a simple way to build dynamic interfaces using PHP.

In this guide, we'll walk through the process of creating a dynamic analytics dashboard for a SaaS application using Laravel Breeze for authentication, Livewire Charts for data visualization, and Neon Postgres for data storage.

We'll build interactive charts that display key metrics such as daily active users, feature usage trends, and user signups vs. cancellations.

## Prerequisites

Before we begin, make sure you have:

- PHP 8.1 or higher installed
- Composer for managing PHP dependencies
- A [Neon](https://console.neon.tech/signup) account for Postgres hosting
- Basic familiarity with Laravel, Livewire, and Postgres

## Setting up the Project

1. Create a new Laravel project:

   ```bash
   composer create-project laravel/laravel saas-charts
   cd saas-charts
   ```

2. Install Laravel Breeze with Livewire:

   ```bash
   composer require laravel/breeze --dev
   php artisan breeze:install livewire
   ```

3. Install the Livewire Charts package which we'll use for data visualization:

   ```bash
   composer require asantibanez/livewire-charts
   ```

4. Install the Livewire Charts assets which include the necessary JavaScript and CSS files:

   ```bash
   php artisan livewire-charts:install
   ```

5. Set up your Neon Postgres connection in the `.env` file:

   ```env
   DB_CONNECTION=pgsql
   DB_HOST=your-neon-hostname.neon.tech
   DB_PORT=5432
   DB_DATABASE=your_database_name
   DB_USERNAME=your_username
   DB_PASSWORD=your_password
   ```

6. Run the migrations to set up the users table and other Breeze-related tables in your Neon Postgres database:

   ```bash
   php artisan migrate
   ```

   This will create the necessary tables for user authentication and session management.

## Additional Database Tables

Now that we have the `users` table set up by Breeze, let's create migrations for our additional SaaS analytics data.

For the purpose of this guide, we'll track feature usage and subscriptions. You can adjust these tables based on your specific application requirements.

1. Create migrations:

   ```bash
   php artisan make:migration create_feature_usage_table
   php artisan make:migration create_subscriptions_table
   ```

   Note that the naming convention for the migration files is important to make sure that your migrations are named correctly with the `create_` prefix followed by the table name.

2. Update the migration files:

   The above commands will create two migration files in the `database/migrations` directory. Update the migration files as follows:

   For the `create_feature_usage_table` we'll track the usage of different features by users, so we'll store the `user_id`, `feature_name`, and the `used_at` timestamp:

   ```php
   public function up()
   {
       Schema::create('feature_usage', function (Blueprint $table) {
           $table->id();
           $table->foreignId('user_id')->constrained()->onDelete('cascade');
           $table->string('feature_name');
           $table->timestamp('used_at');
           $table->timestamps();
       });
   }
   ```

   For the `create_subscriptions_table` we'll track user subscriptions, including the `user_id`, `plan`, `started_at`, and `ended_at` timestamps:

   ```php
   public function up()
   {
       Schema::create('subscriptions', function (Blueprint $table) {
           $table->id();
           $table->foreignId('user_id')->constrained()->onDelete('cascade');
           $table->string('plan');
           $table->timestamp('started_at');
           $table->timestamp('ended_at')->nullable();
           $table->timestamps();
       });
   }
   ```

3. With the migrations in place, run the migrations to create the tables in your Neon Postgres database:

   ```bash
   php artisan migrate
   ```

   This will create the `feature_usage` and `subscriptions` tables in your database.

## Creating Models

Laravel's Eloquent ORM provides a convenient way to interact with your database.

By defining models, we can represent and manipulate the data in the `FeatureUsage` and `Subscription` tables, which we created earlier through migrations.

In this step, we'll create models and set up relationships to ensure efficient data retrieval and interaction.

### Step 1: Generate the Models

Start by creating the `FeatureUsage` and `Subscription` models using Laravel's Artisan command:

```bash
php artisan make:model FeatureUsage
php artisan make:model Subscription
```

This will generate two model files in the `app/Models` directory corresponding to the `feature_usage` and `subscriptions` tables in your database.

### Step 2: Define Relationships in the Models

Now, let's update the model classes to define relationships between the tables. The `FeatureUsage` and `Subscription` models will be connected to the `User` model via foreign keys.

#### 2.1 `FeatureUsage` Model

In the `app/Models/FeatureUsage.php` file, define the relationship with the `User` model. Since each feature usage entry belongs to a specific user, we will use a `belongsTo` relationship:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class FeatureUsage extends Model
{
    use HasFactory;

    protected $table = 'feature_usage';

    protected $fillable = [
        'user_id',
        'feature_name',
        'used_at',
    ];

    public function user()
    {
        return $this->belongsTo(User::class);
    }

    protected $casts = [
        'used_at' => 'datetime',
    ];
}
```

The above defines the following:

- `fillable`: Specifies which attributes can be mass-assigned, in this case, `user_id`, `feature_name`, and `used_at`.
- `user()`: Defines a `belongsTo` relationship, meaning each `FeatureUsage` belongs to a single `User`.
- `casts`: Automatically casts the `used_at` column to a `datetime` object for easier manipulation in PHP.

#### 2.2 `Subscription` Model

In the `app/Models/Subscription.php` file, define relationships with both the `User` model and handle timestamps (`started_at` and `ended_at`) correctly. This indicates that each subscription belongs to a user and includes a `plan`:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class Subscription extends Model
{
    use HasFactory;

    protected $fillable = [
        'user_id',
        'plan',
        'started_at',
        'ended_at',
    ];

    public function user()
    {
        return $this->belongsTo(User::class);
    }

    protected $casts = [
        'started_at' => 'datetime',
        'ended_at' => 'datetime',
    ];
}
```

The above defines the following:

- `fillable`: Makes sure that the fields `user_id`, `plan`, `started_at`, and `ended_at` are mass assignable.
- `user()`: Defines a `belongsTo` relationship where each `Subscription` is linked to a specific `User`.
- `casts`: Automatically casts the `started_at` and `ended_at` columns to `datetime` objects.

### Step 3: Database Relationships

Once these relationships are defined, Eloquent provides methods to interact with related data. For example:

- Access a user's feature usages with `$user->featureUsages()`.
- Retrieve a user's subscriptions with `$user->subscriptions()`.
- Easily manipulate and retrieve data for timestamps (e.g., `started_at`, `ended_at`, and `used_at`).

## Building the Dashboard

We’ll create a simple SaaS dashboard that showcases our dynamic charts which will include daily active users, feature usage trends, and user signups vs. cancellations. This dashboard will use Livewire for interactivity and Tailwind CSS for styling along with the Livewire Charts package for creating the dynamic charts.

### Step 1: Create the Livewire Component

First, generate a new Livewire component for the dashboard:

```bash
php artisan make:livewire Dashboard
```

This will create both the `Dashboard` class in `app/Http/Livewire` and the a view in `resources/views/livewire/dashboard.blade.php`.

### Step 2: Update `Dashboard.php`

In the `app/Http/Livewire/Dashboard.php` file, we’ll render the dashboard view inside the main layout:

```php
<?php

namespace App\Livewire;

use Livewire\Component;

class Dashboard extends Component
{
    public function render()
    {
        return view('livewire.dashboard')->layout('layouts.app');
    }
}
```

Note that we're using the `layout('layouts.app')` method to specify the main layout file for the dashboard view. This layout file will contains the main structure of the dashboard.

### Step 3: Create the Dashboard View

Now, let’s update the `dashboard.blade.php` view with a grid layout that displays multiple charts, along with some Tailwind CSS styling to improve the design.

```blade
<div class="min-h-screen py-12 bg-gray-100">
    <div class="mx-auto max-w-7xl sm:px-6 lg:px-8">
        <!-- Dashboard Header -->
        <div class="flex items-center justify-between mb-10">
            <h1 class="text-4xl font-bold text-gray-800">SaaS Analytics Dashboard</h1>
            <button class="px-4 py-2 text-white transition duration-200 bg-indigo-600 rounded-lg hover:bg-indigo-700">
                Refresh Data
            </button>
        </div>

        <!-- Dashboard Grid -->
        <div class="grid grid-cols-1 gap-8 sm:grid-cols-2 lg:grid-cols-3">
            <!-- Daily Active Users Chart -->
            <div class="p-6 bg-white rounded-lg shadow-md">
                <h2 class="mb-4 text-xl font-semibold">Daily Active Users</h2>
                <livewire:daily-active-users-chart />
            </div>

            <!-- Feature Usage Trends Chart -->
            <div class="p-6 bg-white rounded-lg shadow-md">
                <h2 class="mb-4 text-xl font-semibold">Feature Usage Trends</h2>
                <livewire:feature-usage-trends-chart />
            </div>

            <!-- User Signups vs Cancellations Chart -->
            <div class="p-6 bg-white rounded-lg shadow-md">
                <h2 class="mb-4 text-xl font-semibold">User Signups vs. Cancellations</h2>
                <livewire:user-signups-vs-cancellations-chart />
            </div>
        </div>
    </div>
</div>
```

This view includes a header with a title and a refresh button, followed by a grid layout that displays the three charts: 'Daily Active Users', 'Feature Usage Trends', and 'User Signups vs. Cancellations'. We will create those charts components next.

## Setting up Routes

With the charts dashboard view and the Livewire component in place, let's set up the routes to display the dashboard.

```php
use App\Livewire\Dashboard;

Route::get('/charts', Dashboard::class)->middleware(['auth'])->name('dashboard');
```

This route will display the dashboard view when the `/charts` URL is accessed. The `auth` middleware ensures that only authenticated users can access the dashboard.

## Set Up Livewire Charts for the Dashboard

Now with everything in place, let's implement individual chart components.

The Livewire Charts package provides a wide range of chart types, including area charts, radar charts, and treemaps, offering flexibility to create various data visualizations.

We'll use `LivewireLineChart` for 'Daily Active Users', `LivewireColumnChart` for 'Feature Usage Trends', and `LivewirePieChart` for 'User Signups vs. Cancellations'. To get a full list of available chart types, check out the [Livewire Charts documentation](https://github.com/asantibanez/livewire-charts/).

### 2.1 Daily Active Users Chart

Create a Livewire component for the daily active users chart:

```bash
php artisan make:livewire DailyActiveUsersChart
```

In `app/Livewire/DailyActiveUsersChart.php`, define the logic to fetch the data:

```php
<?php

namespace App\Livewire;

use Livewire\Component;
use App\Models\FeatureUsage;
use Asantibanez\LivewireCharts\Models\LineChartModel;

class DailyActiveUsersChart extends Component
{
    public function render()
    {
        // Example query to fetch daily active users
        $activeUsers = FeatureUsage::selectRaw('DATE(used_at) as date, COUNT(DISTINCT user_id) as users')
            ->groupBy('date')
            ->get();

        // Prepare data for the chart
        $lineChartModel = (new LineChartModel())
            ->setTitle('Daily Active Users')
            ->setAnimated(true)
            ->setSmoothCurve()
            ->withOnPointClickEvent('onPointClick');

        foreach ($activeUsers as $activeUser) {
            $lineChartModel->addPoint($activeUser->date, $activeUser->users);
        }

        return view('livewire.daily-active-users-chart', [
            'lineChartModel' => $lineChartModel
        ]);
    }
}
```

Create the corresponding Blade view in `resources/views/livewire/daily-active-users-chart.blade.php`:

```blade
<div class="h-64">
    <livewire:livewire-line-chart
        :line-chart-model="$lineChartModel"
    />
</div>
```

### 2.2 Feature Usage Trends Chart

Create another Livewire component for feature usage trends:

```bash
php artisan make:livewire FeatureUsageTrendsChart
```

In `app/Livewire/FeatureUsageTrendsChart.php`, define the data logic:

```php
<?php

namespace App\Livewire;

use Livewire\Component;
use App\Models\FeatureUsage;
use Asantibanez\LivewireCharts\Models\ColumnChartModel;

class FeatureUsageTrendsChart extends Component
{
    public function render()
    {
        // Example query to fetch feature usage trends
        $featureUsages = FeatureUsage::selectRaw('feature_name, COUNT(*) as usage_count')
            ->groupBy('feature_name')
            ->get();

        // Prepare the chart data
        $columnChartModel = (new ColumnChartModel())
            ->setTitle('Feature Usage Trends')
            ->setAnimated(true);

        foreach ($featureUsages as $usage) {
            $columnChartModel->addColumn($usage->feature_name, $usage->usage_count, '#f6ad55');
        }

        return view('livewire.feature-usage-trends-chart', [
            'columnChartModel' => $columnChartModel
        ]);
    }
}
```

In `resources/views/livewire/feature-usage-trends-chart.blade.php`:

```blade
<div class="h-64">
    <livewire:livewire-column-chart
        :column-chart-model="$columnChartModel"
    />
</div>
```

### 2.3 User Signups vs. Cancellations Chart

Create a Livewire component for user signups vs. cancellations:

```bash
php artisan make:livewire UserSignupsVsCancellationsChart
```

In `app/Livewire/UserSignupsVsCancellationsChart.php`, define the data logic:

```php
<?php

namespace App\Livewire;

use Livewire\Component;
use App\Models\Subscription;
use Asantibanez\LivewireCharts\Models\PieChartModel;

class UserSignupsVsCancellationsChart extends Component
{
    public function render()
    {
        // Example query to fetch signups vs cancellations
        $signups = Subscription::whereNotNull('started_at')->count();
        $cancellations = Subscription::whereNotNull('ended_at')->count();

        // Prepare the chart data
        $pieChartModel = (new PieChartModel())
            ->setTitle('Signups vs Cancellations')
            ->addSlice('Signups', $signups, '#90cdf4')
            ->addSlice('Cancellations', $cancellations, '#fc8181');

        return view('livewire.user-signups-vs-cancellations-chart', [
            'pieChartModel' => $pieChartModel
        ]);
    }
}
```

In `resources/views/livewire/user-signups-vs-cancellations-chart.blade.php`:

```blade
<div class="h-64">
    <livewire:livewire-pie-chart
        :pie-chart-model="$pieChartModel"
    />
</div>
```

## Step 3: Add Chart Scripts

Include the chart scripts in your main layout file (`resources/views/layouts/app.blade.php`) by adding:

```blade
@livewireScripts
@livewireChartsScripts
```

This will load the necessary JavaScript files for Livewire and Livewire Charts to render the interactive charts on the dashboard.

## Step 4: Test the Dashboard

Run the server to access your charts dashboard if you haven't already:

```bash
php artisan serve
```

Navigate to the `/charts` route, and you should see the real-time interactive charts displayed on your dashboard.

### Seeding the Database with Sample Data

If you don't have any data yet, you can seed the database with sample data to test the charts. First, create a seeder that populates the `FeatureUsage` and `Subscription` tables with mock data:

1. Generate the seeder:

   ```bash
   php artisan make:seeder SampleDataSeeder
   ```

2. Open the newly created seeder file (`database/seeders/SampleDataSeeder.php`) and populate it with sample data for feature usage and subscriptions:

   ```php
   <?php

   namespace Database\Seeders;

   use Illuminate\Database\Seeder;
   use App\Models\FeatureUsage;
   use App\Models\Subscription;
   use App\Models\User;
   use Carbon\Carbon;

   class SampleDataSeeder extends Seeder
   {
       public function run()
       {
           // Create users
           $users = User::factory(10)->create();

           // Seed FeatureUsage data
           foreach ($users as $user) {
               for ($i = 0; $i < 5; $i++) {
                   FeatureUsage::create([
                       'user_id' => $user->id,
                       'feature_name' => 'Feature ' . rand(1, 5),
                       'used_at' => Carbon::now()->subDays(rand(0, 30)),
                   ]);
               }
           }

           // Seed Subscription data
           foreach ($users as $user) {
               Subscription::create([
                   'user_id' => $user->id,
                   'plan' => 'Basic',
                   'started_at' => Carbon::now()->subMonths(2),
                   'ended_at' => rand(0, 1) ? Carbon::now()->subMonth() : null,
               ]);
           }
       }
   }
   ```

3. Run the seeder to populate the database with test data:

   ```bash
   php artisan db:seed --class=SampleDataSeeder
   ```

Once the database is seeded, refresh the charts dashboard, and you should see the charts populated with real-time data.

For more information on seeding the database, check out the [Laravel documentation](https://laravel.com/docs/11.x/seeding).

## Optimizing Performance

When working with large datasets, you will have to make sure that your application is optimized for performance. This includes optimizing database queries, caching results, and using efficient algorithms.

We will cover some optimization techniques for improving the performance of your Neon Postgres application below but you should also check out the [Performance tips for Neon Postgres](/blog/performance-tips-for-neon-postgres) blog post for more specific tips.

### 1. Database Indexing for Frequently Queried Columns

Database indexing is a key technique to speed up query execution, especially for columns used frequently in `WHERE`, `JOIN`, and `ORDER BY` clauses. With indexes in place, the database can find records faster, making your queries more efficient.

This can be especially useful for tables like `FeatureUsage` and `Subscription`, where you might frequently query by `user_id`, `used_at`, `started_at`, and `ended_at`.

Here’s how to add indexes for the `FeatureUsage` and `Subscription` tables:

```php
Schema::table('feature_usage', function (Blueprint $table) {
    $table->index(['user_id', 'used_at']);  // Index on user_id and used_at to speed up queries
});

Schema::table('subscriptions', function (Blueprint $table) {
    $table->index(['user_id', 'started_at', 'ended_at']);  // Index on user_id, started_at, and ended_at for faster lookups
});
```

These indexes will optimize queries related to filtering or grouping by `user_id`, `used_at`, `started_at`, and `ended_at`, which are common in analytics.

To learn more about indexing in Neon Postgres, check out the [Neon documentation](/docs/postgres/indexes) on indexes.

### 2. Implement Caching for Expensive Queries

Caching is a great way to reduce the load on your database by storing the results of expensive queries and retrieving them from memory when needed. This avoids running the same query multiple times for data that doesn't change frequently.

Here's how you can cache the results of a query for daily active users for a specific time period:

```php
use Illuminate\Support\Facades\Cache;

// In your Livewire component
$dailyActiveUsers = Cache::remember('daily_active_users_' . $this->selectedDays, 60 * 5, function () {
    return FeatureUsage::selectRaw('DATE(used_at) as date, COUNT(DISTINCT user_id) as count')
        ->whereDate('used_at', '>=', now()->subDays($this->selectedDays))
        ->groupBy('date')
        ->orderBy('date')
        ->get();
});
```

Quick explanation of the code:

- `Cache::remember`: Caches the query result for 5 minutes (`60 * 5` seconds). If the data is already cached, it retrieves the result from the cache; otherwise, it runs the query and stores the result.
- This is useful for queries that don’t need real-time updates and can tolerate slight delays, such as historical data or reports.

The `Cache::remember` method is a convenient way to cache query results in Laravel. You can adjust the cache duration based on your application's requirements. However, be cautious with caching, as it can lead to stale data if not managed properly.

## Conclusion

In this guide, we've built a simple dynamic SaaS dashboard using Laravel Breeze for authentication, Livewire Charts for data visualization, and Neon Postgres for data storage. This setup provides a good starting point for tracking key metrics in your SaaS or web application.

To go further, consider the following next steps:

1. Implementing more detailed drill-down features for each chart.
2. Adding user-specific analytics for personalized insights.
3. Implementing real-time updates using Livewire's polling feature or websockets.

## Additional Resources

- [Laravel Documentation](https://laravel.com/docs)
- [Neon Documentation](/docs)
- [Livewire Documentation](https://livewire.laravel.com/)
- [Livewire Charts Documentation](https://github.com/asantibanez/livewire-charts/)

<NeedHelp />


# Building a Simple Real-Time Search with Laravel, Livewire, and Neon

---
title: Building a Simple Real-Time Search with Laravel, Livewire, and Neon
subtitle: Learn how to integrate Laravel with Postgres on Neon, using Laravel's Eloquent ORM and migrations for efficient database management.
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-06-29T00:00:00.000Z'
updatedOn: '2024-06-29T00:00:00.000Z'
---

Laravel is a powerful PHP framework known for its elegant syntax and feature-rich ecosystem. Livewire, a full-stack framework for Laravel, allows us to build dynamic interfaces with minimal JavaScript. Together, they provide a robust foundation for creating interactive web applications.

In this guide, we'll build a simple real-time search feature using Laravel, Livewire, and Neon. We'll set up a Laravel project, create a database schema, implement the search functionality with Livewire, and optimize performance with Neon. By the end of this tutorial, you'll have a working real-time search feature that leverages the strengths of Laravel, Livewire, and Neon.

## Prerequisites

Before we begin, you will need to have the following:

- PHP 8.1 or higher installed on your system
- [Composer](https://getcomposer.org/) for managing PHP dependencies
- [Node.js](https://nodejs.org/) and [npm](https://www.npmjs.com/) installed on your local machine for managing front-end assets
- A [Neon](https://console.neon.tech/signup) account
- Basic knowledge of Laravel, Livewire, and Tailwind CSS

## Setting up the Project

Before we dive into building the search functionality, let's set up a new Laravel project and configure the necessary components.

### Creating a New Laravel Project

Open your terminal and run the following command to create a new Laravel project:

```bash
composer create-project laravel/laravel real-time-search
cd real-time-search
```

This command will create a new Laravel project in a directory named `real-time-search`. Navigate to the project directory to continue with the setup.

### Installing and Configuring Livewire

Now that we have a Laravel project, let's install Livewire:

```bash
composer require livewire/livewire
```

Livewire will automatically register its service provider.

### Setting up Tailwind CSS

To use Tailwind CSS, we need to install and configure it as well.

```bash
npm install -D tailwindcss@latest postcss@latest autoprefixer@latest
npx tailwindcss init -p
```

Update your `tailwind.config.js` file to include Laravel and Livewire specific paths:

```javascript
/** @type {import('tailwindcss').Config} */
export default {
  content: ['./resources/**/*.blade.php', './resources/**/*.js', './resources/**/*.vue'],
  theme: {
    extend: {},
  },
  plugins: [],
};
```

The above configuration tells Tailwind to scan the specified files in the `resources` directory for classes to include in the compiled CSS.

This approach ensures that Tailwind's utility classes are available in your Laravel views and Livewire components and keeps your CSS bundle size minimal.

Next, add the `@tailwind` directives to your `resources/css/app.css` file:

```css
@tailwind base;
@tailwind components;
@tailwind utilities;
```

Finally, run the following command to compile your assets with Vite:

```bash
npm run dev
```

Leave the Vite development server running in the background to compile your assets and proceed with the next steps.

### Connecting to Neon Database

To connect your Laravel application to your Neon database, update your `.env` file with the Neon database credentials:

```env
DB_CONNECTION=pgsql
DB_HOST=your-neon-hostname.neon.tech
DB_PORT=5432
DB_DATABASE=your_database_name
DB_USERNAME=your_username
DB_PASSWORD=your_password
```

Make sure to replace the placeholders with your actual Neon database details.

## Building the Search Functionality

Now that our project is set up, let's build the search functionality. We'll create a simple product search feature that filters products based on their name and description.

### Creating the Database Schema

We'll create a simple `products` table for this example. Run the following command to create a migration:

```bash
php artisan make:model Product -m
```

Open the newly created migration file in `database/migrations` and update the `up` method to include the necessary columns:

```php
public function up()
{
    Schema::create('products', function (Blueprint $table) {
        $table->id();
        $table->string('name');
        $table->text('description');
        $table->decimal('price', 8, 2);
        $table->timestamps();
    });
}
```

For the sake of simplicity, we've included the `name`, `description`, and `price` columns in our `products` table.

Run the migration to create the `products` table in your database:

```bash
php artisan migrate
```

### Seeding Sample Data

Laravel provides a convenient way to seed your database with sample data. Let's create some sample data. Create a new seeder:

```bash
php artisan make:seeder ProductSeeder
```

Open `database/seeders/ProductSeeder.php` and add the following:

```php
<?php

namespace Database\Seeders;

use Illuminate\Database\Console\Seeds\WithoutModelEvents;
use Illuminate\Database\Seeder;

class ProductSeeder extends Seeder
{
    /**
     * Run the database seeds.
     */
    public function run(): void
    {
        $products = [
            ['name' => 'Laptop', 'description' => 'High-performance laptop', 'price' => 999.99],
            ['name' => 'Smartphone', 'description' => 'Latest model smartphone', 'price' => 699.99],
            ['name' => 'Headphones', 'description' => 'Noise-cancelling headphones', 'price' => 199.99],
            ['name' => 'Smartwatch', 'description' => 'Fitness tracking smartwatch', 'price' => 249.99],
            ['name' => 'Tablet', 'description' => '10-inch tablet', 'price' => 399.99],
            ['name' => 'Desktop', 'description' => 'High-performance desktop', 'price' => 1499.99],
            ['name' => 'Monitor', 'description' => '27-inch 4K monitor', 'price' => 499.99],
            ['name' => 'Keyboard', 'description' => 'Mechanical gaming keyboard', 'price' => 149.99],
            ['name' => 'Mouse', 'description' => 'Wireless mouse', 'price' => 49.99],
            ['name' => 'Printer', 'description' => 'Wireless all-in-one printer', 'price' => 199.99],
            ['name' => 'Scanner', 'description' => 'High-speed document scanner', 'price' => 299.99],
            ['name' => 'Projector', 'description' => '1080p home theater projector', 'price' => 799.99],
            ['name' => 'Camera', 'description' => 'Mirrorless camera', 'price' => 999.99],
            ['name' => 'Drone', 'description' => '4K camera drone', 'price' => 1199.99],
            ['name' => 'Gaming Console', 'description' => 'Next-gen gaming console', 'price' => 499.99],
            ['name' => 'VR Headset', 'description' => 'Wireless VR headset', 'price' => 299.99],
            ['name' => 'External Hard Drive', 'description' => '2TB external hard drive', 'price' => 99.99],
            ['name' => 'USB Flash Drive', 'description' => '128GB USB flash drive', 'price' => 29.99],
            ['name' => 'Wireless Router', 'description' => 'Dual-band wireless router', 'price' => 99.99],
            ['name' => 'Smart Speaker', 'description' => 'Voice-controlled smart speaker', 'price' => 79.99],
        ];

        foreach ($products as $product) {
            \App\Models\Product::create($product);
        }
    }
}

```

Update `database/seeders/DatabaseSeeder.php` to include the `ProductSeeder` class by adding it to the `run` method:

```php
public function run()
{
    $this->call([
        ProductSeeder::class,
    ]);
}
```

Run the seeder to populate the `products` table with sample data:

```bash
php artisan db:seed
```

This command will insert the sample products into the `products` table so we can test our search functionality. Note that the `db:seed` command will run all seeders by default, and if you run it multiple times, it will insert duplicate records.

### Implementing the Livewire Component

Next, let's create a Livewire component for our search functionality. Run the following command to generate a new Livewire component:

```bash
php artisan make:livewire ProductSearch
```

This command creates two files:

- `app/Livewire/ProductSearch.php`: The Livewire component class, which contains the search logic.
- `resources/views/livewire/product-search.blade.php`: The view file for the Livewire component.

Open `app/Livewire/ProductSearch.php` and update it with the following code which fetches products based on the search query:

```php
<?php

namespace App\Livewire;

use Livewire\Component;
use App\Models\Product;

class ProductSearch extends Component
{
    public $search = '';

    public function render()
    {
        $products = Product::where('name', 'like', '%' . $this->search . '%')
                           ->orWhere('description', 'like', '%' . $this->search . '%')
                           ->get();

        return view('livewire.product-search', [
            'products' => $products
        ]);
    }
}
```

Rundown of the code above:

- We start by defining a `$search` property that will be bound to the search input, and a `render` method that fetches products based on the search query.
- The `render` method queries the `products` table for records that match the search query in the `name` or `description` columns.
- We're using a simple `ILIKE` query to perform a case-insensitive search. You can customize the search logic based on your requirements.
- Next we get all matching products using the `get` method and pass them to the view. Alternatively, you can paginate the results for better performance using Laravel's `paginate` method.
- The `render` method returns the view `livewire.product-search` along with the `$products` variable.

Once you've updated the component class, let's create the view for this component. Open `resources/views/livewire/product-search.blade.php` and add the following content:

```html
<div>
  <div class="mb-4">
    <input
      wire:model.live.debounce.300ms="search"
      type="text"
      placeholder="Search products..."
      class="focus:ring-blue-500 w-full rounded-lg border px-4 py-2 focus:outline-none focus:ring-2"
    />
  </div>

  <div class="grid grid-cols-1 gap-4 lg:grid-cols-3 md:grid-cols-2">
    @forelse($products as $product)
    <div
      class="transform rounded-lg bg-white p-4 shadow transition duration-300 ease-in-out hover:scale-105"
    >
      <h3 class="text-lg font-semibold">{{ $product->name }}</h3>
      <p class="text-gray-600">{{ $product->description }}</p>
      <p class="text-blue-600 mt-2 font-bold">${{ number_format($product->price, 2) }}</p>
    </div>
    @empty
    <div class="rounded-lg bg-white p-4 text-center shadow">No products found.</div>
    @endforelse
  </div>
</div>
```

This view includes an input field for the search query and a grid to display the search results.

The `wire:model.live.debounce.300ms` attribute on the input field binds it to the `$search` property in our Livewire component, with a300ms debounce to reduce the number of database queries triggered by user input changes.

Using the `@forelse` directive, we loop through the `$products` collection and display each product's name, description, and price. If no products match the search query, we display a message indicating that no products were found.

### Updating the Layout

To use our new component, let's update the main layout. Open `resources/views/welcome.blade.php` and replace its content with:

```html
<!doctype html>
<html lang="{{ str_replace('_', '-', app()->getLocale()) }}">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Real-Time Search</title>
    @vite('resources/css/app.css') @livewireStyles
  </head>
  <body class="bg-gray-100">
    <div class="container mx-auto px-4 py-8">
      <h1 class="mb-8 text-center text-3xl font-bold">Real-Time Product Search</h1>
      @livewire('product-search')
    </div>

    @livewireScripts @vite('resources/js/app.js')
  </body>
</html>
```

This layout includes the necessary Livewire scripts and styles, as well as our `ProductSearch` component.

After updating the layout, make sure that your Vite development server is still running to compile the assets.

```bash
npm run dev
```

## Optimizing Search Performance with Neon

To optimize our search performance, we can leverage Neon's indexing capabilities.

Indexing the `name` and `description` columns will speed up search queries by allowing the database to quickly locate matching records.

Let's create an index on the `name` and `description` columns of our `products` table.

Create a new migration:

```bash
php artisan make:migration add_index_to_products_table
```

Open the new migration file and update the `up` and `down` methods to add and remove the index from the `products` table respectively:

```php
public function up()
{
    Schema::table('products', function (Blueprint $table) {
        $table->index(['name', 'description']);
    });
}

public function down()
{
    Schema::table('products', function (Blueprint $table) {
        $table->dropIndex(['name', 'description']);
    });
}
```

Run the migration:

```bash
php artisan migrate
```

This index will significantly improve the performance of our search queries, especially as the number of products grows.

## Testing the Search Functionality

To ensure our search functionality works as expected, let's write a simple test. Run the following command to create a test file:

```bash
php artisan make:test ProductSearchTest
```

Open the newly created test file in `tests/Feature/ProductSearchTest.php` and add the following test:

```php
<?php

namespace Tests\Feature;

use App\Livewire\ProductSearch;
use App\Models\Product;
use Illuminate\Foundation\Testing\RefreshDatabase;
use Livewire\Livewire;
use Tests\TestCase;

class ProductSearchTest extends TestCase
{
    // use RefreshDatabase;

    /** @test */
    public function it_can_search_products()
    {
        // If using RefreshDatabase trait, make sure to seed the database

        Livewire::test(ProductSearch::class)
            ->set('search', 'Laptop')
            ->assertSee('Laptop')
            ->assertDontSee('Phone');
    }
}
```

This test creates two products and then checks if the search functionality correctly filters the results.

> Make sure to only use the `RefreshDatabase` trait when running tests to avoid modifying your production database during testing.

Run the test with:

```bash
php artisan test
```

To learn more about testing in Laravel along with Neon branding, check out the [Testing Laravel Applications with Neon's Database Branching](https://neon.tech/guides/laravel-test-on-branch).

## Conclusion

In this tutorial, we've built a real-time search feature using Laravel, Livewire, and Neon. We've leveraged Livewire's real-time capabilities to create a responsive search component, and utilized Neon's high-performance database to ensure quick and efficient queries.

This implementation provides a solid foundation for a search feature, but there are always ways to enhance and expand its functionality:

- Implement pagination for large result sets
- Add filters for more refined searches
- Incorporate full-text search capabilities for more accurate results
- Implement caching to further improve performance
- Use [Laravel Scout](https://laravel.com/docs/11.x/scout) for full-text search capabilities

By combining the power of Laravel, the simplicity of Livewire, and the performance of Neon, it's easy to create dynamic and responsive web applications that meet your users' needs.

## Additional Resources

- [Laravel Documentation](https://laravel.com/docs)
- [Livewire Documentation](https://laravel-livewire.com/docs)
- [Tailwind CSS Documentation](https://tailwindcss.com/docs)
- [Neon Documentation](/docs)

<NeedHelp />


# Building a TODO Application with Laravel, Livewire, and Volt

---
title: Building a TODO Application with Laravel, Livewire, and Volt
subtitle: Learn how to create a simple yet powerful TODO app using Laravel, Livewire, Volt, and Laravel Breeze for authentication
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-06-30T00:00:00.000Z'
updatedOn: '2024-06-30T00:00:00.000Z'
---

In this guide, we'll walk through the process of building a TODO application using [Laravel](https://laravel.com/), [Livewire](https://livewire.laravel.com/), and [Volt](https://livewire.laravel.com/docs/volt).

We'll use [Laravel Breeze](https://laravel.com/docs/11.x/starter-kits) for authentication and Neon Postgres as our database.

By the end of this tutorial, you'll have a simple yet fully functional TODO application that allows users to create, update, and delete tasks.

## Prerequisites

Before we begin, ensure you have the following:

- PHP 8.1 or higher installed on your system
- [Composer](https://getcomposer.org/) for managing PHP dependencies
- [Node.js](https://nodejs.org/) and npm for managing front-end assets
- A [Neon](https://console.neon.tech/signup) account for database hosting
- Basic knowledge of Laravel and Livewire

## Setting up the Project

Let's start by creating a new Laravel project and setting up the necessary components for our TODO application.

### Install the Laravel Installer

The Laravel installer is a command-line tool that simplifies the process of creating new Laravel projects. If you don't have it installed, run the following command:

```bash
composer global require laravel/installer
```

To verify the installation, run:

```bash
laravel --version
```

This should display the Laravel installer version, confirming that the installation was successful.

### Creating a New Laravel Project

Open your terminal and run the following command to create a new Laravel project using the Laravel installer:

```bash
laravel new laravel-todo-volt
```

Follow the on-screen instructions to create the project by selecting the following options:

- Starter Kit: 'Laravel Breeze'
- Breeze stack: 'Livewire (Volt Class API) with Alpine'
- Dark mode: Based on your preference
- Testing framework: Pest
- Database: PostgreSQL

This command creates a new Laravel project named `laravel-todo-volt` with the selected options and installs the necessary dependencies like Breeze, Livewire, and Volt.

After the project is created, navigate to the project directory:

```bash
cd laravel-todo-volt
```

This can also be done directly via `composer` instead of using the Laravel installer, but the Laravel installer provides an interactive setup process that simplifies the initial project setup rather than running multiple commands manually.

### Setting up the Database

Update your `.env` file with your Neon database credentials:

```env
DB_CONNECTION=pgsql
DB_HOST=your-neon-hostname.neon.tech
DB_PORT=5432
DB_DATABASE=your_database_name
DB_USERNAME=your_username
DB_PASSWORD=your_password
```

Make sure to replace `your-neon-hostname`, `your_database_name`, `your_username`, and `your_password` with your actual Neon database details.

Once you've updated the `.env` file, run the following command to create the default tables:

```bash
php artisan migrate
```

This will create the necessary tables in your Neon database.

### Compiling Assets

Laravel Breeze uses Vite for asset compilation. Run the following commands to install dependencies and compile assets:

```bash
npm install
npm run dev
```

Keep the Vite development server running in the background as you continue with the next steps.

## Creating the TODO Model and Migration

A model in Laravel represents a database table and allows you to interact with the table's data using Eloquent ORM. The migration file defines the structure of the database table and is used to create or modify the table.

The standard convention in Laravel is to create the model with singular naming and the migration with plural naming. For example, a `Todo` model would correspond to a `todos` table in the database, `User` model to `users` table, and so on.

Now, let's create the `Todo` model along with its migration:

```bash
php artisan make:model Todo -m
```

Open the newly created migration file in `database/migrations` and update it to include the necessary columns for the `todos` table:

```php
public function up()
{
    Schema::create('todos', function (Blueprint $table) {
        $table->id();
        $table->foreignId('user_id')->constrained()->onDelete('cascade');
        $table->string('title');
        $table->text('description')->nullable();
        $table->boolean('is_completed')->default(false);
        $table->timestamps();
    });
}
```

This migration creates a `todos` table with columns for the todo title, description, completion status, and a foreign key to the `users` table.

The `onDelete('cascade')` method ensures that todos are deleted when the corresponding user is deleted so that we don't have orphaned records left in the database.

Update the `app/Models/Todo.php` model file to include the relationship with the user and the fillable fields:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class Todo extends Model
{
    use HasFactory;

    protected $fillable = ['title', 'description', 'is_completed'];

    public function user()
    {
        return $this->belongsTo(User::class);
    }
}
```

The `$fillable` property specifies which fields can be mass-assigned when creating or updating a todo task. This helps protect against mass-assignment vulnerabilities and ensures that only the specified fields are allowed to be updated.

Also, update the `app/Models/User.php` file to include the relationship with todos:

```php
public function todos()
{
    return $this->hasMany(Todo::class);
}
```

This method defines a one-to-many relationship between the `User` and `Todo` models, allowing us to retrieve all todos associated with a user. For example, `Auth::user()->todos` will return all todos created by the authenticated user.

Now, run the migrations to create the `todos` table in your Neon database:

```bash
php artisan migrate
```

## Creating the Volt Component

Volt is a new addition to Livewire that allows you to define Livewire components using a class-based API.

This makes it easier to organize and manage your components, especially for larger applications with many components.

Let's create a Volt component for our TODO list:

```bash
php artisan make:volt TodoList
```

This command creates a new Volt component file at `resources/views/livewire/todolist.blade.php`. This single file will contain both the component's logic and its template compared to the traditional Livewire components where the logic is in a separate PHP file.

```php
<?php

use App\Models\Todo;
use Illuminate\Support\Facades\Auth;
use function Livewire\Volt\{state, computed, action};

state(['newTodoTitle' => '']);

$todos = computed(function () {
    return Auth::user()->todos()->latest()->get();
});

$addTodo = action(function () {
    $this->validate([
        'newTodoTitle' => 'required|min:3'
    ]);

    Auth::user()->todos()->create([
        'title' => $this->newTodoTitle,
    ]);

    $this->newTodoTitle = '';
});

$toggleComplete = action(function (Todo $todo) {
    $todo->update(['is_completed' => !$todo->is_completed]);
});

$deleteTodo = action(function (Todo $todo) {
    $todo->delete();
});

?>

<div>
    <h2 class="text-2xl font-semibold mb-4">Your TODO List</h2>

    <form wire:submit="addTodo" class="mb-4">
        <div class="flex">
            <input
                wire:model="newTodoTitle"
                type="text"
                class="flex-1 rounded-l-md border-gray-300 shadow-sm focus:border-indigo-300 focus:ring focus:ring-indigo-200 focus:ring-opacity-50 dark:bg-gray-700 dark:border-gray-600 dark:text-indigo-400 dark:focus:ring-indigo-500 dark:focus:border-indigo-500"
                placeholder="Enter a new TODO item"
            >
            <button type="submit" class="bg-blue-500 text-white px-4 py-2 rounded-r-md hover:bg-blue-600">Add</button>
        </div>
        @error('newTodoTitle') <span class="text-red-500 text-sm">{{ $message }}</span> @enderror
    </form>

    <ul class="space-y-2">
        @foreach($this->todos as $todo)
            <li class="flex items-center justify-between bg-white p-4 rounded-md shadow dark:bg-gray-800">
                <div class="flex items-center">
                    <input
                        type="checkbox"
                        wire:click="toggleComplete({{ $todo->id }})"
                        {{ $todo->is_completed ? 'checked' : '' }}
                        class="mr-2 rounded border-gray-300 text-indigo-600 shadow-sm focus:border-indigo-300 focus:ring focus:ring-indigo-200 focus:ring-opacity-50 dark:bg-gray-700 dark:border-gray-600 dark:text-indigo-400 dark:focus:ring-indigo-500 dark:focus:border-indigo-500"
                    >
                    <span class="{{ $todo->is_completed ? 'line-through text-gray-400' : '' }}">
                        {{ $todo->title }}
                    </span>
                </div>
                <button
                    wire:click="deleteTodo({{ $todo->id }})"
                    class="text-red-500 hover:text-red-700 dark:text-red-400 dark:hover:text-red-600"
                >
                    Delete
                </button>
            </li>
        @endforeach
    </ul>
</div>
```

This Volt component combines the component's logic and template in a single file. Let's break down the key parts:

- We use `state()` to define reactive properties like `newTodoTitle`.
- The `computed()` function is used to create a dynamic property for fetching todos.
- The `action()` defines methods that can be triggered from the template.
- The template section uses Livewire directives like `wire:submit` and `wire:click` to interact with the component's logic.

This approach allows for a more self-contained component definition, making it easier to understand and maintain your Livewire components rather than having the logic and template in separate files.

## Integrating the TODO List into the Dashboard

Now that we have the Volt component ready, let's integrate our TODO list into the main dashboard.

Open `resources/views/dashboard.blade.php` and replace its content with:

```html
<x-app-layout>
  <x-slot name="header">
    <h2 class="text-gray-800 dark:text-gray-200 text-xl font-semibold leading-tight">
      {{ __('Dashboard') }}
    </h2>
  </x-slot>

  <div class="py-12">
    <div class="mx-auto max-w-7xl lg:px-8 sm:px-6">
      <div class="dark:bg-gray-800 overflow-hidden bg-white shadow-sm sm:rounded-lg">
        <div class="text-gray-900 dark:text-gray-100 p-6">
          <livewire:todolist />
        </div>
      </div>
    </div>
  </div>
</x-app-layout>
```

This integrates our `TodoList` component into the Breeze dashboard by using the `livewire:todolist` directive. When you visit the dashboard, you should see the TODO list component displayed on the page.

As we have used the `auth()->user()` method in the `TodoList` component to fetch the user's todos, each user will see their own list of todos when they visit the dashboard, however we have not yet implemented any authorization to ensure that users can only manage their own todos. We'll cover this in the next section.

## Adding Authorization

To ensure users can only manage their own todos, let's implement some basic authorization.

In addition to the authentication provided by Laravel Breeze, Laravel provides an authorization system that allows you to define policies for your models. These policies define the rules for accessing and managing resources, such as todos in our case.

Create a new policy for the `Todo` model using the following command:

```bash
php artisan make:policy TodoPolicy --model=Todo
```

Open `app/Policies/TodoPolicy.php` and update it to define the authorization rules for updating and deleting todo items:

```php
<?php

namespace App\Policies;

use App\Models\Todo;
use App\Models\User;
use Illuminate\Auth\Access\Response;

class TodoPolicy
{
    public function update(User $user, Todo $todo): bool
    {
        return $user->id === $todo->user_id;
    }

    public function delete(User $user, Todo $todo): bool
    {
        return $user->id === $todo->user_id;
    }
}
```

Here, we define two methods: `update` and `delete`. These methods check if the authenticated user is the owner of the todo by comparing the user's ID with the todo's `user_id`. If the user is the owner, the method returns `true`, allowing the user to update or delete the todo. Otherwise, it returns `false` and denies access.

Now, update the `TodoList` component to use these policies when toggling completion status or deleting todos:

```php
$toggleComplete = action(function (Todo $todo) {
    if (auth()->user()->cannot('update', $todo)) {
        return;
    }
    $todo->update(['is_completed' => !$todo->is_completed]);
});

$deleteTodo = action(function (Todo $todo) {
    if (auth()->user()->cannot('delete', $todo)) {
        return;
    }
    $todo->delete();
});
```

In these updated actions:

- We use `auth()->user()->cannot('update', $todo)` and `auth()->user()->cannot('delete', $todo)` to check if the current user is authorized to perform the respective actions based on the `TodoPolicy` rules.
- If the user is not authorized, the function returns early without performing the action preventing unauthorized access.
- If the user is authorized, the action proceeds as before allowing the user to toggle completion status or delete the todo.

This way you can make sure that users can only toggle completion status or delete todos that they own, as defined in the `TodoPolicy`.

## Writing Tests for Your TODO Application with Pest

Testing is an important part of the development process. There are a few different ways to write tests in Laravel, including PHPUnit and Pest. Choosing the right testing framework depends on your preference and the requirements of your project.

In this section, we'll cover writing tests using Pest, a more expressive and minimalistic testing framework for PHP.

Pest provides a [Livewire plugin](https://pestphp.com/docs/plugins#livewire), which allows you to write tests for Livewire components in a more readable and concise way. To install the Pest plugin for Livewire, run the following command:

```bash
composer require pestphp/pest-plugin-livewire --dev
```

### Setting Up the Test Environment

For this example, we will use an in-memory SQLite database for testing. This ensures that tests run quickly and do not affect your production database.

However, to learn more about testing in Laravel along with Neon branding, check out the [Testing Laravel Applications with Neon's Database Branching](https://neon.tech/guides/laravel-test-on-branch). This guide will help you set up a separate database branch for testing, allowing you to test your application with real data rather than an in-memory database.

To get started, ensure your `.env.testing` file is configured to use an in-memory SQLite database for testing:

```env
APP_KEY=base64:kf_your_app_key_here
DB_CONNECTION=sqlite
DB_DATABASE=:memory:
```

This will allow us to use the `RefreshDatabase` trait to reset the database before each test, ensuring a clean slate for testing.

> It is important to note that the `RefreshDatabase` trait will clear the database before each test, so make sure to use a separate database for testing to avoid data loss as the database will be reset for each test meaning that any data that you have in the database will be lost.

### Creating and Using a `TodoFactory`

Factories in Laravel generate sample data for models, useful for testing and database seeding with realistic data thanks to the Faker library.

Let's create a factory for our `Todo` model which will generate random todo items for testing:

```bash
php artisan make:factory TodoFactory --model=Todo
```

Update `database/factories/TodoFactory.php` to add the necessary fields and states for generating todo items:

```php
<?php

namespace Database\Factories;

use App\Models\Todo;
use App\Models\User;
use Illuminate\Database\Eloquent\Factories\Factory;

class TodoFactory extends Factory
{
    protected $model = Todo::class;

    public function definition()
    {
        return [
            'user_id' => User::factory(),
            'title' => $this->faker->sentence(4),
            'description' => $this->faker->paragraph(),
            'is_completed' => $this->faker->boolean(20),
            'created_at' => $this->faker->dateTimeBetween('-1 month', 'now'),
            'updated_at' => $this->faker->dateTimeBetween('-1 month', 'now'),
        ];
    }

    public function completed()
    {
        return $this->state(['is_completed' => true]);
    }

    public function incomplete()
    {
        return $this->state(['is_completed' => false]);
    }
}
```

This factory generates random todo items with titles, descriptions, and completion status. We've also defined two states: `completed` and `incomplete` to create todos with specific completion statuses.

### Creating Feature Tests

Let's create a feature test file for our TODO list functionality using the following command:

```bash
php artisan make:test TodoListTest
```

This command creates a new test file at `tests/Feature/TodoListTest.php`. Open this file and replace its contents with the following:

```php
<?php

use App\Models\User;
use App\Models\Todo;
use function Pest\Laravel\get;
use function Pest\Laravel\{actingAs};
use function Pest\Livewire\livewire;

uses(\Illuminate\Foundation\Testing\RefreshDatabase::class);

test('user can view todos', function () {
    $user = User::factory()->create();

    $todos = Todo::factory()->count(3)->create([
        'user_id' => $user->id,
    ]);

    actingAs($user);

    livewire('todolist')
        ->assertSee($todos[0]->title)
        ->assertSee($todos[1]->title)
        ->assertSee($todos[2]->title);
});

test('user can delete a todo', function () {
    $user = User::factory()->create();

    $todo = Todo::factory()->create([
        'user_id' => $user->id,
    ]);

    actingAs($user);

    livewire('todolist')
        ->call('deleteTodo', $todo->id);

    expect(Todo::find($todo->id))->toBeNull();
});

test('user can not delete a todo that does not belong to them', function () {
    $user = User::factory()->create();
    $otherUser = User::factory()->create();

    $todo = Todo::factory()->create([
        'user_id' => $otherUser->id,
    ]);

    actingAs($user);

    livewire('todolist')
        ->call('deleteTodo', $todo->id);

    expect(Todo::find($todo->id))->not->toBeNull();
});
```

These tests cover the following scenarios:

- A user can view their todos when visiting the dashboard.
- A user can delete a todo that belongs to them.
- A user cannot delete a todo that belongs to another user.

The `actingAs()` function is used to authenticate the user before interacting with the Livewire component. This ensures that the user is authorized to perform the actions.

The `livewire()` function is used to interact with the Livewire component and make assertions based on the component's state.

### Running the Tests

Again, before you run the tests, note that the `RefreshDatabase` trait will clear the database before each test, so make sure to use a separate database for testing to avoid data loss like an in-memory SQLite database or a Neon database branch.

You can run these tests using the following command:

```bash
php artisan test
```

This will execute the tests and provide feedback on the results. Writing tests helps ensure that your application behaves as expected and catches bugs early in the development process.

## Conclusion

In this tutorial, we've built a simple yet functional TODO application using Laravel, Livewire, and Volt. We've covered:

1. Setting up a new Laravel project with Breeze, Livewire, and Volt
1. Creating a `Todo` model and migration
1. Implementing a Volt component for managing todos
1. Integrating the TODO list into the dashboard
1. Adding basic authorization to ensure users can only manage their own todos
1. Writing tests for the TODO application using Pest

This application provides a solid foundation for a TODO list, showing the power and simplicity of Laravel, Livewire, and Volt. From here, you could expand the functionality by adding features such as:

- Due dates for todos
- Categorization or tagging of todos
- Sorting and filtering options
- Sharing todos with other users
- Assigning todos to specific users

## Additional Resources

- [Laravel Documentation](https://laravel.com/docs)
- [Livewire Documentation](https://laravel-livewire.com/docs)
- [Laravel Breeze Documentation](https://laravel.com/docs/breeze)
- [Tailwind CSS Documentation](https://tailwindcss.com/docs)
- [Neon Documentation](/docs)


# Reverting a failed deployment and schema migration in Laravel

---
title: Reverting a failed deployment and schema migration in Laravel
subtitle: Learn how to revert a failed deployment and schema migration in Laravel using built-in tools like `migrate:rollback` and Neon's backup and restore capabilities.
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-05-26T00:00:00.000Z'
updatedOn: '2024-05-26T00:00:00.000Z'
---

Deploying new features and updates is an essential part of maintaining a modern web application. However, not every deployment goes as planned.

When something goes wrong, especially with schema migrations, the consequences can range from data inconsistencies to extended application downtime.

In this guide, you'll learn how to revert a failed deployment and schema migration in Laravel using built-in tools like `migrate:rollback` and the backup and restore capabilities of Neon. We'll cover practical steps and best practices for deployment and recovery, helping you prevent future deployment issues.

## Rolling back migrations in Laravel

Laravel provides a built-in [Artisan migrate command](https://laravel.com/docs/11.x/migrations#rolling-back-migrations) for reverting schema changes when a migration doesn't go as expected.

Here's how to use it:

### Using `php artisan migrate:rollback`

1. **Revert the last batch of migrations:**

   ```bash
   php artisan migrate:rollback
   ```

   This command will revert the last batch of migrations applied to your database.

2. **Revert a specific number of steps:**
   If you need more control, use the `--step` option:

   ```bash
   php artisan migrate:rollback --step=1
   ```

   This command will only roll back the most recent migration file.

3. **Rolling back to a specific point in time:**
   For more granular control over the rollback process, use the `--date` option with a date string.
   ```bash
   php artisan migrate:rollback --date="2024-05-01 12:00:00"
   ```
   This command will roll back all migrations that were executed after the specified date.

### Troubleshooting rollback issues

If you encounter issues while rolling back migrations, follow these troubleshooting steps:

- **Failed rollback command:** Check the Laravel logs for error messages. For instance, if you see an error about missing tables or columns:

  - Revisit the migration files and ensure they are consistent.
  - Adjust or fix the migrations as needed.

- **Ensure idempotency:**
  Make sure all migrations can be run and rolled back without side effects. Here's an example of an idempotent migration:

  ```php
  // database/migrations/2024_05_10_000000_add_status_to_users.php
  use Illuminate\Database\Migrations\Migration;
  use Illuminate\Database\Schema\Blueprint;
  use Illuminate\Support\Facades\Schema;

  class AddStatusToUsers extends Migration
  {
      public function up()
      {
          if (!Schema::hasColumn('users', 'status')) {
              Schema::table('users', function (Blueprint $table) {
                  $table->string('status')->default('active');
              });
          }
      }

      public function down()
      {
          if (Schema::hasColumn('users', 'status')) {
              Schema::table('users', function (Blueprint $table) {
                  $table->dropColumn('status');
              });
          }
      }
  }
  ```

### Database verification after rollback

After rolling back migrations, verify the database schema and data to ensure the rollback was successful.

- **Check migration status:**

  ```bash
  php artisan migrate:status
  ```

  This command lists all migrations, showing which ones have been applied.

- **Inspect database directly:**
  Use your database management tool to directly inspect the schema and data.

> **Note:** `migrate:rollback` can lead to data loss if not used carefully. Ensure you have a backup strategy in place.

## Restoring your data using Neon

If rolling back migrations doesn't solve the issue, [Neon's backup and restore](/docs/manage/backups) capabilities can quickly restore your database to a previous state.

### Key benefits of using Neon for restoration

1. **Point-in-Time Restoration:** Restore to a specific moment before the failed deployment.
2. **Restore from another Branch:** Use Neon's branching feature to restore from a stable branch.

### Restoration steps

To restore your database using Neon, you can either use the Neon dashboard or the Neon CLI or API. Follow the steps outlined in [Neon's Branch Restore Guide](/docs/guides/branch-restore#how-to-use-branch-restore):

After restoring the database, align your codebase with the restored data to ensure consistency.

For detailed steps, refer to the [Neon Branch Restore Guide](/docs/guides/branch-restore#how-to-use-branch-restore).

## Best practices for deployment and recovery

No deployment process is foolproof, but following best practices can help you recover quickly and prevent future issues.

### Use a staging environment

Replicate your production environment for testing before deploying features to production.

1. **Develop and test locally:**
   Run migrations and tests against your development database.

2. **Deploy changes and migrations to staging:**
   Ensure your staging environment closely resembles production.

3. **Perform thorough testing:**
   Use automated and manual testing to validate changes.

4. **Promote changes to production after verification:**
   Deploy to production only after all tests pass.

### Break down database changes

Smaller, manageable migrations make rollbacks simpler.

- **Reduce extensive issues:**
  Break changes into smaller, logical batches.

- **Example workflow:**

  1. Create multiple smaller migrations instead of one large one.

  ```bash
  php artisan make:migration add_status_to_users
  php artisan make:migration add_type_to_users
  ```

  2. Test each migration in isolation.

  ```bash
  php artisan migrate
  ```

### Implement a robust backup strategy

1. **Daily Full Backups:** Schedule daily full backups.
2. **Incremental Backups:** Use frequent snapshots throughout the day.
3. **Retention Policies:** Keep backups long enough for compliance and audits.

It is important to test your backup and restore process regularly to ensure it works as expected.

## Preventing future deployment issues

Even with a solid recovery plan, preventing deployment issues in the first place is the best approach. There are several strategies to minimize deployment problems:

### Automate and validate deployments

Implement a CI/CD pipeline to streamline the deployment process and add safeguards.

**Example CI/CD pipeline:**

1. **Build Stage:** Install dependencies and compile assets.

   ```yaml
   - name: Install Dependencies
     run: composer install
   - name: Compile Assets
     run: npm run build
   ```

2. **Test stage:** Run tests and validate coding standards.

   ```yaml
   - name: Run Unit Tests
     run: php artisan test
   - name: Run Coding Standards Check
     run: php artisan lint
   ```

3. **Deployment Stage:** Deploy to staging, run health checks, and promote to production if all tests pass.

### Use Neon's branching feature

Create isolated environments for testing and staging using [Neon's branching feature](/docs/introduction/branching).

1. **Create a branch:**
   Create a branch from your production database in the Neon dashboard.

2. **Deploy code to staging:**
   Point your staging environment to the new branch.

3. **Test migrations and features thoroughly:**
   Ensure migrations work correctly and features function as expected.

4. **Merge to production:**
   Deploy your changes to production after successful testing.

For a detailed guide on using Neon's branching feature with Laravel for testing and staging, refer to the [Testing Laravel Applications with Neon's Database Branching](/guides/laravel-test-on-branch).

### Set up monitoring and alerts

Proactively monitor your application and database. That way, you can catch issues early and respond quickly.

Some monitoring tools to consider can include tools like New Relic, Sentry, or Datadog.

On the database monitoring side, you can use [Neon's built-in monitoring capabilities](/docs/introduction/monitoring-page) to track performance metrics and receive alerts for potential issues.

## Conclusion

By using Laravel's built-in `migrate:rollback` command and Neon's backup and restore capabilities, you can revert a failed deployment quickly and safely. Follow best practices like testing in staging environments, breaking down database changes, and automating deployments to minimize future issues and maintain a smooth deployment process.

- [Laravel Migrations Documentation](https://laravel.com/docs/11.x/migrations)
- [Neon documentation](/docs)


# Creating a Multi-Tenant Application with Laravel and Neon

---
title: Creating a Multi-Tenant Application with Laravel and Neon
subtitle: Learn how to build a scalable multi-tenant application using Laravel and Neon's powerful database features
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-06-30T00:00:00.000Z'
updatedOn: '2024-06-30T00:00:00.000Z'
---

Multi-tenancy is a software architecture where a single instance of an application serves multiple tenants or clients.

Each tenant's data is isolated and remains invisible to other tenants. This approach is commonly used in Software as a Service (SaaS) applications. In this tutorial, we'll build the foundation for a multi-tenant SaaS application using Laravel and Neon.

By the end of this tutorial, you'll have a fully functional multi-tenant SaaS application where tenants can manage their own books, users, and settings, all while maintaining data isolation between tenants.

## Prerequisites

Before we start, make sure you have the following:

- PHP 8.1 or higher installed on your system
- [Composer](https://getcomposer.org/) for managing PHP dependencies
- [Node.js](https://nodejs.org/) and npm for managing front-end assets
- A [Neon](https://console.neon.tech/signup) account for database hosting
- Basic knowledge of Laravel and Livewire

## Setting up the Project

Let's start by creating a new Laravel project and setting up the necessary components.

### Creating a New Laravel Project

Open your terminal and run the following command to create a new Laravel project:

```bash
composer create-project laravel/laravel laravel-multi-tenant-saas
cd laravel-multi-tenant-saas
```

### Installing Required Packages

For our multi-tenant SaaS application, we'll use the following package:

- `stancl/tenancy`: A flexible multi-tenancy package for Laravel
- Laravel Breeze: A minimal authentication starter kit for Laravel

Start by installing the `stancl/tenancy` package:

```bash
composer require stancl/tenancy
```

After installing the package, let's set up the tenancy:

```bash
php artisan tenancy:install
```

Register the `TenancyServiceProvider` in the `bootstrap/providers.php` file:

```php
return [
    // ...
    App\Providers\TenancyServiceProvider::class,
],
```

Let's install Laravel Breeze with the Blade views:

```bash
composer require laravel/breeze --dev
php artisan breeze:install blade
```

Next, install the required NPM packages:

```bash
npm install
npm run dev
```

### Setting up the Database

Update your `.env` file with your Neon database credentials:

```env
DB_CONNECTION=pgsql
DB_HOST=your-neon-hostname.neon.tech
DB_PORT=5432
DB_DATABASE=your_database_name
DB_USERNAME=your_username
DB_PASSWORD=your_password
```

After updating the `.env` file, run the database migrations:

```bash
php artisan migrate
```

## Implementing Multi-Tenancy

Now that we have our basic setup, let's implement multi-tenancy in our application.

### Creating the Tenant Model

Create a `Tenant` model:

```bash
php artisan make:model Tenant
```

Update the `app/Models/Tenant.php` file:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Stancl\Tenancy\Database\Models\Tenant as BaseTenant;
use Stancl\Tenancy\Contracts\TenantWithDatabase;
use Stancl\Tenancy\Database\Concerns\HasDatabase;
use Stancl\Tenancy\Database\Concerns\HasDomains;

class Tenant extends BaseTenant implements TenantWithDatabase
{
    use HasFactory, HasDatabase, HasDomains;

}
```

This model extends the base `Tenant` model provided by the tenancy package and implements the `TenantWithDatabase` interface. We've also defined the fillable attributes and custom columns for our tenant.

The `HasDatabase` and `HasDomains` traits provided by the tenancy package allow us to manage tenant-specific databases and domains. This essentially means that each tenant will have its own database and domain providing data isolation between tenants.

To learn more about the tenancy package event system and how to customize the tenant model, refer to the [stancl/tenancy documentation](https://tenancyforlaravel.com/docs/v3/event-system).

### Configuring Tenancy

Update the `config/tenancy.php` file to use our custom `Tenant` model:

```php
'tenant_model' => \App\Models\Tenant::class,
```

Also, update the central domains configuration:

```php
'central_domains' => [
    'laravel-multi-tenant-saas.test',
    'localhost',
    'example.com',
],
```

Replace the default central domains with your own domain names.

This is an important part as this is how the tenancy package will determine which domain belongs to which tenant and load the tenant-specific data accordingly.

Feel free to review the other configuration options in the `config/tenancy.php` file to customize the tenancy behavior based on your requirements.

### Creating Tenant Migrations

The tenancy package has built-in event listeners that automatically run tenant-specific migrations when a tenant is created. For this we need to make sure that all of the tenant-specific migrations are in the `database/migrations/tenant` directory.

As each tenant will have its own database, the migrations in the tenant directory will be used to create tenant-specific tables in the tenant's database.

Start by copying the default User migration to the `database/migrations/tenant` directory:

```bash
cp database/migrations/0001_01_01_000000_create_users_table.php database/migrations/tenant
```

This will be the base migration for tenant-specific tables.

### Implementing Tenant Routes

The tenancy package provides middleware to handle tenant-specific routes. This allows you to define routes that are accessible only to tenants and not to central domains.

Start by creating a new file `routes/tenant.php` for tenant-specific routes with the following content:

```php
<?php

use Illuminate\Support\Facades\Route;
use Stancl\Tenancy\Middleware\InitializeTenancyByDomain;
use Stancl\Tenancy\Middleware\PreventAccessFromCentralDomains;

/*
|--------------------------------------------------------------------------
| Tenant Routes
|--------------------------------------------------------------------------
|
| Here you can register the tenant routes for your application.
| These routes are loaded by the TenantRouteServiceProvider.
|
| Feel free to customize them however you want. Good luck!
|
*/

Route::middleware([
    'web',
    InitializeTenancyByDomain::class,
    PreventAccessFromCentralDomains::class,
])->group(function () {
    Route::get('/', function () {
        return 'This is your multi-tenant application. The id of the current tenant is ' . tenant('id');
    });

    // Here you can add more tenant-specific routes
});
```

These routes will be loaded by the `TenantRouteServiceProvider` and will be accessible only to tenants. The `InitializeTenancyByDomain` middleware will set the current tenant based on the domain, and the `PreventAccessFromCentralDomains` middleware will prevent access from central domains.

For more information on how to customize the tenancy routes, refer to the [stancl/tenancy documentation](https://tenancyforlaravel.com/docs/v3/routes).

### Implementing Tenant Creation

Create a controller for tenant registration, this would usually be done by the admin users of the application:

```bash
php artisan make:controller TenantController
```

Update the `app/Http/Controllers/TenantController.php` controller and implement the tenant registration process:

```php
<?php

namespace App\Http\Controllers;

use App\Models\Tenant;
use Illuminate\Http\Request;

class TenantController extends Controller
{
    public function showRegistrationForm()
    {
        return view('tenant.register');
    }

    public function register(Request $request)
    {
        $request->validate([
            'domain' => 'required|string|max:255|unique:domains,domain',
        ]);

        $tenant = Tenant::create();
        $tenant->domains()->create(['domain' => $request->domain]);

        return redirect()->route('tenant.registered', $request->domain);
    }

    public function registered($domain)
    {
        return view('tenant.registered', compact('domain'));
    }
}
```

This controller handles tenant registration, creates a new tenant in the database, and sets up the tenant's domain. The `TenancyServiceProvider` will automatically map the tenancy events to the listener, which will create the tenant's database and run the tenant-specific migrations inside the `database/migrations/tenant` directory for the new tenant.

In a nutshell, the controller has three methods:

- `showRegistrationForm()`: Displays the tenant registration form
- `register()`: Registers a new tenant, which creates a new tenant record and domain
- `registered()`: Displays a success message after registration

This controller will be used to manage tenant registration in our application. Allowing new tenants to register and create their own subdomain and database for their account.

Add routes for tenant registration in `routes/web.php`:

```php
use App\Http\Controllers\TenantController;

Route::get('/register', [TenantController::class, 'showRegistrationForm'])->name('tenant.register');
Route::post('/register', [TenantController::class, 'register']);
Route::get('/registered/{domain}', [TenantController::class, 'registered'])->name('tenant.registered');
```

Create the corresponding views for tenant registration starting by creating the `resources/views/tenant/register.blade.php` file:

```html
<x-guest-layout>
  <form method="POST" action="{{ route('tenant.register') }}">
    @csrf
    <div class="mt-4">
      <x-input-label for="domain" :value="__('Subdomain')" />
      <div class="flex">
        <x-text-input
          id="domain"
          class="mt-1 block w-full"
          type="text"
          name="domain"
          :value="old('domain')"
          required
        />
        <span class="text-gray-600 ml-2 mt-1">.example.com</span>
      </div>
    </div>

    <div class="mt-4 flex items-center justify-end">
      <x-primary-button class="ml-4"> {{ __('Register Tenant') }} </x-primary-button>
    </div>
  </form>
</x-guest-layout>
```

Then create the `resources/views/tenant/registered.blade.php` file to display the success message after registration:

```html
<x-guest-layout>
  <div class="text-gray-600 mb-4 text-sm">
    {{ __('Your tenant has been registered successfully!') }}
  </div>

  <div class="mt-4 flex items-center justify-between">
    <div>
      Your tenant URL:
      <a
        href="https://{{ $domain }}.example.com"
        class="text-gray-600 hover:text-gray-900 text-sm underline"
        target="_blank"
        >https://{{ $domain }}.example.com</a
      >
    </div>
  </div>
</x-guest-layout>
```

This completes the tenant registration process. Tenants can now register and create their own subdomain and database for their account. In a real-world scenario, you would protect the registration routes with authentication middleware to ensure that only authorized admin users can create new tenants.

### Verifying Tenant Registration

To verify that the registration process works, visit `http://laravel-multi-tenant-saas.test/register` and register a new tenant. After registration, you should see the success message with the tenant's domain.

Next go to your Neon dashboard and verify that the new tenant's database has been created:

```sql
SELECT * FROM tenants;
```

You should see the newly created tenant in the `tenants` table. You can also check the `domains` table to verify that the tenant's domain has been added:

```sql
SELECT * FROM domains;
```

And to verify that you actually have a separate database for the new tenant, use the `\l` command in the `psql` console to list all databases or the following SQL query:

```sql
SELECT datname FROM pg_database WHERE datistemplate = false;
```

The tenant's database should be listed in the results and it should be named `tenant{tenant_id}`.

> The tenancy package allows you to configure the database naming convention for tenants. By default, the database name is `tenant{tenant_id}` where `{tenant_id}` is the ID of the tenant. You can also configure the package to use separate schemas instead of separate databases for tenants.

With that done, you've successfully implemented tenant registration in your multi-tenant SaaS application. Next let's implement the tenant onboarding process.

### Implementing Tenant Onboarding

Now that you can register new tenants, let's create an onboarding process.

Each tenant will need to create an account to access their dashboard. The domain will be used to identify the tenant, so we'll use the domain as the tenant's subdomain, e.g., `tenant1.example.com`.

Create a new controller for tenant onboarding:

```bash
php artisan make:controller Tenant/OnboardingController
```

Update the `app/Http/Controllers/Tenant/OnboardingController.php` to handle the onboarding process:

```php
<?php

namespace App\Http\Controllers\Tenant;

use App\Http\Controllers\Controller;
use App\Models\User;
use Illuminate\Http\Request;
use Illuminate\Support\Facades\Hash;

class OnboardingController extends Controller
{
    public function show()
    {
        if (User::count() > 0) {
            return redirect()->route('tenant.dashboard');
        }

        return view('tenant.onboarding');
    }

    public function store(Request $request)
    {
        $validated = $request->validate([
            'name' => 'required|string|max:255',
            'email' => 'required|string|email|max:255|unique:users',
            'password' => 'required|string|min:8|confirmed',
        ]);

        $user = User::create([
            'name' => $validated['name'],
            'email' => $validated['email'],
            'password' => Hash::make($validated['password']),
        ]);

        auth()->login($user);

        return redirect()->route('tenant.dashboard')->with('success', 'Welcome to your new account!');
    }
}
```

Add routes for the onboarding process in `routes/tenant.php` inside the `Route::middleware` group for tenant routes:

```php
use App\Http\Controllers\Tenant\OnboardingController;

Route::middleware([
    'web',
    InitializeTenancyByDomain::class,
    PreventAccessFromCentralDomains::class,
])->group(function () {
    // Existing routes
    // ...

    Route::get('/onboarding', [OnboardingController::class, 'show'])->name('tenant.onboarding');
    Route::post('/onboarding', [OnboardingController::class, 'store'])->name('tenant.onboarding.store');

});
```

Create the onboarding view in `resources/views/tenant/onboarding.blade.php`:

```html
<x-guest-layout>
  <form method="POST" action="{{ route('tenant.onboarding.store') }}">
    @csrf

    <div>
      <x-input-label for="name" :value="__('Name')" />
      <x-text-input
        id="name"
        class="mt-1 block w-full"
        type="text"
        name="name"
        :value="old('name')"
        required
        autofocus
        autocomplete="name"
      />
    </div>

    <div class="mt-4">
      <x-input-label for="email" :value="__('Email')" />
      <x-text-input
        id="email"
        class="mt-1 block w-full"
        type="email"
        name="email"
        :value="old('email')"
        required
        autocomplete="username"
      />
    </div>

    <div class="mt-4">
      <x-input-label for="password" :value="__('Password')" />
      <x-text-input
        id="password"
        class="mt-1 block w-full"
        type="password"
        name="password"
        required
        autocomplete="new-password"
      />
    </div>

    <div class="mt-4">
      <x-input-label for="password_confirmation" :value="__('Confirm Password')" />
      <x-text-input
        id="password_confirmation"
        class="mt-1 block w-full"
        type="password"
        name="password_confirmation"
        required
        autocomplete="new-password"
      />
    </div>

    <div class="mt-4 flex items-center justify-end">
      <x-primary-button class="ml-4"> {{ __('Complete Setup') }} </x-primary-button>
    </div>
  </form>
</x-guest-layout>
```

For simplicity, we're extending the Breeze guest layout for the onboarding form. But you can customize the layout to match your application's design and even have different layouts for the onboarding process based on each tenant's requirements.

To test the onboarding process, visit `http://tenant1.example.com/onboarding` and complete the onboarding form. After submitting the form, you should be redirected to the tenant dashboard which we'll implement next.

### Implementing Tenant Dashboard

Create a new controller for the tenant dashboard:

```bash
php artisan make:controller Tenant/DashboardController
```

Update the `app/Http/Controllers/Tenant/DashboardController.php` to display the tenant dashboard:

```php
<?php

namespace App\Http\Controllers\Tenant;

use App\Http\Controllers\Controller;
use Illuminate\Http\Request;

class DashboardController extends Controller
{
    public function index()
    {
        return view('tenant.dashboard');
    }
}
```

Create the dashboard view in `resources/views/tenant/dashboard.blade.php`:

```html
<x-app-layout>
  <x-slot name="header">
    <h2 class="text-gray-800 text-xl font-semibold leading-tight">{{ __('Dashboard') }}</h2>
  </x-slot>

  <div class="py-12">
    <div class="mx-auto max-w-7xl lg:px-8 sm:px-6">
      <div class="overflow-hidden bg-white shadow-sm sm:rounded-lg">
        <div class="text-gray-900 p-6">{{ __("You're logged in!") }}</div>
      </div>
    </div>
  </div>
</x-app-layout>
```

Add a route for the tenant dashboard in `routes/tenant.php` inside the `Route::middleware` group for tenant routes:

```php
use App\Http\Controllers\Tenant\DashboardController;

Route::middleware([
    'web',
    InitializeTenancyByDomain::class,
    PreventAccessFromCentralDomains::class,
])->group(function () {
    // Existing routes
    // ...
    Route::get('/dashboard', [DashboardController::class, 'index'])->name('tenant.dashboard');

});
```

To test the tenant dashboard, visit `http://tenant1.example.com/dashboard` after completing the onboarding process. You should see the dashboard view with a welcome message.

You can also check the `users` table in the tenant's database to verify that the user account created during onboarding has been added:

```sql
SELECT * FROM users;
```

This will show you the user account created during the onboarding process for that specific tenant in the tenant's database rather than the central database.

## Conclusion

In this tutorial, we've built a simple multi-tenant application using Laravel and Neon. We've covered:

1. Setting up the project and implementing multi-tenancy
2. Creating a tenant registration process
3. Implementing tenant onboarding
4. Adding a tenant dashboard for individual tenants

This implementation provides a foundation for building more complex SaaS applications with Laravel and Neon. You can further expand on this system by:

- Adding more features to the tenant dashboard
- Implementing billing and subscription management
- Enhancing security with two-factor authentication
- Adding more tenant-specific customizations

Using the `stancl/tenancy` package along with Neon, each tenant will have its own database. Thanks to Neon's autoscaling feature, you can easily scale your application as you onboard more tenants.

There are other packages and tools available to help you build multi-tenant applications with Laravel. You can explore these options based on your requirements and choose the one that best fits your needs. Some of the popular packages include:

- [spatie/laravel-multitenancy](https://spatie.be/docs/laravel-multitenancy/v3/introduction)
- [tenancy/tenancy](https://github.com/tenancy/tenancy)

## Additional Resources

- [Laravel Documentation](https://laravel.com/docs)
- [stancl/tenancy Documentation](https://tenancyforlaravel.com/)
- [Neon Documentation](https://neon.tech/docs/)


# An Overview of Laravel and Postgres on Neon

---
title: An Overview of Laravel and Postgres on Neon
subtitle: Learn how to integrate Laravel with Postgres on Neon, leveraging Laravel's Eloquent ORM and migrations for efficient database management.
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-05-25T00:00:00.000Z'
updatedOn: '2024-05-25T00:00:00.000Z'
---

When combining the robust features of [Laravel](https://laravel.com/), a highly expressive PHP framework, with the efficiency and scalability of Postgres on Neon, developers gain a powerful toolset for web development.

Laravel's native support for Postgres ensures a smooth integration process. When working with Neon Postgres, the transition is nearly seamless, thanks to Laravel's database agnostic [migrations](https://laravel.com/docs/11.x/migrations) and [Eloquent ORM](https://laravel.com/docs/11.x/eloquent), which effortlessly maps application objects to database tables.

## Create a Neon project

If you do not have one already, create a Neon project. Save your connection details including your password. They are required when defining connection settings.

1. Navigate to the [Projects](https://console.neon.tech/app/projects) page in the Neon Console.
2. Click **New Project**.
3. Specify your project settings and click **Create Project**.

## Setting Up Your Environment for Laravel and Neon Postgres

Start by installing Laravel. For installation instructions, refer to the [Laravel documentation](https://laravel.com/docs/11.x/installation).

To get Laravel working with Neon Postgres, you'll need to configure your environment settings.

This process involves updating the `.env` file in your Laravel project to include the details for your Neon Postgres database connection.

Here's what you need to update in the `.env` file:

```env
DB_CONNECTION=pgsql
DB_HOST=your-neon-hostname.neon.tech
DB_PORT=5432
DB_DATABASE=<your-database-name>
DB_USERNAME=<your-username>
DB_PASSWORD=<your-password>
```

- `DB_CONNECTION`: This tells Laravel that you're using a PostgreSQL database.
- `DB_HOST`: Here, you'll put the address of your Neon database.
- `DB_PORT`: This is the port number for PostgreSQL, which is usually 5432.
- `DB_DATABASE`: The name of your database on Neon.
- `DB_USERNAME` and `DB_PASSWORD`: Your login credentials for the Neon database.

With these settings, Laravel can connect to your Neon Postgres database, allowing your application to interact with it.

## Using Eloquent and Migrations in Laravel

Laravel's migration system and Eloquent ORM are powerful tools that simplify database management and interaction.

When you use Eloquent with Neon Postgres, it allows you to handle database operations without writing any SQL queries directly, thanks to Laravel's expressive syntax. Along with the Laravel migration system, you can easily manage your database schema and perform operations like creating tables, defining relationships, and querying data.

### Database Migrations and Schema Management

Laravel's migration system is an essential tool for database schema management. Migrations ensure that your database structure is properly version-controlled.

To create a new migration in Laravel, you use the `make:migration` Artisan command:

```bash
php artisan make:migration create_books_table --create=books
```

This will create a new migration file in the `database/migrations` directory. In this file, you define the schema for the `books` table:

```php
Schema::create('books', function (Blueprint $table) {
    $table->id();
    $table->string('title');
    $table->unsignedBigInteger('author_id');
    $table->year('publication_year');
    $table->timestamps();
});
```

Once you've defined the schema, you can run the migration to create the table in your Neon Postgres database:

```bash
php artisan migrate
```

This will execute the migration and create the `books` table in your database and keep track of the migration history.

### Defining Models in Laravel

First, you define a model that represents a table in your database. Each model corresponds to a table and allows you to interact with the table's records. For example, to create a `Book` model, you would create a `Book.php` file in the `app/Models` directory or run the following Artisan command:

```bash
php artisan make:model Book
```

The command will create a `Book.php` file in the `app/Models` directory.

In the model file, you can define details about the table and its columns:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class Book extends Model
{
    protected $fillable = ['title', 'author', 'publication_year'];
}
```

The `fillable` array in the model protects your application from mass-assignment vulnerabilities by specifying which attributes should be assignable.

Additionally, in the model, you can define relationships with other models, set up mutators and accessors, and perform various database operations.

### Creating and Saving Records

With the model set up, you can create and save new records to your database using Eloquent.

In your controller or wherever you need to create a new record, you can instantiate the model, set its properties, and save it:

```php
$book = new Book;
$book->title = 'Sample Book';
$book->author_id = 1;
$book->publication_year = 2021;
$book->save();
```

This creates a new instance of the `Book` model, sets its properties (`title`, `author_id`, `publication_year`), and then saves the new record to the `books` table in your Neon Postgres database.

Using Eloquent, you can manage your database records with simple, expressive syntax, making your code cleaner and more maintainable.

## Using Queries Efficiently

When building an application, it's important to write efficient queries. The same is true when working with Laravel and Postgres.

Laravel's Eloquent ORM has features like relationships and eager loading that help you write better queries.

For example, if you're fetching authors and their books, instead of making a separate database query for each author's books (which can slow things down), you can use eager loading.

Let's say you have an `Author` model with a `books` relationship:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class Author extends Model
{
    use HasFactory;

    protected $fillable = ['name', 'email', 'biography'];

    public function books()
    {
        return $this->hasMany(Book::class);
    }
}
```

With this relationship defined, you can use eager loading to fetch authors and their books in a more efficient way:

```php
$authors = Author::with('books')->get();
```

This code fetches all authors and their associated books, reducing the number of queries made to the database.

## Postgres specific features in Laravel

Leveraging Postgres's advanced features within your Laravel application can significantly boost performance, enhance data integrity, and provide flexible data storage options.

### Utilizing Indexes

Indexes are a crucial aspect of database optimization, especially when dealing with large datasets. They work by creating a data structure that allows the database engine to quickly locate and retrieve the data without scanning the entire table.

For instance, if your application includes a feature that allows users to search for books by their titles, querying a large database without an index can be time-consuming. By indexing the `title` column, you significantly improve query performance:

```php
Schema::table('books', function (Blueprint $table) {
    $table->index('title');
});
```

After adding the index, queries that search for books by title, like the one below, become much more efficient, reducing the response time for your users:

```php
$books = Book::where('title', 'like', '%search-term%')->paginate(10);
```

### Ensuring Data Integrity with Foreign Key Constraints

Foreign key constraints are vital for maintaining referential integrity between tables. They ensure that relationships between tables remain consistent, preventing orphaned records and ensuring data accuracy.

In the context of a book application, where each book is linked to an author, defining a foreign key constraint ensures that every book in your database is tied to an existing author. Here's how you can define such a constraint in a Laravel migration:

```php
Schema::table('books', function (Blueprint $table) {
    $table->foreign('author_id')->references('id')->on('authors')->onDelete('cascade');
});
```

By using the `foreign()` method, you can ensure that when an author is deleted, all their books are also removed from the database, preventing orphaned records and maintaining the integrity of your data.

### Leveraging JSONB for Flexible Data Storage

Postgres's JSONB data type offers a powerful way to store and query JSON data, providing flexibility for your application's data storage needs. Unlike the standard JSON data type, JSONB stores data in a decomposed binary format, allowing for efficient querying.

Imagine you want to store various metadata about each book, such as tags, reviews, or custom attributes. The JSONB data type allows you to store this information in a structured, queryable format:

```php
Schema::table('books', function (Blueprint $table) {
    $table->jsonb('metadata')->nullable();
});
```

With this `metadata` column, you can easily store and retrieve structured data related to each book, making your application more flexible and adaptable to changing requirements.

By integrating these Postgres features into your Laravel application, you can enhance its performance, maintain data integrity, and provide a scalable solution for managing complex data structures.

## Testing and Neon Postgres Branches

When integrating Neon Postgres with your Laravel application, leveraging database branches for testing is a robust strategy to ensure the reliability and consistency of your tests.

Neon Postgres Branches allow you to create isolated database environments, similar to branching in version control systems like Git. By using a separate database branch for testing, you ensure that your test executions are isolated from your production data, maintaining data integrity and consistency.

Usually, when running tests in Laravel, you would use a separate database for testing to avoid affecting your production data. In most cases, developers use an in-memory SQLite database for testing. However, Neon Postgres branches offer a more solid solution for testing your Laravel application.

1. Neon Postgres lets you create branches of your database. This means you can have a dedicated branch just for testing purposes, where you can freely run tests, apply migrations, and modify data without affecting your production database.

2. With a testing branch, you can execute your entire suite of tests in an environment that mirrors production without the risk of corrupting your actual production data. This is particularly useful for integration tests that interact with the database.

3. Configuring your Laravel application to use a separate database branch for testing is straightforward. You adjust your testing environment configuration to point to the testing branch of your Neon Postgres database, ensuring that when Laravel runs tests, it uses this isolated database instance.

## Conclusion

Combining Laravel with Postgres on Neon offers a powerful and efficient environment for developing web applications. Laravel's seamless integration with Postgres allows developers to take advantage of the full power of both the framework and the database, providing a flexible, scalable, and developer-friendly platform.

The ability to use database branches for testing with Neon Postgres brings an additional layer of robustness to your development process, allowing for isolated testing environments that mirror your production setup without risking data integrity.

Laravel's expressive syntax combined with Neon Postgres's powerful features allow developers to build complex, data-driven applications efficiently and effectively.


# Implementing Queue Workers and Job Processing in Laravel with Neon Postgres

---
title: Implementing Queue Workers and Job Processing in Laravel with Neon Postgres
subtitle: Learn how to implement efficient background processing in Laravel using queue workers and Neon Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-07-14T00:00:00.000Z'
updatedOn: '2024-07-14T00:00:00.000Z'
---

Laravel provides a powerful and flexible system for handling background processing through queues and scheduling. This allows you to improve your application's performance by offloading time-consuming tasks and automating recurring processes. In this comprehensive guide, we'll explore how to implement queue workers, job processing, and scheduled tasks in Laravel using Postgres as the queue driver.

By the end of this tutorial, you'll know how to build a system for background processing and task automation, using the power of Laravel queues and the scheduler with Neon Postgres.

## Prerequisites

Before we begin, ensure you have the following:

- PHP 8.1 or higher installed on your system
- [Composer](https://getcomposer.org/) for managing PHP dependencies
- A [Neon](https://console.neon.tech/signup) account for Postgres database hosting
- Basic knowledge of Laravel and database operations

## Setting up the Project

Let's start by creating a new Laravel project and setting up the necessary components. We'll use Composer to create a new Laravel project and configure it to use Postgres as the queue driver.

### Creating a New Laravel Project

Open your terminal and run the following command to create a new Laravel project:

```bash
composer create-project laravel/laravel laravel-queue-demo
cd laravel-queue-demo
```

### Setting up the Database

Update your `.env` file with your Neon Postgres database credentials:

```env
DB_CONNECTION=pgsql
DB_HOST=your-neon-hostname.neon.tech
DB_PORT=5432
DB_DATABASE=your_database_name
DB_USERNAME=your_username
DB_PASSWORD=your_password
```

Run the migrations:

```bash
php artisan migrate
```

This will create the necessary tables in your Neon Postgres database.

## Implementing Laravel Queues with Postgres

Out of the box, Laravel provides a unified API for working with queues, allowing you to push jobs onto the queue and process them in the background. We'll configure Laravel to use Postgres as the queue driver and create a sample job to demonstrate the queue processing.

### Configuring the Queue Connection

To configure Laravel to use Postgres as the queue driver, update the `QUEUE_CONNECTION` in the `.env` file:

```env
QUEUE_CONNECTION=database
```

This tells Laravel to use the database driver for the queue system. Some of the other available drivers are `sync`, `redis`, `beanstalkd`, `sqs`, and `null`.

### Creating the Jobs Table

As we're using the database driver for queues, we need to create a table to store the jobs.

If you don't already have a `jobs` table in your database, Laravel provides an Artisan command to generate the migration for the jobs table:

```bash
php artisan queue:table
php artisan migrate
```

The `jobs` table will be created in your Postgres database. It has the following columns:

- `id`: The unique identifier for the job.
- `queue`: The name of the queue the job belongs to.
- `payload`: The serialized job payload.
- `attempts`: The number of times the job has been attempted.
- `reserved_at`: The timestamp when the job was reserved by a worker.
- `available_at`: The timestamp when the job is available to be processed.
- `created_at`: The timestamp when the job was created.

## Creating and Dispatching Jobs

Now that we've set up the queue system, let's create a sample job and dispatch it to the queue.

We'll create a job called `GenerateDatabaseReport` that simulates generating a complex report from your database. When the job is dispatched, it will log a message indicating that the report has been processed.

Such jobs can be used to perform time-consuming tasks like sending emails, processing images, or interacting with external APIs without blocking the main application.

### Creating a Job

Let's start by creating a job called `GenerateDatabaseReport` using the following Artisan command:

```bash
php artisan make:job GenerateDatabaseReport
```

This will create a file at `app/Jobs/GenerateDatabaseReport.php`. Update the job class with the following content to simulate processing a report:

```php
<?php

namespace App\Jobs;

use Illuminate\Bus\Queueable;
use Illuminate\Contracts\Queue\ShouldQueue;
use Illuminate\Foundation\Bus\Dispatchable;
use Illuminate\Queue\InteractsWithQueue;
use Illuminate\Queue\SerializesModels;
use Illuminate\Support\Facades\Log;

class GenerateDatabaseReport implements ShouldQueue
{
    use Dispatchable, InteractsWithQueue, Queueable, SerializesModels;

    protected $reportId;

    public function __construct($reportId)
    {
        $this->reportId = $reportId;
    }

    public function handle()
    {
        sleep(5);
        Log::info("Processed report: {$this->reportId}");
    }
}
```

Let's break down the key components of this job class:

1. `implements ShouldQueue`: This interface tells Laravel that this job should be pushed onto the queue instead of running synchronously.

2. Use statements:

   - `Dispatchable`: Allows the job to be dispatched to the queue.
   - `InteractsWithQueue`: Provides methods for interacting with the queue.
   - `Queueable`: Allows the job to be pushed onto queues.
   - `SerializesModels`: Automatically serializes and deserializes Eloquent models in the job.

3. `protected $reportId`: This property stores the ID of the report to be processed.

4. `__construct($reportId)`: The constructor accepts a report ID and assigns it to the `$reportId` property.

5. `handle()` method: This is where the main logic of the job is implemented. In this example:
   - `sleep(5)` simulates a time-consuming process.
   - `Log::info(...)` logs a message indicating that the report has been processed.

When this job is dispatched, Laravel will serialize it and store it in the database. When a queue worker picks up the job, it will deserialize it and call the `handle()` method.

The `sleep(5)` call is just for demonstration purposes. In a real-world scenario, you'd replace this with actual report processing logic, such as fetching data from the database, generating a report, and storing it in a file or sending it via email.

### Dispatching the Job

For testing purposes, let's dispatch the `GenerateDatabaseReport` job when a specific route is accessed. This will simulate pushing the job onto the queue for processing, but in a real-world scenario, you'd dispatch jobs from your application logic based on specific events or triggers (e.g., user registration, order completion) or using the Laravel scheduler for recurring tasks like sending daily reports.

Add a route in `routes/web.php`:

```php
use App\Jobs\GenerateDatabaseReport;

Route::get('/dispatch-job', function () {
    GenerateDatabaseReport::dispatch(1);
    return 'Job dispatched!';
});
```

This route will dispatch the `GenerateDatabaseReport` job with the report ID `1` when accessed. You can test this by visiting `/dispatch-job` in your browser or using a tool like Postman or `curl`, which will trigger the job processing in the background, returning a response immediately instead of waiting for the job to complete.

As we are using the `database` queue driver, the job will be stored in the `jobs` table in your Neon Postgres database.

If you were to check the `jobs` table in your database, you would see an entry for the dispatched job with the serialized payload and other metadata:

```sql
SELECT * FROM jobs;
```

This will show you the job entry in the `jobs` table, which includes the serialized payload, queue name, and other metadata.

## Running Queue Workers

Now that we've dispatched a job to the queue, we need to run a queue worker to process the job. Queue workers listen for new jobs on the queue and execute them in the background.

Start a queue worker using the following Artisan command:

```bash
php artisan queue:work
```

This will start a queue worker that listens for new jobs on the default queue. You can specify the queue name using the `--queue` option if you have multiple queues. Having multiple queues allows you to prioritize jobs based on their importance or processing requirements.

You will see the following output:

```php
   INFO  Processing jobs from the [default] queue.

  2024-07-13 16:29:51 App\Jobs\GenerateDatabaseReport ........... RUNNING
  2024-07-13 16:29:58 App\Jobs\GenerateDatabaseReport ........... 6s DONE
```

The job will be picked up by the queue worker, processed, and the log message will be written to the log file. You can check the log file to verify that the job was processed successfully. It should take approximately 5 seconds to process the job due to the `sleep(5)` call in the job logic but in a real-world scenario, the processing time will depend on the actual logic implemented in the job.

If you were to check the `jobs` table in your database after the job has been processed, you would see that the job has been removed from the table, indicating that it has been successfully processed.

```sql
SELECT * FROM jobs;
```

This will show you that the job has been removed from the `jobs` table after processing.

## Handling Job Failures and Retries

Laravel provides mechanisms for handling failed jobs and configuring job retries. Let's explore how to configure failed job storage, handle failed jobs, and set up job retries.

### Configuring Failed Job Storage

If you don't already have a table for failed jobs in your database, you can use Laravel's Artisan commands to create one. Run the following commands:

```bash
php artisan queue:failed-table
php artisan migrate
```

This will create a `failed_jobs` table in your Postgres database to store information about failed jobs for debugging and analysis. The failed jobs will be stored in this table when a job fails to process and later retried or manually processed as needed.

### Handling Failed Jobs

Add a `failed` method to your job class to handle failed jobs:

```php
public function failed(\Throwable $exception)
{
    Log::error("Failed to process report {$this->reportId}: {$exception->getMessage()}");
}
```

This method will be called when a job fails to process. You can log the error message or perform additional actions based on the failure, such as sending an email notification or updating a status in the database.

### Configuring Job Retries

In some cases, you may want to retry a job if it fails to process. Laravel allows you to configure the number of retries and the timeout for a job.

Add retry and timeout configurations to your job class, for example:

```php
public $tries = 3;
public $timeout = 120; // 2 minutes
```

This configuration will retry the job up to 3 times if it fails and set a timeout of 2 minutes for each job execution. You can adjust these values based on your application's requirements.

To simulate a job failure, you can throw an exception in the `handle` method:

```php
public function handle()
{
    throw new \Exception("Failed to process report: {$this->reportId}");
}
```

Visit the `/dispatch-job` route to dispatch the job with the exception thrown and run the queue worker to process the job.

```bash
php artisan queue:work
```

When the job is dispatched and processed, it will fail due to the exception thrown in the `handle` method. The job will be retried based on the configuration you've set.

After the job fails to process, you can check the `failed_jobs` table in your database to see the failed job entry:

```sql
SELECT * FROM failed_jobs;
```

In the `failed_jobs` table, you'll see the failed job entry with information about the job, the exception message, and the number of attempts made.

You can also manually retry a failed job using the `queue:retry` Artisan command:

```bash
php artisan queue:retry job-id
```

Replace `job-id` with the ID of the failed job you want to retry. This will requeue the job for processing.

## Implementing Scheduled Jobs

Laravel's scheduler allows you to expressively define your command schedule within your Laravel application itself. In Laravel 11, this is done using the `routes/console.php` file, simplifying the process and keeping all routing-related code in one place.

### Configuring the Scheduler

Let's add a scheduled task that dispatches our `GenerateDatabaseReport` job every hour:

```php
<?php

use Illuminate\Support\Facades\Schedule;
use App\Jobs\GenerateDatabaseReport;

Schedule::job(new GenerateDatabaseReport(1))->hourly();
```

This will dispatch the `GenerateDatabaseReport` job every hour. You can define more complex schedules using the `Schedule` facade, such as daily, weekly, or custom schedules based on your application's requirements.

### Running the Scheduler

To run the scheduler, you still need to add the following Cron entry to your server:

```shell
* * * * * cd /path-to-your-project && php artisan schedule:run >> /dev/null 2>&1
```

This Cron will call the Laravel command scheduler every minute. Laravel then evaluates your scheduled tasks and runs the tasks that are due.

This approach provides a way to automate recurring tasks in your application, such as sending daily reports, cleaning up temporary files, or updating data from external sources, all without having to manage multiple Cron jobs.

## Additional Queue Processing Techniques

Laravel offers several handy techniques for working with queues, allowing you to build more complex and efficient job processing systems. Let's explore some of these techniques in detail, focusing on how they work with Postgres as our queue driver.

### Job Chaining

Job chaining is a powerful feature in Laravel that allows you to specify a sequence of jobs that should be run in order. This is particularly useful when you have a series of related tasks that need to be executed sequentially.

Here's how job chaining works:

```php
use App\Jobs\GenerateDatabaseReport;
use App\Jobs\VerifyDatabaseReport;
use App\Jobs\GeneratedDatabaseReport;

GenerateDatabaseReport::dispatch(1)
    ->chain([
        new VerifyDatabaseReport(1),
        new GeneratedDatabaseReport(1)
    ]);
```

In this example:

1. The `GenerateDatabaseReport` job is dispatched first.
2. Once `GenerateDatabaseReport` completes successfully, `VerifyDatabaseReport` is automatically dispatched.
3. After `VerifyDatabaseReport` finishes, `GeneratedDatabaseReport` is dispatched.

If any job in the chain fails, the subsequent jobs won't be executed. This ensures that your entire process maintains integrity.

You can also add delays between chained jobs:

```php
GenerateDatabaseReport::dispatch(1)
    ->chain([
        new VerifyDatabaseReport(1),
        (new GeneratedDatabaseReport(1))->delay(now()->addMinutes(10))
    ]);
```

This will delay the `GeneratedDatabaseReport` job by 10 minutes after `VerifyDatabaseReport` completes.

### Job Batching

Job batching allows you to group related jobs together, monitor their execution as a single unit, and perform actions when the entire batch completes. This is incredibly useful for processing large datasets or performing complex, multi-step operations.

To use job batching with Postgres, you first need to create a batches table:

```bash
php artisan queue:batches-table
php artisan migrate
```

Here's an example of job batching in Laravel:

```php
use Illuminate\Support\Facades\Bus;
use App\Jobs\GenerateDatabaseReport;

$batch = Bus::batch([
    new GenerateDatabaseReport(1),
    new GenerateDatabaseReport(2),
    new GenerateDatabaseReport(3),
])->then(function (Batch $batch) {
    // All jobs completed successfully...
    Log::info('All reports processed successfully');
})->catch(function (Batch $batch, Throwable $e) {
    // First batch job failure detected...
    Log::error('Batch job failed: ' . $e->getMessage());
})->finally(function (Batch $batch) {
    // The batch has finished executing...
    Log::info('Batch processing completed');
})->dispatch();
```

Key points about job batching:

- The `then()` callback is executed if all jobs in the batch complete successfully.
- The `catch()` callback is executed if any job in the batch fails.
- The `finally()` callback is always executed when the batch finishes, regardless of success or failure.

You can also add jobs to an existing batch:

```php
$batch->add(new GenerateDatabaseReport(4));
```

And you can check the progress of a batch using the `progress()` method:

```php
$batchId = $batch->id;

// Later...
$batch = Bus::findBatch($batchId);
$progress = $batch->progress(); // Returns a percentage
```

### Rate Limiting

Rate limiting is very helpful for preventing your application from overwhelming external services or your own database. When using Postgres as your queue driver, you can implement rate limiting at the application level. Here's an example of how you might do this:

```php
use Illuminate\Support\Facades\DB;
use Carbon\Carbon;

class GenerateDatabaseReport implements ShouldQueue
{
    public function handle()
    {
        $key = 'process-report';
        $limit = 10; // Number of jobs
        $duration = 60; // Time period in seconds

        $count = DB::table('jobs')
            ->where('queue', $key)
            ->where('created_at', '>=', Carbon::now()->subSeconds($duration))
            ->count();

        if ($count < $limit) {
            // Job logic...
            Log::info('Processing report...');
        } else {
            // Rate limit exceeded, release the job back to the queue
            $this->release(10); // Release back to queue after 10 seconds
        }
    }
}
```

In this example:

- We're using the `jobs` table in Postgres to track our rate limit.
- We allow 10 jobs to be processed every 60 seconds.
- If a job can be processed within this limit, the job logic is executed.
- If the rate limit has been exceeded, the job is released back to the queue with a 10-second delay.

You can also use different keys for different types of jobs:

```php
$key = "process-report:{$this->reportId}";
```

This allows you to have separate rate limits for different reports or different types of jobs.

## Monitoring and Managing Queues

Laravel provides several Artisan commands for monitoring and managing your queues:

- `queue:work`: Process new jobs as they are pushed onto the queue
- `queue:listen`: Similar to `queue:work`, but will reload the worker after each job
- `queue:retry`: Retry a failed job
- `queue:failed`: List all of the failed jobs
- `queue:flush`: Delete all of the failed jobs

## Implementing Supervisor for Queue Workers

Running queue workers using the `queue:work` command works well for development environments, but it's not suitable for production environments.

For production environments, use [Supervisor](http://supervisord.org/) to ensure your queue workers are always running.

### Installing and Configuring Supervisor

Install Supervisor:

```bash
sudo apt-get install supervisor
```

Create a new Supervisor configuration file:

```bash
sudo nano /etc/supervisor/conf.d/laravel-worker.conf
```

Add the following content:

```ini
[program:laravel-worker]
process_name=%(program_name)s_%(process_num)02d
command=php /path/to/your/project/artisan queue:work database --sleep=3 --tries=3 --max-time=3600
autostart=true
autorestart=true
user=www-data
numprocs=8
redirect_stderr=true
stdout_logfile=/path/to/your/project/worker.log
stopwaitsecs=3600
```

Start Supervisor:

```bash
sudo supervisorctl reread
sudo supervisorctl update
sudo supervisorctl start laravel-worker:*
```

## Conclusion

In this guide, we've explored how to implement queue workers, job processing, and scheduled tasks in Laravel using Postgres as the queue driver. We've covered creating and dispatching jobs, running queue workers, handling job failures and retries, implementing scheduled jobs, and setting up Supervisor for production environments.

## Additional Resources

- [Laravel Queues Documentation](https://laravel.com/docs/11.x/queues)
- [Laravel Task Scheduling](https://laravel.com/docs/11.x/scheduling)
- [Supervisor Documentation](http://supervisord.org/)
- [Neon Documentation](/docs)


# A Deep Dive into Laravel's Routes, Middleware, and Validation: Optimizing Database Interactions

---
title: "A Deep Dive into Laravel's Routes, Middleware, and Validation: Optimizing Database Interactions"
subtitle: Explore Laravel's core features to build efficient and secure web applications with optimized database interactions using Neon Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-07-14T00:00:00.000Z'
updatedOn: '2024-07-14T00:00:00.000Z'
---

Laravel, a popular PHP framework, provides a wide range of tools for building web applications. Among its core features are routing, middleware, and validation, which work together to create secure, efficient, and well-structured applications. In this guide, we'll explore these concepts, with a particular focus on how they interact with and optimize database operations.

By the end of this tutorial, you'll have a good understanding of how to structure your Laravel application's request lifecycle, from the initial route hit to the final database query, all while ensuring proper validation and middleware checks.

## Prerequisites

Before we begin, ensure you have the following:

- PHP 8.1 or higher installed on your system
- [Composer](https://getcomposer.org/) for managing PHP dependencies
- A [Neon](https://console.neon.tech/signup) account for Postgres database hosting
- Basic knowledge of Laravel and database operations

## Setting up the Project

Let's start by creating a new Laravel project and setting up the necessary components.

### Creating a New Laravel Project

Open your terminal and run the following command to create a new Laravel project:

```bash
composer create-project laravel/laravel laravel-routes-middleware-validation
cd laravel-routes-middleware-validation
```

### Setting up the Database

Update your `.env` file with your Neon Postgres database credentials:

```env
DB_CONNECTION=pgsql
DB_HOST=your-neon-hostname.neon.tech
DB_PORT=5432
DB_DATABASE=your_database_name
DB_USERNAME=your_username
DB_PASSWORD=your_password
```

### Understanding Laravel Routing

Routing in Laravel is a fundamental concept that defines how your application responds to incoming HTTP requests. It's the entry point for all requests to your application, determining which code should be executed based on the URL and HTTP method.

#### Basic Routing

Let's start with a basic route that interacts with the database. We'll create a route to fetch and display a list of users.

Open `routes/web.php` and add the following route:

```php
use App\Models\User;
use Illuminate\Support\Facades\Route;

Route::get('/users', function () {
    $users = User::all();
    return view('users.index', ['users' => $users]);
});
```

This route does the following:

1. It responds to GET requests to the `/users` URL.
2. It uses a closure function to define the route's behavior.
3. Inside the closure, it fetches all users from the database using the `User` model.
4. It returns a view named `users.index`, passing the fetched users to the view.

While this approach works for simple routes, it's generally not recommended for larger applications. As your application grows, putting logic directly in route closures can lead to cluttered and hard-to-maintain code.

#### Introducing Controllers

In practice, it's better to use controllers to handle the logic for your routes. Controllers group related request handling logic into a single class. Let's create a controller for our user-related routes:

```bash
php artisan make:controller UserController
```

This command creates a new `UserController` in `app/Http/Controllers/UserController.php`. Now, let's modify our route to use this controller:

```php
use App\Http\Controllers\UserController;

Route::get('/users', [UserController::class, 'index']);
```

In `UserController.php` is where you define your logic, like fetching users from the database:

```php
namespace App\Http\Controllers;

use App\Models\User;

class UserController extends Controller
{
    public function index()
    {
        $users = User::all();
        return view('users.index', ['users' => $users]);
    }
}
```

This approach separates our route definition from its logic, making our code more organized and easier to maintain.

#### Route Parameters

Route parameters allow you to capture parts of the URI as variables. They're particularly helpful for creating dynamic routes. Let's create a route to display a specific user's details:

```php
Route::get('/users/{id}', [UserController::class, 'show']);
```

In `UserController.php`, add the `show` method to fetch and display a specific user:

```php
public function show($id)
{
    $user = User::findOrFail($id);
    return view('users.show', ['user' => $user]);
}
```

This route and method do the following:

1. The `{id}` in the route definition is a route parameter.
2. Laravel passes this parameter to the `show` method.
3. We use `findOrFail` to fetch the user by ID.
4. If the user is not found, Laravel automatically returns a 404 response.
5. If found, we return a view with the user's details.

#### Route Model Binding

Laravel offers an even more elegant way to handle route parameters with Eloquent models. It's called implicit route model binding:

```php
Route::get('/users/{user}', [UserController::class, 'show']);
```

And in the controller we can type-hint the `User` model:

```php
public function show(User $user)
{
    return view('users.show', ['user' => $user]);
}
```

With this approach:

1. Laravel automatically resolves `{user}` to an instance of the `User` model.
2. If no matching model is found, it automatically returns a 404 response.
3. This reduces boilerplate code and uses Laravel's model binding feature.

#### Route Groups

Route groups allow you to share route attributes across multiple routes. This is particularly useful for applying middleware, prefixes, or namespaces to a set of routes.

```php
Route::middleware(['auth'])->group(function () {
    Route::get('/dashboard', [DashboardController::class, 'index']);
    Route::get('/profile', [ProfileController::class, 'show']);
});
```

This group does the following:

1. It applies the `auth` middleware to all routes within the group.
2. The `dashboard` and `profile` routes are now protected and only accessible to authenticated users.
3. It keeps our routes DRY (Don't Repeat Yourself) by applying shared attributes in one place.

You can also nest route groups for more complex structures:

```php
Route::prefix('admin')->middleware(['auth', 'admin'])->group(function () {
    Route::get('/users', [AdminUserController::class, 'index']);
    Route::get('/posts', [AdminPostController::class, 'index']);
});
```

This creates a group of admin routes that:

1. All start with `/admin`
2. Require authentication and admin privileges
3. Are handled by admin-specific controllers

## Implementing Middleware

Middleware acts as a powerful mechanism for filtering HTTP requests hitting your application. It's essential for implementing features like authentication, CORS handling, and request/response modifications. In Laravel 11, the way middleware is handled has been streamlined for better performance and easier configuration.

By using middleware, you can:

1. Perform actions before the request reaches your application
2. Perform actions after the application generates a response
3. Modify the request or response as needed

### Creating Custom Middleware

Let's create a custom middleware to check if a user has admin privileges. You can use the following Artisan command:

```bash
php artisan make:middleware CheckAdminStatus
```

This creates a new file `app/Http/Middleware/CheckAdminStatus.php`. Let's update it with our logic to check for admin status:

```php
<?php

namespace App\Http\Middleware;

use Closure;
use Illuminate\Http\Request;
use Symfony\Component\HttpFoundation\Response;

class CheckAdminStatus
{
    public function handle(Request $request, Closure $next): Response
    {
        if (!$request->user() || !$request->user()->is_admin) {
            return redirect('/')->with('error', 'You do not have admin access.');
        }

        return $next($request);
    }
}
```

This middleware:

1. Checks if there's an authenticated user and if they have admin status
2. If not, it redirects to the home page with an error message
3. If the user is an admin, it allows the request to proceed

### Registering Middleware

In Laravel 11, middleware registration has been simplified. You no longer need to register middleware in the `Kernel.php` file. Instead, you can register middleware directly in your `bootstrap/app.php` file:

```php
$app->routeMiddleware([
    'auth' => App\Http\Middleware\Authenticate::class,
    'admin' => App\Http\Middleware\CheckAdminStatus::class,
]);
```

This registers the `CheckAdminStatus` middleware with the key `admin`, allowing you to apply it to specific routes.

### Applying Middleware to Routes

Now you can apply this middleware to routes that require admin access:

```php
use App\Http\Controllers\AdminController;

Route::middleware(['auth', 'admin'])->group(function () {
    Route::get('/admin/dashboard', [AdminController::class, 'dashboard']);
    Route::get('/admin/users', [AdminController::class, 'users']);
});
```

This route group:

1. Applies both the `auth` and `admin` middleware
2. Groups together routes that should only be accessible to authenticated admin users
3. Uses controller methods to handle the requests, keeping the route file clean

### Middleware Parameters

Laravel allows you to pass parameters to your middleware. This can be useful when you need to customize middleware behavior based on the route or request context.

Let's modify our `CheckAdminStatus` middleware to accept a required permission level:

```php
public function handle(Request $request, Closure $next, int $requiredLevel): Response
{
    if (!$request->user() || $request->user()->admin_level < $requiredLevel) {
        return redirect('/')->with('error', 'You do not have sufficient privileges.');
    }

    return $next($request);
}
```

You can then use this middleware with parameters in your routes:

```php
Route::get('/admin/users', [AdminController::class, 'users'])
    ->middleware('admin:2'); // Requires admin level 2 or higher
```

As a good practice, each middleware should have a single responsibility.

## Implementing Validation

Validation is an important aspect of any web application. It allows you to check that incoming data meets specific criteria before processing. Laravel provides a validation system that integrates easily with your routes, controllers, and database operations.

### Basic Validation

Let's start with a basic example of validating user input when creating a new user:

```php
use App\Models\User;
use Illuminate\Http\Request;
use Illuminate\Support\Facades\Hash;
use Illuminate\Validation\Rules\Password;

Route::post('/users', function (Request $request) {
    $validated = $request->validate([
        'name' => 'required|string|max:255',
        'email' => 'required|string|email|max:255|unique:users',
        'password' => ['required', 'confirmed', Password::min(8)],
    ]);

    $user = User::create([
        'name' => $validated['name'],
        'email' => $validated['email'],
        'password' => Hash::make($validated['password']),
    ]);

    return redirect()->route('users.show', ['id' => $user->id])
        ->with('success', 'User created successfully');
});
```

This example demonstrates several key points:

1. The `validate` method automatically returns a 422 response with validation errors if validation fails.
2. Validated data is returned if validation passes, allowing you to safely use it.
3. The `unique:users` rule checks the database to ensure the email isn't already in use.

### Validation Error Handling

By default, Laravel automatically redirects the user back to the previous page with the validation errors and old input if validation fails. You can access these in your views:

```php
@if ($errors->any())
    <div class="alert alert-danger">
        <ul>
            @foreach ($errors->all() as $error)
                <li>{{ $error }}</li>
            @endforeach
        </ul>
    </div>
@endif

<form method="POST" action="/users">
    @csrf
    <input type="text" name="name" value="{{ old('name') }}">
    <!-- Other form fields -->
</form>
```

This code snippet displays validation errors and repopulates your form fields with old user input.

### Custom Error Messages

You can customize validation error messages by passing an array of messages as the second argument to the `validate` method:

```php
$validated = $request->validate([
    'name' => 'required|string|max:255',
    'email' => 'required|string|email|max:255|unique:users',
], [
    'name.required' => 'A name is required',
    'email.unique' => 'This email is already registered',
]);
```

This allows you to provide more user-friendly error messages.

### Form Request Validation

For more complex validation scenarios, Laravel provides Form Request classes. These are particularly useful when you have validation logic that you want to reuse across multiple controllers or routes.

Let's create a form request for updating user profiles:

```bash
php artisan make:request UpdateUserProfileRequest
```

Now, let's update `app/Http/Requests/UpdateUserProfileRequest.php`:

```php
<?php

namespace App\Http\Requests;

use Illuminate\Foundation\Http\FormRequest;
use Illuminate\Validation\Rule;

class UpdateUserProfileRequest extends FormRequest
{
    public function authorize()
    {
        return $this->user() !== null;
    }

    public function rules()
    {
        return [
            'name' => 'required|string|max:255',
            'email' => [
                'required',
                'string',
                'email',
                'max:255',
                Rule::unique('users')->ignore($this->user()->id),
            ],
            'bio' => 'nullable|string|max:1000',
            'avatar' => 'nullable|image|max:1024',
        ];
    }

    public function messages()
    {
        return [
            'email.unique' => 'This email is already in use by another account.',
            'avatar.max' => 'The avatar must not be larger than 1MB.',
        ];
    }
}
```

Now we can use this Form Request in our controller:

```php
use App\Http\Requests\UpdateUserProfileRequest;

class ProfileController extends Controller
{
    public function update(UpdateUserProfileRequest $request)
    {
        $user = $request->user();
        $user->update($request->validated());

        if ($request->hasFile('avatar')) {
            $user->avatar = $request->file('avatar')->store('avatars', 'public');
            $user->save();
        }

        return redirect()->route('profile')
            ->with('success', 'Profile updated successfully');
    }
}
```

This approach offers several benefits:

1. Validation logic is encapsulated and reusable across multiple routes or controllers.
2. The controller stays clean and focused on its primary responsibility of handling requests and responses.
3. The `authorize` method allows for permission checks before validation.

### Custom Validation Rules

Laravel allows you to create custom validation rules. This is useful when you have specific validation requirements that aren't covered by Laravel's built-in rules and when you want to reuse these rules across your application.

Let's create a rule to ensure a string contains no spaces:

```bash
php artisan make:rule NoSpaces
```

Update the `app/Rules/NoSpaces.php` file to add the validation logic:

```php
<?php

namespace App\Rules;

use Closure;
use Illuminate\Contracts\Validation\ValidationRule;

class NoSpaces implements ValidationRule
{
    public function validate(string $attribute, mixed $value, Closure $fail): void
    {
        if (str_contains($value, ' ')) {
            $fail('The :attribute must not contain spaces.');
        }
    }
}
```

All that we validate is that the string doesn't contain any spaces using the `str_contains` function.

You can now use this rule in your validations:

```php
use App\Rules\NoSpaces;

$request->validate([
    'username' => ['required', 'string', new NoSpaces],
]);
```

This custom rule ensures that the `username` field doesn't contain any spaces before it's stored in the database, but you can use it for any other validation logic you need.

## Optimizing Database Interactions

Efficient database interactions are very important when building high-performance Laravel applications. As your application scales, optimizing these interactions becomes increasingly important. Let's explore various techniques to improve database performance.

### Understanding the N+1 Query Problem and Eager Loading

The N+1 query problem is a common performance issue in ORM systems. It occurs when you fetch a list of records and then make additional queries for each record to retrieve related data.

#### Example of N+1 Problem:

```php
$posts = Post::all();
foreach ($posts as $post) {
    echo $post->author->name; // This causes an additional query for each post
}
```

This code results in 1 query to fetch all posts, plus N queries (where N is the number of posts) to fetch each post's author. This can lead to a large number of queries and slow performance.

#### Solving with Eager Loading:

Eager loading solves this by loading all related data in a single query:

```php
Route::get('/posts', function () {
    $posts = Post::with('author')->paginate(20);
    return view('posts.index', ['posts' => $posts]);
});
```

This loads all posts and their authors in just 2 queries, regardless of the number of posts. The `with('author')` method specifies the relationship to eager load and prevents the N+1 problem.

#### Advanced Eager Loading:

You can eager load multiple relationships and even nest them:

```php
$posts = Post::with(['author', 'comments.user'])->get();
```

This loads posts, their authors, comments on each post, and the user who made each comment.

### Query Optimization Techniques

#### Indexing

Indexes are crucial for query performance. They allow the database to find data without scanning the entire table.

You can learn more about indexing in the [Neon documentation](https://neon.tech/docs/postgres/indexes/).

In Laravel migrations, you can add indexes like this:

```php
Schema::table('users', function (Blueprint $table) {
    $table->index('email');
    $table->index(['last_name', 'first_name']);
});
```

Consider indexing:

- Foreign keys
- Columns used in WHERE clauses
- Columns used for sorting (ORDER BY)

Remember, while indexes speed up reads, they can slow down writes, so use them judiciously.

#### Chunking Results

When working with large datasets, use chunking to process records in batches:

```php
User::chunk(100, function ($users) {
    foreach ($users as $user) {
        // Process user
    }
});
```

This prevents loading all records into memory at once, reducing memory usage and improving performance. This can be especially useful for tasks like sending emails to all users or processing large datasets.

### Caching Strategies

Caching can significantly reduce database load for frequently accessed, rarely changing data. Cache data that's expensive to compute or retrieve from the database and doesn't change frequently.

#### Basic Caching:

Using the Laravel cache facade, you can cache data like this:

```php
use Illuminate\Support\Facades\Cache;

Route::get('/stats', function () {
    $stats = Cache::remember('site_stats', 3600, function () {
        return [
            'user_count' => User::count(),
            'post_count' => Post::count(),
            'comment_count' => Comment::count(),
        ];
    });

    return view('stats', ['stats' => $stats]);
});
```

This caches the stats for an hour (3600 seconds). The data is recalculated and cached if it's not found in the cache. The cache is stored in the default cache store as configured in your `.env` file.

After the data is cached, subsequent requests will retrieve the data from the cache instead of querying the database again.

#### Model Caching:

For individual models, you can cache queries:

```php
$user = Cache::remember('user:' . $id, 3600, function () use ($id) {
    return User::find($id);
});
```

### Query Builder Optimization

When using Laravel's query builder, there are several techniques to optimize your queries:

#### Select Specific Columns:

Instead of selecting all columns, specify only the ones you need. This is particularly useful when fetching large datasets from tables with many columns, not all of which are required.

Let's say you only need the `id`, `name`, and `email` columns from the `users` table:

```php
$users = DB::table('users')->select('id', 'name', 'email')->get();
```

This reduces the amount of data transferred from the database. To verify the generated SQL query, you can use the `toSql` method:

```php
$sql = DB::table('users')->select('id', 'name', 'email')->toSql();
dd($sql);
```

This will output the generated SQL query for debugging purposes.

#### Use Proper Data Types:

This is not specific to Laravel but is important for query performance. You should always make sure that you're using appropriate data types in your migrations. For example, use `tinyInteger` for boolean fields instead of `integer`.

#### Avoid Using `orWhere` Excessively:

Excessive use of `orWhere` can lead to slow queries. Consider using `whereIn` instead:

```php
// Instead of:
$users = User::where('status', 'active')
             ->orWhere('status', 'pending')
             ->get();

// Use:
$users = User::whereIn('status', ['active', 'pending'])->get();
```

### Eloquent Performance Tips

Besides the above techniques, there are some additional tips to optimize Eloquent queries:

#### Use Lazy Collections for Large Datasets:

When working with large datasets, use lazy collections to conserve memory:

```php
User::cursor()->each(function ($user) {
    // Process user
});
```

This loads users one at a time from the database instead of loading all at once, reducing memory usage.

#### Leverage Raw Queries for Complex Operations:

For very complex queries, sometimes a raw query can be more efficient. You can use Laravel's `DB` facade to run raw SQL queries:

```php
$users = DB::select('SELECT * FROM users WHERE id > ? AND email = ?', [1, 'example@example.com']);
```

You can abstract raw queries into a repository or service class to keep your controllers clean.

## Conclusion

In this guide, we've explored Laravel's routing system, middleware, and validation, with a focus on optimizing database interactions.

Always consider the performance implications of your routes and database queries, especially as your application scales. Use middleware to keep your routes clean and secure, and implement thorough validation to ensure data integrity.

By following these practices and continually refining your approach, you'll be well-equipped to build Laravel applications that are both powerful and performant.

## Additional Resources

- [Laravel Routing Documentation](https://laravel.com/docs/11.x/routing)
- [Laravel Middleware Documentation](https://laravel.com/docs/11.x/middleware)
- [Laravel Validation Documentation](https://laravel.com/docs/11.x/validation)
- [Laravel Query Builder Documentation](https://laravel.com/docs/11.x/queries)
- [Neon Documentation](/docs)


# Implementing Soft Deletes in Laravel and Postgres

---
title: Implementing Soft Deletes in Laravel and Postgres
subtitle: Learn how to implement and optimize soft deletes in Laravel for improved data management and integrity.
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-07-20T00:00:00.000Z'
updatedOn: '2024-07-20T00:00:00.000Z'
---

Laravel is a PHP framework that offers a lot of features to simplify database operations. One such feature is soft deletes, which allows you to "delete" records without actually removing them from your database.

This approach is particularly useful when you need to maintain data integrity, implement data recovery features, or comply with data retention policies.

In this guide, we'll explore Laravel's soft delete functionality, covering everything to get you started, from setting up soft deletes to performance considerations.

## Prerequisites

Before we dive in, ensure you have:

- PHP 8.1 or higher installed
- Laravel 10.x or 11.x set up
- A [Neon](https://neon.tech) account for Postgres database hosting
- Basic understanding of Laravel and Eloquent ORM

## Understanding Soft Deletes

When enabling soft deletes, you essentially add a `deleted_at` timestamp to your database records. When a record is "deleted", Laravel sets this timestamp instead of removing the record entirely. This allows you to:

1. Recover accidentally deleted data.
2. Maintain referential integrity.
3. Implement data archiving strategies.
4. Comply with data retention policies.

Let's explore how to implement soft deletes in Laravel along with Neon Postgres.

## Setting up the Project

Before we go further into implementing soft deletes, let's set up a new Laravel project. If you already have a Laravel project, you can skip this step.

### Creating a New Laravel Project

Open your terminal and run the following command to create a new Laravel project:

```bash
composer create-project laravel/laravel soft-deletes
cd soft-deletes
```

This will create a new Laravel project in a directory named `soft-deletes`. Navigate to the project directory to continue with the setup.

## Implementing Soft Deletes

Implementing soft deletes in Laravel involves two main steps: preparing the database and updating the model.

For this guide, we'll use a `posts` table as an example. You can apply the same steps to any other table in your application.

### Step 1: Creating the Model and Migration

If you don't already have a model for the table you want to apply soft deletes to, you'll need to create one. Let's start by creating a `Post` model along with a migration file. Laravel provides an Artisan command that can do both in one go:

```bash
php artisan make:model Post -m
```

This command creates two files:

1. `app/Models/Post.php`: The `Post` model file.
2. `database/migrations/xxxx_xx_xx_xxxxxx_create_posts_table.php`: A migration file to create the `posts` table.

The `-m` flag tells Artisan to create a migration file along with the model.

### Step 2: Updating the Migration

Now, let's update the migration file to include the `deleted_at` column required for soft deletes. Open the newly created migration file in the `database/migrations` directory and update the `up` method:

```php
public function up(): void
{
    Schema::create('posts', function (Blueprint $table) {
        $table->id();
        $table->string('title');
        $table->text('content');
        $table->timestamps();
        $table->softDeletes();
    });
}
```

The `softDeletes()` method adds a nullable `deleted_at` timestamp column to your table which Laravel uses for soft deletes.

### Step 3: Running the Migration

With our migration file prepared, we can now run it to create the `posts` table in our database:

```bash
php artisan migrate
```

This command executes all pending migrations, creating the `posts` table with the `deleted_at` column.

If you were to connect to your database, you'd see a new `posts` table with the following columns:

```sql
SELECT * FROM posts;

+----+-------+---------+------------+------------+------------+
| id | title | content | created_at | updated_at | deleted_at |
+----+-------+---------+------------+------------+------------+
```

### Step 4: Updating the Model

Finally, we need to update our `Post` model to use the `SoftDeletes` trait. Open `app/Models/Post.php` and update it as follows:

```php {6,10}
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Model;
use Illuminate\Database\Eloquent\SoftDeletes;

class Post extends Model
{
    use SoftDeletes;

    protected $fillable = ['title', 'content'];
}
```

By adding the `use SoftDeletes;` line, we're telling Laravel that this model should use soft delete functionality when deleting records.

With these steps completed, your `Post` model is now set up to use soft deletes. When you call `$post->delete()`, Laravel will set the `deleted_at` timestamp instead of actually removing the record from the database.

### Adding Soft Deletes to an Existing Table

If you're adding soft deletes to an existing table, you'll need to create a separate migration to add the `deleted_at` column. You can do this with the command:

```bash
php artisan make:migration add_soft_deletes_to_posts_table --table=posts
```

This command creates a new migration file where you can add the `deleted_at` column to the `posts` table:

```php
public function up(): void
{
    Schema::table('posts', function (Blueprint $table) {
        $table->softDeletes();
    });
}
```

After creating the migration, run `php artisan migrate` to apply the changes to your database.

## Using Soft Deletes

Now that we've set up soft deletes in our Laravel application, let's explore how to use them in practice. We'll cover basic operations like deleting, restoring, and permanently deleting records, as well as querying with soft deletes.

### Basic Operations

#### Deleting a Record

To soft-delete a record, you can use the `delete()` method just as you would for a regular delete operation:

```php
$post = Post::find(1);
$post->delete();
```

When this code runs, several things happen behind the scenes:

1. Laravel checks if the `SoftDeletes` trait is used in the `Post` model.
2. Instead of running a SQL `DELETE` query, it performs an `UPDATE` query.
3. The `deleted_at` column is set to the current timestamp.
4. The model's `deleted_at` attribute is updated in memory.

This approach allows you to maintain the record in the database while marking it as deleted. It's beneficial when you need to keep records for auditing purposes or when you want to implement a "trash" feature in your application.

If you did not have soft deletes enabled, the `$post->delete()` method would generate the following SQL query:

```sql
DELETE FROM posts WHERE id = {id};
```

However, with soft deletes enabled, the query looks like this:

```sql
UPDATE posts SET deleted_at = '2024-05-26 12:00:00' WHERE id = {id};
```

This way, the record is not removed from the database but is instead marked as "deleted".

If you were to now try to retrieve the post with `Post::find(1)`, it would not return the record because it has been "soft deleted". Under the hood, Laravel automatically adds a `WHERE deleted_at IS NULL` clause to your queries to exclude soft-deleted records, e.g.:

```sql
SELECT * FROM posts WHERE id = 1 AND deleted_at IS NULL;
```

So, you won't see the soft-deleted record in your query results unless you explicitly ask for it.

#### Restoring a Soft-Deleted Record

To bring back a soft-deleted record, you use the `restore()` method on the model:

```php
$post = Post::withTrashed()->find(1);
$post->restore();
```

Here's what happens when you run this code:

1. `withTrashed()` tells Laravel to include soft-deleted records in the query.
2. `find(1)` retrieves the post, even if it's soft-deleted.
3. `restore()` sets the `deleted_at` column back to `NULL`.

This process effectively "undeletes" the record, making it visible in normal queries again.

#### Permanently Deleting a Record

If you need to remove a record from the database permanently, use `forceDelete()`:

```php
$post = Post::withTrashed()->find(1);
$post->forceDelete();
```

This method:

1. Bypasses the soft delete mechanism.
2. Executes a SQL `DELETE` query to permanently remove the record.
3. Removes any associated files or resources if you've set up your model to handle this.

Use `forceDelete()` with caution, as it permanently removes data and can't be undone, unless you have a backup strategy in place.

### Querying with Soft Deletes

Soft deletes affect how you query your database. Laravel provides methods to control whether soft-deleted records are included in query results or not.

#### Retrieving Only Non-Deleted Records

By default, Laravel excludes soft-deleted records from query results:

```php
$activePosts = Post::all(); // Only returns non-deleted posts
```

As we mentioned earlier, Laravel automatically adds a where clause to your query:

```sql
SELECT * FROM posts WHERE deleted_at IS NULL
```

This ensures that your queries don't return "deleted" records unless you explicitly ask for them.

#### Including Soft-Deleted Records

To include soft-deleted records in your query, use `withTrashed()`:

```php
$allPosts = Post::withTrashed()->get();
```

This method removes the `WHERE deleted_at IS NULL` clause from the query, allowing you to retrieve all records, regardless of their deleted status. The generated SQL query looks like this:

```sql
SELECT * FROM posts;
```

So using `withTrashed()` is useful when you need to access soft-deleted records for auditing or recovery purposes.

#### Retrieving Only Soft-Deleted Records

In some cases, you may need to retrieve only soft-deleted records. Laravel provides the `onlyTrashed()` method for this purpose:

```php
$deletedPosts = Post::onlyTrashed()->get();
```

This method adds a `WHERE deleted_at IS NOT NULL` clause to your query, returning only the "deleted" records.

### Using the `DB` Facade

While Eloquent provides a high-level API for working with soft deletes, sometimes you might need to use raw SQL queries or the Query Builder. The `DB` facade in Laravel allows you to work with soft deletes at a lower level, giving you more control over your database operations.

Here are some examples with explanations:

```php
use Illuminate\Support\Facades\DB;

// Soft delete a record
DB::table('posts')->where('id', 1)->update(['deleted_at' => now()]);
```

This query manually sets the `deleted_at` column to the current timestamp, effectively soft-deleting the record. Unlike Eloquent's `delete()` method, this doesn't trigger any model events.

```php
// Restore a soft-deleted record
DB::table('posts')->where('id', 1)->update(['deleted_at' => null]);
```

Here, we're restoring a soft-deleted record by setting its `deleted_at` column back to null. This makes the record visible to normal queries again.

```php
// Query including soft-deleted records
$allPosts = DB::table('posts')->get();
```

This query retrieves all records, including soft-deleted ones. The `DB` facade doesn't automatically exclude soft-deleted records like Eloquent does.

```php
// Query only non-deleted records
$activePosts = DB::table('posts')->whereNull('deleted_at')->get();
```

To exclude soft-deleted records, we explicitly add a `whereNull('deleted_at')` clause. This mimics Eloquent's default behavior.

```php
// Query only soft-deleted records
$deletedPosts = DB::table('posts')->whereNotNull('deleted_at')->get();
```

This query retrieves only soft-deleted records by checking for non-null `deleted_at` values.

```php
// Permanently delete a soft-deleted record
DB::table('posts')->where('id', 1)->delete();
```

This operation permanently removes the record from the database, regardless of its soft-deleted status. Be cautious with this as it's irreversible.

The `DB` facade bypasses Eloquent's model events and global scopes, so you'll need to handle any related logic manually if needed.

## General Best Practices

When working with soft deletes in Laravel, there are several best practices to consider for optimal performance and data integrity. Here are some recommendations:

### 1. Regular Cleanup of Old Soft-Deleted Records

One of the main downsides of soft deletes is that records remain in your database even after they're "deleted". This can lead to unnecessary data bloat over time.

To prevent your database from growing too large, consider implementing a cleanup routine:

```php
Post::onlyTrashed()
    ->where('deleted_at', '<', now()->subYears(2))
    ->forceDelete();
```

This code permanently removes records that have been soft-deleted for more than two years. Here's why this is important:

- Soft-deleted records still occupy space in your database. Regular cleanup prevents unnecessary database growth.
- Fewer records generally mean faster queries, even when using `withTrashed()`.
- Some data protection regulations require data to be permanently deleted after a certain period.

You can schedule this command to run regularly using Laravel's task scheduler so that old soft-deleted records are cleaned up automatically.

### 2. Use Soft Deletes Carefully

While soft deletes are useful, they're not always necessary for every model. You should consider the following factors when deciding whether to use soft deletes:

- Use soft deletes for important data that might need to be restored.
- If a model has many important relationships, soft deletes can help maintain data integrity.
- For tables with a very high volume of records, consider the potential performance impact of soft deletes.
- For data privacy or compliance reasons, permanent deletion might be more appropriate.

### 3. Implement Access Controls

If your application allows users to access soft-deleted records, ensure that unauthorized users can't access them:

```php
public function show(Post $post)
{
    if ($post->trashed()) {
        abort(404);
    }
    return view('posts.show', compact('post'));
}
```

This prevents unauthorized access to soft-deleted records, which could contain sensitive or outdated information. If you need to allow certain users to access soft-deleted records, implement appropriate access controls based on user roles or permissions.

### 4. Be Cautious with Indexing

Regarding indexing the `deleted_at` column, there's debate in the community. Some argue against it because:

- Most queries filter for non-deleted records (`WHERE deleted_at IS NULL`), which may not benefit from an index on `deleted_at`.
- An index on `deleted_at` could potentially slow down write operations.

Instead, consider your specific use case:

- If you frequently query for soft-deleted records or restore them, an index might be beneficial.
- If your primary operations are on non-deleted records, you might not need an index on `deleted_at`.

Always measure the performance impact in your specific scenario before deciding on indexing strategy.

For more information about indexes in general, refer to Neon's documentation on [indexes](https://neon.tech/docs/postgres/indexes).

## Testing Soft Deletes

As with anything, testing is important, that way you can make sure your soft delete implementation works correctly. Here's an example test case:

Laravel provides several tools and assertions specifically for testing soft deletes. Let's go over some common tests you might want to include in your test suite.

### Testing Soft Delete Functionality

Let's start with a test to ensure a post is correctly soft deleted:

```php
public function it_soft_deletes_a_post()
{
    $post = Post::factory()->create();

    $post->delete();

    $this->assertSoftDeleted($post);
    $this->assertDatabaseHas('posts', ['id' => $post->id]);
    $this->assertDatabaseMissing('posts', [
        'id' => $post->id,
        'deleted_at' => null
    ]);
}
```

This test:

1. Creates a post using a factory.
2. Soft deletes the post.
3. Asserts that the post is soft deleted using Laravel's `assertSoftDeleted` method.
4. Checks that the post still exists in the database.
5. Verifies that there's no record with a null `deleted_at` for this post.

### Testing Restore Functionality

Next, let's test the restore functionality:

```php
public function it_restores_a_soft_deleted_post()
{
    $post = Post::factory()->create();
    $post->delete();

    $post->restore();

    $this->assertDatabaseHas('posts', [
        'id' => $post->id,
        'deleted_at' => null
    ]);
    $this->assertNotSoftDeleted($post);
}
```

This test:

1. Creates and soft deletes a post.
2. Restores the post.
3. Checks that the post exists in the database with a null `deleted_at`.
4. Uses Laravel's `assertNotSoftDeleted` to confirm the post is no longer soft deleted.

### Testing Query Scopes

It's also important to test that your queries are correctly scoping soft deleted records:

```php
public function it_excludes_soft_deleted_posts_from_regular_queries()
{
    $activePost = Post::factory()->create();
    $deletedPost = Post::factory()->create();
    $deletedPost->delete();

    $posts = Post::all();

    $this->assertTrue($posts->contains($activePost));
    $this->assertFalse($posts->contains($deletedPost));
}

public function it_includes_soft_deleted_posts_when_using_with_trashed()
{
    $activePost = Post::factory()->create();
    $deletedPost = Post::factory()->create();
    $deletedPost->delete();

    $posts = Post::withTrashed()->get();

    $this->assertTrue($posts->contains($activePost));
    $this->assertTrue($posts->contains($deletedPost));
}
```

These two tests ensure that:

1. Regular queries exclude soft deleted records and only return active posts.
2. Queries using `withTrashed()` include soft deleted records.

### Testing Force Delete

Finally, let's test the force delete functionality:

```php
public function it_permanently_deletes_a_post()
{
    $post = Post::factory()->create();

    $post->forceDelete();

    $this->assertDatabaseMissing('posts', ['id' => $post->id]);
    $this->assertDatabaseCount('posts', 0);
}
```

This test verifies that force deleting a post removes it entirely from the database.

## Conclusion

Laravel's soft delete feature provides a way to manage data deletion without losing valuable information. By using soft deletes, you can improve your application's data integrity and provide features like data recovery or undo functionality to your users.

Consider the performance implications of soft deletes, especially when working with large datasets. Utilize Neon Postgres's capabilities, such as [indexing](https://neon.tech/docs/postgres/indexes) and [table partitioning](https://neon.tech/docs/postgres/ddl-partitioning), to maintain high performance as your application scales.

When implementing soft deletes, always think about the lifecycle of your data. Plan on implementing policies for permanent deletion of old soft-deleted records to manage database growth optimally and comply with data retention regulations.

## Additional Resources

- [Laravel Documentation on Soft Deletes](https://laravel.com/docs/eloquent#soft-deleting)
- [Neon Postgres Documentation](/docs)
- [Laravel Eloquent Performance Tips](https://laravel.com/docs/eloquent-relationships#eager-loading)


# Testing Laravel Applications with Neon's Database Branching

---
title: Testing Laravel Applications with Neon's Database Branching
subtitle: Leveraging Realistic Production Data for Robust Testing with Laravel and Neon Branching
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-05-26T00:00:00.000Z'
updatedOn: '2024-05-26T00:00:00.000Z'
---

[Laravel](https://laravel.com) is a popular PHP framework widely used for building web applications. It includes powerful tools for automated testing, with [PEST](https://pestphp.com/) being a preferred option due to its simplicity and effectiveness.

Testing with realistic data is crucial as it helps ensure that your application performs well under real-world conditions. Neon's database branching feature offers a unique solution by allowing you to test with actual production data without affecting your live database, thus maintaining data integrity and security.

## 1. Understanding Laravel Testing Approaches

In Laravel, developers commonly use a local SQLite database for testing. This method is favored because it allows for starting with a clean state for each test run by applying all database migrations and seeders. This setup is ideal for parallel testing, ensuring that tests run quickly and do not interfere with each other.

However, testing with SQLite can differ significantly from a production environment that might use a different database system, such as MySQL or PostgreSQL. These differences can affect the application's behavior and lead to unexpected issues in production. Therefore, testing with real data can provide a more accurate assessment of how the application will perform in its live environment. Using production data, though, presents risks and challenges related to security and data management.

## 2. Neon Branching

Neon offers a database [branching feature](/docs/introduction/branching) that allows you to create isolated branches of your database for development, testing, and more.

A branch in Neon is a copy-on-write clone of your data that can be made from the current database state or any past state. This means you can have an exact copy of your production data at a specific point in time to use for testing.

Some key benefits of Neon branching include:

- **Isolation:** Branches are completely isolated from the original database and other branches, ensuring that the operations performed on one do not affect others.
- **Efficiency:** Branching is quick and does not burden the parent database, as it uses a copy-on-write mechanism. This means the original database's performance remains unaffected, even when multiple branches are in use.
- **Flexibility:** You can modify or delete branches without impacting the original data. Changes to a branch are independent and only record the differences from the point of branch creation.
- **Simplicity:** Every Neon project starts with a main branch, and new branches can be created from this root or from any other branch. This structure simplifies managing different versions of your database.

Neon's branching is particularly useful in continuous integration and delivery pipelines, enhancing developer productivity by reducing the setup time needed for test environments.

This feature allows you to test with realistic data scenarios without the overhead of maintaining multiple separate databases.

## 3. Setting Up Your Testing Environment

Setting up a robust testing environment with Neon and Laravel involves several steps, from configuring your Neon account to setting up Laravel for testing with PEST. This section will guide you through the prerequisites, installation, configuration, and initial test creation to ensure your application is ready for effective testing.

### Prerequisites

Before you begin, ensure you have the following:

- **Neon Account:** You need an account with Neon to access their database services. You can sign up at [Neon's official website](https://neon.tech).
- **API Keys:** Generate API keys from the Neon console. These keys will allow your Laravel application to interact with the Neon database programmatically.
- **Local Development Environment:** Laravel requires a PHP environment (PHP 7.4 or later) and [Composer](https://getcomposer.org/) for managing dependencies.
- **Laravel Installation:** A Laravel project set up on your local machine. For installation instructions, refer to the [Laravel documentation](https://laravel.com/docs/11.x/installation).

### Installation and Configuration

To set up your testing environment with Neon and Laravel, follow these steps:

1. **Configure Database Connection:**

   - After creating your Neon account and a new database branch, obtain the connection details from the Neon dashboard.
   - Open your Laravel project and update the `.env` file with the Neon database connection parameters:

     ```env
     DB_CONNECTION=pgsql
     DB_HOST=your-neon-hostname.neon.tech
     DB_PORT=5432
     DB_DATABASE=<your-database-name>
     DB_USERNAME=<your-username>
     DB_PASSWORD=<your-password>
     ```

2. **Install PEST PHP:**

   - PEST is a testing framework for PHP that works seamlessly with Laravel. Install PEST via Composer with the following command:

     ```
     composer require pestphp/pest --dev
     composer require pestphp/pest-plugin-laravel --dev
     ```

#### Creating a Migration and Seeder

1. **Generate Migration and Model:**

   - Run the following command to create a new migration file for a `questions` table and its associated model:

   ```
   php artisan make:model Question -m
   ```

   - Open the generated migration file in the `database/migrations` directory and add fields to the `questions` table schema:

     ```php
     Schema::create('questions', function (Blueprint $table) {
        $table->id();
        $table->string('title');
        $table->text('description');
        $table->timestamps();
     });
     ```

2. **Create Seeder:**

   - Generate a seeder to populate the `questions` table:

     ```
     php artisan make:seeder QuestionsTableSeeder
     ```

   - Open the `database/seeders/QuestionsTableSeeder.php` and in the `run` method, add code to create sample questions:

     ```php
     public function run()
     {
        $questions = [
            ['What is Laravel?', 'A PHP framework for web artisans.'],
            ['What is MVC?', 'A design pattern called Model-View-Controller.'],
            ['What is PHP?', 'A popular general-purpose scripting language.'],
            ['How do databases work?', 'Databases store data in an organized manner.'],
            ['What is OOP?', 'Object-Oriented Programming is a programming paradigm.'],
            ['What is a variable in programming?', 'A variable is used to store information.'],
            ['What is an API?', 'Application Programming Interface, a way for systems to interact.'],
            ['What are webhooks?', 'Webhooks allow applications to send automated messages or information.'],
            ['What is JSON?', 'JSON is a format for storing and transporting data.'],
            ['What is a function in programming?', 'A function is a block of code designed to perform a particular task.']
        ];

        foreach ($questions as $q) {
            Question::create([
                'title' => $q[0],
                'description' => $q[1]
            ]);
        }
     }
     ```

   - Register the seeder in `DatabaseSeeder.php`:

     ```php
     $this->call(QuestionsTableSeeder::class);
     ```

3. **Run Migrations and Seeders:**

   - Migrate the database to create the `questions` table:

     ```
     php artisan migrate
     ```

   - Seed the database with test data:

     ```
     php artisan db:seed
     ```

#### Creating a Questions Controller

1. **Generate the Controller:**
   A controller is a PHP class that handles HTTP requests. You can create a controller to manage questions data in your Laravel application.

   - Use Artisan to create a new controller named `QuestionController`:

     ```
     php artisan make:controller QuestionController
     ```

2. **Add a Method to Retrieve Questions:**

   - Open the newly created `QuestionController` in the `app/Http/Controllers` directory.
   - Add a method to fetch and return all questions:

     ```php
     public function index() {
         $questions = \App\Models\Question::all();
         return response()->json($questions);
     }
     ```

3. **Update Routes:**

   - Open the `routes/web.php` file and add a route to handle GET requests for questions:

     ```php
     Route::get('/questions', [\App\Http\Controllers\QuestionController::class, 'index']);
     ```

This setup provides a simple API endpoint to retrieve all questions from the database.

To, verify the setup, you can run the Laravel development server:

```
php artisan serve
```

Access the `/questions` endpoint in your browser or a tool like Postman to see the JSON response with the seeded questions.

#### Writing a PEST Test for the `QuestionController`

1. **Create the Test File:**

   - PEST allows you to write tests in a very expressive way. You can create a test file specifically for the `QuestionController`:

     ```
     php artisan pest:test QuestionTest
     ```

2. **Write the Test:**
   Usually, you would write a test that uses the `RefreshDatabase` trait to migrate the database and then seed it with test data before each test. But in this case, we will use the Neon branch to test with real data instead.

   - Open the generated test file in `tests/Feature` and add a test to check the `/questions` endpoint:

     ```php
     it('can retrieve questions from the database', function () {
        $response = $this->get('/questions');
        $response->assertStatus(200)
                ->assertJsonStructure([
                    '*' => ['id', 'title', 'description', 'created_at', 'updated_at']
                ]);
     });
     ```

   - This test does the following:
     - It sends a GET request to the `/questions` endpoint.
     - It asserts that the HTTP status is `200` and checks the JSON structure to match the expected fields for questions.

### Running the Tests

Run the updated tests to ensure your controller behaves correctly:

```
./vendor/bin/pest
```

PEST will execute the test and provide feedback on the test results.

## 4. Using Neon Branching with Laravel

You should never run tests against your production database, as it can lead to data corruption and security risks. Especially if you are using `RefreshDatabase` or `DatabaseTransactions` traits, which can delete or modify data during testing. This is where Neon branching comes in handy.

Neon's branching feature enables you to create isolated database environments, which is ideal for testing changes without impacting the production database.

This can be particularly useful when testing complex features or changes that require realistic data scenarios. Especially when there are schema changes or data migrations involved, Neon branching provides a safe and efficient way to validate your application's behavior on a copy of your production data.

### Creating a Neon Branch

1. **Log In to Neon Dashboard:**

   - Access your Neon dashboard by logging in at [Neon's official website](https://neon.tech).

2. **Select Your Database:**

   - Navigate to the database project that you are using for your production environment.

3. **Create a New Branch:**
   - Click on "Branches" in the sidebar menu.
   - Click on "Create Branch."
   - Name your new branch (e.g., "testing-branch") and specify if it should be created from the current state of the database or from a specific point in time. This creates a copy-on-write clone of your database.
   - Wait for the branch to be fully provisioned, which usually takes just a few seconds.

### Integrating Neon Branching with Laravel Testing

Go back to your Laravel project and integrate the Neon branch into your testing setup:

1. **Update Environment Configuration:**

   - Once your branch is created, obtain the connection details (hostname, database name, username, and password) from the Neon dashboard.
   - Create a new environment file in your Laravel project, such as `.env.testing`, and configure it to use the Neon testing branch. This ensures that your testing environment uses its database configuration.

     ```env
     DB_CONNECTION=pgsql
     DB_HOST=your-neon-testing-hostname.neon.tech
     DB_PORT=5432
     DB_DATABASE=<your-testing-database-name>
     DB_USERNAME=<your-testing-username>
     DB_PASSWORD=<your-testing-password>
     ```

1. **Update PHPUnit Configuration:**

   - Ensure that PHPUnit (used by PEST for running tests) is configured to use the `.env.testing` file. Update your `phpunit.xml` file to specify the environment file:

     ```xml
     <php>
         <env name="APP_ENV" value="testing"/>
         <env name="DB_CONNECTION" value="pgsql"/>
     </php>
     ```

1. **Run Tests:**

   - With the testing branch configured, you can write tests that interact with the database as if it were production data, without the risk of affecting real user data. Use PEST to run your tests:

     ```bash
     ./vendor/bin/pest
     ```

   - Examine the output from PEST to ensure your application behaves as expected against the testing branch. This approach allows you to test changes in a controlled environment that mirrors your production setup.

## 5. Managing Neon Branches with `neonctl` CLI

Automated testing is an essential aspect of software development, ensuring that new code contributions don't break existing functionality. Neon's database branching feature enables you to create isolated environments for testing changes without affecting your production database.

With the `neonctl` CLI tool, managing these branches becomes straightforward and seamless.

### Installing `neonctl`

Before you can start using `neonctl`, you need to install it on your local machine. Follow the installation instructions provided in the [Neon CLI documentation](/docs/reference/cli-install) to set up `neonctl` on your system.

### Using `neonctl` to Manage Branches

Once `neonctl` is installed, you can use it to interact with your Neon database branches. Here are the basic commands for managing branches:

#### 1. [Creating a Branch](/docs/reference/cli-branches#create)

To create a new branch, use the `neonctl branches create` command:

```bash
neonctl branches create --project-id PROJECT_ID --parent PARENT_BRANCH_ID --name BRANCH_NAME
```

Replace `PROJECT_ID`, `PARENT_BRANCH_ID`, and `BRANCH_NAME` with the appropriate values for your Neon project. This command will create a new branch based on the specified parent branch.

#### 2. [Listing Branches](/docs/reference/cli-branches#list)

To list all branches in your Neon project, use the `neonctl branches list` command:

```bash
neonctl branches list --project-id PROJECT_ID
```

Replace `PROJECT_ID` with your Neon project ID. This command will display a list of all branches along with their IDs, names, and other relevant information.

#### 3. [Obtaining Connection String](/docs/reference/cli-connection-string)

Once you've created a branch, you'll need to obtain the connection string to configure your Laravel application. Use the `neonctl connection-string` command:

```bash
neonctl connection-string BRANCH_ID
```

Replace `BRANCH_ID` with the ID of the branch you want to connect to. This command will output the connection string that you can use to configure your Laravel `.env` file.

#### 4. [Deleting a Branch](/docs/reference/cli-branches#delete)

After you've finished testing with a branch, you can delete it using the `neonctl branches delete` command:

```bash
neonctl branches delete BRANCH_ID
```

Replace `BRANCH_ID` with the ID of the branch you want to delete. This command will remove the branch from your Neon project, ensuring that resources are not left unused.

### Integrating Neon Branches with Laravel Testing

Once you've created a Neon branch using `neonctl`, you can integrate it into your Laravel testing workflow:

1. **Obtain Connection Details:** Use `neonctl connection-string` to get the connection details for the branch.
2. **Update `.env.testing` File:** Update your Laravel `.env.testing` file with the connection details obtained from `neonctl`.
3. **Run Tests:** Execute your Laravel tests as usual, ensuring that they interact with the Neon branch database.
4. **Clean Up:** After testing is complete, use `neonctl branches delete` to delete the branch and clean up resources.

## Conclusion

Testing Laravel applications with Neon's database branching offers a solution for ensuring the reliability and performance of your codebase.

By using realistic production data in a controlled testing environment, developers can confidently validate their changes without risking the integrity of live databases.

Neon's branching feature provides isolation, efficiency, flexibility, and simplicity, making it a valuable tool for streamlining the testing process.

## Additional Resources

- [Laravel Documentation](https://laravel.com/docs) - Official documentation for the Laravel PHP framework, covering installation, configuration, and usage guides.
- [Neon Documentation](/docs) - Comprehensive documentation for Neon's database services, including guides, tutorials, and API references.
- [GitHub Actions Tutorials](https://docs.github.com/en/actions/learn-github-actions) - Learn how to automate your workflow with GitHub Actions, including tutorials on setting up continuous integration for Laravel applications.
- [Neon Branching GitHub Actions Guide](/docs/guides/branching-github-actions) - Step-by-step guide on integrating Neon database branching with GitHub Actions for automated testing workflows.


# Building a Multi-Step Form with Laravel Volt, Folio, and Neon Postgres

---
title: Building a Multi-Step Form with Laravel Volt, Folio, and Neon Postgres
subtitle: Learn how to create a multi-step form with Laravel Volt, Folio, and Neon Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-10-19T00:00:00.000Z'
updatedOn: '2024-10-19T00:00:00.000Z'
---

In this guide, we'll walk through the process of building a multi-step form using Laravel [Volt](https://livewire.laravel.com/docs/volt), [Folio](https://laravel.com/docs/11.x/folio), and Neon Postgres.

Laravel Volt provides reactivity for dynamic form interactions, Folio offers file-based routing for a clean project structure, and Neon Postgres serves as our scalable database solution.

Our example app will be a job application form with multiple steps, including personal information, education, and work experience.

## Prerequisites

Before we begin, make sure you have:

- PHP 8.1 or higher installed
- Composer for managing PHP dependencies
- A [Neon](https://console.neon.tech/signup) account for Postgres hosting
- Basic familiarity with Laravel and Postgres

## Setting up the Project

Let's start by creating a new Laravel project and setting up the necessary components.

1. Create a new Laravel project:

   ```bash
   composer create-project laravel/laravel job-application-form
   cd job-application-form
   ```

2. Install Laravel Folio for file-based routing:

   ```bash
   composer require laravel/folio
   ```

3. Install the Volt Livewire adapter for Laravel, this will also install the Livewire package:

   ```bash
   composer require livewire/volt
   ```

4. After installing Volt, you can install the Volt service provider:

   ```bash
   php artisan volt:install
   ```

## Configuring the Database Connection

Update your `.env` file with your Neon Postgres credentials:

```env
DB_CONNECTION=pgsql
DB_HOST=your-neon-hostname.neon.tech
DB_PORT=5432
DB_DATABASE=your_database_name
DB_USERNAME=your_username
DB_PASSWORD=your_password
```

Replace `your-neon-hostname.neon.tech`, `your_database_name`, `your_username`, and `your_password` with your Neon Postgres connection details.

## Database Design

Let's create the database migrations for our job application form. We'll use separate tables for each section and leverage Postgres JSON columns for flexible data storage for additional information.

First, let's create the migration for the applicants table using the following `artisan` command:

```bash
php artisan make:migration create_applicants_table
```

Note that the `create_applicants_table` migration name follows the Laravel convention of `create_{table_name}_table`, where `{table_name}` is the name of the table you're creating. That way, Laravel can automatically determine the table name from the migration name, and also it will be easier to identify the purpose of the migration file by its name for other developers.

This command generates a new migration file in the `database/migrations` directory. Open the newly created file and update its content as follows:

```php
<?php

use Illuminate\Database\Migrations\Migration;
use Illuminate\Database\Schema\Blueprint;
use Illuminate\Support\Facades\Schema;

return new class extends Migration
{
    public function up()
    {
        Schema::create('applicants', function (Blueprint $table) {
            $table->id();
            $table->string('first_name');
            $table->string('last_name');
            $table->string('email')->unique();
            $table->jsonb('additional_info')->nullable();
            $table->timestamps();
        });
    }

    public function down()
    {
        Schema::dropIfExists('applicants');
    }
};
```

This migration creates the `applicants` table with fields for `first_name`, `last_name`, and `email`. The `email` field is set as unique to prevent duplicate applications. We've also included a `jsonb` column called `additional_info` for storing any extra data that doesn't fit into the predefined columns. This flexibility is one of the advantages of using Postgres with Laravel.

Next, let's create the migration for the educations table:

```bash
php artisan make:migration create_educations_table
```

Update the newly created migration file with the following content:

```php
<?php

use Illuminate\Database\Migrations\Migration;
use Illuminate\Database\Schema\Blueprint;
use Illuminate\Support\Facades\Schema;

return new class extends Migration
{
    public function up()
    {
        Schema::create('educations', function (Blueprint $table) {
            $table->id();
            $table->foreignId('applicant_id')->constrained()->onDelete('cascade');
            $table->string('institution');
            $table->string('degree');
            $table->date('start_date');
            $table->date('end_date')->nullable();
            $table->jsonb('additional_info')->nullable();
            $table->timestamps();
        });
    }

    public function down()
    {
        Schema::dropIfExists('educations');
    }
};
```

This migration creates the `educations` table. It includes a foreign key `applicant_id` that references the `id` column in the `applicants` table. The `onDelete('cascade')` ensures that if an applicant is deleted, their education records are also removed. We've included fields for the institution, degree, and start/end dates. Again, we have an `additional_info` jsonb column for flexibility.

Finally, let's create the migration for the work experiences table:

```bash
php artisan make:migration create_work_experiences_table
```

Update this migration file with the following content:

```php
<?php

use Illuminate\Database\Migrations\Migration;
use Illuminate\Database\Schema\Blueprint;
use Illuminate\Support\Facades\Schema;

return new class extends Migration
{
    public function up()
    {
        Schema::create('work_experiences', function (Blueprint $table) {
            $table->id();
            $table->foreignId('applicant_id')->constrained()->onDelete('cascade');
            $table->string('company');
            $table->string('position');
            $table->date('start_date');
            $table->date('end_date')->nullable();
            $table->text('responsibilities');
            $table->jsonb('additional_info')->nullable();
            $table->timestamps();
        });
    }

    public function down()
    {
        Schema::dropIfExists('work_experiences');
    }
};
```

This migration creates the `work_experiences` table. Similar to the `educations` table, it has a foreign key relationship with the `applicants` table. It includes fields for the company, position, start/end dates, and responsibilities. The `responsibilities` field is of type `text` to allow for longer descriptions. We've also included an `additional_info` jsonb column here.

Now that we've created all our migrations, we can run them to create the tables in our database:

```bash
php artisan migrate
```

This command will execute all the migrations we've just created, setting up the database schema for our job application form.

One thing to note is that we've used the `jsonb` column type for storing additional information in each table. This allows us to store flexible data structures without needing to define a fixed schema. Postgres' JSONB data type is ideal for this use case.

For your Laravel migrations, you should not use the Neon Postgres Pooler. The Pooler is designed to manage connections for long-running processes, such as web servers, and is not necessary for short-lived processes like migrations.

## Creating Models

Next, let's create models for our `Applicant`, `Education`, and `WorkExperience` tables. Models in Laravel are used to interact with database tables and represent the data in your application in an object-oriented way.

Laravel provides an easy way to generate models using the `artisan` command. To create the `Applicant` model run:

```bash
php artisan make:model Applicant
```

This command creates a new file `app/Models/Applicant.php`. Open this file and update it with the following content:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class Applicant extends Model
{
    use HasFactory;

    protected $fillable = [
        'first_name',
        'last_name',
        'email',
        'additional_info'
    ];

    protected $casts = [
        'additional_info' => 'array',
    ];

    public function educations()
    {
        return $this->hasMany(Education::class);
    }

    public function workExperiences()
    {
        return $this->hasMany(WorkExperience::class);
    }
}
```

Now, create the `Education` model:

```bash
php artisan make:model Education
```

Update the newly created file at `app/Models/Education.php` with the following content:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class Education extends Model
{
    use HasFactory;

    public $table = 'educations';

    protected $fillable = [
        'applicant_id',
        'institution',
        'degree',
        'start_date',
        'end_date',
        'additional_info'
    ];

    protected $casts = [
        'start_date' => 'date',
        'end_date' => 'date',
        'additional_info' => 'array',
    ];

    public function applicant()
    {
        return $this->belongsTo(Applicant::class);
    }
}
```

Finally, create the `WorkExperience` model:

```bash
php artisan make:model WorkExperience
```

And update the `app/Models/WorkExperience.php` file with the following content:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class WorkExperience extends Model
{
    use HasFactory;

    protected $fillable = [
        'applicant_id',
        'company',
        'position',
        'start_date',
        'end_date',
        'responsibilities',
        'additional_info'
    ];

    protected $casts = [
        'start_date' => 'date',
        'end_date' => 'date',
        'additional_info' => 'array',
    ];

    public function applicant()
    {
        return $this->belongsTo(Applicant::class);
    }
}
```

Let's quickly note down the most important parts in these model definitions:

- We've used the `$fillable` property to specify which attributes can be mass-assigned. This is a security feature to prevent unintended mass assignment vulnerabilities.
- We've defined relationships between models. An `Applicant` has many `Education` and `WorkExperience` records, while `Education` and `WorkExperience` belong to an `Applicant`.
- We've used the `$casts` property to automatically cast certain attributes to specific types. For example, we're casting the `additional_info` field to an array, which works well with Postgres' JSONB column type.
- The `start_date` and `end_date` fields are cast to date objects, which allows for easy date manipulation in PHP.

These models will allow us to easily interact with our database tables using Laravel's Eloquent ORM. They provide a convenient way to retrieve, create, update, and delete records, as well as define relationships between different tables.

## Creating a layout for the multi-step form

Before we create the form components, let's set up a layout for our multi-step form. We'll create a main layout file that includes the necessary CSS and JavaScript assets including the Livewire scripts.

Create a new Blade layout file at `resources/views/layouts/app.blade.php`:

```blade
<!DOCTYPE html>
<html lang="{{ str_replace('_', '-', app()->getLocale()) }}">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Job Application Form</title>
    <script src="https://cdn.tailwindcss.com"></script>
    @vite(['resources/css/app.css', 'resources/js/app.js'])
    @livewireStyles
</head>
<body>
    <div class="container mx-auto mt-8">
        @yield('content')
    </div>

    @livewireScripts
</body>
</html>
```

In this layout file:

- We've included the necessary meta tags for character encoding, viewport settings and the page title.
- We've used the `@vite` directive to include the CSS and JavaScript assets. This directive is provided by the Laravel Vite package, which integrates Laravel with the Vite build tool for modern frontend development.
- We've included the Livewire styles and scripts. Livewire is a full-stack framework for Laravel that allows you to build dynamic interfaces without writing JavaScript.

To compile the frontend assets, you'll need to run the following commands:

```bash
npm install
npm run build
```

## Implementing File-based Routing with Folio

Laravel Folio was introduced in 2023, and it offers a new approach to routing in Laravel applications.

It simplifies routing by allowing you to create routes simply by adding Blade templates to a specific directory. This file-based routing system makes your project structure cleaner and more intuitive.

It is not a replacement for Laravel's built-in routing system but rather a complementary feature that simplifies routing for certain types of applications.

First, let's set up the directory structure for our multi-step form. Create the following directory structure in your `resources/views/pages` folder:

```shell
resources/
└── views/
    └── pages/
        ├── index.blade.php
        └── apply/
            ├── index.blade.php
            ├── personal-info.blade.php
            ├── education.blade.php
            ├── work-experience.blade.php
            └── review.blade.php
            └── confirmation.blade.php
```

With Folio, each of these Blade files automatically becomes a route. For example:

- `pages/index.blade.php` will be accessible at the root URL `/`
- `pages/apply/personal-info.blade.php` will be accessible at `/apply/personal-info`

To create a Folio page, you can use the `php artisan folio:page` command. For example, to create a page for the personal information step:

```bash
php artisan folio:page apply/personal-info
```

The above will create a blade file for the in `resources/views/pages/apply/personal-info.blade.php`:

```blade
<div>
    <h2>Personal Information</h2>
    <!-- Your form content will go here -->
</div>
```

You can list all available Folio routes using the following Artisan command:

```bash
php artisan folio:list
```

You can create similar pages for the education, work experience, and review steps:

```bash
php artisan folio:page apply/education
php artisan folio:page apply/work-experience
php artisan folio:page apply/review
```

We will update these files with the form components later in the guide.

The main thing to remember here is that with Folio, you don't need to manually define routes in a separate routes file. The mere presence of a Blade file in the `pages` directory automatically creates a corresponding route.

## Building the Multi-Step Form with Volt

Volt is a powerful addition to Laravel Livewire that allows you to build reactive components without writing JavaScript. Unlike traditional Livewire components, Volt lets you define your component's state and validation rules directly in the view file, eliminating the need for a separate component class.

Let's create Volt components for each step of our multi-step form.

### Personal Information Form

First, create the personal information form component:

```bash
php artisan make:volt personal-info-form
```

That will create a file at `resources/views/livewire/personal-info-form.blade.php`. Update the file with the following content:

```blade
<?php

use function Livewire\Volt\state;
use function Livewire\Volt\rules;

state([
    'first_name' => '',
    'last_name' => '',
    'email' => '',
]);

rules([
    'first_name' => 'required|min:2',
    'last_name' => 'required|min:2',
    'email' => 'required|email|unique:applicants,email',
]);

$saveAndContinue = function () {
    $this->validate();

    $applicant = \App\Models\Applicant::create($this->only(['first_name', 'last_name', 'email']));
    session(['applicant_id' => $applicant->id]);

    return redirect()->route('apply.education');
};

?>

<div class="max-w-lg p-8 mx-auto bg-white rounded-lg shadow-md">
    <h2 class="mb-6 text-2xl font-semibold text-gray-800">Personal Information</h2>

    <form wire:submit.prevent="saveAndContinue">
        <!-- First Name -->
        <div class="mb-4">
            <label for="first_name" class="block text-sm font-medium text-gray-700">First Name</label>
            <input type="text" id="first_name" wire:model="first_name" class="block w-full px-3 py-2 mt-1 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-indigo-500 focus:border-indigo-500 sm:text-sm">
            @error('first_name')
                <span class="text-sm text-red-500">{{ $message }}</span>
            @enderror
        </div>

        <div class="mb-4">
            <label for="last_name" class="block text-sm font-medium text-gray-700">Last Name</label>
            <input type="text" id="last_name" wire:model="last_name" class="block w-full px-3 py-2 mt-1 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-indigo-500 focus:border-indigo-500 sm:text-sm">
            @error('last_name')
                <span class="text-sm text-red-500">{{ $message }}</span>
            @enderror
        </div>

        <div class="mb-6">
            <label for="email" class="block text-sm font-medium text-gray-700">Email</label>
            <input type="email" id="email" wire:model="email" class="block w-full px-3 py-2 mt-1 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-indigo-500 focus:border-indigo-500 sm:text-sm">
            @error('email')
                <span class="text-sm text-red-500">{{ $message }}</span>
            @enderror
        </div>

        <div>
            <button type="submit" class="w-full px-4 py-2 text-white transition duration-200 bg-indigo-600 rounded-md hover:bg-indigo-700">
                Next
            </button>
        </div>
    </form>
</div>
```

Quick explanation of the code above:

- We define the component's state using the `state` function, which initializes the form fields.
- The `rules` function sets up validation rules for each field.
- The `saveAndContinue` function handles form submission. It validates the form, creates a new `Applicant` record, stores the `applicant_id` in the session, and redirects to the next step.
- The form fields are bound to the component's state using `wire:model`.
- Validation errors are displayed using `@error`.

In the same way, you can create components for the education, work experience, and review steps.

### Education Form

Next, create the education form component:

```bash
php artisan make:volt education-form
```

Update `resources/views/livewire/education-form.blade.php`:

```blade
<?php

use function Livewire\Volt\state;
use function Livewire\Volt\rules;

state([
    'institution' => '',
    'degree' => '',
    'start_date' => '',
    'end_date' => '',
]);

rules([
    'institution' => 'required|min:2',
    'degree' => 'required|min:2',
    'start_date' => 'required|date',
    'end_date' => 'nullable|date|after:start_date',
]);

$saveAndContinue = function () {
    $this->validate();

    $applicantId = session('applicant_id');
    \App\Models\Education::create(array_merge($this->all(), ['applicant_id' => $applicantId]));

    return redirect()->route('apply.work-experience');
};

?>

<div class="max-w-lg p-8 mx-auto bg-white rounded-lg shadow-md">
    <h2 class="mb-6 text-2xl font-semibold text-gray-800">Education</h2>

    <form wire:submit.prevent="saveAndContinue">
        <div class="mb-4">
            <label for="institution" class="block text-sm font-medium text-gray-700">Institution</label>
            <input type="text" id="institution" wire:model="institution" class="block w-full px-3 py-2 mt-1 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-indigo-500 focus:border-indigo-500 sm:text-sm">
            @error('institution')
                <span class="text-sm text-red-500">{{ $message }}</span>
            @enderror
        </div>

        <div class="mb-4">
            <label for="degree" class="block text-sm font-medium text-gray-700">Degree</label>
            <input type="text" id="degree" wire:model="degree" class="block w-full px-3 py-2 mt-1 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-indigo-500 focus:border-indigo-500 sm:text-sm">
            @error('degree')
                <span class="text-sm text-red-500">{{ $message }}</span>
            @enderror
        </div>

        <div class="mb-4">
            <label for="start_date" class="block text-sm font-medium text-gray-700">Start Date</label>
            <input type="date" id="start_date" wire:model="start_date" class="block w-full px-3 py-2 mt-1 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-indigo-500 focus:border-indigo-500 sm:text-sm">
            @error('start_date')
                <span class="text-sm text-red-500">{{ $message }}</span>
            @enderror
        </div>

        <div class="mb-6">
            <label for="end_date" class="block text-sm font-medium text-gray-700">End Date</label>
            <input type="date" id="end_date" wire:model="end_date" class="block w-full px-3 py-2 mt-1 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-indigo-500 focus:border-indigo-500 sm:text-sm">
            @error('end_date')
                <span class="text-sm text-red-500">{{ $message }}</span>
            @enderror
        </div>

        <div>
            <button type="submit" class="w-full px-4 py-2 text-white transition duration-200 bg-indigo-600 rounded-md hover:bg-indigo-700">
                Next
            </button>
        </div>
    </form>
</div>
```

### Work Experience Form

Next, let's create the work experience form component:

```bash
php artisan make:volt work-experience-form
```

Update `resources/views/livewire/work-experience-form.blade.php` similar to the previous components:

```blade
<?php

use function Livewire\Volt\state;
use function Livewire\Volt\rules;

state([
    'company' => '',
    'position' => '',
    'start_date' => '',
    'end_date' => '',
    'responsibilities' => '',
]);

rules([
    'company' => 'required|min:2',
    'position' => 'required|min:2',
    'start_date' => 'required|date',
    'end_date' => 'nullable|date|after:start_date',
    'responsibilities' => 'required|min:10',
]);

$saveAndContinue = function () {
    $this->validate();

    $applicantId = session('applicant_id');
    \App\Models\WorkExperience::create(array_merge($this->all(), ['applicant_id' => $applicantId]));

    return redirect()->route('apply.review');
};

?>

<div class="max-w-lg p-8 mx-auto bg-white rounded-lg shadow-md">
    <h2 class="mb-6 text-2xl font-semibold text-gray-800">Work Experience</h2>

    <form wire:submit.prevent="saveAndContinue">
        <div class="mb-4">
            <label for="company" class="block text-sm font-medium text-gray-700">Company</label>
            <input type="text" id="company" wire:model="company" class="block w-full px-3 py-2 mt-1 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-indigo-500 focus:border-indigo-500 sm:text-sm">
            @error('company')
                <span class="text-sm text-red-500">{{ $message }}</span>
            @enderror
        </div>

        <div class="mb-4">
            <label for="position" class="block text-sm font-medium text-gray-700">Position</label>
            <input type="text" id="position" wire:model="position" class="block w-full px-3 py-2 mt-1 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-indigo-500 focus:border-indigo-500 sm:text-sm">
            @error('position')
                <span class="text-sm text-red-500">{{ $message }}</span>
            @enderror
        </div>

        <div class="mb-4">
            <label for="start_date" class="block text-sm font-medium text-gray-700">Start Date</label>
            <input type="date" id="start_date" wire:model="start_date" class="block w-full px-3 py-2 mt-1 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-indigo-500 focus:border-indigo-500 sm:text-sm">
            @error('start_date')
                <span class="text-sm text-red-500">{{ $message }}</span>
            @enderror
        </div>

        <div class="mb-4">
            <label for="end_date" class="block text-sm font-medium text-gray-700">End Date</label>
            <input type="date" id="end_date" wire:model="end_date" class="block w-full px-3 py-2 mt-1 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-indigo-500 focus:border-indigo-500 sm:text-sm">
            @error('end_date')
                <span class="text-sm text-red-500">{{ $message }}</span>
            @enderror
        </div>

        <div class="mb-6">
            <label for="responsibilities" class="block text-sm font-medium text-gray-700">Responsibilities</label>
            <textarea id="responsibilities" wire:model="responsibilities" class="block w-full px-3 py-2 mt-1 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-indigo-500 focus:border-indigo-500 sm:text-sm"></textarea>
            @error('responsibilities')
                <span class="text-sm text-red-500">{{ $message }}</span>
            @enderror
        </div>

        <div>
            <button type="submit" class="w-full px-4 py-2 text-white transition duration-200 bg-indigo-600 rounded-md hover:bg-indigo-700">
                Next
            </button>
        </div>
    </form>
</div>
```

### Review Form

Finally, create the review form component:

```bash
php artisan make:volt review-form
```

Update `resources/views/livewire/review-form.blade.php` as we did for the other components:

```blade
<?php

use function Livewire\Volt\state;
use function Livewire\Volt\{mount};
use App\Models\Applicant;

state(['applicant' => null]);

mount(function () {
    $applicantId = session('applicant_id');
    $this->applicant = Applicant::with(['educations', 'workExperiences'])->findOrFail($applicantId);
});

$submit = function () {
    session()->flash('message', 'Your application has been submitted successfully!');
    return redirect()->route('apply.confirmation');
};

?>

<div class="max-w-2xl p-8 mx-auto bg-white rounded-lg shadow-md">
    <h2 class="mb-6 text-2xl font-semibold text-gray-800">Review Your Application</h2>

    <div class="mb-6">
        <h3 class="mb-3 text-xl font-semibold text-gray-700">Personal Information</h3>
        <p><strong>Name:</strong> {{ $applicant->first_name }} {{ $applicant->last_name }}</p>
        <p><strong>Email:</strong> {{ $applicant->email }}</p>
    </div>

    <div class="mb-6">
        <h3 class="mb-3 text-xl font-semibold text-gray-700">Education</h3>
        @foreach($applicant->educations as $education)
            <div class="p-4 mb-4 rounded-lg shadow-sm bg-gray-50">
                <p><strong>Institution:</strong> {{ $education->institution }}</p>
                <p><strong>Degree:</strong> {{ $education->degree }}</p>
                <p><strong>Period:</strong> {{ $education->start_date }} - {{ $education->end_date ?: 'Present' }}</p>
            </div>
        @endforeach
    </div>

    <div class="mb-6">
        <h3 class="mb-3 text-xl font-semibold text-gray-700">Work Experience</h3>
        @foreach($applicant->workExperiences as $experience)
            <div class="p-4 mb-4 rounded-lg shadow-sm bg-gray-50">
                <p><strong>Company:</strong> {{ $experience->company }}</p>
                <p><strong>Position:</strong> {{ $experience->position }}</p>
                <p><strong>Period:</strong> {{ $experience->start_date }} - {{ $experience->end_date ?: 'Present' }}</p>
                <p><strong>Responsibilities:</strong> {{ $experience->responsibilities }}</p>
            </div>
        @endforeach
    </div>

    <div>
        <button wire:click="submit" class="w-full px-4 py-2 text-white transition duration-200 bg-indigo-600 rounded-md hover:bg-indigo-700">
            Submit Application
        </button>
    </div>
</div>
```

These Volt components handle the state management, validation, and submission logic for each step of the multi-step form. That way Volt simplifies the process of creating interactive components by allowing you to define both the logic and the template in a single file.

To use these components in your Folio pages and make the routes named, you can include them like this. Named routes allow you to easily reference routes by name throughout your application. We also need to extend a layout for each page to ensure a consistent structure.

First, in each file, you will define a named route using the `name` function and extend the layout.

- For the `resources/views/pages/apply/personal-info.blade.php` file:

  ```blade
  <?php
  use function Laravel\Folio\name;

  name('apply.personal-info');
  ?>

  @extends('layouts.app')

  @section('title', 'Personal Information')

  @section('content')
  <div>
      <livewire:personal-info-form />
  </div>
  @endsection
  ```

We need to do the same for the other pages:

- For the `resources/views/pages/apply/education.blade.php` file:

  ```blade
  <?php
  use function Laravel\Folio\name;

  name('apply.education');
  ?>

  @extends('layouts.app')

  @section('title', 'Education')

  @section('content')
  <div>
      <livewire:education-form />
  </div>
  @endsection
  ```

- For the `resources/views/pages/apply/work-experience.blade.php` file:

  ```blade
  <?php
  use function Laravel\Folio\name;

  name('apply.work-experience');
  ?>

  @extends('layouts.app')

  @section('title', 'Work Experience')

  @section('content')
  <div>
      <livewire:work-experience-form />
  </div>
  @endsection
  ```

- And for the `resources/views/pages/apply/review.blade.php` file:

  ```blade
  <?php
  use function Laravel\Folio\name;

  name('apply.review');
  ?>

  @extends('layouts.app')

  @section('title', 'Review')

  @section('content')
  <div>
      <livewire:review-form />
  </div>
  @endsection
  ```

### Confirmation Page

Finally, create a confirmation page for the application submission:

```bash
php artisan folio:page apply/confirmation
```

Update the `resources/views/pages/apply/confirmation.blade.php` file:

```blade
<?php
use function Laravel\Folio\name;

name('apply.confirmation');
?>

@extends('layouts.app')

@section('title', 'Confirmation')

@section('content')
<div class="max-w-md p-8 mx-auto text-center bg-white rounded-lg shadow-md">
    <h2 class="mb-4 text-2xl font-semibold text-green-600">Application Submitted</h2>

    <p class="mb-6 text-gray-700">{{ session('message') }}</p>

    <a href="/" class="inline-block px-4 py-2 text-white transition duration-200 bg-indigo-600 rounded-md hover:bg-indigo-700">
        Return to Homepage
    </a>
</div>
@endsection
```

This page displays a success message after the application is submitted and provides a link to return to the homepage.

## Testing the Multi-Step Form

To manually verify that everything works as expected, follow these steps:

1. If you haven't already, start the Laravel development server:

   ```
   php artisan serve
   ```

1. Open your browser and navigate to `http://localhost:8000/apply/personal-info`.

1. Fill out the personal information form and submit it. You should be redirected to the education form.

1. Fill out the education form and submit it. You should be redirected to the work experience form.

1. Fill out the work experience form and submit it. You should be redirected to the review page.

1. On the review page, verify that all the information you entered is displayed correctly.

1. Submit the application and verify that you see a success message.

1. To check if the data was persisted correctly:

   - Open a database client (like pgAdmin for Postgres) and connect to your Neon database.
   - Check the `applicants`, `educations`, and `work_experiences` tables. You should see your submitted data.
   - Verify that the `applicant_id` in the `educations` and `work_experiences` tables matches the `id` in the `applicants` table for your submission.

1. Try refreshing the page or closing and reopening your browser, then navigate back to `http://localhost:8000/apply/review`. You should still see your submitted data, demonstrating that the data persists across sessions.

## Testing

Besides manual testing, you can also write automated tests to make sure your multi-step form works correctly. Laravel provides a testing suite that allows you to write unit, feature, and browser tests.

Create feature tests for your multi-step form to ensure each step works correctly. Here's an example for the personal info step:

```php
<?php

namespace Tests\Feature;

use Tests\TestCase;
use App\Models\Applicant;
use Livewire\Volt\Volt;
use Illuminate\Foundation\Testing\RefreshDatabase;

class PersonalInfoTest extends TestCase
{
    use RefreshDatabase;

    public function test_can_submit_personal_info()
    {
        Volt::test('personal-info-form')
            ->set('first_name', 'John')
            ->set('last_name', 'Doe')
            ->set('email', 'john@example.com')
            ->call('saveAndContinue')
            ->assertRedirect('/apply/education');

        $this->assertDatabaseHas('applicants', [
            'first_name' => 'John',
            'last_name' => 'Doe',
            'email' => 'john@example.com',
        ]);

        $this->assertNotNull(session('applicant_id'));
    }
}
```

This test checks if:

1. The form can be submitted with valid data.
1. The data is correctly stored in the database.
1. The `applicant_id` is stored in the session.
1. The user is redirected to the next step after submission.

You can create similar tests for the education and work experience steps.

To learn more about testing in Laravel, check out the [Testing Laravel Applications with Neon's Database Branching](/guides/laravel-test-on-branch) guide.

## Conclusion

In this guide, we've built a multi-step form using Laravel Volt, Folio, and Neon Postgres. We've covered form validation, data storage, and routing, demonstrating how these tools can be used together to create a dynamic and interactive form.

To further improve this project, consider adding features like:

- File uploads for resumes
- Email notifications to applicants
- An admin interface to review applications

One thing to keep in mind is always to validate and sanitize user inputs, optimize your database queries, and thoroughly test your application before deploying to production.

## Additional Resources

- [Laravel Documentation](https://laravel.com/docs)
- [Neon Documentation](/docs)
- [Neon Branching GitHub Actions Guide](/docs/guides/branching-github-actions)

<NeedHelp />


# Building a Real-Time Task Board with Laravel, Neon, and WebSockets

---
title: Building a Real-Time Task Board with Laravel, Neon, and WebSockets
subtitle: Learn how to create a collaborative task management system using Laravel, Neon Postgres, and WebSockets
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-08-17T00:00:00.000Z'
updatedOn: '2024-08-17T00:00:00.000Z'
---

Real-time features can significantly improve user experience in web applications. They allow users to see updates immediately without refreshing the page. In this guide, we'll demonstrate how to add real-time functionality to a Laravel application using Neon Postgres and WebSockets.

We'll build a collaborative task board where team members can create, update, and move tasks in real-time. By the end of this guide, you'll understand how to set up WebSockets in Laravel, store and retrieve data using Neon Postgres, and broadcast updates to connected clients instantly.

## Prerequisites

Before we begin, make sure you have the following:

- PHP 8.1 or higher installed on your system
- Composer for managing PHP dependencies
- A [Neon](https://console.neon.tech/signup) account and project

## Setting up the Laravel project

To get started we will need to create a new Laravel project and configuring it with Neon Postgres.

1. Create a new Laravel project:

   ```bash
   composer create-project laravel/laravel realtime-taskboard
   cd realtime-taskboard
   ```

   This will create a new Laravel project in a directory named `realtime-taskboard`. And using the `cd` command, we'll navigate to the project directory.

2. Configure the Neon database connection. Open your `.env` file and update the database settings:

   ```env
   DB_CONNECTION=pgsql
   DB_HOST=your-neon-hostname.neon.tech
   DB_PORT=5432
   DB_DATABASE=your_database_name
   DB_USERNAME=your_username
   DB_PASSWORD=your_password
   ```

   Replace the placeholders with your Neon database details which you can find in the Neon console.

3. Laravel provides a few starter kits for authentication. We'll use Laravel Breeze for this project to set up authentication:

   ```bash
   composer require laravel/breeze --dev
   php artisan breeze:install blade
   php artisan migrate
   npm install
   npm run dev
   ```

   This will install Laravel Breeze, set up authentication views using Blade, run the migrations to create the necessary tables. The `npm install` and `npm run dev` commands install the frontend dependencies and compile the assets.

Now that we've set up the Laravel project and connected it to Neon Postgres, let's create the task board.

## Creating the Task model and migration

Laravel uses models to interact with the database and migrations to create database tables. The task board will consist of tasks that users can create, update, and move between different statuses (e.g., 'To Do', 'In Progress', 'Done').

Let's create a model and migration for our tasks table.

1. Generate the model and migration:

   ```bash
   php artisan make:model Task -m
   ```

   This command will create a `Task` model and a migration file for the `tasks` table.

2. Update the migration file in `database/migrations` to create the `tasks` table with the necessary columns:

   ```php
   public function up()
   {
       Schema::create('tasks', function (Blueprint $table) {
           $table->id();
           $table->foreignId('user_id')->constrained()->onDelete('cascade');
           $table->string('title');
           $table->text('description')->nullable();
           $table->enum('status', ['todo', 'in_progress', 'done'])->default('todo');
           $table->timestamps();
       });
   }
   ```

   This migration creates a `tasks` table with columns for the task `title`, `description`, `status`, and the user who created the task.

3. Run the migration:

   ```bash
   php artisan migrate
   ```

   This will create the `tasks` table in your Neon Postgres database with the specified columns and constraints in the migration file.

4. Update the `Task` model in `app/Models/Task.php`:

   ```php
   <?php

   namespace App\Models;

   use Illuminate\Database\Eloquent\Factories\HasFactory;
   use Illuminate\Database\Eloquent\Model;

   class Task extends Model
   {
       use HasFactory;

       protected $fillable = ['title', 'description', 'status'];

       public function user()
       {
           return $this->belongsTo(User::class);
       }
   }
   ```

   This model defines the relationship between tasks and users. Each task belongs to a user thanks to the `user()` method defined in the model. The `fillable` property specifies which attributes can be mass-assigned. Laravel then makes it easy to create, update, and retrieve tasks using the `Task` model.

5. One more thing that we will have to do is to update the `User` model in `app/Models/User.php` to define the relationship between users and tasks:

   ```php
   public function tasks()
   {
       return $this->hasMany(Task::class);
   }
   ```

   This defines the relationship between users and tasks. Each user can have multiple tasks and can be retrieved using the `tasks()` method on the `User` model.

## Setting up WebSockets

Laravel provides built-in support for broadcasting events using WebSockets. We'll use WebSockets to broadcast task creation and updates in real-time to connected clients using Pusher.

Instead of using Pusher, there are other options like [Laravel WebSockets](https://beyondco.de/docs/laravel-websockets) by Beyond Code, which is a self-hosted WebSockets server for Laravel applications. However, for this guide, we'll use Pusher, as it takes care of the WebSockets infrastructure for us.

With Laravel 11 to install broadcasting, you can run the following command:

```bash
php artisan install:broadcasting
```

Follow the prompts to set up broadcasting and when asked for `reverb`, select "No" as we are going to use Pusher instead.

To set up Pusher, you need to do the following:

1. Sign up for a free account at [Pusher](https://pusher.com/).

2. After creating an account and a new app, update your `.env` file with your Pusher credentials:

   ```env
   PUSHER_APP_ID="your-pusher-app-id"
   PUSHER_APP_KEY="your-pusher-key"
   PUSHER_APP_SECRET="your-pusher-secret"
   PUSHER_HOST=
   PUSHER_PORT=443
   PUSHER_SCHEME="https"
   PUSHER_APP_CLUSTER="mt1"

   VITE_APP_NAME="${APP_NAME}"
   VITE_PUSHER_APP_KEY="${PUSHER_APP_KEY}"
   VITE_PUSHER_HOST="${PUSHER_HOST}"
   VITE_PUSHER_PORT="${PUSHER_PORT}"
   VITE_PUSHER_SCHEME="${PUSHER_SCHEME}"
   VITE_PUSHER_APP_CLUSTER="${PUSHER_APP_CLUSTER}"

   BROADCAST_DRIVER=pusher
   ```

   The `PUSHER_APP_ID`, `PUSHER_APP_KEY`, and `PUSHER_APP_SECRET` values can be found in your Pusher dashboard. The `PUSHER_APP_CLUSTER` value depends on the region where your app is hosted. For example, `mt1` is for the `mt1` region.

   The `VITE_` variables are used for client-side configuration in our JavaScript code using Vite. In our case, we'll use Vite to manage our frontend assets along with Laravel Echo and Pusher.

3. Install Laravel Echo and Pusher JS:

   ```bash
   npm install --save-dev laravel-echo pusher-js
   ```

   Laravel Echo is a JavaScript library that makes it easy to work with WebSockets and listen for events. Pusher JS is the JavaScript client library for Pusher that Laravel Echo uses to communicate with the Pusher service where our events are broadcasted to.

4. Open `resources/js/echo.js` and update it with your Pusher credentials:

   ```javascript
   import Echo from 'laravel-echo';

   import Pusher from 'pusher-js';
   window.Pusher = Pusher;

   window.Echo = new Echo({
     broadcaster: 'pusher',
     key: import.meta.env.VITE_PUSHER_APP_KEY,
     cluster: import.meta.env.VITE_PUSHER_APP_CLUSTER,
     forceTLS: true,
   });
   ```

   Make sure that you still have the `npm run dev` command running in the background to compile the assets and make the changes available in the browser.

## Creating the task board interface

Now that we've set up the database, models, and WebSockets, let's create the task board interface where users can view, create, and update tasks.

Create a new blade file at `resources/views/taskboard.blade.php` where we'll build the task board interface:

```html
<x-app-layout>
  <x-slot name="header">
    <h2 class="text-gray-800 text-xl font-semibold leading-tight">{{ __('Task Board') }}</h2>
  </x-slot>

  <div class="py-12">
    <div class="mx-auto max-w-7xl lg:px-8 sm:px-6">
      <div class="overflow-hidden bg-white shadow-sm sm:rounded-lg">
        <div class="border-gray-200 border-b bg-white p-6">
          <div class="grid grid-cols-1 gap-6 md:grid-cols-3">
            <div id="todo" class="bg-gray-100 rounded-lg p-4 shadow">
              <h3 class="text-blue-600 mb-4 text-lg font-semibold">To Do</h3>
              <div class="task-column space-y-4">
                <!-- Tasks will be inserted here -->
              </div>
            </div>
            <div id="in_progress" class="bg-gray-100 rounded-lg p-4 shadow">
              <h3 class="text-yellow-600 mb-4 text-lg font-semibold">In Progress</h3>
              <div class="task-column space-y-4">
                <!-- Tasks will be inserted here -->
              </div>
            </div>
            <div id="done" class="bg-gray-100 rounded-lg p-4 shadow">
              <h3 class="text-green-600 mb-4 text-lg font-semibold">Done</h3>
              <div class="task-column space-y-4">
                <!-- Tasks will be inserted here -->
              </div>
            </div>
          </div>

          <div class="mt-8">
            <h3 class="mb-4 text-lg font-bold">Add a New Task</h3>
            <form id="task-form" class="space-y-4">
              @csrf
              <div>
                <label for="title" class="text-gray-700 block text-sm font-medium"
                  >Task Title</label
                >
                <input
                  type="text"
                  name="title"
                  id="title"
                  placeholder="Enter task title"
                  class="border-gray-300 focus:ring-blue-500 focus:border-blue-500 mt-1 block w-full rounded-md border p-2 shadow-sm"
                  required
                />
              </div>
              <div>
                <label for="description" class="text-gray-700 block text-sm font-medium"
                  >Task Description</label
                >
                <textarea
                  name="description"
                  id="description"
                  placeholder="Enter task description"
                  class="border-gray-300 focus:ring-blue-500 focus:border-blue-500 mt-1 block w-full rounded-md border p-2 shadow-sm"
                ></textarea>
              </div>
              <input type="hidden" name="status" value="todo" />
              <div>
                <button
                  type="submit"
                  class="bg-blue-500 hover:bg-blue-600 focus:ring-blue-500 w-full rounded-md px-4 py-2 text-white shadow focus:outline-none focus:ring-2 focus:ring-offset-2"
                >
                  Add Task
                </button>
              </div>
            </form>
          </div>
        </div>
      </div>
    </div>
  </div>

  <script>
    document.addEventListener('DOMContentLoaded', function () {
      const taskForm = document.getElementById('task-form');
      const columns = ['todo', 'in_progress', 'done'];

      function addTaskToColumn(task) {
        const column = document.querySelector(`#${task.status} .task-column`);
        const taskElement = document.createElement('div');
        taskElement.id = `task-${task.id}`;
        taskElement.className = 'bg-white p-4 rounded-lg shadow';
        taskElement.innerHTML = `
                    <h4 class="font-semibold text-lg mb-2">${task.title}</h4>
                    <p class="text-gray-600 mb-4">${task.description || 'No description'}</p>
                    <select class="task-status w-full p-2 border border-gray-300 rounded-md" data-task-id="${task.id}">
                        ${columns.map((status) => `<option value="${status}" ${status === task.status ? 'selected' : ''}>${status.replace('_', ' ')}</option>`).join('')}
                    </select>
                `;
        column.appendChild(taskElement);
      }

      function loadTasks() {
        axios
          .get('/tasks')
          .then((response) => {
            response.data.forEach(addTaskToColumn);
          })
          .catch((error) => {
            console.error('Error loading tasks:', error);
          });
      }

      taskForm.addEventListener('submit', function (e) {
        e.preventDefault();
        const formData = new FormData(this);
        axios
          .post('/tasks', Object.fromEntries(formData))
          .then((response) => {
            this.reset();
            addTaskToColumn(response.data);
          })
          .catch((error) => {
            console.error('Error creating task:', error);
          });
      });

      document.addEventListener('change', function (e) {
        if (e.target.classList.contains('task-status')) {
          const taskId = e.target.dataset.taskId;
          const newStatus = e.target.value;
          axios
            .put(`/tasks/${taskId}`, { status: newStatus })
            .then((response) => {
              const taskElement = document.getElementById(`task-${taskId}`);
              if (taskElement) {
                taskElement.remove();
              }
              addTaskToColumn(response.data);
            })
            .catch((error) => {
              console.error('Error updating task:', error);
            });
        }
      });

      Echo.channel('taskboard')
        .listen('TaskCreated', (e) => {
          addTaskToColumn(e.task);
        })
        .listen('TaskUpdated', (e) => {
          const taskElement = document.getElementById(`task-${e.task.id}`);
          if (taskElement) {
            taskElement.remove();
          }
          addTaskToColumn(e.task);
        });

      loadTasks();
    });
  </script>
</x-app-layout>
```

The majority of the code is HTML and JavaScript that creates the task board interface and handles task creation and updates. The real-time functionality of our task board is powered by WebSockets, implemented using Laravel Echo and Pusher. Here's a breakdown of how it works:

1. **Channel Setup**: We create a channel named 'taskboard' for our real-time communications:

   ```javascript
   Echo.channel('taskboard');
   ```

   This channel will be used for broadcasting and listening to task-related events.

2. **Event Listening**: We set up listeners for two types of events:

   ```javascript
   .listen('TaskCreated', (e) => {
       addTaskToColumn(e.task);
   })
   .listen('TaskUpdated', (e) => {
       const taskElement = document.getElementById(`task-${e.task.id}`);
       if (taskElement) {
           taskElement.remove();
       }
       addTaskToColumn(e.task);
   });
   ```

   - `TaskCreated`: When a new task is created, we add it to the appropriate column.
   - `TaskUpdated`: When a task is updated, we remove the old task element and add the updated one.

   For more information on how events are broadcasted and listened to, check out the [Getting Started with Laravel Events and Listeners](/guides/laravel-events-and-listeners) guide.

3. **Server-Side Broadcasting**: For the above to work, in our Laravel controllers, we will broadcast these events after creating or updating a task:

   ```php
   broadcast(new TaskCreated($task))->toOthers();
   broadcast(new TaskUpdated($task))->toOthers();
   ```

   The `toOthers()` method ensures that the event is not sent back to the user who initiated the action.

4. **Real-Time Updates**: When these events are received, the task board updates instantly for all connected users, providing a collaborative, real-time experience.

This WebSockets implementation allows for immediate synchronization across all clients without the need for polling or page refreshes, creating a smooth and responsive user experience.

## Handling tasks

As mentioned earlier, we'll use Laravel controllers to handle task creation and updates and broadcast events to connected clients.

1. Create a controller for tasks:

   ```bash
   php artisan make:controller TaskController
   ```

2. Update `app/Http/Controllers/TaskController.php`:

   ```php
   <?php

   namespace App\Http\Controllers;

   use App\Models\Task;
   use App\Events\TaskCreated;
   use App\Events\TaskUpdated;
   use Illuminate\Http\Request;

   class TaskController extends Controller
   {
       public function index()
       {
           return Task::all();
       }

       public function store(Request $request)
       {
           $validatedData = $request->validate([
               'title' => 'required|string|max:255',
               'description' => 'nullable|string',
           ]);

           $task = $request->user()->tasks()->create($validatedData);
           broadcast(new TaskCreated($task))->toOthers();
           return $task;
       }

       public function update(Request $request, Task $task)
       {
           $validatedData = $request->validate([
               'status' => 'required|in:todo,in_progress,done',
           ]);

           $task->update($validatedData);
           broadcast(new TaskUpdated($task))->toOthers();
           return $task;
       }
   }
   ```

   These methods handle task creation, retrieval, and updates. When a task is created or updated, the corresponding event is broadcast to all connected clients using the `TaskCreated` or `TaskUpdated` event and the `broadcast` method.

3. Create events for task creation and updates:

   ```bash
   php artisan make:event TaskCreated
   php artisan make:event TaskUpdated
   ```

   These commands will create two event classes in the `app/Events` directory. We'll update these classes to broadcast the task data to the 'taskboard' channel.

4. Update `app/Events/TaskCreated.php`:

   ```php
   <?php

   namespace App\Events;

   use App\Models\Task;
   use Illuminate\Broadcasting\Channel;
   use Illuminate\Broadcasting\InteractsWithSockets;
   use Illuminate\Contracts\Broadcasting\ShouldBroadcast;
   use Illuminate\Foundation\Events\Dispatchable;
   use Illuminate\Queue\SerializesModels;

   class TaskCreated implements ShouldBroadcast
   {
       use Dispatchable, InteractsWithSockets, SerializesModels;

       public $task;

       public function __construct(Task $task)
       {
           $this->task = $task;
       }

       public function broadcastOn()
       {
           return new Channel('taskboard');
       }
   }
   ```

   This class defines the `TaskCreated` event, which broadcasts the newly created task to the 'taskboard' channel, allowing all connected clients to receive the update.

   If you need to broadcast the event to a specific user, you can use the `private` channel instead of the `public` channel.

5. Update `app/Events/TaskUpdated.php` similarly, just change the class name to `TaskUpdated`:

   ```php
   <?php

   namespace App\Events;

   use App\Models\Task;
   use Illuminate\Broadcasting\Channel;
   use Illuminate\Broadcasting\InteractsWithSockets;
   use Illuminate\Contracts\Broadcasting\ShouldBroadcast;
   use Illuminate\Foundation\Events\Dispatchable;
   use Illuminate\Queue\SerializesModels;

   class TaskUpdated implements ShouldBroadcast
   {
       use Dispatchable, InteractsWithSockets, SerializesModels;

       public $task;

       public function __construct(Task $task)
       {
           $this->task = $task;
       }

       public function broadcastOn()
       {
           return new Channel('taskboard');
       }
   }
   ```

6. Update `routes/web.php` to add routes for tasks:

   ```php
   use App\Http\Controllers\TaskController;

   Route::middleware(['auth'])->group(function () {
       Route::get('/taskboard', function () {
           return view('taskboard');
       })->name('taskboard');

       Route::get('/tasks', [TaskController::class, 'index']);
       Route::post('/tasks', [TaskController::class, 'store']);
       Route::put('/tasks/{task}', [TaskController::class, 'update']);
   });
   ```

   These routes allow users to view the task board, retrieve tasks, create new tasks, and update existing tasks.

   For more information on Laravel routing, check out the [Laravel's Routes, Middleware, and Validation guide](/guides/laravel-routes-middleware-validation).

## Testing the real-time task board

Now that everything is set up, let's test our real-time task board.

1. Start your Laravel development server:

   ```bash
   php artisan serve
   ```

   This will start the Laravel development server on `http://localhost:8000`. If you already have a server running, you can skip this step.

2. In another terminal, start the Laravel queue worker to process the broadcast events:

   ```bash
   php artisan queue:work
   ```

   This will ensure that the broadcast events are processed and sent to connected clients.

   To learn more about Laravel queues, check out the [Implementing Queue Workers and Job Processing in Laravel with Neon Postgres](/guides/laravel-queue-workers-job-processing) guide.

3. Open two different browsers and visit `http://localhost:8000/taskboard`.

4. Log in with two different user accounts.

5. Start creating and moving tasks in one browser. You should see the tasks appear and move in real-time in the other browser.

6. If you were to visit your Pusher dashboard, you should see the events being broadcasted.

## How it works

Here's how the whole process of the real-time updates work:

1. When a user creates or updates a task, it's sent to the server.

2. The server saves the task in the Neon Postgres database.

3. After saving the task, the server broadcasts a `TaskCreated` or `TaskUpdated` event using Pusher.

4. Pusher sends this event to all connected users except the sender using the 'taskboard' channel.

5. The JavaScript code listening for these events receives the new or updated task and adds or moves it on the task board.

This process happens very quickly, giving the appearance of real-time updates.

## Optimizing for larger applications

As your task board grows, you might need to optimize it for better performance:

1. **Pagination**: Instead of loading all tasks at once, implement pagination to load tasks in smaller batches. Currently, we are loading all tasks using `Task::all()`, which can be inefficient for large datasets.

2. **Caching**: Use Laravel's caching features to cache frequently accessed data, reducing database queries.

3. **Database Indexing**: Add indexes to frequently queried columns in your Neon Postgres database to speed up queries. For more information, check out the Neon Postgres documentation on [Indexes](/docs/postgres/indexes).

4. **Queue Workers**: Use multiple queue workers to process broadcast events concurrently, especially in high-traffic applications. Also, consider using Laravel Horizon for monitoring and managing your queue workers.

5. **Private Channels**: If you need to broadcast events to specific users or groups, use private channels to ensure data privacy. This is useful for applications with user-specific data or private conversations between users where data should not be shared with others.

## Conclusion

In this guide, we've built a simple real-time collaborative task board using Laravel, Neon Postgres, and WebSockets. This example shows how you can create interactive, real-time web applications that update instantly across multiple users using Laravel's broadcasting feature.

## Additional Resources

- [Laravel Broadcasting Documentation](https://laravel.com/docs/broadcasting)
- [Pusher Documentation](https://pusher.com/docs)
- [Neon Documentation](/docs)

<NeedHelp />


# Building a Todo CLI App with Laravel Zero and Neon Postgres

---
title: Building a Todo CLI App with Laravel Zero and Neon Postgres
subtitle: Learn how to create a command-line interface (CLI) application using Laravel Zero and Neon Postgres for efficient task management.
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-07-01T00:00:00.000Z'
updatedOn: '2024-07-01T00:00:00.000Z'
---

[Laravel Zero](https://laravel-zero.com/) is a micro-framework that provides a starting point for your console application.

Combined with Neon's serverless Postgres database, you can create powerful CLI tools with persistent storage.

In this guide, we'll build a Todo CLI app that allows users to manage their tasks efficiently from the command line.

By the end of this tutorial, you'll have a fully functional Todo CLI app where users can add, list, update, and delete tasks. We'll also implement task prioritization and due dates to enhance the app's functionality.

## Prerequisites

Before we start, make sure you have the following:

- PHP 8.1 or higher installed on your system
- [Composer](https://getcomposer.org/) for managing PHP dependencies
- A [Neon](https://console.neon.tech/signup) account for database hosting
- Basic knowledge of PHP and Laravel

## Setting up the Project

Let's start by creating a new Laravel Zero project and setting up the necessary components.

### Creating a New Laravel Zero Project

Open your terminal and run the following command to create a new Laravel Zero project:

```bash
composer create-project laravel-zero/laravel-zero todo-cli
cd todo-cli
```

This command creates a new Laravel Zero project in a directory named `todo-cli` and installs all the necessary dependencies.

Once the project is created, you can run the following command to rename the default `app` namespace to `todo`:

```bash
php application app:rename todo
```

To test the application, run:

```bash
php todo inspire
```

This command should display an inspirational quote from Laravel as a confirmation that the application is set up correctly.

### Installing Required Add-Ons

Out of the box Laravel Zero provides a basic structure for CLI applications. Laravel Zero offers a variety of add-ons to extend the functionality of your CLI app which includes database support, testing, logging, file system, scheduler, and more.

To install the database package, run:

```bash
php todo app:install database
```

> The `todo` command is the name of the executable file we defined in the previous step using the `app:rename` command.

Additionally, we can install the `fakerphp/faker` package to generate sample data for testing later on:

```bash
composer require fakerphp/faker --dev
```

### Configuring the Database

The `config/database.php` file will include the database configuration for your application looking like this:

```php
'connections' => [
    // ... other connections ...

    'neon' => [
        'driver' => 'pgsql',
        'url' => env('DATABASE_URL'),
        'host' => env('DB_HOST', '127.0.0.1'),
        'port' => env('DB_PORT', '5432'),
        'database' => env('DB_DATABASE', 'forge'),
        'username' => env('DB_USERNAME', 'forge'),
        'password' => env('DB_PASSWORD', ''),
        'charset' => 'utf8',
        'prefix' => '',
        'prefix_indexes' => true,
        'schema' => 'public',
        'sslmode' => 'prefer',
    ],
],
```

Rather than hardcoding the database credentials in the configuration file, we can use environment variables to store sensitive information securely. Create a `.env` file in the root of your project and add your Neon database credentials:

```env
DB_CONNECTION=neon
DATABASE_URL=postgres://your-username:your-password@your-neon-hostname/your-database
DB_HOST=your-neon-hostname
DB_PORT=5432
DB_DATABASE=your-database
DB_USERNAME=your-username
DB_PASSWORD=your-password
```

Make sure to replace the placeholders with your actual Neon database details.

## Creating the Todo App Structure

Now that we have our basic setup, let's create the structure for our Todo CLI app, including models, migrations, and commands.

### Creating the Task Model and Migration

Just like a standard Laravel application, we'll create a `Task` model to represent the tasks in our Todo app.

```bash
php todo make:model Task -m
```

Update the migration file in `database/migrations` to define the structure of our tasks table:

```php
public function up()
{
    Schema::create('tasks', function (Blueprint $table) {
        $table->id();
        $table->string('title');
        $table->text('description')->nullable();
        $table->enum('status', ['pending', 'in_progress', 'completed'])->default('pending');
        $table->enum('priority', ['low', 'medium', 'high'])->default('medium');
        $table->date('due_date')->nullable();
        $table->timestamps();
    });
}
```

This migration creates a `tasks` table with columns for the task title, description, status, priority, and due date.

Now, update the `app/Task.php` model to define the fillable attributes:

```php
<?php

namespace App;

use Illuminate\Database\Eloquent\Model;

class Task extends Model
{
    protected $fillable = ['title', 'description', 'status', 'priority', 'due_date'];

    protected $casts = [
        'due_date' => 'date',
    ];
}
```

Run the migration to create the tasks table in your Neon database:

```bash
php todo migrate
```

This command will create the `tasks` table in your Neon database.

### Creating Commands

Laravel Zero uses commands to define the CLI functionality. Let's create commands for adding, listing, updating, and deleting tasks.

#### Add Task Command

Create a new command to add tasks:

```bash
php todo make:command AddTaskCommand
```

Update the `app/Commands/AddTaskCommand.php` the file to define the command signature and functionality:

```php
<?php

namespace App\Commands;

use App\Task;
use Carbon\Carbon;
use LaravelZero\Framework\Commands\Command;

class AddTaskCommand extends Command
{
    protected $signature = 'task:add {title} {--description=} {--priority=medium} {--due-date=}';
    protected $description = 'Add a new task';

    public function handle()
    {
        $task = Task::create([
            'title' => $this->argument('title'),
            'description' => $this->option('description'),
            'priority' => $this->option('priority'),
            'due_date' => $this->option('due-date') ? Carbon::parse($this->option('due-date')) : null,
        ]);

        $this->info("Task added successfully! ID: {$task->id}");
    }
}
```

This command allows users to add a new task with a title, description, priority, and due date.

Rundown of the command signature:

- `{title}`: The title of the task (required argument). Required arguments are passed without the `--` prefix.
- `--description`: The description of the task (optional option)
- `--priority`: The priority of the task (optional option with a default value of `medium`)
- `--due-date`: The due date of the task (optional option)

The `handle` method creates a new task record in the database with the provided details and displays a success message with the ID of the newly created task.

To verify that the command works, run:

```bash
php todo task:add "Complete Laravel Zero guide" --description="Write a guide on creating a Todo CLI app" --priority=high --due-date=2024-07-15
```

You should see a success message with the ID of the newly created task.

#### List Tasks Command

Next, let's create a command to list tasks so we can view all tasks or filter them by status or priority:

```bash
php todo make:command ListTasksCommand
```

Update the `app/Commands/ListTasksCommand.php` file:

```php
<?php

namespace App\Commands;

use App\Task;
use LaravelZero\Framework\Commands\Command;

class ListTasksCommand extends Command
{
    protected $signature = 'task:list {--status=} {--priority=}';
    protected $description = 'List all tasks';

    public function handle()
    {
        $query = Task::query();

        if ($this->option('status')) {
            $query->where('status', $this->option('status'));
        }

        if ($this->option('priority')) {
            $query->where('priority', $this->option('priority'));
        }

        $tasks = $query->get();

        $headers = ['ID', 'Title', 'Status', 'Priority', 'Due Date'];
        $rows = $tasks->map(function ($task) {
            return [
                $task->id,
                $task->title,
                $task->status,
                $task->priority,
                $task->due_date ? $task->due_date->format('Y-m-d') : 'N/A',
            ];
        });

        $this->table($headers, $rows);
    }
}
```

Rundown of the command signature:

- `--status`: Filter tasks by status (optional option)
- `--priority`: Filter tasks by priority (optional option)

The `handle` method retrieves tasks from the database based on the provided filters (if any) and displays them in a table format.

Now you can list all tasks or filter them by status or priority:

```bash
php todo task:list
```

The result will be displayed in a table format with columns for ID, Title, Status, Priority, and Due Date:

```sql
+----+-----------------------------+---------+----------+------------+
| ID | Title                       | Status  | Priority | Due Date   |
+----+-----------------------------+---------+----------+------------+
| 1  | Complete Laravel Zero guide | pending | high     | 2024-07-15 |
+----+-----------------------------+---------+----------+------------+
```

To filter tasks by status or priority, you can use the `--status` and `--priority` options:

```bash
php todo task:list --status=pending
```

#### Update Task Command

Now that we have the ability to add and list tasks, let's create a command to update tasks:

```bash
php todo make:command UpdateTaskCommand
```

Update the `app/Commands/UpdateTaskCommand.php` file and define the command signature and functionality:

```php
<?php

namespace App\Commands;

use App\Task;
use Carbon\Carbon;
use LaravelZero\Framework\Commands\Command;

class UpdateTaskCommand extends Command
{
    protected $signature = 'task:update {id} {--title=} {--description=} {--status=} {--priority=} {--due-date=}';
    protected $description = 'Update an existing task';

    public function handle()
    {
        $task = Task::find($this->argument('id'));

        if (!$task) {
            $this->error("Task not found!");
            return;
        }

        $updates = [];

        if ($this->option('title')) {
            $updates['title'] = $this->option('title');
        }

        if ($this->option('description')) {
            $updates['description'] = $this->option('description');
        }

        if ($this->option('status')) {
            $updates['status'] = $this->option('status');
        }

        if ($this->option('priority')) {
            $updates['priority'] = $this->option('priority');
        }

        if ($this->option('due-date')) {
            $updates['due_date'] = Carbon::parse($this->option('due-date'));
        }

        $task->update($updates);

        $this->info("Task updated successfully!");
    }
}
```

Rundown of the command signature:

- `{id}`: The ID of the task to update (required argument)
- `--title`: The new title of the task (optional option)
- `--description`: The new description of the task (optional option)
- `--status`: The new status of the task (optional option)
- `--priority`: The new priority of the task (optional option)
- `--due-date`: The new due date of the task (optional option)

The `handle` method retrieves the task by ID, checks if it exists, and updates the task with the provided details. It displays a success message if the task is updated successfully.

To update a task, run:

```bash
php todo task:update 1 --status=in_progress
```

You will get a `Task updated successfully!` message if the task is updated successfully.

To verify the update, list the tasks again:

```bash
php todo task:list
```

Now you should see the updated status of the task:

```sql
+----+-----------------------------+-------------+----------+------------+
| ID | Title                       | Status      | Priority | Due Date   |
+----+-----------------------------+-------------+----------+------------+
| 1  | Complete Laravel Zero guide | in_progress | high     | 2024-07-15 |
+----+-----------------------------+-------------+----------+------------+
```

Try to also update other fields like the title, description, priority, or due date.

#### Delete Task Command

Finally to complete the basic CRUD operations, let's create a command to delete tasks:

```bash
php todo make:command DeleteTaskCommand
```

Update the `app/Commands/DeleteTaskCommand.php` file:

```php
<?php

namespace App\Commands;

use App\Task;
use LaravelZero\Framework\Commands\Command;

class DeleteTaskCommand extends Command
{
    protected $signature = 'task:delete {id}';
    protected $description = 'Delete a task';

    public function handle()
    {
        $task = Task::find($this->argument('id'));

        if (!$task) {
            $this->error("Task not found!");
            return;
        }

        $task->delete();

        $this->info("Task deleted successfully!");
    }
}
```

Here the `id` argument is required to identify the task to be deleted.

The `handle` method retrieves the task by ID, checks if it exists, deletes the task, and displays a success message if the task is deleted successfully.

To delete a task, run:

```bash
php todo task:delete 1
```

You should see a `Task deleted successfully!` message if the task is deleted successfully.

You can also try deleting a task that doesn't exist to see the error message:

```bash
php todo task:delete 2
```

This will display a `Task not found!` error message.

## Implementing Additional Features

To enhance our Todo CLI app, let's implement some additional features like task prioritization and due date reminders.

### Task Prioritization

We've already included a priority field in our tasks table. Let's update the `ListTasksCommand` to sort tasks by priority:

Update the `handle` method in `app/Commands/ListTasksCommand.php`:

```php {14-19}
public function handle()
{
    $query = Task::query();

    if ($this->option('status')) {
        $query->where('status', $this->option('status'));
    }

    if ($this->option('priority')) {
        $query->where('priority', $this->option('priority'));
    }

    $tasks = $query->get(); // [!code --]
    $tasks = $query->orderByRaw("CASE
        WHEN priority = 'high' THEN 1
        WHEN priority = 'medium' THEN 2
        WHEN priority = 'low' THEN 3
        ELSE 4
    END")->get();

    // ... rest of the method
}
```

This modification sorts the tasks by priority, with high priority tasks appearing first.

### Due Date Reminders

Let's create a new command to show tasks that are due soon:

```bash
php todo make:command DueTasksCommand
```

Update the `app/Commands/DueTasksCommand.php` to include the due date reminder functionality:

```php
<?php

namespace App\Commands;

use App\Task;
use Carbon\Carbon;
use LaravelZero\Framework\Commands\Command;

class DueTasksCommand extends Command
{
    protected $signature = 'task:due {days=7}';
    protected $description = 'Show tasks due within the specified number of days';

    public function handle()
    {
        $days = (int) $this->argument('days');
        $dueDate = Carbon::now()->addDays($days);

        $tasks = Task::where('status', '!=', 'completed')
            ->whereDate('due_date', '<=', $dueDate)
            ->orderBy('due_date')
            ->get();

        if ($tasks->isEmpty()) {
            $this->info("No tasks due within the next {$days} days.");
            return;
        }

        $headers = ['ID', 'Title', 'Priority', 'Due Date', 'Days Left'];
        $rows = $tasks->map(function ($task) {
            $daysLeft = Carbon::now()->diffInDays($task->due_date, false);
            return [
                $task->id,
                $task->title,
                $task->priority,
                $task->due_date->format('Y-m-d'),
                $daysLeft,
            ];
        });

        $this->table($headers, $rows);
    }
}
```

This command shows tasks that are due within a specified number of days (default is 7) and aren't completed yet.

The `handle` method calculates the due date based on the provided number of days, retrieves tasks that are due within that period, and displays them in a table format with columns for ID, Title, Priority, Due Date, and Days Left.

To show tasks due within the next 14 days, run:

```bash
php todo task:due 14
```

## Testing the Todo CLI App

Now that we have implemented our Todo CLI app, let's try the complete workflow to ensure everything works as expected.

1. Add a task:

```bash
php todo task:add "Complete Laravel Zero guide" --description="Write a guide on creating a Todo CLI app" --priority=high --due-date=2024-07-15
```

2. List all tasks:

```bash
php todo task:list
```

3. Update a task:

```bash
php todo task:update 1 --status=in_progress
```

4. Show due tasks:

```bash
php todo task:due 14
```

5. Delete a task:

```bash
php todo task:delete 1
```

## Building the Application

Rather than having to run the `php todo` command each time, Laravel Zero allows you to build your application into a single executable file. To do this, run:

```bash
php todo app:build
```

This command will generate a standalone executable in the `builds` directory, which you can distribute and run on other systems without requiring PHP or Composer to be installed.

You will be prompted to choose the version of the application you want to build. Select the version that best suits your needs.

To run the built application, use the following command:

```bash
./builds/todo
```

To handle your database environment variables, you can create a `.env` file in the same directory as the built executable or set the environment variables directly in your system.

## Conclusion

In this tutorial, we've built a fully functional Todo CLI app using Laravel Zero and Neon Postgres. We've implemented features such as adding, listing, updating, and deleting tasks, as well as task prioritization and due date reminders.

This implementation provides a solid foundation for a CLI-based task management system, but there are always ways to improve and expand its functionality:

- Implement task categories or tags
- Add support for recurring tasks
- Implement data export and import functionality
- Add user authentication for multi-user support
- Implement task dependencies (subtasks)

By combining the power of Laravel Zero and the scalability of Neon Postgres, you can quickly create efficient and powerful CLI applications that meet your specific needs.

## Additional Resources

- [Laravel Zero Documentation](https://laravel-zero.com/docs/introduction)
- [Illuminate Database Documentation](https://laravel.com/docs/database)
- [Carbon Documentation](https://carbon.nesbot.com/docs/)
- [Neon Documentation](/docs)


# Using LlamaIndex with Postgres to Build your own Reverse Image Search Engine

---
title: Using LlamaIndex with Postgres to Build your own Reverse Image Search Engine
subtitle: A step-by-step guide to build your own Reverse Image Search engine in an Astro application with LlamaIndex and Postgres
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-06-11T00:00:00.000Z'
updatedOn: '2024-06-11T00:00:00.000Z'
---

Have you ever searched for an image using an... image? [Google Images](https://images.google.com/) is a widely used example of such reverse image search engine. Do you wonder how it's able to show highly similar images in the search results? Well, in this guide you will learn how to create such an image engine on your own. You will learn how to create a system that's able to index images into a collection, and return images that are highly similar to the uploaded one.

## Prerequisites

To follow along this guide, you will need the following:

- [Node.js 18](https://nodejs.org/en) or later
- A [Neon](https://console.neon.tech/signup) account
- A [Vercel](https://vercel.com) account

## Steps

- [Provisioning a Serverless Postgres](#provisioning-a-serverless-postgres)
- [Create a new Astro application](#create-a-new-astro-application)
- [Enabling Server Side Rendering in Astro with Vercel](#enabling-server-side-rendering-in-astro-with-vercel)
- [Setting up a Postgres Database Connection](#setting-up-a-postgres-database-connection)
  - [Initialize Postgres Vector Store in LlamaIndex](#initialize-postgres-vector-store-in-llamaindex)
- [Build the Image Indexing API Endpoint](#build-the-image-indexing-api-endpoint)
- [Build the Reverse Image Search API Endpoint](#build-the-reverse-image-search-api-endpoint)
- [Build Reverse Image Search User Interface](#build-reverse-image-search-user-interface)
- [Deploy to Vercel](#deploy-to-vercel)

## Provisioning a Serverless Postgres

Using Serverless Postgres database helps you scale down to zero. With Neon, you only have to pay for what you use.

To get started, go to the [Neon console](https://console.neon.tech/app/projects) and enter the name of your choice as the project name.

You will then be presented with a dialog that provides a connecting string of your database. Click on **Pooled connection** on the top right of the dialog and the connecting string automatically updates in the box below it.

![](/guides/images/llamaindex-postgres-search-images/create-database.png)

All Neon connection strings have the following format:

```bash
postgres://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>?sslmode=require
```

- `user` is the database user.
- `password` is the database user’s password.
- `endpoint_hostname` is the host with neon.tech as the [TLD](https://www.cloudflare.com/en-gb/learning/dns/top-level-domain/).
- `port` is the Neon port number. The default port number is 5432.
- `dbname` is the name of the database. “neondb” is the default database created with each Neon project.
- `?sslmode=require` an optional query parameter that enforces the [SSL](https://www.cloudflare.com/en-gb/learning/ssl/what-is-ssl/) mode while connecting to the Postgres instance for better security.

Save this connecting string somewhere safe to be used as the `POSTGRES_URL` further in the guide. Proceed further in this guide to create a Astro application.

## Create a new Astro application

Let’s get started by creating a new Astro project. Open your terminal and run the following command:

```bash
npm create astro@latest my-app
```

`npm create astro` is the recommended way to scaffold an Astro project quickly.

When prompted, choose:

- `Empty` when prompted on how to start the new project.
- `Yes` when prompted if plan to write Typescript.
- `Strict` when prompted how strict Typescript should be.
- `Yes` when prompted to install dependencies.
- `Yes` when prompted to initialize a git repository.

Once that’s done, you can move into the project directory and start the app:

```bash
cd my-app
npm run dev
```

The app should be running on [localhost:4321](http://localhost:4321/). Let's close the development server for now.

Next, execute the command in your terminal window below to install the necessary libraries and packages for building the application:

```bash
npm install dotenv llamaindex@0.3.4 uuid
```

The above command installs the following packages:

- [dotenv](https://npmjs.com/package/dotenv): A library for handling environment variables.
- [llamaindex](https://npmjs.com/package/llamaindex): A data framework for creating LLM applications.
- [uuid](https://npmjs.com/package/uuid): A library to generate RFC-compliant UUIDs in JavaScript.

Then, make the following additions in your `tsconfig.json` file to make relative imports within the project easier:

```json
{
  "extends": "astro/tsconfigs/base",
  "compilerOptions": {
    // [!code ++]
    "baseUrl": ".", // [!code ++]
    "paths": {
      // [!code ++]
      "@/*": ["src/*"] // [!code ++]
    } // [!code ++]
  } // [!code ++]
}
```

To complete the initial set up, let's move on to enabling server-side rendering in the Astro application.

## Enabling Server Side Rendering in Astro with Vercel

To index and search from images, you are going to require server-side operations to be executed. Enable server side rendering in your Astro project by executing the following command:

```bash
npx astro add vercel
```

When prompted, choose:

- `Yes` when prompted to install the Vercel dependencies.
- `Yes` when prompted to make changes to Astro configuration file.

The command above installed the following dependency:

- `@astrojs/vercel`: The adapter that allows you to server-side render your Astro application on Vercel.

With this, your Astro application is all set to run in the development mode and deploy to Vercel without changes. Now, let' configure connection to Postgres to index and query from a given set of images.

## Setting up a Postgres Database Connection

First, create an `.env` file in the root directory of your project with the following environment variable to initiate the setup of a database connection:

```bash
# Neon Postgres Pooled Connection URL

POSTGRES_URL="postgres://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>?sslmode=require"
```

The file, `.env` should be kept secret and not included in Git history. Ensure that `.env` is added to the `.gitignore` file in your project.

Now, let's move on to using Postgres as the vector store to power your image search application with reverse image search capabilities.

### Initialize Postgres Vector Store in LlamaIndex

To index and query images (via their vector embeddings), you will use the Postgres-compatible `PGVectorStore` class by llamaindex. It enables you to write minimal code by extracting image features and creating vector embeddings under the hood. Inside `src` directory, create a `neon.ts` file with the following code:

```tsx
// File: src/neon.ts

import 'dotenv/config';
import { PGVectorStore } from 'llamaindex';

// Create and export a new instance of PGVectorStore
// This instance represents the vector store using PostgreSQL as the backend
export default new PGVectorStore({
  connectionString: process.env.POSTGRES_URL,
  dimensions: 512,
});
```

The code above begins with importing the `dotenv/config`, loading all the environment variables into the scope. Further, it exports an instance of `PGVectorStore` initialized using the Postgres Pooled Connection URL obtained earlier.

Now, let's move on to writing an API endpoint in the Astro application with which you can index the given set of image URLs in your Postgres database.

## Build the Image Indexing API Endpoint

LlamaIndex in combination with ClipEmbedding, internally retrieves the remote images, extracts their features using the CLIP model, and generates their vector embeddings. This process involves analyzing various visual and semantic aspects of the image to create a numerical representation that captures its essence. By handling feature extraction and embedding generation, llamaindex enables you to focus on building the rest of your application.

To index images via an API endpoint, create a file `src/pages/api/upsert.ts` with the following code:

```tsx
// File: src/pages/api/upsert.ts

import { v4 as uuidv4 } from 'uuid';
import imageVectorStore from '@/neon';
import type { APIContext } from 'astro';
import { ClipEmbedding, ImageDocument, Settings, VectorStoreIndex } from 'llamaindex';

// Set the embedding model to Clip for image embeddings
Settings.embedModel = new ClipEmbedding();

export async function POST({ request }: APIContext) {
  // Parse the JSON body of the request to get the list of image URLs
  const { images = [] }: { images: string[] } = await request.json();
  // Convert image URLs into ImageDocument objects
  const documents = images.map(
    (imageURL: string) =>
      new ImageDocument({
        // Generate a unique ID for each image document
        id_: uuidv4(),
        // Convert imageURL to a URL object
        image: new URL(imageURL),
        // Attach metadata with the image URL
        metadata: { url: imageURL },
      })
  );
  // Index the ImageDocument objects in the vector store
  await VectorStoreIndex.fromDocuments(documents, { imageVectorStore });
}
```

The code above begins with importing modules including `uuid`, `llamaindex`, and `imageVectorStore` (an alias for the instance of the `PGVectorStore` instance created earlier). Further, the code **sets the embedding model to Clip for image embeddings**. This takes care of extracting features from images, and creating their vector embeddings.

In the `POST` function, it handles incoming requests, expecting a JSON body with an array of image URLs. It converts each URL into an `ImageDocument` object, generating a unique ID for each, and attaching metadata containing the original URL.

The `ImageDocument` objects are then indexed in the vector store using `VectorStoreIndex.fromDocuments()`, which takes the documents array and options object as parameters. **The `imageVectorStore` is specified as the target store for indexing**. The entire process allows for efficient storage (and retrieval) of image embeddings in your application.

With indexing complete, let's move on to building the reverse image search API endpoint.

## Build the Reverse Image Search API Endpoint

First, let's walk through the process of reverse image search. A user would upload an image and then you'd need to find similar images. Greater the similarity, higher the priority in the image search results. The common ground for computing similarities between images is a numerical representation of their visual and semantic features. The computation of these features is done via CLIP model (by OpenAI) internally in the LlamaIndex library. LlamaIndex, upon querying would return a set of images along with the similarity score, i.e. how closely are the two images related. This allows you to efficiently handles this similarity search of images based on their embeddings.

To reverse image search via an API endpoint, create a file `src/pages/api/query.ts` with the following code:

```tsx
// File: src/pages/api/query.ts

import type { APIContext } from 'astro';

export async function POST({ request }: APIContext) {
  // Parse the form data from the request to get the file
  const data = await request.formData();
  const file = data.get('file') as File;
  // If no file is provided, return a 400 Bad Request response
  if (!file) return new Response(null, { status: 400 });
  // Read the file contents into a buffer
  const fileBuffer = await file.arrayBuffer();
  // Create a Blob from the buffer with the correct MIME type
  const fileBlob = new Blob([fileBuffer], { type: file.type });
  // ...
}
```

The code above implements a `POST` function, expecting a form data request with a file attached. It retrieves and validates the presence of file in the request.

Further, it reads its contents into a buffer using `arrayBuffer()`, and creates a Blob with the correct MIME type. This allows the API endpoint to accept images directly in the request, saving compute (on the server-side) for fetching remote image URL to search on.

Now with the following code you get to the final set of operations, i.e. creating vector embeddings of the image and returning highly similar images.

```tsx
// File: src/pages/api/query.ts

// ...

import neonStore from '@/neon';
import { ClipEmbedding, VectorStoreQueryMode } from 'llamaindex';

export async function POST({ request }: APIContext) {
  // ...
  // Get the image embedding using ClipEmbedding
  const image_embedding = await new ClipEmbedding().getImageEmbedding(fileBlob);
  // Query the Neon Postgres vector store for similar images
  const { similarities, nodes } = await neonStore.query({
    similarityTopK: 100,
    queryEmbedding: image_embedding,
    mode: VectorStoreQueryMode.DEFAULT,
  });
  // Initialize an array to store relevant image URLs
  const relevantImages: string[] = [];
  if (nodes) {
    similarities.forEach((similarity: number, index: number) => {
      // Check if similarity is greater than 90% (i.e., similarity threshold)
      if (100 - similarity > 90) {
        const document = nodes[index];
        relevantImages.push(document.metadata.url);
      }
    });
  }
  return new Response(JSON.stringify(relevantImages), {
    headers: { 'Content-Type': 'application/json' },
  });
}
```

The code above adds two new imports: `neonStore` (an alias for the `PGVectorStore` instance) and `ClipEmbedding` from `llamaindex`. **It initializes the embedding model as Clip for processing image embeddings**. It then utilizes the Clip embedding model to extract the image embedding. Further, it queries the Neon Postgres vector store for similar images using the extracted embedding. The query parameters include a similarity threshold and the image embedding.

The relevant images are filtered based on a similarity threshold of 90%, and their URLs are stored in an array. Finally, the endpoint returns a JSON response containing the URLs of the relevant images. This process enables efficient and high quality retrieval of similar images based on their embeddings, completing the reverse image search functionality within the application.

Now let's get to the final aspect of your Astro application, i.e. the user interface.

## Build Reverse Image Search User Interface

For the users to interact with your API endpoints, they require two things: an area on the webpage where they can upload the image they want to search similar ones for, and being able to see the search results.

Update the `index.astro` file in your `src/pages` directory with the following code to allow them to upload an image to search with.

```html
---
// File: src/pages/index.astro
---

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" type="image/svg+xml" href="/favicon.svg" />
    <meta name="viewport" content="width=device-width" />
    <meta name="generator" content="{Astro.generator}" />
    <title>Astro</title>
  </head>
  <body class="flex flex-col items-center">
    <form class="flex flex-col" id="fileUploadForm" enctype="multipart/form-data">
      <input
        class="rounded border px-4 py-3"
        type="file"
        id="fileInput"
        name="file"
        accept="image/*"
      />
      <button id="query" class="mt-3 max-w-max rounded bg-black px-4 py-1 text-white" type="submit">
        Query &rarr;
      </button>
    </form>
  </body>
</html>
```

The HTML above contains a form element with the id `fileUploadForm`, which allows users to upload image files. It consists of an input field of type file and a submit button labeled "Query". **The form is set to handle multipart/form-data encoding**.

To programtically render the search results, you would need to fetch the response from `/api/query` endpoint and then create `img` HTML elements on the webpage. Add the following JavaScript to your index route.

```html
---
// File: src/pages/index.astro
---

<html lang="en">
  <head>
    <!-- Head -->
  </head>
  <body class="flex flex-col items-center">
    <!-- Form -->
    <script>
      document.getElementById('fileUploadForm')?.addEventListener('submit', async function (event) {
        event.preventDefault()
        // remove the previous search results
        document.getElementById('searchResults')?.remove()
        // create a new form data object that contains the uploaded file
        const formData = new FormData()
        const fileInput = document.getElementById('fileInput') as HTMLInputElement
        if (!fileInput || !fileInput.files || fileInput.files.length === 0) return
        formData.append('file', fileInput.files[0])
        // query for similar images
        const queryCall = await fetch('/api/query', { method: 'POST', body: formData })
        const queryResp = await queryCall.json()
        // create the search results div
        const searchResultsDiv = document.createElement('div')
        searchResultsDiv.setAttribute('id', 'searchResults')
        // append all the image results to the search results div
        queryResp.forEach((eachImage: string) => {
          const img = document.createElement('img')
          img.setAttribute('class', 'size-100')
          img.setAttribute('src', eachImage)
          searchResultsDiv.append(img)
        })
        document.body.append(searchResultsDiv)
      })
    </script>
  </body>
</html>
```

The code above adds an event listener that is invoked in case the form is submitted. Upon submission, it extracts the uploaded file, creates a FormData object containing the file, and sends a POST request to `/api/query` with the file data.

The response from the query API is received and parsed as JSON. A new div element is dynamically created to hold the search results, and each image URL from the response is used to create an image element, which is then appended to the search results div.

The resulting search results, consisting of dynamically generated images are then visible to the users. This approach enables real-time querying and display of similar images based on the user upload(s).

With all that, your Astro application is ready to be deployed on Vercel with ease.

## Deploy to Vercel

The repository is now ready to deploy to Vercel. Use the following steps to deploy:

- Start by creating a GitHub repository containing your app's code.
- Then, navigate to the Vercel Dashboard and create a **New Project**.
- Link the new project to the GitHub repository you've just created.
- In **Settings**, update the **Environment Variables** to match those in your local `.env` file.
- Deploy.

## Summary

In this guide, you learned how to build a reverse image search engine in an Astro application using LlamaIndex and Serverless Postgres Database. During the process, you learned how to create vector embeddings of the images using ClipEmbeddings, to index and search from them using LlamaIndex Postgres vector store.

<NeedHelp />


# Local Development with Neon

---
title: Local Development with Neon
subtitle: Learn how to develop applications locally with Neon
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2024-11-05T00:00:00.000Z'
updatedOn: '2024-11-05T00:00:00.000Z'
---

Setting up your development environment should be simple and fast. With Neon's modern approach to PostgreSQL, you get exactly that. Here's how to create the perfect setup for your applications.

<Admonition type="note">
The setups described in this guide use the **Neon serverless driver** for connecting to a Postgres database hosted locally or on Neon over HTTP or WebSockets. To learn more, see [The Neon Serverless driver](https://neon.tech/docs/serverless/serverless-driver).
</Admonition>

## Two ways to develop

When setting up a development environment with Neon, there are a couple of different approaches you can take:

1. **Database branching**
2. **Local PostgreSQL**

Let's explore both options to help you pick the right one.

## Database branching

Imagine creating a complete copy of your database as easily as creating a Git branch. That's [database branching](https://neon.tech/docs/introduction/branching) with Neon – perfect for testing new features or updates without touching production data.

### Why use it?

- **Fast setup**: Create new environments in ~1 second
- **Zero configuration**: No local PostgreSQL installation required
- **True isolation**: Test changes without fear of breaking production
- **Cost-efficient**: Pay only for unique data and actual compute usage
- **Team-friendly**: Share database branches as easily as sharing Git branches
- **Autoscaling**: Resources scale to zero when you're not coding
- **Data reset**: Need a fresh start or a do-over? Reset your branch to match production in seconds

### Quickstart

1. Install the [**Neon CLI**](/docs/reference/neon-cli) by following the guide [here](/docs/reference/neon-cli#install).

2. **Connect your account**

   ```bash
   neonctl auth
   ```

3. **Create your branch**

   ```bash
   neonctl branches create --name dev/your-name

   # Get your connection details
   neonctl connection-string dev/your-name
   ```

   <Admonition type="note">
   You can also create branches through the Neon Console by navigating to your project and clicking the "Branches" tab. This provides a visual interface for branch management and configuration
   </Admonition>

4. **Set up your environment**

   ```bash
   # .env.development
   DATABASE_URL='postgresql://[user]:[password]@[endpoint]/[dbname]'
   ```

5. **Install dependencies**

   Dependencies include [Neon's serverless driver](https://neon.tech/docs/serverless/serverless-driver) and a WebSockets library.

   <Admonition type="note">
   The Neon serverless driver supports connections over HTTP and WebSockets, depending on your requirements. This setup assumes that you could be using either. For the differences, refer to the [Neon's serverless driver docs](https://neon.tech/docs/serverless/serverless-driver).
   </Admonition>

   <CodeTabs labels={["npm", "yarn", "pnpm"]}>

   ```bash
   npm install @neondatabase/serverless ws
   ```

   ```bash
   yarn add @neondatabase/serverless ws
   ```

   ```bash
   pnpm add @neondatabase/serverless ws
   ```

   </CodeTabs>

6. **Connect your app**

   ```javascript
   import { Pool, neon, neonConfig } from '@neondatabase/serverless';

   // Uncomment the following lines if you are on environments that do not support WebSocket, e.g, Node.js
   // import ws from 'ws';
   // neonConfig.webSocketConstructor = ws;

   export const pool = new Pool({ connectionString: process.env.DATABASE_URL });
   export const sql = neon(process.env.DATABASE_URL);
   ```

    <Admonition type="note">
      If you're using Drizzle or Prisma, replace your database connection string in your environment variables with your development branch's connection string.
    </Admonition>

### Tips and tricks

- **Stay organized**: Use prefixes like `dev/feature-auth` or `dev/alice`
- **Reset data**: Start fresh when needed:
  ```bash
  neon branches reset dev/your-name
  ```
- **Feature work**: Create dedicated branches:
  ```bash
  neon branches create --name dev/auth-system --parent main
  ```

## Local PostgreSQL

Sometimes you need to work offline or want full control over your database. Here's how to set up a local PostgreSQL instance that works perfectly with the Neon. This method uses:

- The [Neon Serverless driver](https://neon.tech/docs/serverless/serverless-driver) to connect to your local database (same as the database branching setup described above)
- A Docker compose file that installs a local instance of PostgreSQL 17 and the Neon Proxy. The Neon Proxy lets you to connect to your local PostgreSQL database using the Neon serverless driver.

<Admonition type="note" title="kudos">
The Neon Proxy setup uses the [local-neon-http-proxy](https://github.com/TimoWilhelm/local-neon-http-proxy) Dockerfile, developed by [TimoWilhelm](https://github.com/TimoWilhelm).
</Admonition>

### Why use this method?

- **Full control**: Your own PostgreSQL instance
- **Offline work**: Code without internet dependency
- **Fast queries**: Zero network latency
- **Free development**: Use your local resources

### Docker Compose setup

Create a `docker-compose.yml` file with the following content:

```yaml
services:
  postgres:
    image: postgres:17
    command: '-d 1'
    volumes:
      - db_data:/var/lib/postgresql/data
    ports:
      - '5432:5432'
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=main
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U postgres']
      interval: 10s
      timeout: 5s
      retries: 5

  neon-proxy:
    image: ghcr.io/timowilhelm/local-neon-http-proxy:main
    environment:
      - PG_CONNECTION_STRING=postgres://postgres:postgres@postgres:5432/main
    ports:
      - '4444:4444'
    depends_on:
      postgres:
        condition: service_healthy

volumes:
  db_data:
```

Run the following command to start local PostgreSQL and the Neon Proxy, which helps you connect to your local database:

```bash
docker-compose up -d
```

<Admonition type="tip" title="Working offline?">
The [local-neon-http-proxy](https://github.com/TimoWilhelm/local-neon-http-proxy) Dockerfile setup uses [*.localtest.me](https://readme.localtest.me/) to enable testing with local URLs without adding entires to your host file. The `localtest.me` domain and all wildcard subdomains point to `127.0.0.1`.

However, this solution requires an internet connection. To work offline, you'll need to add an entry to your system's hosts file to map `db.localtest.me` to localhost:

```bash
127.0.0.1 db.localtest.me
```

For instructions on editing your hosts file on different operating systems, see [this guide](https://www.hostinger.in/tutorials/how-to-edit-hosts-file).

[dnsmask](https://help.ubuntu.com/community/Dnsmasq) is another option [suggested by a Neon user](https://github.com/neondatabase/website/issues/2690) for resolving domain names when there is no internet connection.
</Admonition>

### Connect your app

<Tabs labels={["Using neondatabase/serverless", "Using drizzle", "Using prisma"]}>

<TabItem>

1. Install Dependencies

   <CodeTabs labels={["npm", "yarn", "pnpm"]}>

   ```bash
   npm install @neondatabase/serverless ws
   ```

   ```bash
   yarn add @neondatabase/serverless ws
   ```

   ```bash
   pnpm add @neondatabase/serverless ws
   ```

   </CodeTabs>

2. **Configure the connection**

   ```typescript
   import { neon, neonConfig, Pool } from '@neondatabase/serverless';
   import ws from 'ws';

   let connectionString = process.env.DATABASE_URL;

   // Configuring Neon for local development
   if (process.env.NODE_ENV === 'development') {
     connectionString = 'postgres://postgres:postgres@db.localtest.me:5432/main';
     neonConfig.fetchEndpoint = (host) => {
       const [protocol, port] = host === 'db.localtest.me' ? ['http', 4444] : ['https', 443];
       return `${protocol}://${host}:${port}/sql`;
     };
     const connectionStringUrl = new URL(connectionString);
     neonConfig.useSecureWebSocket = connectionStringUrl.hostname !== 'db.localtest.me';
     neonConfig.wsProxy = (host) => (host === 'db.localtest.me' ? `${host}:4444/v2` : `${host}/v2`);
   }
   neonConfig.webSocketConstructor = ws;

   // Neon supports both HTTP and WebSocket clients. Choose the one that fits your needs:

   // HTTP Client (sql)
   // - Best for serverless functions and Lambda environments
   // - Ideal for stateless operations and quick queries
   // - Lower overhead for single queries
   // - Better for applications with sporadic database access
   export const sql = neon(connectionString);

   // WebSocket Client (pool)
   // - Best for long-running applications (like servers)
   // - Maintains a persistent connection
   // - More efficient for multiple sequential queries
   // - Better for high-frequency database operations
   export const pool = new Pool({ connectionString });
   ```

</TabItem>
<TabItem>

1. Install Dependencies

   <CodeTabs labels={["npm", "yarn", "pnpm"]}>

   ```bash
   npm install drizzle-orm @neondatabase/serverless ws
   ```

   ```bash
   yarn add drizzle-orm @neondatabase/serverless ws
   ```

   ```bash
   pnpm add drizzle-orm @neondatabase/serverless ws
   ```

   </CodeTabs>

2. **Configure the connection**

   ```typescript
   import { neon, neonConfig, Pool } from '@neondatabase/serverless';
   import { drizzle as drizzleWs } from 'drizzle-orm/neon-serverless';
   import { drizzle as drizzleHttp } from 'drizzle-orm/neon-http';
   import ws from 'ws';

   let connectionString = process.env.DATABASE_URL;

   // Configuring Neon for local development
   if (process.env.NODE_ENV === 'development') {
     connectionString = 'postgres://postgres:postgres@db.localtest.me:5432/main';
     neonConfig.fetchEndpoint = (host) => {
       const [protocol, port] = host === 'db.localtest.me' ? ['http', 4444] : ['https', 443];
       return `${protocol}://${host}:${port}/sql`;
     };
     const connectionStringUrl = new URL(connectionString);
     neonConfig.useSecureWebSocket = connectionStringUrl.hostname !== 'db.localtest.me';
     neonConfig.wsProxy = (host) => (host === 'db.localtest.me' ? `${host}:4444/v2` : `${host}/v2`);
   }
   neonConfig.webSocketConstructor = ws;

   const sql = neon(connectionString);
   const pool = new Pool({ connectionString });

   // Drizzle supports both HTTP and WebSocket clients. Choose the one that fits your needs:

   // HTTP Client:
   // - Best for serverless functions and Lambda environments
   // - Ideal for stateless operations and quick queries
   // - Lower overhead for single queries
   // - Better for applications with sporadic database access
   export const drizzleClientHttp = drizzleHttp({ client: sql });

   // WebSocket Client:
   // - Best for long-running applications (like servers)
   // - Maintains a persistent connection
   // - More efficient for multiple sequential queries
   // - Better for high-frequency database operations
   export const drizzleClientWs = drizzleWs({ client: pool });
   ```

3. **Migration setup**

   To ensure your Drizzle migrations run smoothly and without errors in your development environment, you can install the `postgres` package as a development dependency.

   <CodeTabs labels={["npm", "yarn", "pnpm"]}>

   ```bash
   npm i -D postgres
   ```

   ```bash
   yarn add -D postgres
   ```

   ```bash
   pnpm add -D postgres
   ```

   </CodeTabs>

</TabItem>

<TabItem>

Note that Driver Adapters are still in preview for Prisma. Please refer to the [Prisma documentation](https://www.prisma.io/docs/orm/overview/databases/neon) for the latest information.

1. Install Dependencies

   <CodeTabs labels={["npm", "yarn", "pnpm"]}>

   ```bash
   npm install @prisma/client @prisma/adapter-neon @neondatabase/serverless ws
   ```

   ```bash
   yarn add @prisma/client @prisma/adapter-neon @neondatabase/serverless ws
   ```

   ```bash
   pnpm add @prisma/client @prisma/adapter-neon @neondatabase/serverless ws
   ```

   </CodeTabs>

2. **Enable the Preview Flag**

   To use the Neon serverless driver with Prisma, enable the preview flag in your `schema.prisma` file.

   ```prisma
     generator client {
       provider        = "prisma-client-js"
       previewFeatures = ["driverAdapters"]
     }
   ```

3. **Configure the connection**

   ```typescript
   import { neon, neonConfig, Pool } from '@neondatabase/serverless';
   import { PrismaNeon, PrismaNeonHTTP } from '@prisma/adapter-neon';
   import { PrismaClient } from '@prisma/client';
   import ws from 'ws';

   let connectionString = process.env.DATABASE_URL;

   // Configuring Neon for local development
   if (process.env.NODE_ENV === 'development') {
     connectionString = 'postgres://postgres:postgres@db.localtest.me:5432/main';
     neonConfig.fetchEndpoint = (host) => {
       const [protocol, port] = host === 'db.localtest.me' ? ['http', 4444] : ['https', 443];
       return `${protocol}://${host}:${port}/sql`;
     };
     const connectionStringUrl = new URL(connectionString);
     neonConfig.useSecureWebSocket = connectionStringUrl.hostname !== 'db.localtest.me';
     neonConfig.wsProxy = (host) => (host === 'db.localtest.me' ? `${host}:4444/v2` : `${host}/v2`);
   }
   neonConfig.webSocketConstructor = ws;

   const sql = neon(connectionString);
   const pool = new Pool({ connectionString });

   // Prisma supports both HTTP and WebSocket clients. Choose the one that fits your needs:

   // HTTP Client:
   // - Ideal for stateless operations and quick queries
   // - Lower overhead for single queries
   const adapterHttp = new PrismaNeonHTTP(sql);
   export const prismaClientHttp = new PrismaClient({ adapter: adapterHttp });

   // WebSocket Client:
   // - Best for long-running applications (like servers)
   // - Maintains a persistent connection
   // - More efficient for multiple sequential queries
   // - Better for high-frequency database operations
   const adapterWs = new PrismaNeon(pool);
   export const prismaClientWs = new PrismaClient({ adapter: adapterWs });
   ```

</TabItem>

</Tabs>

## Which development approach should you use?

Before choosing between cloud-hosted or local development, it's important to understand the benefits of each approach.

Cloud-hosted branches offer several compelling advantages:

### Cost-efficient development

- **Minimal storage costs**: Branches are extremely cost-effective as you only pay for unique data changes
- **Smart compute usage**: Development happens on small computes (0.25 vCPU) that scale to zero by default
- **Free Plan benefits**: Even the Free Plan includes 5 compute hours on dev branches
  - This translates to 20 hours of development time on a 0.25 vCPU compute
  - One compute hour at 1 vCPU equals four hours at 0.25 vCPU

### Developer-friendly features

- **Instant deployment**: Branches are created in seconds, just like Git branches
- **Branch reset**: Easily refresh your development data from the parent branch
- **Zero maintenance**: No need to manage local PostgreSQL installations

| Feature             | Database Branching                           | Local PostgreSQL                     |
| ------------------- | -------------------------------------------- | ------------------------------------ |
| Setup Time          | ✅ Instant (~1 second)                       | ⏱️ Requires initial configuration    |
| Configuration       | ✅ Zero configuration needed                 | 🔧 Requires local setup              |
| Team Collaboration  | ✅ Easy branch sharing and management        | 🤝 Requires additional setup         |
| Cost Management     | ✅ Pay only for unique data and compute time | 💻 Local resources only              |
| Resource Scaling    | ✅ Scale to zero when not in use             | ❌ Always consuming resources        |
| Offline Development | ❌ Requires internet connection              | ✅ Works offline                     |
| Network Latency     | 🌐 Depends on connection                     | ✅ Zero latency                      |
| Production Parity   | ✅ Identical to production                   | 🔄 Requires additional configuration |

## When to use each approach

### Choose database branching when:

- You want instant development environments
- You need efficient resource utilization
- You're working with a team

**Perfect for:**

- Most development workflows
- Team environments
- Rapid prototyping
- Feature development
- Testing database changes

### Consider local PostgreSQL when:

- Offline development is crucial
- You need zero network latency
- You require complete database control
- You have specific local testing requirements

## Best practices for cloud-hosted development with Neon branching

### Environment tips

- Keep development and production database branches separate
- Always Use clear branch naming
- Never commit credentials to a version control system

### Resource tips

- Use scale to zero for development branches
- Clean up unused branches
- Reset branches to match production when needed

### Security tips

- Use separate development credentials
- Rotate credentials regularly
- Keep production credentials isolated

## Start building

You're now ready to create a powerful development environment with Neon. Choose the approach that fits your team best and start building.

<NeedHelp/>


# Setting up Neon serverless Postgres as an Azure Native Integration

---
title: 'Setting up Neon serverless Postgres as an Azure Native Integration'
subtitle: "A step-by-step guide to deploying Neon's serverless Postgres via the Azure Marketplace"
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2024-12-12T00:00:00.000Z'
updatedOn: '2024-12-12T00:00:00.000Z'
---

Neon's serverless Postgres is now available as a native integration within the Azure ecosystem, allowing organizations to provision Neon organizations directly from the Azure portal. This integration combines the power of Neon's serverless architecture with Azure's robust cloud infrastructure, offering features like Single Sign-On (SSO), unified billing, and resource management. This guide walks you through the process of setting up and managing Neon Serverless Postgres via the Azure Marketplace.

## Prerequisites

- An [Azure account](https://azure.microsoft.com/free/) with an active subscription

## Key benefits of Neon on Azure

The Azure Native integration delivers significant advantages that enhance both developer productivity and business operations:

- **Azure Portal Integration**: Create and manage Neon organizations directly through the Azure portal, eliminating the need to switch between platforms
- **Enhanced Development Experience**: Leverage familiar tools, including the Azure CLI and popular SDKs (Node.js, Python, .NET, Java, Go) for consistent workflow management
- **Single Sign-On**: Access Neon using your Microsoft credentials, improving security and simplifying authentication
- **Unified Billing**: Track and manage Neon expenses alongside other Azure services in one consolidated bill
- **Cost Optimization**: Optimize your cloud spend by applying Neon usage toward your [Microsoft Azure Consumption Commitment](https://learn.microsoft.com/en-us/marketplace/azure-consumption-commitment-benefit).

The Neon integration is available through the [Azure Marketplace](https://neon.tech/docs/introduction/billing-azure-marketplace).

## Creating your Neon organization

To get started with Neon on Azure, you'll need to create a Neon organization within your Azure environment. This process is streamlined through the Azure portal, allowing you to manage your Neon resources alongside other Azure services.

### Access Neon through Azure Marketplace

1. Navigate to the [Azure Portal](https://portal.azure.com).
2. Search for **Neon Serverless Postgres** in the search bar.
3. Select the offering from the Marketplace results.
   ![Search for Neon Serverless Postgres on Azure](/docs/guides/search-for-neon-on-azure.png)

## Choosing the right plan

You will be prompted to select a plan based on your organization's requirements:
While starting with the Free Plan is an excellent way to explore Neon's capabilities, understanding the full range of plans helps you make informed decisions as your needs evolve.

### Free plan: Perfect for getting started

The Free Plan provides an excellent entry point for developers and small projects. With 10 projects, 0.5 GB-month storage, and 190 compute hours, you can build and test applications while experiencing Neon's core features like [database branching](/docs/introduction/branching), [read replicas](/docs/introduction/read-replicas) and [Postgres extensions](https://neon.tech/docs/extensions/pg-extensions)

### Scale plan: Growing with your application

As your application grows, the Scale Plan offers substantial increases in resources and features for growing organizations, including:

- 50 GB-month storage + 250 GB-month archive storage
- 750 compute hours
- 1,000 projects
- Autoscaling up to 8 vCPUs
- Point-in-time restore (PITR) up to 14 days
- IP Allow Rules for enhanced security
- [Datadog integration](/docs/guides/datadog) for advanced monitoring

### Business plan: Enterprise-ready features

For organizations requiring enterprise-grade features, the Business Plan provides everything on the Scale plan plus more resources and features:

- 500 GB storage + 2500 GB-month archive storage
- 1000 compute hours
- 5,000 projects
- Autoscaling up to 16 vCPUs
- Larger fixed compute sizes up to 56 vCPU
- Point-in-time restore (PITR) up to 30 days
- Enhanced compliance with SOC 2 and ISO certifications
- Guaranteed 99.95% SLA for mission-critical applications
- Priority support for quick issue resolution
- Private Link support for secure connectivity

<Admonition type="note" title="Note">
For custom enterprise requirements beyond the Business Plan, [you can reach out to Neon's sales team](https://neon.tech/contact-sales).
</Admonition>

## Configuring your resource

For this guide, we'll use the Free Tier, but you can easily upgrade later as your needs grow. We will proceed to the resource configuration stage by clicking on **Subscribe** with the Free Plan selected.

![Subscribe to Neon on Azure](/docs/guides/subscribe-to-neon-on-azure.png)

After selecting your plan, you'll need to configure your Neon resource within Azure. Begin by selecting your Azure subscription and creating or choosing an existing resource group. The resource group helps you organize and manage related Azure resources together. Next, provide a meaningful resource name that helps you identify this Neon deployment within your Azure environment.

![Neon Resource Configuration on Azure](/docs/guides/neon-resource-configuration-on-azure.png)

You can create a Neon resource in the `East US 2`, `Germany West Central`, and `West US 3` regions, but keep in mind that this is the region for the Azure resource, not your Neon database — you'll select a region for your actual database in a later step. Finally, create a distinctive organization name that reflects your company or project. This organization name will be visible in both Azure and the Neon Console.

Click **Review + Create** to proceed to the final stage of the deployment process. The Azure portal will validate your configuration settings, ensuring that all required fields are correctly filled. Once the validation is successful, click **Create** to deploy your Neon resource. Azure will begin provisioning your resource, which typically takes a few minutes. You should see a notification indicating that your Neon resource has been successfully deployed.

![Neon Resource Deployment Complete on Azure](/docs/guides/neon-resource-deployment-complete-on-azure.png)

## Managing your Neon resource

### Accessing the Neon Console

After successful deployment, accessing your Neon Console is straightforward through Azure's SSO integration. Navigate to your newly created Neon resource in the Azure portal, where you'll find the **SSO link** under **Portal URL**.

![Neon SSO Link in Azure Portal](/docs/guides/neon-sso-link-in-azure-portal.png)

This link provides access to the Neon Console using your Azure credentials, eliminating the need for separate authentication processes.

### Creating your first project

Upon accessing the Neon Console, you'll be directed to the project creation view. Here, you can create your first Neon project.

![Neon Azure Create Project](/docs/guides/neon-azure-create-project.png)

Neon organizes your databases into projects, each serving as an independent unit for database management and development. You can customize your project with the following details:

- **Project name:** Typically named for your application or user.
- **Database name:** Specify whatever database name makes sense. The default name is `neondb`. The name will form part of your database connection string.
- **Region:** When selecting a region, consider the geographical location of your applications and users. Choose the region that provides the lowest latency for your primary user base.
- **Advanced options:** Neon uses branches for data, similar to git branches. The default branch is named `main`. If you want, you can specify a different name.
- **Autoscaling range**: Set your project's autoscaling range. Compute resources will scale up and down within the specified range to meet demand.

### Exploring your project and database

Once your project is created, a default branch and Postgres database are automatically generated within it. You will be redirected to the project dashboard, which displays details of your project. This dashboard is your central hub for monitoring resources, setting up connections, and managing your database.

![Neon Project Dashboard](/docs/guides/neon-project-dashboard.png)

Here are some key features you'll find:

- **Connection string:** The connection string provides the necessary details to connect your applications to the database.
- **Branching:** You can create new database branches from the default one. Learn more about [database branching here](/docs/introduction/branching).
- **Monitoring**: The dashboard allows you to monitor resource usage, including compute, storage, and connections count.

### Connecting your application

To connect your application to the Neon database, copy the connection URL from the project dashboard and use it in your application stack. This URL contains the necessary credentials and connection details to establish a connection to your Neon database.

For more about connecting your application to Neon, see [Connect to Neon](https://neon.tech/docs/connect/connect-intro). Neon also provides the following resources to help get you up and running:

- [Neon framework guides](https://neon.tech/docs/get-started-with-neon/frameworks)
- [Neon language guides](https://neon.tech/docs/get-started-with-neon/languages)
- [Connection examples repo](https://github.com/neondatabase/examples)
- [Application templates repo](https://neon.tech/templates)

## Platform-Specific Management: Azure Portal vs. Neon Console

### Azure Portal: Your Administrative hub

The Azure portal provides a unified management experience for your Neon resources, serving as the primary entry point within the Azure ecosystem. It allows you to create and manage Neon organizations, which can then be accessed through the dedicated Neon Console. The Azure portal also centralizes your Neon financial management, providing cost views, allowing application of your [Microsoft Azure Consumption Commitment](https://learn.microsoft.com/en-us/marketplace/azure-consumption-commitment-benefit), and enabling unified billing reports.

Additionally, accessing Neon via the Azure portal enables SSO, which simplifies access to the Neon Console, and lets you manage Neon organization access via Azure RBAC.

In essence, the Azure portal provides a centralized platform for managing the administrative and financial aspects of your Neon integration within Azure. On the other hand, the management of databases, individual projects, and their advanced features is managed in the Neon Console, which we'll discuss next.

### Neon Console: Your Database management and development hub

The Neon Console is your dedicated environment for the day-to-day management of your Postgres databases and development workflows, providing a specialized set of tools beyond what's available in the Azure portal. This is where you create and manage individual database projects, configure autoscaling to adapt to your workload, and access database monitoring metrics to ensure optimal performance.

In the Neon Console, you can create read replicas to scale your read operations without impacting your primary database. You can also leverage Neon's branching capabilities, enabling you to quickly create instant copies of your database for isolated development and testing purposes. This can be instrumental when implementing database changes or developing new features.

If needed, the Neon Console facilitates instant database restoration via it's point in time restore feature. Additionally, the console provides performance and usage monitoring, allowing you to track your usage, monitor database connections, and more. The Neon Console also allows for various integrations with popular developer tools and services, enhancing the overall workflow, which is essential to ensure that your database functions are streamlined and integrated into your development ecosystem.

In short, the Neon Console provides the essential capabilities needed for database administration, development, and optimization of your Postgres databases, giving you granular control over your data and your development workflow.

## Support

Neon offers a range of support options to help you get the most out of your Neon on Azure deployment. The following table outlines the support channels available at each plan level:

| Support channels                                                                                | Free Plan | Scale Plan | Business Plan | Enterprise |
| :---------------------------------------------------------------------------------------------- | :-------: | :--------: | :-----------: | :--------: |
| [Neon Discord Server](/docs/introduction/support#neon-discord-server) (not an official channel) |  &check;  |  &check;   |    &check;    |  &check;   |
| [Neon AI Chat](/docs/introduction/support#neon-ai-chat) (not an official channel)               |  &check;  |  &check;   |    &check;    |  &check;   |
| [Support tickets](/docs/introduction/support#support-tickets)                                   |     -     |  &check;   |    &check;    |  &check;   |
| [Prioritized support tickets](/docs/introduction/support#prioritized-support-tickets)           |     -     |     -      |    &check;    |  &check;   |
| [Video chat](/docs/introduction/support#video-chat)                                             |     -     |     -      |      \*       |     \*     |
| [SLAs](/docs/introduction/support#slas)                                                         |     -     |     -      |    &check;    |  &check;   |

<div style={{margin: '-30px 0 30px 0'}}>
<small><sup>*</sup>Video chats may be scheduled on a case-by-case basis. See [Video chat](/docs/introduction/support#video-chat).</small>
</div>

## Additional Resources

- [Neon Documentation](/docs)
- [Microsoft Azure Documentation](https://docs.microsoft.com/en-us/azure/)
- [Azure Marketplace](https://azuremarketplace.microsoft.com/en-us/)
- [Microsoft Azure Consumption Commitment Benefit](https://learn.microsoft.com/en-us/marketplace/azure-consumption-commitment-benefit)

<NeedHelp />


# Automated Database Branching with GitHub Actions

---
title: Automated Database Branching with GitHub Actions
subtitle: Learn how to automate database branching for your application using Neon and GitHub Actions
enableTableOfContents: true
author: dhanush-reddy
createdAt: '2024-11-29T00:00:00.000Z'
---

Database changes can be one of the trickiest parts of application development. When multiple developers work on features that require database modifications, they often face challenges like conflicting schema changes, difficulty in testing migrations, and the risk of breaking the production database.

Database branching solves these problems by allowing developers to create isolated database environments for each feature branch, just like they do with code.
This guide demonstrates how to implement automated database branching using Neon and GitHub Actions, where each pull request gets its own database branch, complete with the necessary schema changes. You'll build a Next.js Todo application that showcases this workflow, which automates several critical database operations, including:

- Creating a new database branch when a pull request is opened
- Automatically applying schema migrations to the new branch
- Showing schema diffs directly in your pull request
- Syncing schema changes to production when the PR is merged

By the end of this guide, you'll have a system where database changes are as seamless as code changes, with each feature safely isolated in its own environment until it's ready for production. This approach not only makes database changes safer but also gives developers the confidence to experiment with schema changes without fear of breaking the production environment.

## Prerequisites

- A [Neon account](https://console.neon.tech)
- A [GitHub account](https://github.com/)
- Node.js installed on your machine
- Basic familiarity with Next.js and TypeScript

## Setting Up Your Neon Database

1. Create a new Neon project from the [Neon Console](https://console.neon.tech). For instructions, see [Create a project](/docs/manage/projects#create-a-project).
2. Note your connection string from the connection details page.

   Your connection string will look similar to this:

   ```shell
   postgres://[user]:[password]@[neon_hostname]/[dbname]?sslmode=require
   ```

## Set up the project

1. Create a new Next.js project with TypeScript:

   ```bash shouldWrap
   npx create-next-app@14 todo-app --typescript --tailwind --use-npm --eslint --app --no-src-dir --import-alias "@/*"
   cd todo-app
   ```

2. Install the required dependencies:

   ```bash
   npm install drizzle-orm @neondatabase/serverless dotenv
   npm install -D drizzle-kit
   ```

## Configure the database schema

1. Create `app/db/schema.ts`:

   ```typescript
   import { integer, text, boolean, pgTable } from 'drizzle-orm/pg-core';

   export const todo = pgTable('todo', {
     id: integer('id').primaryKey(),
     text: text('text').notNull(),
     done: boolean('done').default(false).notNull(),
   });
   ```

2. Create `drizzle.config.ts` in your project root:

   ```typescript
   import { config } from 'dotenv';
   import { defineConfig } from 'drizzle-kit';

   config({ path: '.env' });

   export default defineConfig({
     schema: './app/db/schema.ts',
     out: './migrations',
     dialect: 'postgresql',
     dbCredentials: {
       url: process.env.DATABASE_URL!,
     },
   });
   ```

3. Add database scripts to your `package.json`:

   ```json
   {
    ...
     "scripts": {
        ...
       "db:generate": "drizzle-kit generate",
       "db:migrate": "drizzle-kit migrate"
     }
   }
   ```

4. Create a `.env` file in your project root:

   ```bash shouldWrap
   DATABASE_URL=postgres://[user]:[password]@[neon_hostname]/[dbname]?sslmode=require
   ```

5. Push your code to a Github repository.

## Set up the Neon GitHub integration

The [Neon GitHub integration](/docs/guides/neon-github-integration) connects your Neon project to your application repository and automatically sets a `NEON_API_KEY` secret and `NEON_PROJECT_ID` variable for you. These variables will support the GitHub Actions workflow we'll create in a later step.

1. In the Neon Console, navigate to the **Integrations** page in your Neon project.
2. Locate the **GitHub** card and click **Add**.
   ![GitHub App card](/docs/guides/github_card.png)
3. On the **GitHub** drawer, click **Install GitHub App**.
4. If you have more than one GitHub account, select the account where you want to install the GitHub app.
5. Select the GitHub repository to connect to your Neon project, and click **Connect**.

   The final page of the GitHub integration setup provides a sample GitHub Actions workflow. With this workflow as a example, we'll create a custom GitHub Actions workflow in the next steps.

## Create the GitHub Actions workflow

Create `.github/workflows/neon_workflow.yaml` file and add the following code:

```yaml
name: Create/Delete Branch for Pull Request

on:
  pull_request:
    types:
      - opened
      - reopened
      - synchronize
      - closed

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}

jobs:
  setup:
    name: Setup
    outputs:
      branch: ${{ steps.branch_name.outputs.current_branch }}
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write
    steps:
      - name: Get branch name
        id: branch_name
        uses: tj-actions/branch-names@v8

  create_neon_branch:
    name: Create Neon Branch
    outputs:
      db_url: ${{ steps.create_neon_branch_encode.outputs.db_url }}
      db_url_with_pooler: ${{ steps.create_neon_branch_encode.outputs.db_url_with_pooler }}
    needs: setup
    if: |
      github.event_name == 'pull_request' && (
      github.event.action == 'synchronize'
      || github.event.action == 'opened'
      || github.event.action == 'reopened')
    runs-on: ubuntu-latest
    steps:
      - name: Create Neon Branch
        id: create_neon_branch
        uses: neondatabase/create-branch-action@v5
        with:
          project_id: ${{ vars.NEON_PROJECT_ID }}
          branch_name: preview/pr-${{ github.event.number }}-${{ needs.setup.outputs.branch }}
          api_key: ${{ secrets.NEON_API_KEY }}

      - name: Checkout
        uses: actions/checkout@v4

      - name: Run Migrations on Preview Branch
        run: npm install && npm run db:generate && npm run db:migrate
        env:
          DATABASE_URL: '${{ steps.create_neon_branch.outputs.db_url }}'

      - name: Post Schema Diff Comment to PR
        uses: neondatabase/schema-diff-action@v1
        with:
          project_id: ${{ vars.NEON_PROJECT_ID }}
          compare_branch: preview/pr-${{ github.event.number }}-${{ needs.setup.outputs.branch }}
          api_key: ${{ secrets.NEON_API_KEY }}

  delete_neon_branch:
    name: Delete Neon Branch and Apply Migrations on Production Database
    needs: setup
    if: |
      github.event_name == 'pull_request' &&
      github.event.action == 'closed'
    runs-on: ubuntu-latest
    steps:
      - name: Delete Neon Branch
        uses: neondatabase/delete-branch-action@v3
        with:
          project_id: ${{ vars.NEON_PROJECT_ID }}
          branch: preview/pr-${{ github.event.number }}-${{ needs.setup.outputs.branch }}
          api_key: ${{ secrets.NEON_API_KEY }}

      - name: Checkout
        if: github.event.pull_request.merged == true
        uses: actions/checkout@v4

      - name: Apply migrations to production
        if: github.event.pull_request.merged == true
        run: |
          npm install
          npm run db:generate
          npm run db:migrate
        env:
          DATABASE_URL: '${{ secrets.DATABASE_URL }}'
```

<Admonition type="note" title="Note">
To set up GitHub Actions correctly:

1. **Enable Workflow Permissions**:
   Go to your repository's GitHub Actions settings, navigate to **Actions** > **General**, and set **Workflow permissions** to **Read and write permissions**.

2. **Add Database Connection String**:
   Add a `DATABASE_URL` secret to your repository under **Settings** > **Secrets and variables** > **Actions**, using the connection string for your production database that you noted earlier. While you're here, you should see the `NEON_API_KEY` secret and `NEON_PROJECT_ID` variable that have already been set by the Neon GitHub integration.

</Admonition>

## Understanding the workflow

The GitHub Actions workflow automates database branching and schema management for pull requests. Here's a breakdown of the workflow:

### Create Branch Job

This job runs when a pull request is opened, reopened, or synchronized:

1. **Branch Creation**:

   - Uses Neon's `create-branch-action` to create a new database branch
   - Names the branch using the pattern `preview/pr-{number}-{branch_name}`
   - Inherits the schema and data from the parent branch

2. **Migration Handling**:

   - Installs project dependencies
   - Generates migration files using Drizzle
   - Applies migrations to the newly created branch
   - Uses the branch-specific `DATABASE_URL` for migration operations

3. **Schema Diff Generation**:
   - Uses Neon's `schema-diff-action`
   - Compares the schema of the new branch with the parent branch
   - Automatically posts the differences as a comment on the pull request
   - Helps reviewers understand database changes at a glance

### Delete Branch Job

This job executes when a pull request is closed (either merged or rejected):

1. **Production Migration**:

   - If the PR is merged, applies migrations to the production database
   - Uses the main `DATABASE_URL` stored in repository secrets
   - Ensures production database stays in sync with merged changes

2. **Cleanup**:
   - Removes the preview branch using Neon's `delete-branch-action`

## Flow Summary

Here's how the entire process works from start to finish:

1. Developer creates a new feature branch and makes schema changes
2. When they open a pull request:
   - A new database branch is automatically created
   - Schema migrations are generated and applied
   - A schema diff comment is posted on the PR
3. During PR review:
   - Reviewers can see exactly what database changes are being made
   - The isolated database branch prevents conflicts with other features
   - Additional commits trigger automatic migration updates
4. When the PR is approved and merged:
   - Migrations are automatically applied to the production database
   - The preview branch is deleted
   - The schema changes are now live in production
5. If the PR is closed without merging:
   - The preview branch is automatically deleted
   - No changes are made to the production database

This automated workflow ensures that:

1. Every feature gets its own isolated database environment
2. Schema changes are automatically tracked and documented in the pull request
3. Migrations are consistently applied across environments
4. Production database stays in sync with merged code
5. Database resources are efficiently managed
6. The risk of manual migration errors is minimized

## Test the workflow

To test the workflow, perform the following steps:

1. Create a new feature branch:

   ```bash
   git checkout -b feature/add-todo-created-at
   ```

2. Modify the schema in `db/schema/todos.ts`:

   ```typescript
   export const todo = pgTable('todo', {
     id: integer('id').primaryKey(),
     text: text('text').notNull(),
     done: boolean('done').default(false).notNull(),
     created_at: timestamp('created_at').notNull().defaultNow(),
   });
   ```

3. Commit and push your changes:

   ```bash
   git add .
   git commit -m "feat: add created_at field to todo"
   git push origin feature/add-todo-created-at
   ```

4. Open a pull request on GitHub

The workflow will:

- Create a new database branch for your PR
- Apply the schema migration
- Post a schema diff comment on the PR
  ![Schema Diff Comment](/docs/guides/github_schema_diff_example_comment.png)
- After merging, apply the changes to production

## Source code

You can find the complete source code for this example on GitHub.

<DetailIconCards>
<a href="https://github.com/neondatabase-labs/neon-github-actions-integration" description="Get started with automated database branching using Neon and GitHub Actions" icon="github">Get started with automated database branching</a>
</DetailIconCards>

## Resources

- [Neon GitHub Integration Documentation](/docs/guides/neon-github-integration)
- [Database Branching Workflows](https://neon.tech/flow)
- [GitHub Actions Documentation](https://docs.github.com/en/actions)

<NeedHelp/>


# Query Postgres in Next.js Server Actions

---
title: Query Postgres in Next.js Server Actions
subtitle: Learn how to query Postgres in Next.js with Server Actions
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-05-13T13:24:36.612Z'
updatedOn: '2024-05-13T13:24:36.612Z'
---

In this guide, you will learn the process of creating a simple web application using Next.js Server Actions that allows you to capture user input via forms, and insert them into Postgres via `@neondatabase/serverless` and `pg`.

To create a Neon project and access it from an Next.js application:

- [Create a Neon project](#create-a-neon-project)
- [Create a Next.js project and add dependencies](#create-a-nextjs-project-and-add-dependencies)
- [Store your Neon credentials](#store-your-neon-credentials)
- [Create a Form with Server Actions](#create-a-form-with-server-actions)
- [Implement Next.js Server Actions](#implement-nextjs-server-actions)
- [Run the app](#run-the-app)

## Create a Neon project

If you do not have one already, create a Neon project. Save your connection details including your password. They are required when defining connection settings.

1. Navigate to the [Projects](https://console.neon.tech/app/projects) page in the Neon Console.
2. Click **New Project**.
3. Specify your project settings and click **Create Project**.

## Create a Next.js project and add dependencies

1. Create an Next.js project if you do not have one. For instructions, see [Automatic Installation](https://nextjs.org/docs/getting-started/installation#automatic-installation) in the Next.js documentation.

2. Add project dependencies using one of the following commands:

   <CodeTabs reverse={true} labels={["node-postgres", "Neon serverless driver"]}>

   ```shell
   npm install pg
   ```

   ```shell
   npm install @neondatabase/serverless
   ```

   </CodeTabs>

## Store your Neon credentials

Add a `.env` file to your project directory and add your Neon connection string to it. You can find the connection string for your database in the **Connection Details** widget on the Neon **Dashboard**. For more information, see [Connect from any application](/docs/connect/connect-from-any-app).

```shell shouldWrap
DATABASE_URL="postgres://[user]:[password]@[neon_hostname]/[dbname]"
```

## Create a Form with Server Actions

Create a simple form that allows users to input a comment. For now, add an action named `create` that you will create in next step with Next.js server actions.

```tsx
// File: app/page.tsx

export default function Page() {
  return (
    <form action={create}>
      <input type="text" placeholder="write a comment" name="comment" />
      <button type="submit">Submit</button>
    </form>
  );
}
```

## Implement Next.js Server Actions

Now, let's add the server action to insert the data into your Postgres.

<CodeTabs reverse={true} labels={["node-postgres", "Neon serverless driver"]}>

```tsx {3,6-16}
// File: app/page.tsx

import { Client } from 'pg';

export default function Page() {
  async function create(formData: FormData) {
    'use server';
    // Create a client instance using `node-postgres`
    const client = new Client(`${process.env.DATABASE_URL}`);
    await client.connect();
    // Create the comments table if it does not exist
    await client.query('CREATE TABLE IF NOT EXISTS comments (comment TEXT)');
    const comment = formData.get('comment');
    // Insert the comment from the form into the Postgres (powered by Neon)
    await client.query('INSERT INTO comments (comment) VALUES ($1)', [comment]);
  }
  return (
    <form action={create}>
      <input type="text" placeholder="write a comment" name="comment" />
      <button type="submit">Submit</button>
    </form>
  );
}
```

```tsx {3,6-15}
// File: app/page.tsx

import { neon } from '@neondatabase/serverless';

export default function Page() {
  async function create(formData: FormData) {
    'use server';
    // Create an instance of Neon's TS/JS driver
    const sql = neon(`${process.env.DATABASE_URL}`);
    // Create the comments table if it does not exist
    await sql('CREATE TABLE IF NOT EXISTS comments (comment TEXT)');
    const comment = formData.get('comment');
    // Insert the comment from the form into the Postgres (powered by Neon)
    await sql('INSERT INTO comments (comment) VALUES ($1)', [comment]);
  }
  return (
    <form action={create}>
      <input type="text" placeholder="write a comment" name="comment" />
      <button type="submit">Submit</button>
    </form>
  );
}
```

</CodeTabs>

## Run the app

Execute the following command to run your application locally:

```shell
npm run dev
```

<NeedHelp />


# How to upload to S3 in Next.js and save references in Postgres

---
title: How to upload to S3 in Next.js and save references in Postgres
subtitle: Let users upload files directly to S3 by creating presigned URLs in Next.js and saving the references in a Postgres database.
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-05-16T00:00:00.000Z'
updatedOn: '2024-05-16T00:00:00.000Z'
---

In this guide, you will learn how to add a feature to a Next.js app that allows users to upload files to Amazon S3, and insert the references to them in Postgres (powered by Neon) via `pg` and `@neondatabase/serverless`.

## Steps

- [Create a Neon project](#create-a-neon-project)
- [Store your Neon credentials](#store-your-neon-credentials)
- [Create an Amazon S3 Bucket](#create-an-amazon-s3-bucket)
- [Create access keys for IAM users (in AWS)](#create-access-keys-for-iam-users-in-aws)
- [Create a new Next.js application](#create-a-new-nextjs-application)
- [Create a Presigned URL with Amazon S3 SDK](#create-a-presigned-url-with-amazon-s3-sdk)
- [Save Reference to S3 objects in Postgres](#save-reference-to-s3-objects-in-postgres)
- [Upload to Presigned URL with in-browser JavaScript](#upload-to-presigned-url-with-in-browser-javascript)
- [Run the app](#run-the-app)

## Create a Neon project

If you do not have one already, create a Neon project.

1. Navigate to the [Projects](https://console.neon.tech/app/projects) page in the Neon Console.
2. Click **New Project**.
3. Specify your project settings and click **Create Project**.
4. Copy the database connection string to add to your Next.js app later. The connection string looks like `postgres://[user]:[password]@[neon_hostname]/[dbname]` and can be found in the **Connection Details** widget on the Neon **Dashboard**.

## Create an Amazon S3 Bucket

Open the [Amazon S3 Bucket](https://console.aws.amazon.com/s3), and click **Create bucket**.

![](/guides/images/s3-1.png)

Enter a repository name, say `my-custom-bucket-0` for example. Copy the bucket name to be used as **AWS_S3_BUCKET_NAME** in your application.

```shell shouldWrap
AWS_S3_BUCKET_NAME="my-custom-bucket-0"
```

![](/guides/images/s3-2.png)

In the **Policy** section, use the following json to define the actions allowed with the bucket:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "PublicReadGetObject",
      "Effect": "Allow",
      "Principal": "*",
      "Action": ["s3:PutObject", "s3:GetObject"],
      "Resource": "arn:aws:s3:::launchfast-bucket-0/*"
    }
  ]
}
```

In the **CORS** section, use the following json to define the actions allowed with the bucket:

```json
[
  {
    "AllowedHeaders": ["*"],
    "AllowedMethods": ["GET", "PUT"],
    "AllowedOrigins": ["*"],
    "ExposeHeaders": [],
    "MaxAgeSeconds": 9000
  }
]
```

Finally, complete the bucket creation process by clicking the **Create bucket** at the end.

## Create access keys for IAM users (in AWS)

In the navigation bar on the upper right in your AWS account, click on your name, and then choose **Security credentials**.

![](/guides/images/iam-1.png)

Scroll down to **Access keys** and click on **Create access key**.

![](/guides/images/iam-2.png)

Again, click on **Create access key**.

![](/guides/images/iam-3.png)

Copy the Access key and Secret access key, you will add them to your Next.js project later.

```shell shouldWrap
AWS_KEY_ID="..."
AWS_SECRET_ACCESS_KEY=".../...+"
```

![](/guides/images/iam-4.png)

## Create a new Next.js application

Let’s get started by creating a new Next.js project. Open your terminal and run the following command:

```shell shouldWrap
npx create-next-app@latest my-app
```

When prompted, choose:

- `Yes` when prompted to use TypeScript.
- `No` when prompted to use ESLint.
- `Yes` when prompted to use Tailwind CSS.
- `No` when prompted to use `src/` directory.
- `Yes` when prompted to use App Router.
- `No` when prompted to customize the default import alias (`@/*`).

Once that is done, move into the project directory and start the app in developement mode by executing the following command:

```shell shouldWrap
cd my-app
npm run dev
```

The app should be running on [localhost:3000](http://localhost:3000). Stop the development server to install the libraries necessary to build the application:

```shell shouldWrap
npm install @aws-sdk/client-s3 @aws-sdk/s3-request-presigner @neondatabase/serverless
```

The command installed the following libraries:

- `@aws-sdk/client-s3`: AWS SDK for JavaScript S3 Client for Node.js, Browser and React Native.
- `@aws-sdk/s3-request-presigner`: SDK to generate signed url for S3.
- `@neondatabase/serverless`: Neon's PostgreSQL driver for JavaScript and TypeScript.

Now, create a `.env` file at the root of your project. You are going to add the credentials you obtained earlier.

It should look something like this:

```shell shouldWrap
# AWS Environment Variables
AWS_KEY_ID="..."
AWS_SECRET_ACCESS_KEY=".../...+"
AWS_S3_BUCKET_NAME="...-bucket-0"

# Postgres (powered by Neon) Environment Variable
DATABASE_URL="postgresql://neondb_owner:...@...-pooler.us-east-2.aws.neon.tech/neondb?sslmode=require"
```

Now, let's move on to creating an API route to obtain a presigned URL to upload objects to.

## Create a Presigned URL with Amazon S3 SDK

[Presigned URLs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-presigned-url.html) allow you to upload large chunks of data directly at the source (here, `Amazon S3`).

This saves you from a couple limitations of a server-based upload operation:

- maximum request payload restrictions (on a hosting service, especially in serverless)
- huge RAM required to process multiple large file buffers at the same time

You will create an API endpoint that accepts the file name and it's content type to be uploaded via a presigned URL. In Next.js, you can create an API endpoint by creating a `route.ts` file at any directory level inside the `app` directory. To use `/api/presigned` as the desired API route, create a file `app/api/presigned/route.ts` with the following code:

```tsx
// File: app/api/presigned/route.ts

import { NextResponse, type NextRequest } from 'next/server';

export async function GET(request: NextRequest) {
  const accessKeyId = process.env.AWS_KEY_ID;
  const secretAccessKey = process.env.AWS_SECRET_ACCESS_KEY;
  const s3BucketName = process.env.AWS_S3_BUCKET_NAME;
  if (!accessKeyId || !secretAccessKey || !s3BucketName) {
    return new Response(null, { status: 500 });
  }
  const searchParams = request.nextUrl.searchParams;
  const fileName = searchParams.get('fileName');
  const contentType = searchParams.get('contentType');
  if (!fileName || !contentType) {
    return new Response(null, { status: 500 });
  }
}
```

The code above defines a `GET` handler that validates the presence of all the environment variables required, and the file name and it's content type.

Next, append the following code to return a JSON from the endpoint containing the presigned URL as `signedUrl`:

```tsx {4,5,20-34}
// File: app/api/presigned/route.ts

import { NextResponse, type NextRequest } from 'next/server';
import { getSignedUrl } from '@aws-sdk/s3-request-presigner';
import { S3Client, PutObjectCommand } from '@aws-sdk/client-s3';

export async function GET(request: NextRequest) {
  const accessKeyId = process.env.AWS_KEY_ID;
  const secretAccessKey = process.env.AWS_SECRET_ACCESS_KEY;
  const s3BucketName = process.env.AWS_S3_BUCKET_NAME;
  if (!accessKeyId || !secretAccessKey || !s3BucketName) {
    return new Response(null, { status: 500 });
  }
  const searchParams = request.nextUrl.searchParams;
  const fileName = searchParams.get('fileName');
  const contentType = searchParams.get('contentType');
  if (!fileName || !contentType) {
    return new Response(null, { status: 500 });
  }
  const client = new S3Client({
    region: 'eu-north-1',
    credentials: {
      accessKeyId,
      secretAccessKey,
    },
  });
  const command = new PutObjectCommand({
    Bucket: s3BucketName,
    Key: fileName,
    ContentType: contentType,
  });
  const signedUrl = await getSignedUrl(client, command, { expiresIn: 3600 });
  if (signedUrl) return NextResponse.json({ signedUrl });
  return new Response(null, { status: 500 });
}
```

The code above creates an S3 client using the `@aws-sdk/client-s3` SDK. Then, it uses the `getSignedUrl` utility (from `@aws-sdk/s3-request-presigner`) to sign the URL.

Now, let's move on to building an endpoint to insert the reference to the uploaded object in Postgres.

## Save Reference to S3 objects in Postgres

You will create an API endpoint that accepts the URL to the publicly accessible object. In this example, we'll create a table in Postgres, and associate the object URL with a user, for demonstration purposes. To use `/api/user/image` as the desired API route, create a file `app/api/user/image/route.ts` with the following code:

<CodeTabs reverse={true} labels={["node-postgres", "postgres.js", "Neon serverless driver"]}>

```tsx {3,10-11,14,16,18-21}
// File: app/api/user/image/route.ts

import { Client } from 'pg';
import { NextResponse, type NextRequest } from 'next/server';

export async function POST(request: NextRequest) {
  const { objectUrl } = await request.json();
  if (!process.env.DATABASE_URL) return new Response(null, { status: 500 });
  // Create a client instance using `node-postgres`
  const client = new Client(process.env.DATABASE_URL);
  await client.connect();
  try {
    // Create the user table if it does not exist
    await client.query('CREATE TABLE IF NOT EXISTS "user" (name TEXT, image TEXT)');
    // Mock call to get the user
    const user = 'rishi'; // getUser();
    // Insert the user name and the reference to the image into the user table
    await client.query('INSERT INTO "user" (name, image) VALUES ($1, $2)', [user, objectUrl]);
    return NextResponse.json({ code: 1 });
  } catch (e) {
    return NextResponse.json({
      code: 0,
      message: e instanceof Error ? e.message : e?.toString(),
    });
  }
}
```

```tsx {3,10-11,14,16,18-21}
// File: app/api/user/image/route.ts

import postgres from 'postgres';
import { NextResponse, type NextRequest } from 'next/server';

export async function POST(request: NextRequest) {
  const { objectUrl } = await request.json();
  if (!process.env.DATABASE_URL) return new Response(null, { status: 500 });
  // Create a client instance
  const sql = postgres(process.env.DATABASE_URL, { ssl: 'require' });
  try {
    // Create the user table if it does not exist
    await sql`CREATE TABLE IF NOT EXISTS "user" (name TEXT, image TEXT)`;
    // Mock call to get the user
    const user = 'rishi'; // getUser();
    // Insert the user name and the reference to the image into the user table
    await sql`INSERT INTO "user" (name, image) VALUES (${user}, ${objectUrl})`;
    return NextResponse.json({ code: 1 });
  } catch (e) {
    return NextResponse.json({
      code: 0,
      message: e instanceof Error ? e.message : e?.toString(),
    });
  }
}
```

```tsx {3,12,14,16-19}
// File: app/api/user/image/route.ts

import { neon } from '@neondatabase/serverless';
import { NextResponse, type NextRequest } from 'next/server';

export async function POST(request: NextRequest) {
  const { objectUrl } = await request.json();
  if (!process.env.DATABASE_URL) return new Response(null, { status: 500 });
  const sql = neon(process.env.DATABASE_URL);
  try {
    // Create the user table if it does not exist
    await sql('CREATE TABLE IF NOT EXISTS "user" (name TEXT, image TEXT)');
    // Mock call to get the user
    const user = 'rishi'; // getUser();
    // Insert the user name and the reference to the image into the user table
    await sql('INSERT INTO "user" (name, image) VALUES ($1, $2)', [user, objectUrl]);
    return NextResponse.json({ code: 1 });
  } catch (e) {
    return NextResponse.json({
      code: 0,
      message: e instanceof Error ? e.message : e?.toString(),
    });
  }
}
```

</CodeTabs>

The code above defines a POST endpoint, which first validates the presence of `DATABASE_URL` environment variable. Further, it creates a table named `user` if it does not exist, and inserts the record for a user named `rishi` with the object URL.

Now, let's move on to learning how to call these APIs in the front-end built with React.

## Upload to Presigned URL with in-browser JavaScript

With the API routes defined, the flow to upload the objects and save references to it in the database, is in three steps:

### 1. Accept a file from the user

Using the HTML `<input />` element, accept a file from the user to be uploaded to S3. Attach a listener to change in the file attached to upload programtically.

```tsx
// File: app/page.tsx

'use client';

import { ChangeEvent } from 'react';

export default function Home() {
  const uploadFile = (e: ChangeEvent<HTMLInputElement>) => {
    const file: File | null | undefined = e.target.files?.[0];
    if (!file) return;
    const reader = new FileReader();
    reader.onload = async (event) => {
      const fileData = event.target?.result;
      if (fileData) {
        // Fetch presigned URL and save reference in Postgres (powered by Neon)
      }
    };
    reader.readAsArrayBuffer(file);
  };
  return <input onChange={uploadFile} type="file" />;
}
```

### 2. Fetch the Presigned URL using the file name and type

Perform a GET call to `/api/presigned` API route with the file name and type as the query params. Obtain the presigned URL, and then upload the file as a Blob to it.

```tsx {15-28}
// File: app/page.tsx

'use client';

import { ChangeEvent } from 'react';

export default function Home() {
  const uploadFile = (e: ChangeEvent<HTMLInputElement>) => {
    const file: File | null | undefined = e.target.files?.[0];
    if (!file) return;
    const reader = new FileReader();
    reader.onload = async (event) => {
      const fileData = event.target?.result;
      if (fileData) {
        const presignedURL = new URL('/api/presigned', window.location.href);
        presignedURL.searchParams.set('fileName', file.name);
        presignedURL.searchParams.set('contentType', file.type);
        fetch(presignedURL.toString())
          .then((res) => res.json())
          .then((res) => {
            const body = new Blob([fileData], { type: file.type });
            fetch(res.signedUrl, {
              body,
              method: 'PUT',
            }).then(() => {
              // Save reference to the object in Postgres (powered by Neon)
            });
          });
      }
    };
    reader.readAsArrayBuffer(file);
  };
  return <input onChange={uploadFile} type="file" />;
}
```

### 3. Insert the reference to the object in the Postgres

Perform a `POST` to the `/api/user/image` route, with the presigned URL configured to **not contain the query parameters**. The stripped URL is an absolute reference to the publicly available object uploaded.

```tsx {26-32}
// File: app/page.tsx

'use client';

import { ChangeEvent } from 'react';

export default function Home() {
  const uploadFile = (e: ChangeEvent<HTMLInputElement>) => {
    const file: File | null | undefined = e.target.files?.[0];
    if (!file) return;
    const reader = new FileReader();
    reader.onload = async (event) => {
      const fileData = event.target?.result;
      if (fileData) {
        const presignedURL = new URL('/api/presigned', window.location.href);
        presignedURL.searchParams.set('fileName', file.name);
        presignedURL.searchParams.set('contentType', file.type);
        fetch(presignedURL.toString())
          .then((res) => res.json())
          .then((res) => {
            const body = new Blob([fileData], { type: file.type });
            fetch(res.signedUrl, {
              body,
              method: 'PUT',
            }).then(() => {
              fetch('/api/user/image', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                  objectUrl: res.signedUrl.split('?')[0],
                }),
              });
            });
          });
      }
    };
    reader.readAsArrayBuffer(file);
  };
  return <input onChange={uploadFile} type="file" />;
}
```

## Run the app

Execute the following command to run your application locally:

```shell
npm run dev
```

You should now be able to go through the entire workflow of selecting a file, uploading it to S3, and referencing it later by saving it in the database.

<NeedHelp />


# Using Payload CMS with Neon Postgres to Build an E-commerce Store in Next.js

---
title: Using Payload CMS with Neon Postgres to Build an E-commerce Store in Next.js
subtitle: Build your own E-commerce Store in a Next.js application with Payload CMS and Postgres (powered by Neon).
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-06-06T00:00:00.000Z'
updatedOn: '2024-06-06T00:00:00.000Z'
---

In this guide, you will learn how to set up a serverless Postgres database with Neon, configure Payload CMS with Postgres, and seed the Postgres database using the pre-populated information in Payload CMS Ecommerce template.

## Prerequisites

To follow the steps in this guide, you will need the following:

- [Node.js 18](https://nodejs.org/en) or later
- A [Neon](https://console.neon.tech/signup) account

## Steps

- [Provisioning a Serverless Postgres database powered by Neon](#provisioning-a-serverless-postgres-database-powered-by-neon)
- [Create a new Payload CMS application with Next.js](#create-a-new-payload-cms-application-with-nextjs)
- [Seed your Postgres database](#seed-your-postgres-database)
- [Build and Test your E-commerce Store (locally)](#build-and-test-your-e-commerce-store-locally)
- [Scale-to-zero with Postgres (powered by Neon)](#scale-to-zero-with-postgres-powered-by-neon)

## Provisioning a Serverless Postgres database powered by Neon

Using a serverless Postgres database powered by Neon lets you scale down to zero, which helps you save on compute costs.

To get started, go to the [Neon console](https://console.neon.tech/app/projects) and create a project.

Enable **Pooled connection** in the **Connection String** section of the **Connection Details** panel to obtain the Postgres connection string.

![](/guides/images/payload/98592ce7-3b8a-411b-a769-a0b89eaac8a3.png)

All Neon connection strings have the following format:

```bash
postgres://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>?sslmode=require
```

- `<user>` is the database user.
- `<password>` is the database user’s password.
- `<endpoint_hostname>.neon.tech` is the host with `neon.tech` as the [top-level domain (TLD)](https://www.cloudflare.com/en-gb/learning/dns/top-level-domain/).
- `<port>` is the Neon port number. The default port number is 5432.
- `<dbname>` is the name of the database. **neondb** is the default database created with each Neon project if you do not define your own.
- `?sslmode=require` is an optional query parameter that enforces [SSL](https://www.cloudflare.com/en-gb/learning/ssl/what-is-ssl/) mode for better security when connecting to the Postgres instance.

Save the connecting string somewhere safe. You will use it later to configure the `POSTGRES_URL` variable.

## Create a new Payload CMS application with Next.js

Let's begin with creating a Payload CMS backend to serve all the content for your e-commerce store in Next.js. Open your terminal and run the following command:

```bash
npx create-payload-app@latest payload-neon-ecommerce-store
```

`npx create-payload-app` is the recommended way to scaffold a Payload CMS + Next.js project quickly.

When prompted, choose the following:

![Welcome to Payload command line](/guides/images/payload/6c1f1650-7cc6-4b37-b293-611ba32dc6cc.png)

- `ecommerce` as the project template.
- `PostgreSQL (beta)` as the database.
- The connection string you obtained earlier as the PostgreSQL connection string: `postgres://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>?sslmode=require` .

Once that's done, change to the project directory and start the app:

```bash
cd payload-neon-ecommerce-store
yarn && yarn dev
```

![E-commerce template page](/guides/images/payload/e736400e-e52a-4b28-bb61-7f10fa7c2bc4.png)

The app should be running on [localhost:3000](http://localhost:3000). Let's keep the development server running as we work through the next steps.

Next, let's move on to adding e-commerce seed data to your Neon Postgres database.

## Seed your Postgres database

The `ecommerce` template is pre-configured with a seed database. To add the seed data to your Postgres database, navigate to the Payload CMS admin console at [localhost:3000/admin](http://localhost:3000/admin). Enter only the required attributes: **Email**, **Password**, **Name** and **Role**.

![Sign-in page](/guides/images/payload/fd54ff4f-400b-43fb-a08d-6f4fb0f8dd99.png)

Once you are inside the admin view, click **Seed your database** to start the process of seeding your database with e-commerce data.

![Welcome to your dashboard page](/guides/images/payload/086ae87d-d994-4fbf-b2fd-031ac711a4d1.png)

Once you see the following message in your terminal, you are done with adding seed data to the database.

```bash
INFO (payload): Seeded database successfully!
```

Now, let's move on to building the application and previewing it in action.

## Build and Test your E-commerce Store (Locally)

To test the e-commerce store in action, prepare a build and run the preview server using the following command:

```bash
yarn build && yarn serve
```

The app should now be running on [localhost:3000](http://localhost:3000). Navigate to http://localhost:3000/products/online-course to view the product display page of **Online Course** product.

![Online course page](/guides/images/payload/906a90a5-a17c-4573-8e45-87b67606f0c6.png)

Congratulations! You have now completed creating your own e-commerce store ✨

## Scale-to-zero with Postgres (powered by Neon)

Interestingly, during the entire process of setting up this app, you were using Neon's **Scale-to-zero** feature, which places your Postgres compute endpoint into an idle state when the database is inactive for more than 5 minutes. Click the **Operations** button in your Neon console sidebar to see when the compute was started and automatically suspended to reduce compute usage.

![Neon Monitoring page](/guides/images/payload/74a2aa54-6d28-4f47-b181-077957df6779.png)

## Summary

In this guide, you learned how to build an e-commerce store in Next.js using Payload CMS and a serverless Postgres database (powered by Neon). Furthermore, using Postgres (powered by Neon) allowed you to save on cloud compute resources when the compute endpoint that runs Postgres was idle for more than 5 minutes.

<NeedHelp />


# Real-Time Notifications using pg_notify with Neon Postgres

---
title: Real-Time Notifications using pg_notify with Neon Postgres
subtitle: A step-by-step guide describing how to implement real-time notifications using pg_notify in Postgres
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-07-02T13:24:36.612Z'
updatedOn: '2024-07-02T13:24:36.612Z'
---

This step-by-step guide shows how you can implement real-time notifications in Postgres (powered by Neon). Real-time notifications provide a way to instantly notify users in an application. With [pg_notify](https://www.postgresql.org/docs/current/sql-notify.html) and [Postgres triggers](https://www.postgresql.org/docs/current/triggers.html), you can create a webhook-like system to invoke external services on specific database operations.

## Prerequisites

To follow the steps in this guide, you will need the following:

- [Node.js 18](https://nodejs.org/en/blog/announcements/v18-release-announce) or later
- A [Neon](https://console.neon.tech/signup) account

## Steps

- [Provisioning a Postgres database powered by Neon](#provisioning-a-postgres-database-powered-by-neon)
- [Creating a new Node.js application](#creating-a-new-nodejs-application)
- [Set up triggers](#set-up-triggers)
- [Set up a Notifications Listener](#set-up-a-notifications-listener)
- [Notify using triggers](#notify-using-triggers)

## Provisioning a Postgres database powered by Neon

To get started, go to the [Neon console](https://console.neon.tech/app/projects) and enter the name of your choice as the project name.

You will then be presented with a dialog that provides a connecting string of your database. Make sure to **uncheck** the **Pooled connection checkbox** on the top right of the dialog and the connecting string automatically updates in the box below it.

![](/guides/images/pg-notify/index.png)

All Neon connection strings have the following format:

```bash
postgres://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>
```

- `user` is the database user.
- `password` is the database user’s password.
- `endpoint_hostname` is the host with neon.tech as the [TLD](https://www.cloudflare.com/en-gb/learning/dns/top-level-domain/).
- `port` is the Neon port number. The default port number is 5432.
- `dbname` is the name of the database. “neondb” is the default database created with each Neon project.
- `?sslmode=require` an optional query parameter that enforces the [SSL](https://www.cloudflare.com/en-gb/learning/ssl/what-is-ssl/) mode while connecting to the Postgres instance for better security.

Save this connecting string somewhere safe to be used as the `DATABASE_URL` further in the guide. Proceed further in this guide to create a Node.js application.

## Creating a new Node.js application

To start building the application, create a new Node.js project. Open your terminal and run the following command:

```bash
npm init -y
```

Further, execute the following command to install the dependencies to read the environment variables and connect to Postgres:

```bash
npm install pg dotenv
```

The libraries installed include:

- `pg`: A Postgres client for Node.js.
- `dotenv`: A library for handling environment variables.

Now, let's move on to setting up event triggers that will send notifications upon insertion of a row in a specific table.

## Set up triggers

To set up event triggers for a specific table (say `my_table`), you will define a trigger function called `my_trigger_function`. Create a file named `setup.js` with the following code:

```js shouldWrap
// File: setup.js

// Load all the environment variables
require('dotenv').config();

const { Client } = require('pg');

const connectionString = process.env.DATABASE_URL;

const client = new Client({ connectionString });

async function setupTrigger() {
  try {
    // Connect to Postgres
    await client.connect();
    // Create a my_table if it does not already exist
    await client.query(`CREATE TABLE IF NOT EXISTS
    my_table (id SERIAL PRIMARY KEY, message TEXT)`);
    // Define the my_trigger_function function to send notifications
    await client.query(`
    CREATE OR REPLACE FUNCTION my_trigger_function() RETURNS trigger AS $$
    BEGIN
      PERFORM pg_notify('channel_name', NEW.message);
      RETURN NEW;
    END;
    $$ LANGUAGE plpgsql;`);
    // Create the my_trigger to call the my_trigger_function after each insert
    await client.query(`
    CREATE TRIGGER my_trigger
    AFTER INSERT ON my_table
    FOR EACH ROW
    EXECUTE FUNCTION my_trigger_function();`);
    console.log('Event triggers setup complete.');
    await client.end();
  } catch (e) {
    console.log(e);
  }
}

setupTrigger().catch(console.log);
```

In the code above, `my_trigger_function` function uses `pg_notify` to send a notification to the channel named `my_channel` with the content of the newly inserted `message`. Finally, it creates a trigger named `my_trigger` that will execute this function after each insertion in table named `my_table`. This ensures that any new message inserted into the table triggers the notification.

To execute the script as above, run the following command:

```bash
node setup.js
```

Now, let's move to setting up a notifications listener in Node.js.

## Set up a Notifications Listener

To listen for notifications in Node.js intended for the channel named `my_channel`, create a file `listen.js` with the following code:

```js
// File: listen.js

// Load all the environment variables
require('dotenv').config();

const { Client } = require('pg');

const connectionString = process.env.DATABASE_URL;

const client = new Client({ connectionString });

async function listenToNotifications() {
  try {
    // Connect to Postgres
    await client.connect();
    // Listen to specific channel in Postgres
    // Attach a listener to notifications received
    client.on('notification', (msg) => {
      console.log('Notification received', msg.payload);
    });
    await client.query('LISTEN channel_name');
    console.log('Listening for notifications on my_channel');
  } catch (e) {
    console.log(e);
  }
}

listenToNotifications().catch(console.log);
```

The code above begins with importing `pg` and loading all the enviroment variables into scope. Further, it initializes a client connection to your Postgres. In the `listenToNotifications` function, it sets up a listener to notifications using `client.on('notification', ...)` callback. To invoke the callback, it starts listening for notifications to channel named `my_channel`, using `LISTEN my_channel` command.

To keep listening to the notifications, you would want to keep running the following command:

```bash
node listen.js
```

Now, let's insert a row to invoke the triggers that will notify the listeners.

## Notify using triggers

To notify the listeners, you will use Postgres triggers. To programtically trigger an event that will be created upon insertion into the table named `my_table`, create a file `send.js` with the following code:

```js
// File: send.js

// Load all the environment variables
require('dotenv').config();

const { Client } = require('pg');

const connectionString = process.env.DATABASE_URL;

const client = new Client({ connectionString });

async function insertRow(message) {
  try {
    // Connect to Postgres
    await client.connect();
    // Insert a row into Postgres table
    await client.query('INSERT INTO my_table (message) VALUES ($1)', [message]);
    console.log("Inserted a row in the 'my_table' table.");
    await client.end();
  } catch (e) {
    console.log(e);
  }
}

insertRow('Hello, world!').catch(console.log);
```

The code above begins with importing `pg` and loading all the enviroment variables into scope. Further, it initializes a client connection to your Postgres. In the `insertRow` function, it simply inserts a row into the table named `my_table`.

To execute the script as above, run the following command:

```bash
node send.js
```

<Admonition type="note" title="Note">
By default, Neon scales to zero after 5 minutes of inactivity, which ends any running sessions. As a result, `NOTIFY` and `LISTEN` commands only persist for the duration of the current session and are lost when the session ends.
</Admonition>

## Summary

In this guide, you learned how to receive and send real-time notifications using `pg_notify` in Serverless Postgres. Using Postgres triggers, you can selectively listen to changes happening in specific database table(s), and perform a function that invokes `pg_notify` to send out the notifications to the connected listeners.

## Source code

You can find the source code for the application described in this guide on GitHub.

<DetailIconCards>
<a href='https://github.com/neondatabase/examples/tree/main/with-nodejs-pg-notify' description='pg_notify with Node.js and Neon' icon='github'>Set up notifications using pg_notify with Node.js and Neon</a>
</DetailIconCards>

<NeedHelp />


# Using pgAdmin4 with a Hosted Postgres

---
title: Using pgAdmin4 with a Hosted Postgres
subtitle: A comprehensive guide on how to manage your Postgres database using pgAdmin4.
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-11-29T00:00:00.000Z'
updatedOn: '2024-11-29T00:00:00.000Z'
---

pgAdmin4 is a powerful web-based administration tool for managing PostgreSQL databases. This guide will walk you through the steps to set up and use pgAdmin4 with a hosted Postgres database, enabling you to perform various database operations efficiently.

## Table of Contents

- [Setting Up pgAdmin4](#setting-up-pgadmin4)
- [Connecting to Your Hosted Postgres Database](#connecting-to-your-hosted-postgres-database)
- [Basic Operations in pgAdmin4](#basic-operations-in-pgadmin4)

## Setting Up pgAdmin4

1. **Download and Install pgAdmin4**: If you haven't already, download pgAdmin4 from the [official website](https://www.pgadmin.org/download/). Follow the installation instructions for your operating system.

2. **Launch pgAdmin4**: Open pgAdmin4 from your applications menu or web browser.

## Provisioning a Serverless Postgres

To get started, go to the [Neon console](https://console.neon.tech/app/projects) and enter the name of your choice as the project name.

![](/guides/images/pg-notify/index.png)

All Neon connection strings have the following format:

```bash
postgres://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>
```

- `user` is the database user.
- `password` is the database user’s password.
- `endpoint_hostname` is the host with neon.tech as the [TLD](https://www.cloudflare.com/en-gb/learning/dns/top-level-domain/).
- `port` is the Neon port number. The default port number is 5432.
- `dbname` is the name of the database. “neondb” is the default database created with each Neon project.
- `?sslmode=require` an optional query parameter that enforces the [SSL](https://www.cloudflare.com/en-gb/learning/ssl/what-is-ssl/) mode while connecting to the Postgres instance for better security.

You will be using these connecting string components further in the guide. Proceed further in this guide to connect pgAdmin4 to your Postgres.

## Connecting to Your Hosted Postgres Database

1. **Open pgAdmin4**: Once pgAdmin4 is running, you will see the dashboard.

2. **Create a New Server Connection**:

   - Right-click on "Servers" in the left sidebar and select "Create" > "Server...".
   - In the "Create - Server" dialog, enter a name for your server connection.

3. **Configure Connection Settings**:

   - Go to the "Connection" tab.
   - Enter the following details:
     - **Host**: The endpoint of your hosted Postgres database (e.g., `ep-...us-east-2.aws.neon.tech`).
     - **Port**: The port number (default is `5432`).
     - **Maintenance database**: Your database name.
     - **Username**: Your database username.
     - **Password**: Your database password (you can save the password if desired).

4. **Save the Connection**: Click "Save" to create the server connection. You should now see your server listed in the left sidebar.

![](/guides/images/pg-notify/pgAdmin4.png)

## Basic Operations in pgAdmin4

### 1. Running SQL Queries

- Click on your database in the left sidebar.
- Click on the "Query Tool" icon (or right-click the database and select "Query Tool").
- Enter your SQL queries in the editor and click the "Execute" button (play icon) to run them.

### 2. Managing Tables

- Expand your database in the left sidebar, then expand the "Schemas" > "public" > "Tables" section.
- Right-click on "Tables" to create a new table or manage existing ones.

### 3. Importing and Exporting Data

- To import data, right-click on a table and select "Import/Export".
- Follow the prompts to upload a CSV file or export data to a file.

## Conclusion

pgAdmin4 is an essential tool for managing your hosted Postgres database. With its user-friendly interface, you can easily perform various database operations, from creating databases and tables to running complex queries. By following this guide, you should be well-equipped to utilize pgAdmin4 effectively.

<NeedHelp />


# Building a Real-Time AI Voice Assistant with ElevenLabs

---
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-12-17T00:00:00.000Z'
updatedOn: '2024-12-17T00:00:00.000Z'
title: Building a Real-Time AI Voice Assistant with ElevenLabs
subtitle: A step-by-step guide to building your own AI Voice Assistant in a Next.js application with ElevenLabs and Postgres
---

Imagine having an AI voice assistant like Iron Man's [J.A.R.V.I.S.](https://en.wikipedia.org/wiki/J.A.R.V.I.S.), capable of understanding and responding to your needs in real-time. In this guide, you will learn how to build your very own real-time AI voice assistant using ElevenLabs, store each conversation in a Postgres database, and index them for faster retrieval.

<Admonition type="note" title="Deal alert">
Take advantage of the [AI Engineer Starter Pack](https://www.aiengineerpack.com) by ElevenLabs to get discounts for the tools used in this guide.
</Admonition>

## Prerequisites

To follow this guide, you’ll need the following:

- [Node.js 18](https://nodejs.org/en) or later
- A [Neon](https://console.neon.tech/signup) account
- A [ElevenLabs](https://elevenlabs.io/) account
- A [Vercel](https://vercel.com) account

## Create a new Next.js application

Let’s get started by creating a new Next.js project with the following command:

```shell shouldWrap
npx create-next-app@latest pulse
```

When prompted, choose:

- `Yes` when prompted to use TypeScript.
- `No` when prompted to use ESLint.
- `Yes` when prompted to use Tailwind CSS.
- `No` when prompted to use `src/` directory.
- `Yes` when prompted to use App Router.
- `No` when prompted to use Turbopack for `next dev`.
- `No` when prompted to customize the default import alias (`@/*`).

Once that is done, move into the project directory and install the necessary dependencies with the following command:

```shell
cd pulse
npm install @11labs/react @neondatabase/serverless motion framer-motion react-feather sonner
npm install -D tsx
```

The libraries installed include:

- `framer-motion`: A library for animations in React.
- `react-feather`: A collection of open-source icons for React.
- `motion`: A library to create animations in React applications.
- `sonner`: A notification library for React to display toast notifications.
- `@11labs/react`: A React library to interact with [ElevenLabs API](https://elevenlabs.io/api).
- `@neondatabase/serverless`: A library to connect and interact with Neon’s serverless Postgres database.

The development-specific libraries include:

- `tsx`: To execute and rebuild TypeScript efficiently.

## Provision a Serverless Postgres

To set up a serverless Postgres, go to the [Neon console](https://console.neon.tech/app/projects) and create a new project. Once your project is created, you will receive a connection string that you can use to connect to your Neon database. The connection string will look like this:

```bash shouldWrap
postgresql://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>?sslmode=require
```

Replace `<user>`, `<password>`, `<endpoint_hostname>`, `<port>`, and `<dbname>` with your specific details.

Use this connection string as an environment variable designated as `DATABASE_URL` in the `.env` file.

## Create an AI Agent with ElevenLabs

To create a customizable agent, go to ElevenLabs' [AI Agents](https://elevenlabs.io/app/conversational-ai) and then click on `Create an AI agent` button.

![](/guides/images/pulse/agent-1.png)

Next, give it a personalized name and select the kind of Agent you would want. For demonstration purposes, let's start with a `Blank template`.

![](/guides/images/pulse/agent-2.png)

Next, copy the Agent ID displayed just below the customized name of your agent (here, `Custom`). You will use this Agent ID as the `AGENT_ID` environment variable in your application.

![](/guides/images/pulse/agent-3.png)

Next, go to `Advanced > Client Events` in your Agent settings, and add two events named `agent_response` and `user_transcript`.

![](/guides/images/pulse/agent-4.png)

Finally, go to [API Keys](https://elevenlabs.io/app/settings/api-keys), create an API key and use the value obtained as `XI_API_KEY` enviroment variable in your application.

## Database Schema Setup

Create a file named `schema.tsx` at the root of your project directory with the following code:

```tsx
// File: schema.tsx

import { neon } from '@neondatabase/serverless';
import 'dotenv/config';

const createMessagesTable = async () => {
  if (!process.env.DATABASE_URL) throw new Error(`DATABASE_URL environment variable not found.`);
  const sql = neon(process.env.DATABASE_URL);
  try {
    await sql(
      `CREATE TABLE IF NOT EXISTS messages (created_at SERIAL, id TEXT PRIMARY KEY, session_id TEXT, content_type TEXT, content_transcript TEXT, object TEXT, role TEXT, status TEXT, type TEXT);`
    );
    await sql(
      `CREATE INDEX IF NOT EXISTS idx_session_created_at ON messages (session_id, created_at);`
    );
    console.log('Setup schema succesfully.');
  } catch (error) {
    console.error(error);
    console.log('Failed to set up schema.');
  }
};

createMessagesTable();
```

The code above defines an asynchronous function `createMessagesTable` that connects to a Neon serverless Postgres database using a connection string stored in the `DATABASE_URL` environment variable, creates a `messages` table if it doesn't already exist, and sets up an index on the `session_id` and `created_at` columns for faster retrievals.

To run the migrations, execute the following command:

```
npx tsx schema.tsx
```

If it runs succesfully, you should see `Setup schema succesfully.` in the terminal.

## Build Reusable React Components and Hooks

### 1. Typing Effect Animation

To enhance the user experience by simulating real-time interactions, implement a typing effect in the UI to render AI responses incrementally. Create a file named `useTypingEffect.ts` in the `components` directory with the following code:

```tsx
// File: components/useTypingEffect.ts

import { useEffect, useState } from 'react';

export const useTypingEffect = (text: string, duration: number = 50, isTypeByLetter = false) => {
  const [currentPosition, setCurrentPosition] = useState(0);
  const items = isTypeByLetter ? text.split('') : text.split(' ');
  useEffect(() => {
    setCurrentPosition(0);
  }, [text]);
  useEffect(() => {
    if (currentPosition >= items.length) return;
    const intervalId = setInterval(() => {
      setCurrentPosition((prevPosition) => prevPosition + 1);
    }, duration);
    return () => {
      clearInterval(intervalId);
    };
  }, [currentPosition, items, duration]);
  return items.slice(0, currentPosition).join(isTypeByLetter ? '' : ' ');
};
```

The provided code exports a custom React hook called `useTypingEffect`. This hook simulates a typing effect for a specified text over a given duration, enhancing the user interface by rendering text incrementally.

### 2. Conversation Message

To render each message in the conversation history, you need to dynamically indicate whether the message is from the User or the AI. Create a file named `Message.tsx` in the `components` directory with the following code:

```tsx
// File: components/Message.tsx

import { Cpu, User } from 'react-feather';

export default function ({
  conversationItem,
}: {
  conversationItem: { role: string; formatted: { transcript: string } };
}) {
  return (
    <div className="flex max-w-full flex-row flex-wrap items-start gap-x-3">
      <div className="max-w-max rounded border p-2">
        {conversationItem.role === 'user' ? <User /> : <Cpu />}
      </div>
      <div className="flex flex-col gap-y-2">{conversationItem.formatted.transcript}</div>
    </div>
  );
}
```

The code above exports a React component that renders a message. It conditionally displays a `Cpu` icon for messages from the AI and a `User` icon for messages from the user, along with the message content.

### 3. Various States During AI Interaction

Create a file named `TextAnimation.tsx` in the `components` directory with the following code:

```tsx
// File: components/TextAnimation.tsx

'use client';

import { useTypingEffect } from '@/components/useTypingEffect';
import { motion } from 'framer-motion';
import { useEffect, useState } from 'react';

type AIState = 'idle' | 'listening' | 'speaking';

interface Props {
  onStartListening?: () => void;
  onStopListening?: () => void;
  isAudioPlaying?: boolean;
  currentText: string;
}

export default function AiTalkingAnimation({
  onStartListening,
  onStopListening,
  isAudioPlaying,
  currentText,
}: Props) {
  const [aiState, setAiState] = useState<AIState>('idle');
  const animatedCurrentText = useTypingEffect(currentText, 20);
  const displayedText = useTypingEffect('Click the circle to start the conversation', 20);

  const handleCircleClick = () => {
    if (aiState === 'listening' || aiState === 'speaking') {
      onStopListening?.();
      setAiState('idle');
    } else if (!isAudioPlaying) {
      onStartListening?.();
      setAiState('listening');
    }
  };

  useEffect(() => {
    if (isAudioPlaying) setAiState('speaking');
    else if (aiState === 'speaking' && currentText) setAiState('listening');
  }, [isAudioPlaying]);

  return (
    <div className="bg-gray-100 flex min-h-screen flex-col items-center justify-center p-4">
      <div
        role="button"
        onClick={handleCircleClick}
        className="relative mb-8 cursor-pointer"
        aria-label={aiState === 'listening' ? 'Stop listening' : 'Start listening'}
      >
        <motion.div
          className="from-pink-500 to-violet-600 flex h-20 w-20 items-center justify-center rounded-full bg-gradient-to-br shadow-lg"
          animate={
            aiState === 'idle'
              ? { scale: [1, 1.1, 1] }
              : aiState === 'speaking'
                ? { scale: [1, 1.2, 0.8, 1.2, 1] }
                : {}
          }
          transition={{
            repeat: Infinity,
            ease: 'easeInOut',
            duration: aiState === 'speaking' ? 0.8 : 1.5,
          }}
        />
        {aiState === 'listening' && (
          <svg
            viewBox="0 0 100 100"
            className="absolute left-1/2 top-1/2 h-24 w-24 -translate-x-1/2 -translate-y-1/2"
          >
            <motion.circle
              cx="50"
              cy="50"
              r="48"
              fill="none"
              strokeWidth="4"
              stroke="#8B5CF6"
              transition={{
                duration: 10,
                ease: 'linear',
                repeat: Infinity,
              }}
              strokeLinecap="round"
              initial={{ pathLength: 0, rotate: -90 }}
              animate={{ pathLength: 1, rotate: 270 }}
            />
          </svg>
        )}
      </div>
      <div className="w-full max-w-md rounded-lg bg-white p-6 shadow-lg">
        <p className="text-gray-800 font-mono text-lg" aria-live="polite">
          {aiState === 'listening'
            ? 'Listening...'
            : aiState === 'speaking'
              ? animatedCurrentText
              : displayedText}
        </p>
        {aiState === 'idle' && (
          <motion.div
            animate={{
              opacity: [0, 1, 0],
            }}
            transition={{
              duration: 0.8,
              repeat: Infinity,
              ease: 'easeInOut',
            }}
            className="bg-violet-600 mt-2 h-5 w-2"
          />
        )}
      </div>
    </div>
  );
}
```

The code above exports a React component that creates an interactive UI for the AI voice assistant. It utilizes the `useTypingEffect` hook to simulate a typing effect for the AI's responses and displays different states of interaction, such as "idle," "listening," and "speaking." The component also includes a clickable circle that toggles between starting and stopping the listening state, providing visual feedback through animations.

## Generate a Signed URL for private conversations with ElevenLabs

To create a secure access between user and AI (powered by ElevenLabs), create a new file named `route.ts` in the `app/api/i` directory with the following code:

```tsx
// File: app/api/i/route.ts

export const runtime = 'edge';

export const dynamic = 'force-dynamic';

export const fetchCache = 'force-no-store';

import { NextResponse } from 'next/server';

export async function POST(request: Request) {
  const agentId = process.env.AGENT_ID;
  if (!agentId) throw Error('AGENT_ID is not set or received.');
  const apiKey = process.env.XI_API_KEY;
  if (!apiKey) throw Error('XI_API_KEY is not set or received.');
  try {
    const apiUrl = new URL('https://api.elevenlabs.io/v1/convai/conversation/get_signed_url');
    apiUrl.searchParams.set('agent_id', agentId);
    const response = await fetch(apiUrl.toString(), {
      headers: { 'xi-api-key': apiKey },
    });
    if (!response.ok) throw new Error(response.statusText);
    const data = await response.json();
    return NextResponse.json({ apiKey: data.signed_url });
  } catch (error) {
    // @ts-ignore
    const message = error.message || error.toString();
    return NextResponse.json({ error: message }, { status: 500 });
  }
}
```

The code above defines an API route that generates a signed URL using ElevenLabs API. You will want to use signed URL instead of connecting to a fixed point server so as to allow connection to your personalized, private agents created in ElevenLabs.

## Sync Conversations to a Postgres database

Create a file named `route.ts` in the `app/api/c` directory with the following code:

```tsx
// File: app/api/c/route.ts

export const runtime = 'edge';

export const dynamic = 'force-dynamic';

export const fetchCache = 'force-no-store';

import { neon, neonConfig } from '@neondatabase/serverless';
import { NextResponse } from 'next/server';

neonConfig.poolQueryViaFetch = true;

export async function POST(request: Request) {
  const { id, item } = await request.json();
  if (!id || !item || !process.env.DATABASE_URL) return NextResponse.json({}, { status: 400 });
  const sql = neon(process.env.DATABASE_URL);
  const rows = await sql('SELECT COUNT(*) from messages WHERE session_id = $1', [id]);
  await sql(
    'INSERT INTO messages (created_at, id, session_id, content_type, content_transcript, object, role, status, type) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9) ON CONFLICT DO NOTHING',
    [
      rows[0].count,
      item.id,
      id,
      item.content[0].type,
      item.content[0].transcript,
      item.object,
      item.role,
      item.status,
      item.type,
    ]
  );
  return NextResponse.json({});
}

export async function GET(request: Request) {
  const id = new URL(request.url).searchParams.get('id');
  if (!id || !process.env.DATABASE_URL) return NextResponse.json([]);
  const sql = neon(process.env.DATABASE_URL);
  const rows = await sql('SELECT * from messages WHERE session_id = $1', [id]);
  return NextResponse.json(rows);
}
```

The code above defines two endpoint handlers on `/api/c`:

- A `POST` endpoint that allows you to insert a new message into the `messages` table. It expects a JSON payload containing the `id` of the session and the `item` to be inserted. If the session ID or item is missing, it returns a 400 status code.

- A `GET` endpoint that retrieves all messages associated with a specific session ID. It extracts the session ID from the request URL and queries the `messages` table, returning the results as a JSON response. If the session ID is not provided, it returns an empty array.

## Create the UI for Starting Conversations and Synchronizing Chat History

Create a file named `page.tsx` in the `app/c/[slug]` directory with the following code:

```tsx
// File: app/c/[slug]/page.tsx

'use client';

import { toast } from 'sonner';
import { useParams } from 'next/navigation';
import { useCallback, useEffect, useState } from 'react';
import { type Role, useConversation } from '@11labs/react';

export default function () {
  const { slug } = useParams();
  const [currentText, setCurrentText] = useState('');
  const [messages, setMessages] = useState<any[]>([]);
  const loadConversation = () => {
    fetch(`/api/c?id=${slug}`)
      .then((res) => res.json())
      .then((res) => {
        if (res.length > 0) {
          setMessages(
            res.map((i: any) => ({
              ...i,
              formatted: {
                text: i.content_transcript,
                transcript: i.content_transcript,
              },
            }))
          );
        }
      });
  };
  const conversation = useConversation({
    onError: (error: string) => {
      toast(error);
    },
    onConnect: () => {
      toast('Connected to ElevenLabs.');
    },
    onMessage: (props: { message: string; source: Role }) => {
      const { message, source } = props;
      if (source === 'ai') setCurrentText(message);
      fetch('/api/c', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          id: slug,
          item: {
            type: 'message',
            status: 'completed',
            object: 'realtime.item',
            id: 'item_' + Math.random(),
            role: source === 'ai' ? 'assistant' : 'user',
            content: [{ type: 'text', transcript: message }],
          },
        }),
      }).then(loadConversation);
    },
  });
  const connectConversation = useCallback(async () => {
    toast('Setting up ElevenLabs...');
    try {
      await navigator.mediaDevices.getUserMedia({ audio: true });
      const response = await fetch('/api/i', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
      });
      const data = await response.json();
      if (data.error) return toast(data.error);
      await conversation.startSession({ signedUrl: data.apiKey });
    } catch (error) {
      toast('Failed to set up ElevenLabs client :/');
    }
  }, [conversation]);
  const disconnectConversation = useCallback(async () => {
    await conversation.endSession();
  }, [conversation]);
  const handleStartListening = () => {
    if (conversation.status !== 'connected') connectConversation();
  };
  const handleStopListening = () => {
    if (conversation.status === 'connected') disconnectConversation();
  };
  useEffect(() => {
    return () => {
      disconnectConversation();
    };
  }, [slug]);
  return <></>;
}
```

The code above does the following:

- Defines a `loadConversation` function which calls the `/api/c` route to fetch the conversation history based on the particular slug (i.e. the conversation ID).
- Uses the `useConversation` hook by ElevenLabs to display the toast when the instance is connected, and to sync the real-time message to Postgres using the `onMessage` callback.
- Defines a `connectConversation` function that instantiates a private conversation with the agent after obtaining a signed URL using the `/api/i` route.
- Defines a `disconnectConversation` function that disconnects the ongoing conversation with the agent.
- Creates a `useEffect` handler which on unmount, ends the ongoing conversation with the agent.

Next, import the `TextAnimation` component which displays different state of the conversation, whether AI is listening or speaking (and what if so).

```tsx ins={4,10-15}
'use client';

// ... Existing imports ...
import TextAnimation from '@/components/TextAnimation';

export default function () {
  // ... Existing code ...
  return (
    <>
      <TextAnimation
        currentText={currentText}
        onStopListening={handleStopListening}
        onStartListening={handleStartListening}
        isAudioPlaying={conversation.isSpeaking}
      />
    </>
  );
}
```

Finally, add a `Show Transcript` button that displays the conversation history stored in Neon to the user.

```tsx ins={4,5,9,13-37}
'use client';

// ... Existing imports ...
import { X } from 'react-feather';
import Message from '@/components/Message';

export default function () {
  // ... Existing code ...
  const [isTranscriptOpen, setIsTranscriptOpen] = useState(false);
  return (
    <>
      {/* Existing code */}
      {messages.length > 0 && (
        <button
          className="fixed right-4 top-2 text-sm underline"
          onClick={() => setIsTranscriptOpen(!isTranscriptOpen)}
        >
          Show Transcript
        </button>
      )}
      {isTranscriptOpen && (
        <div className="fixed inset-0 z-50 flex items-center justify-center bg-black bg-opacity-50">
          <div className="max-h-[90%] max-w-[90%] overflow-y-scroll rounded bg-white p-4 text-black shadow-lg">
            <div className="flex flex-row items-center justify-between">
              <span>Transcript</span>
              <button onClick={() => setIsTranscriptOpen(false)}>
                <X />
              </button>
            </div>
            <div className="mt-4 flex flex-col gap-y-4 border-t py-4">
              {messages.map((conversationItem) => (
                <Message key={conversationItem.id} conversationItem={conversationItem} />
              ))}
            </div>
          </div>
        </div>
      )}
    </>
  );
}
```

Now, let's move on to deploying the application to Vercel.

## Deploy to Vercel

The repository is now ready to deploy to Vercel. Use the following steps to deploy:

- Start by creating a GitHub repository containing your app's code.
- Then, navigate to the Vercel Dashboard and create a **New Project**.
- Link the new project to the GitHub repository you've just created.
- In **Settings**, update the **Environment Variables** to match those in your local `.env` file.
- Deploy.

<DetailIconCards>

<a target="_blank" href="https://github.com/neondatabase-labs/pulse" description="A Real-Time AI Voice Assistant" icon="github">Pulse</a>

</DetailIconCards>

## Summary

In this guide, you learned how to build a real-time AI voice assistant using ElevenLabs and Next.js, integrating it with a Postgres database to store and retrieve conversation histories. You explored the process of setting up a serverless database, creating a customizable AI agent, and implementing a user-friendly interface with animations and message handling. By the end, you gained hands-on experience connecting various technologies to create a fully functional AI voice assistant application.

<NeedHelp />


# Query your Postgres Database Using Azure Functions

---
title: Query your Postgres Database Using Azure Functions
subtitle: Learn how to query your Postgres database using Azure Functions
author: adalbert-pungu
enableTableOfContents: true
createdAt: '2024-10-19T21:00:00.000Z'
---

In this guide, we will explore how to query a **Postgres** database hosted on **Neon** using **Azure Functions**. This combination allows you to take advantage of a flexible, high-performance infrastructure without worrying about server management.

## Prerequisites

You will need:

- An [Azure](https://azure.microsoft.com/) account with a subscription to deploy Azure Functions.
- A Neon account. If you don’t have one yet, you can [sign up](https://console.neon.tech/signup).
- Basic knowledge of Node.js and SQL.
- Familiarity with using Visual Studio Code.

## Why Neon?

Neon stands out as a cloud-native Postgres solution with an innovative architecture that separates compute and storage, offering a truly serverless database. This means Neon automatically adjusts its resources based on your application’s needs, making it ideal for projects that require flexible scalability without directly managing the infrastructure. In other words, Neon allows you to accelerate project delivery by focusing solely on development, while having an infrastructure that scales on demand.

> Neon Database is cloud-native Serverless Postgres, meaning it has completly separated storage from compute. This means Neon automatically adjusts its resources based on your application’s needs, making it ideal for projects that require flexible scaling without directly managing the infrastructure. In other words, Neon Database is a fully managed database service that allows you to focus on building your application without worrying about the underlying infrastructure.

Azure Functions are also a serverless compute service, which enables you to run event-driven code without having to manage infrastructure. It is a great choice for building serverless applications, microservices, and APIs. By combining Neon Database with Azure Functions, you can create a powerful and scalable application that can handle a wide range of use cases.

At the same time, Azure Functions enables you to run code in response to events without worrying about the underlying infrastructure. It will create microservices that respond to events, such as **HTTP requests**, without the need to deploy or manage servers.

To illustrate this, we will discuss an example of client management (hotel reservation management), which is a common use case in application development. We will use the technologies mentioned above to query and process data.

## Context

Imagine you are developing a solution to manage hotel reservations. You want to allow users (via an app or website) to view available reservations and interact with a Postgres database hosted on **Neon**.

The application's features will include:

- **View available rooms**: The application will allow users to check available hotel rooms for booking.
- **Add a new reservation**: When a customer makes a reservation, their information will be stored in the Neon.
- **Cancel a reservation**: Customers can cancel a reservation by deleting the corresponding record from the database.

---

## Step 1: Create and Configure the Database on Neon

**Sign up and create the database**

Sign up on [Neon](https://neon.tech/) and follow the steps to create a Postgres database. The database will be named **neondb**.

After creating the database, make sure to copy the connection details (such as **host**, **user**, **password**, **database**) somewhere safe, as they will be used to configure **Azure Functions** to connect to **Neon**.

1. **Creating the tables**

   Once the database is created, you should see an option named "SQL Editor" on the left to write and execute queries.

   In the query editor, copy and paste the SQL code below to create the `clients` and `hotels` tables. These are reference tables, as the `reservations` table will refer to these tables via foreign keys:

   ```sql
   CREATE TABLE clients (
       client_id SERIAL PRIMARY KEY,
       first_name VARCHAR(100),
       last_name VARCHAR(100),
       email VARCHAR(100),
       phone_number VARCHAR(20)
   );

   CREATE TABLE hotels (
       hotel_id SERIAL PRIMARY KEY,
       name VARCHAR(100),
       location VARCHAR(100)
   );
   ```

   Here is the SQL script to create the `reservations` table:

   ```sql
   CREATE TABLE reservations (
       reservation_id SERIAL PRIMARY KEY,
       client_id INT NOT NULL,
       hotel_id INT NOT NULL,
       check_in_date DATE NOT NULL,
       check_out_date DATE NOT NULL,
       number_of_guests INT NOT NULL,
       created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
       FOREIGN KEY (client_id) REFERENCES clients(client_id),
       FOREIGN KEY (hotel_id) REFERENCES hotels(hotel_id)
   );
   ```

2. **Inserting test data**

   You can insert some example data into the database to ensure that everything is working fine up to this point.

   Here is the SQL script to insert data into the `clients` table:

   ```sql
   INSERT INTO clients (first_name, last_name, email, phone_number) VALUES
   ('Alice', 'Dupont', 'alice.dupont@example.com', '0123456789'),
   ('Bob', 'Martin', 'bob.martin@example.com', '0987654321'),
   ('Chloé', 'Lefevre', 'chloe.lefevre@example.com', '0147253689');
   ```

   Here is the SQL script to insert data into the `hotels` table:

   ```sql
   INSERT INTO hotels (name, location) VALUES
   ('Hôtel Le Paris', 'Paris'),
   ('Hôtel des Alpes', 'Annecy'),
   ('Hôtel de la Plage', 'Nice');
   ```

   Here is the SQL script to insert data into the `reservations` table:

   ```sql
   INSERT INTO reservations (client_id, hotel_id, check_in_date, check_out_date, number_of_guests) VALUES
   (1, 1, '2024-11-01', '2024-11-05', 2),
   (2, 2, '2024-11-10', '2024-11-15', 1),
   (3, 3, '2024-12-01', '2024-12-10', 4);
   ```

## Step 2: Create an Azure Function to Manage Products

1.  **Sign in to Azure**

    If you don't already have an account, sign up on the Microsoft [Azure](https://portal.azure.com/) portal.

    We will initialize an Azure Functions project where we will create an **HTTP Trigger function** in Visual Studio Code (VS Code) using the **Azure Functions extension**.

2.  **Install the Azure Functions extension**:

    - Open VS Code, or install [Visual Studio Code](https://code.visualstudio.com/) if it's not yet installed.
    - Go to the extensions tab or press `Ctrl+Shift+X`.
    - Search for "Azure Functions" and install the official extension.

3.  **Create an Azure Functions Project**

    Open the command palette or press `Ctrl+Shift+P` to open the command palette.

    - Type `Azure Functions: Create New Project...` and select that option.
    - Choose a directory where you want to create the project.
    - Select the programming language (`JavaScript` in our case).
    - Choose a JavaScript programming model (`Model V4`).
    - Choose a function template, and select `HTTP trigger`.
    - Give your function a name, for example, `manageClients`.

    Once confirmed, the project will be created with some default code.

4.  **Install the Postgres client**

    In the terminal of your Azure Functions project, install either [Neon serverless driver](/docs/serverless/serverless-driver) or the `node-postgres` (`pg`) package, which will be used to connect to Postgres:

    <CodeTabs labels={["Neon serverless driver", "node-postgres"]}>

    ```bash
    npm install @neondatabase/serverless
    ```

    ```bash
    npm install pg
    ```

    </CodeTabs>

5.  **Azure Functions Core Tools**

    Install Azure Functions Core Tools to run functions locally.

    ```bash
    npm install -g azure-functions-core-tools@4 --unsafe-perm true
    ```

    <Admonition type="note" title="suggested folder structure">

    Since there are three tables in the database (`Clients`, `Hotels`, and `Reservations`), using a separate file for each feature or interaction with the database is a good practice to maintain clear and organized code.

    ```
    src/
      ├──index.js
      ├──functions/
      │   ├──manageClients.js
      │   ├──manageHotels.js
      │   └──manageReservations.js
      └──database/
          ├──client.js
          ├──hotel.js
          └──reservation.js
    ```

    </Admonition>

6.  **Configure Environment Variables**

    On the Neon dashboard, go to `Connection string`, select `Node.js`, and click `.env`. Then, click `show password` and copy the database connection string. If you don't click `show password`, you'll copy a connection string without the password (which is masked).

    Create a `.env` file at the root of the project to store your database connection information from the Neon.

    Here's an example of the connection string you'll copy:

    ```bash shouldWrap
    DATABASE_URL='postgresql://neondb_owner:************@ep-quiet-leaf-a85k5wbg.eastus2.azure.neon.tech/neondb?sslmode=require'
    ```

7.  **Modify the `local.settings.json` file**

    The `local.settings.json` file is used by Azure Functions for **local executions**. Azure Functions does not directly read the `.env` file. Instead, it relies on `local.settings.json` to inject environment variable values during local execution. In production, you will define the same settings through `App Settings` in the Azure portal.

    Here's an example of the `local.settings.json` file :

    ```JSON
    {
      "IsEncrypted": false,
      "Values": {
        "AzureWebJobsStorage": "",
        "FUNCTIONS_WORKER_RUNTIME": "node",
        "DATABASE_URL": "postgresql://neondb_owner:************@ep-quiet-leaf-a85k5wbg.eastus2.azure.neon.tech/neondb?sslmode=require"
      }
    }
    ```

    Install the `dotenv` package by opening the terminal in your Azure Functions project. This package will allow you to load environment variables from the `.env` file:

    ```bash
    npm install dotenv
    ```

8.  **Manage Each Table**

    a. Create a separate file for each table in the `database/` folder.

    Here, you can use either the `neon` package or the `pg` package to connect to the database.

    **Example code for `client.js`**

    <CodeTabs labels={["Neon serverless driver", "node-postgres"]}>

    ```javascript
    import { neon } from '@neondatabase/serverless';
    import dotenv from 'dotenv';

    dotenv.config();

    const sql = neon(process.env.DATABASE_URL);

    const getAllClients = async () => {
      const rows = await sql`SELECT * FROM clients`;
      return rows;
    };

    const addClient = async (first_name, last_name, email, phone_number) => {
      const [newClient] = await sql`
        INSERT INTO clients (first_name, last_name, email, phone_number)
        VALUES (${first_name}, ${last_name}, ${email}, ${phone_number})
        RETURNING *`;
      return newClient;
    };

    export { getAllClients, addClient };
    ```

    ```javascript
    const { Client } = require('pg');
    require('dotenv').config();

    const client = new Client({
      connectionString: process.env.DATABASE_URL,
    });

    const connectDB = async () => {
      if (!client._connected) {
        await client.connect();
      }
    };

    const getAllClients = async () => {
      await connectDB();
      const result = await client.query('SELECT * FROM clients');
      return result.rows;
    };

    const addClient = async (first_name, last_name, email, phone_number) => {
      await connectDB();
      const result = await client.query(
        `INSERT INTO clients (first_name, last_name, email, phone_number)
            VALUES ($1, $2, $3, $4)
            RETURNING *`,
        [first_name, last_name, email, phone_number]
      );
      return result.rows[0];
    };

    module.exports = {
      getAllClients,
      addClient,
    };
    ```

    </CodeTabs>

    **Example code for `hotel.js`**

    <CodeTabs labels={["Neon serverless driver", "node-postgres"]}>

    ```javascript
    import { neon } from '@neondatabase/serverless';
    import dotenv from 'dotenv';

    dotenv.config();

    const sql = neon(process.env.DATABASE_URL);

    const getAllHotels = async () => {
      const rows = await sql`SELECT * FROM hotels`;
      return rows;
    };

    export { getAllHotels };
    ```

    ```javascript
    const { Client } = require('pg');
    require('dotenv').config();

    const client = new Client({
        connectionString: process.env.DATABASE_URL,
    });

    const connectDB = async () => {
        if (!client.\_connected) {
            await client.connect();
        }
    };

    const getAllHotels = async () => {
        await connectDB();
        const result = await client.query('SELECT \* FROM hotels');
        return result.rows;
    };

    module.exports = {
        getAllHotels,
        client,
    };
    ```

    </CodeTabs>

    **Example code for `reservation.js`**

    <CodeTabs labels={["Neon serverless driver", "node-postgres"]}>

    ```javascript
    import { neon } from '@neondatabase/serverless';
    import dotenv from 'dotenv';

    dotenv.config();

    const sql = neon(process.env.DATABASE_URL);

    const getAvailableReservations = async () => {
      const rows = await sql`
            SELECT * FROM reservations WHERE status = ${'available'}
        `;
      return rows;
    };

    export { getAvailableReservations };
    ```

    ```javascript
    const { Client } = require('pg');
    require('dotenv').config();

    const client = new Client({
      connectionString: process.env.DATABASE_URL,
    });

    const connectDB = async () => {
      if (!client._connected) {
        await client.connect();
      }
    };

    const getAvailableReservations = async () => {
      await connectDB();
      const result = await client.query('SELECT * FROM reservations WHERE status = $1', [
        'available',
      ]);
      return result.rows;
    };

    module.exports = {
      getAvailableReservations,
      client,
    };
    ```

    </CodeTabs>

    b. Modify the `functions/` folder by adding the function files:

    In the `functions/` folder, remove the default file, and then add three function management files (`manageClients.js`, `manageHotels.js`, and `manageReservations.js`).

    **Example for `manageClients.js`**

    ```javascript
    // src/functions/manageClients.js
    const { app } = require('@azure/functions');
    const { getAllClients, addClient } = require('../database/client');

    app.http('manageClients', {
      methods: ['GET', 'POST'],
      authLevel: 'anonymous',
      handler: async (request, context) => {
        context.log(`HTTP function processed request for url "${request.url}"`);

        if (request.method === 'GET') {
          try {
            const clients = await getAllClients();
            console.table(clients);
            return {
              body: clients,
            };
          } catch (error) {
            context.log('Error fetching clients:', error);
            return {
              status: 500,
              body: 'Error retrieving clients.',
            };
          }
        }

        if (request.method === 'POST') {
          try {
            const { first_name, last_name, email, phone_number } = await request.json();

            if (!first_name || !last_name || !email || !phone_number) {
              return {
                status: 400,
                body: 'Missing required fields: first_name, last_name, email, phone_number.',
              };
            }

            const newClient = await addClient(first_name, last_name, email, phone_number);
            console.table(newClient);
            return {
              status: 201,
              body: newClient,
            };
          } catch (error) {
            context.log('Error adding client:', error);
            return {
              status: 500,
              body: 'Error adding client.',
            };
          }
        }
      },
    });
    ```

    **Example for `manageHotels.js`**

    ```javascript
    // src/functions/manageHotels.js
    const { app } = require('@azure/functions');
    const { getAllHotels } = require('../database/hotel');

    app.http('manageHotels', {
      methods: ['GET'],
      authLevel: 'anonymous',
      handler: async (request, context) => {
        context.log(`HTTP function processed request for url "${request.url}"`);

        try {
          const hotels = await getAllHotels();
          return {
            body: hotels,
          };
        } catch (error) {
          context.log('Error fetching hotels:', error);
          return {
            status: 500,
            body: 'Error retrieving hotels.',
          };
        }
      },
    });
    ```

    **Example for `manageReservations.js`**

    ```javascript
    // src/functions/manageReservations.js
    const { app } = require('@azure/functions');
    const { getAvailableReservations } = require('../database/reservation');

    app.http('manageReservations', {
      methods: ['GET'],
      authLevel: 'anonymous',
      handler: async (request, context) => {
        context.log(`HTTP function processed request for url "${request.url}"`);

        try {
          const reservations = await getAvailableReservations();
          return {
            body: reservations,
          };
        } catch (error) {
          context.log('Error fetching reservations:', error);
          return {
            status: 500,
            body: 'Error retrieving available reservations.',
          };
        }
      },
    });
    ```

    Feel free to extend this structure to include features such as adding new clients, creating new reservations, or even updating and deleting data, each with **its own file** and **its own logic**.

## Step 3: Test the Function Locally

1. **Run the Function Locally**:

   - Open the integrated terminal in VS Code.
   - Run the following command `npm run start`, which will execute `func start` to start the project and launch the functions:

     ```bash
     npm run start
     ```

2. **Test with a Browser or Postman**:

   - Open a browser and navigate to `http://localhost:7071/api/manageClients` to test your function.
   - You can also use a tool like **Postman** to send **HTTP requests**.

## Step 4: Test and Deploy the Function to Azure

1. **Deploy Your Function**:

   - Open the command palette with `Ctrl+Shift+P` and type `Azure Functions: Deploy to Function App...`.
   - Follow the instructions to select your Azure subscription and choose or create a Function App, then complete the deployment process.

2. **Test the Function**:

   Use a tool like Postman to send an HTTP request to the Azure Function, for example:

   ```arduino
   https://your-azure-function-url?client_id=1234
   ```

   This will return the information of the client with the ID **1234**, if present in the database.

## Conclusion

We have demonstrated how combining Neon and Azure Functions enables the development of fast, scalable applications while reducing the complexity associated with managing infrastructure. With this combination, you can efficiently query your Postgres database without worrying about server maintenance. Moreover, Neon simplifies the scalability of your applications, making it an ideal choice for many modern projects.

## Additional Resources

- [Neon Documentation](/docs) - Comprehensive documentation for Neon's database services, including guides, tutorials, and API references.
- [Azure Functions Documentation](https://learn.microsoft.com/en-us/azure/azure-functions/)

<NeedHelp />


# Building a Full-Stack Portfolio Website with a RAG Powered Chatbot

---
title: Building a Full-Stack Portfolio Website with a RAG Powered Chatbot
subtitle: Develop a modern React portfolio website featuring a chatbot powered by pgvector, Neon Postgres, FastAPI, and OpenAI.
author: sam-harri
enableTableOfContents: true
createdAt: '2024-10-17T00:00:00.000Z'
updatedOn: '2024-10-17T00:00:00.000Z'
---

In this guide, you will build a full-stack portfolio website using `React` for the frontend and `FastAPI` for the backend, featuring a Retrieval-Augmented Generation (RAG) chatbot that leverages `pgvector` on `Neon`'s serverless Postgres to store and retrieve embeddings created with `OpenAI`'s embedding model.

This project is perfect for showcasing technical skills through a portfolio that not only demonstrates front-end and back-end capabilities but also integrates AI to answer questions about your experience.

## Prerequisites

Before you start, ensure that you have the following tools and services ready:

- `pip`: This is required for installing and managing Python packages, including [uv](https://docs.astral.sh/uv/) for creating virtual environments. You can check if `pip` is installed by running the following command:
  ```bash
  pip --version
  ```
- Neon serverless Postgres : You will need a Neon account for provisioning and scaling your `PostgreSQL` database. If you don't have an account yet, [sign up here](https://console.neon.tech/signup).
- Node.js: Needed for developing the frontend using React, you can download it following the [official installation guide](https://nodejs.org/en/learn/getting-started/how-to-install-nodejs).
- OpenAI API key: You need access to the OpenAI API for generating embeddings, you can [sign up here](https://platform.openai.com/signup).

## Setting up the Backend

Follow these steps to set up your backend for the full-stack portfolio website:

1.  Create the project structure.

    Since this is a full-stack project, your backend and frontend will be in separate directories within a single parent folder. Begin by creating the parent folder and moving into it

    ```bash
    mkdir portfolio_project
    cd portfolio_project
    ```

2.  Create a `uv` Python virtual environment.

    If you don't already have uv installed, you can install it with:

    ```bash
    pip install uv
    ```

    Once `uv` is installed, create a new project:

    ```bash
    uv init portfolio_backend
    ```

    This will create a new project directory called `portfolio_backend`. Open this directory in your code editor of your choice.

3.  Set up the virtual environment.

        You will now create and activate a virtual environment in which your project's dependencies will be installed.

        <CodeTabs labels={["Linux/macOS", "Windows"]}>

            ```bash
            uv venv
            source .venv/bin/activate
            ```

            ```bash
            uv venv
            .venv\Scripts\activate
            ```

        </CodeTabs>

        You should see `(portfolio_backend)` in your terminal now, this means that your virtual environment is activated.

4.  Install dependencies.

    Next, add all the necessary dependencies for your project:

    ```bash
    uv add fastapi asyncpg uvicorn loguru python-dotenv openai pgvector
    ```

    where each package does the following:

    - `FastAPI` : A Web / API framework
    - `AsyncPG` : An asynchronous PostgreSQL client
    - `Uvicorn` : An ASGI server for our app
    - `Loguru` : A logging library
    - `Python-dotenv` : To load environment variables from a .env file
    - `openai` : The OpenAI API client for generating embeddings and chatbot responses
    - `pgvector` : A Python client for working with pgvector in PostgreSQL

5.  Create the project structure.

    Create the following directory structure to organize your project files:

    ```md
    portfolio_backend
    ├── src/
    │ ├── database/
    │ │ └── postgres.py
    │ ├── models/
    │ │ └── product_models.py
    │ ├── routes/
    │ │ └── product_routes.py
    │ └── main.py
    ├── .env  
    ├── .python-version
    ├── README.md  
    ├── pyproject.toml  
    └── uv.lock
    ```

## Setting up your Database

In this section, you will set up the `pgvector` extension using Neon's console, add the database's schema, and create the database connection pool and lifecycle management logic in FastAPI.

First, add pgvector to Postgres:

```sql
CREATE EXTENSION IF NOT EXISTS vector;
```

Next, add the schema to your database:

```sql
CREATE TABLE portfolio_embeddings (
    id SERIAL PRIMARY KEY,
    content TEXT NOT NULL,
    embedding VECTOR(1536) NOT NULL
);
```

This table will store the embeddings generated by OpenAI for the chatbot responses. The `content` column will store the text for which the embedding was generated, and the `embedding` column will store the 1536-dimensional vector. An embedding is a representation of text in a high-dimensional space that captures the meaning of the text, allowing you to compare and search for similar text.

With your schema in place, you're now ready to connect to your database in the FastAPI application. To do this you must create a `.env` file in the root of the project to hold environment-specific variables, such as the connection string to your Neon PostgreSQL database, and API keys.

```bash
DATABASE_URL=postgres://user:password@your-neon-hostname.neon.tech/neondb?sslmode=require
OPENAI_API_KEY=your-api-key
OPENAI_ORG_ID=your-org-id
OPENAI_PROJECT_ID=your-project-id
```

Make sure to replace the placeholders (user, password, your-neon-hostname, etc.) with your actual Neon database credentials, which are available in the console, and to fetch the OpenAI key from the OpenAI console.

In your project, the `database.py` file manages the connection to `PostgreSQL` using `asyncpg` and its connection pool, which is a mechanism for managing and reusing database connections efficiently. With this, you can use asynchronous queries, allowing the application to handle multiple requests concurrently.

```python
import os
import asyncpg
from loguru import logger
from typing import Optional
from pgvector.asyncpg import register_vector

conn_pool: Optional[asyncpg.Pool] = None


async def init_postgres() -> None:
    """
    Initialize the PostgreSQL connection pool
    """
    global conn_pool
    try:
        logger.info("Initializing PostgreSQL connection pool...")

        async def initalize_vector(conn):
            await register_vector(conn)

        conn_pool = await asyncpg.create_pool(
            dsn=os.getenv("DATABASE_URL"), init=initalize_vector
        )
        logger.info("PostgreSQL connection pool created successfully.")

    except Exception as e:
        logger.error(f"Error initializing PostgreSQL connection pool: {e}")
        raise


async def get_postgres() -> asyncpg.Pool:
    """
    Get a reference to the PostgreSQL connection pool.

    Returns
    -------
    asyncpg.Pool
        The connection pool object to the PostgreSQL database.
    """
    global conn_pool
    if conn_pool is None:
        logger.error("Connection pool is not initialized.")
        raise ConnectionError("PostgreSQL connection pool is not initialized.")
    try:
        return conn_pool
    except Exception as e:
        logger.error(f"Failed to return PostgreSQL connection pool: {e}")
        raise


async def close_postgres() -> None:
    """
    Close the PostgreSQL connection pool.
    """
    global conn_pool
    if conn_pool is not None:
        try:
            logger.info("Closing PostgreSQL connection pool...")
            await conn_pool.close()
            logger.info("PostgreSQL connection pool closed successfully.")
        except Exception as e:
            logger.error(f"Error closing PostgreSQL connection pool: {e}")
            raise
    else:
        logger.warning("PostgreSQL connection pool was not initialized.")

```

`init_postgres` is responsible for opening the connection pool to the `PostgreSQL` database and `close_postgres` is responsible for gracefully closing all connections in the pool when the `FastAPI` app shuts down to properly manage the lifecycle of the database.

Throughout your API you will also need access to the pool to get connection instances and run queries. `get_postgres` returns the active connection pool. If the pool is not initialized, an error is raised.

## Defining the Pydantic Models

Now, you will create the models that represent the data structures used in your API. These models will be used to validate the request and response data in your API endpoints. The API will use these models to serialize and deserialize data between the client and the server. For your API, you will need to complete a chat request and add embeddings to the database.

```python
from pydantic import BaseModel
from typing import Optional


class PortfolioEntryCreate(BaseModel):
    content: str


class PortfolioEntryResponse(BaseModel):
    id: int
    content: str
    embedding: Optional[list[float]]


class QueryRequest(BaseModel):
    query: str


class QueryResponse(BaseModel):
    content: str
    similarity: float
```

Each of the models represent the following:

- `PortfolioEntryCreate`: Represents the input for creating a new embedding
- `PortfolioEntryResponse`: Represents the output of a created embedding
- `QueryRequest`: Represents a question to ask the chatbot
- `QueryResponse`: Represents the response from the chatbot

## Creating the API Endpoints

In this section, you will create the API routes for adding new portfolio entries and chatting with the chatbot. The `add-portfolio-entry` route will add a new portfolio entry to the database and store its embedding. The `chat` route will use the stored embeddings to generate a response to a user query.

A typical RAG chatbot workflow involves first creating an embedding for the user query, then finding the most similar embeddings in the database, and finally using the context of these embeddings to generate a response. Here, embeddings are generated using OpenAI's text-embedding-3-small model, and the similarity between embeddings is calculated using the `<=>` operator in PostgreSQL, which represents the cosine similarity between two vectors. The chatbot response is then generated using OpenAI's GPT-4o-mini model.

```python
from fastapi import APIRouter, HTTPException, Depends
from models.models import PortfolioEntryCreate, PortfolioEntryResponse, QueryRequest
from database.postgres import get_postgres
import asyncpg
from typing import List
import os
from openai import OpenAI
import numpy as np

router = APIRouter()

client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    organization=os.getenv("OPENAI_ORG_ID"),
    project=os.getenv("OPENAI_PROJECT_ID"),
)


async def generate_embedding(content: str) -> List[float]:
    """
    Generate an embedding for the given content using OpenAI API.
    """
    try:
        content = content.replace("\n", " ")
        response = client.embeddings.create(
            input=[content],
            model="text-embedding-3-small",
        )
        return response.data[0].embedding
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error generating embedding: {e}")


@router.post("/add-entry/", response_model=PortfolioEntryResponse)
async def add_portfolio_entry(
    entry: PortfolioEntryCreate, pool: asyncpg.Pool = Depends(get_postgres)
):
    """
    Add a new portfolio entry and store its embedding in PostgreSQL.
    """
    try:
        embedding = await generate_embedding(entry.content)

        embedding_np = np.array(embedding)

        async with pool.acquire() as conn:
            row = await conn.fetchrow(
                """
                INSERT INTO portfolio_embeddings (content, embedding)
                VALUES ($1, $2)
                RETURNING id, content, embedding
                """,
                entry.content,
                embedding_np,
            )
            if row:
                return PortfolioEntryResponse(
                    id=row["id"], content=row["content"], embedding=row["embedding"]
                )
            else:
                raise HTTPException(
                    status_code=500, detail="Failed to insert entry into the database."
                )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to add entry: {e}")


@router.post("/chat/")
async def chat(query: QueryRequest, pool: asyncpg.Pool = Depends(get_postgres)):
    """
    Chat with the portfolio chatbot by retrieving relevant information from stored embeddings
    and using it as context to generate a response.
    """
    try:
        query_embedding = await generate_embedding(query.query)

        query_embedding_np = np.array(query_embedding)

        async with pool.acquire() as conn:
            rows = await conn.fetch(
                """
                SELECT content, embedding <=> $1 AS similarity
                FROM portfolio_embeddings
                ORDER BY similarity
                LIMIT 5
                """,
                query_embedding_np,
            )

            context = "\n".join([row["content"] for row in rows])

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "You are YOUR NAME, and you are answering questions as yourself, using 'I' and 'my' in your responses. "
                        "You should respond to questions about your portfolio, skills, experience, and education. For example, you should answer questions about specific technologies you've worked with, such as Java, React, or other tools. "
                        "If you have relevant experience with a technology, describe it concisely. For example, if asked about Java, describe your experience using it. "
                        "You should also answer questions about your education, including your experience at school, and your work in relevant industries. "
                        "However, if a question is completely unrelated to your professional experience, such as questions about recipes, trivia, or non-technical personal matters, respond with: 'That question isn't relevant to my experience or skills.' "
                        "Focus on answering technical and career-related questions, but only reject questions that are clearly off-topic."
                        "If they ask you about a technology you havent used, you can say: 'I haven't worked with that technology yet, but I'm always eager to learn new things.'"
                        "Answer any personal questions that are related to technology, like 'What are our favorite languages?' or 'What technology/language/anything tech are you most excited about?'"
                    ),
                },
                {
                    "role": "user",
                    "content": f"Context: {context}\n\nQuestion: {query.query}",
                },
            ],
            max_tokens=200,
            stream=False,
        )

        return {"response": response.choices[0].message.content}

    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Failed to process chat request: {e}"
        )

```

In the `chat` route, the chatbot is sent the text obtained from RAG and the user questions, but is also sent a system message that sets the context for the chatbot. You can customize this message to provide additional context or instructions to the chatbot, and to guide the chatbot's responses to your liking.

## Running the Application

After setting up the database, models, and API routes, the next step is to run the `FastAPI` application.

The `main.py` file defines the `FastAPI` application, manages the database lifecycle, and includes the routes you created above.

```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
from database.postgres import init_postgres, close_postgres
from routes.routes import router
import dotenv
import uvicorn


@asynccontextmanager
async def lifespan(app: FastAPI):
    dotenv.load_dotenv()
    await init_postgres()
    yield
    await close_postgres()


app: FastAPI = FastAPI(lifespan=lifespan, title="FastAPI Portfolio RAG ChatBot API")
app.include_router(router)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

```

Since you will be connecting your application from the frontend, you will need to allow CORS (Cross Origin Resource Sharing). The `CORSMiddleware` is added such that the API can accept requests from any origin, including your React app.

To run the application, use uvicorn CLI with the following command:

```bash
uvicorn main:app --host 0.0.0.0 --port 8000
```

## Adding Embeddings to the Database

You can now add embeddings to the database by sending a POST request to the `/add-entry` endpoint with the content you want to store. To do this, you can use a tool like `curl`,`Postman`, `httpie`, or any other HTTP client. For example, using `httpie`:

```bash shouldWrap
http POST http://localhost:8000/add-entry/ content="YOUR CONTENT"
```

For the best results, you should add a variety of portfolio entries that cover different aspects of your experience and skills. This will help the chatbot generate more accurate responses to user queries.

## Testing the Chatbot

Using `httpie`, you can test the chatbot by sending a POST request to the `/chat` endpoint with a user query. For example:

```bash shouldWrap
http POST http://localhost:8000/chat/ query="What is your favorite programming language?"
```

The chatbot will respond with a relevant answer based on the embeddings stored in the database, something like:

```json shouldWrap
{
  "response": "My favorite programming language right now is Rust, with Python coming in a close second. I love the performance and safety features of Rust, and the simplicity and readability of Python."
}
```

## Setting up the Frontend

Now that the backend is set up and running, it's time to set up the frontend using React. The frontend will be responsible for interacting with the FastAPI backend and displaying your portfolio, as well as allowing users to ask questions to the RAG-powered chatbot.

1. Clone the frontend repository.

   First, go back to the parent directory (portfolio_project) you created at the beginning of this guide and clone the frontend repository:

   ```bash
   cd ..
   git clone https://github.com/sam-harri/portfolio_frontend
   ```

   Then, open up that new directory in your code editor.

2. Install the dependencies.

   Once you have cloned the frontend repository, install the necessary dependencies using `npm`:

   ```bash
   npm install
   ```

   This will install all the packages specified in the `package.json` file, which are required for the React app to run.

3. Update the frontend content.

   Now, you will update the content of your portfolio, such as your bio, projects, and skills, and experience to match your personal details. Each of the section in the portfolio is a separate component that you can modify to include your own information.
   These include:

   - Landing: The landing page of the portfolio, which includes your name, bio, and a profile picture.
   - Experience: A section that lists your work experience, including the company name, logo, your position, and a brief description of your role.
   - Skills: A section that lists your technical skills, such as programming languages, frameworks, and tools you are proficient in. Find the logos of your technologies at [Devicon](https://devicon.dev/)
   - Projects: A section that lists your projects, including the project name, description, and a link to the project's GitHub repository or live demo.

   The chatbot component is responsible for sending user queries to the backend and displaying the chatbot responses, and can be found in the `Chatbot.tsx` file.

## Running the Frontend

Once you've updated the content, you can start the frontend development server to preview your portfolio website.

To start the development server, run the following command:

```bash
npm run dev
```

This will start the React development server at http://localhost:3000. Open your browser and navigate to that address to view your portfolio website.

Now, with the React app running, take a look at how the website appears. Ensure that the design, content, and overall presentation are what you expect. You can interact with the chatbot (which will not yet be functional until the backend is also running) to check the layout and form submission.

To test the chatbot functionality, you will need to have the backend running as well. To do this, open another console window, navigate to the `portfolio_backend` directory, and run the FastAPI application using the `uvicorn` command as shown earlier.
Once the API is running, you can interact with the chatbot on the frontend to test the chatbot functionality, try out some prompts, tweak the responses, and see how the chatbot performs.

## Optional: Running the Full-Stack Project Using Docker and Docker Compose

In this optional section, you will containerize both the frontend and backend of your full-stack portfolio website using Docker. By using Docker, you can ensure that your application runs consistently across different environments without worrying about dependencies and setups. Docker Compose will allow you to orchestrate running both services (frontend and backend) with a single command.

The only prerequisite for this section is having Docker installed on your machine. If you don't have it installed, you can follow the [Docker installation guide](https://docs.docker.com/engine/install/).

1. Dockerize the backend.

   To Dockerize the backend, you will create a `Dockerfile` in the `portfolio_backend` directory. This file will define the steps to build the Docker image for your FastAPI application.

   ```Dockerfile
   FROM python:3.10-slim

   ENV PYTHONDONTWRITEBYTECODE=1
   ENV PYTHONUNBUFFERED=1

   WORKDIR /app

   COPY . /app

   RUN pip install uv

   RUN uv pip install -r pyproject.toml --system

   WORKDIR /app/src

   EXPOSE 8000

   CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
   ```

   This Dockerfile:

   - Copies your FastAPI app into the container
   - Installs all necessary Python dependencies
   - Exposes port 8000, which is where FastAPI will run
   - Runs the FastAPI app using uvicorn

2. Dockerize the frontend.

   Dockerizing the frontend is similar to the backend. Create a `Dockerfile` in the root of the `portfolio_frontend` directory:

   ```Dockerfile
   FROM node:18-alpine AS base

   WORKDIR /app

   COPY package.json yarn.lock* package-lock.json* pnpm-lock.yaml* ./

   RUN \
   if [ -f yarn.lock ]; then yarn --frozen-lockfile; \
   elif [ -f package-lock.json ]; then npm ci; \
   elif [ -f pnpm-lock.yaml ]; then corepack enable pnpm && pnpm i --frozen-lockfile; \
   else echo "Lockfile not found." && exit 1; \
   fi

   COPY . .

   RUN npm run build

   RUN npm install -g serve

   EXPOSE 3000

   CMD ["serve", "-s", "dist"]
   ```

   This Dockerfile:

   - Uses Node.js to install frontend dependencies
   - Builds the React application
   - Serves the static build using the serve package
   - Exposes port 3000 where the React app will be available

3. Set up Docker Compose.

   Docker Compose simplifies the process of running multiple containers together. You can define both the frontend and backend in a single configuration and run them together with a single command.

   Below is the `docker-compose.yml` file, placed at the root of the project, which sets up both the services:

   ```yaml
   services:
   api:
     build:
     context: portfolio_backend/
     dockerfile: Dockerfile
     ports:
       - '8000:8000'
     env_file:
       - portfolio_backend/.env
   nextjs-app:
     build:
       context: portfolio_frontend/
       dockerfile: Dockerfile
     ports:
       - '3000:3000'
     environment:
       - NODE_ENV=production
     depends_on:
       - api
   ```

4. Run the application with Docker Compose.

   Once your Dockerfiles and Docker Compose file are ready, you can bring up the entire stack using:

   ```bash
   docker-compose up --build
   ```

   This command will:

   - Build the Docker images for both the backend and frontend
   - Start both the FastAPI backend (on port 8000) and the React frontend (on port 3000)
   - Automatically manage the service dependencies (the frontend will wait until the backend is up before starting)

   Using Docker and Docker Compose to run your full-stack portfolio website simplifies the process of managing dependencies and ensures consistency across different environments. You can now run your entire application, both frontend and backend, in isolated containers with a single command. This setup is also beneficial if you plan to deploy your application to production in a cloud environment or if you want to share the project with others who can run it without manual installation steps.

## Conclusion

You have successfully built and deployed a full-stack portfolio website powered by React, FastAPI, pgvector, Neon Postgres, and OpenAI. By leveraging OpenAI embeddings and the RAG-powered chatbot, you added an AI-driven layer to your portfolio that can dynamically answer questions about your projects, skills, and experience.

The next steps in the project could include deploying the application to a cloud platform like AWS, Azure, or Google Cloud, adding more features to the chatbot, or customizing the frontend design to match your personal style. You can also extend the chatbot's capabilities by fine-tuning it on more data or using a different model for generating responses.


# Scale your Django application with Neon Postgres Read Replicas

---
title: Scale your Django application with Neon Postgres Read Replicas
subtitle: Learn how to scale Django applications with Neon Postgres Read Replicas
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2024-10-20T00:00:00.000Z'
updatedOn: '2024-10-20T00:00:00.000Z'
---

[Neon read replicas](https://neon.tech/docs/introduction/read-replicas) are independent read-only compute instances that can significantly enhance database performance and scalability. By distributing read operations across these replicas, you can reduce latency and improve overall system responsiveness, especially for read-heavy applications. A standout feature of Neon is that adding a read replica doesn't require extra storage. This makes it a cost-effective way to scale your database, suitable for businesses of all sizes.

This guide explains how to integrate Neon read replicas into your Django application. You'll learn how to configure your Django database router to direct read operations to these replicas, optimizing your database performance and overall application speed.

## Prerequisites

Before you begin, make sure you have:

- A Neon account and project. If you don't have one, sign up for a Neon account and create a project by following the [Getting started guide](https://neon.tech/docs/get-started-with-neon/signing-up).
- Basic knowledge of [Django](https://docs.djangoproject.com/en) and Python.
- [Python](https://www.python.org/downloads/) installed on your local machine.

## Build the note-taking app

To demonstrate how to use Neon read replicas with Django, we'll build a simple note-taking application that uses a Neon database. We'll then update the application to use a read replica for read operations, improving the application's performance and scalability.

### Part 1: Build the initial note-taking app with a single database

#### Set up the project

Create a new Django project and app:

```bash
python -m venv venv
source venv/bin/activate # On Windows, use venv\Scripts\activate
pip install django psycopg2-binary
django-admin startproject django_notes
cd django_notes
python manage.py startapp notes
```

This creates a new virtual environment, installs Django, and sets up a new Django project called `django_notes`. We also create a new app called `notes`, which will contain the logic for managing notes. For the database driver, we use `psycopg2-binary` to connect to a PostgreSQL database.

#### Update settings

In `django_notes/settings.py`, add the following:

```python
INSTALLED_APPS = [
    # ... other apps
    'notes',
]

DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'your_neon_database_name',
        'USER': 'your_username',
        'PASSWORD': 'your_password',
        'HOST': 'your_neon_host',
        'PORT': '5432',
    }
}
```

The `INSTALLED_APPS` array is updated to include the `notes` app we just created. The `DATABASES` dictionary is also updated to use a PostgreSQL database on Neon.

#### Create the Note model

In `notes/models.py`, add:

```python
from django.db import models

class Note(models.Model):
    title = models.CharField(max_length=200)
    content = models.TextField()
    created_at = models.DateTimeField(auto_now_add=True)

    def __str__(self):
        return self.title
```

The `Note` model defines the structure for storing notes in the database. Each note will have a `title`, `content`, and a `created_at` field, which is automatically populated with the current timestamp when a note is created.

#### Create views

In `notes/views.py`, add:

```python
from django.shortcuts import render, redirect, get_object_or_404
from .models import Note
from django.views.decorators.http import require_http_methods

@require_http_methods(["GET", "POST"])
def create_note(request):
    if request.method == "POST":
        title = request.POST.get('title')
        content = request.POST.get('content')
        Note.objects.create(title=title, content=content)
        return redirect('list_notes')
    return render(request, 'notes/create_note.html')

def list_notes(request):
    notes = Note.objects.all().order_by('-created_at')
    return render(request, 'notes/list_notes.html', {'notes': notes})

@require_http_methods(["POST"])
def delete_note(request, note_id):
    note = get_object_or_404(Note, id=note_id)
    note.delete()
    return redirect('list_notes')
```

Three views are defined here:

- `create_note`: Handles both displaying the note creation form (GET request) and saving the new note (POST request).
- `list_notes`: Fetches and displays all the notes ordered by the creation time, showing the newest ones first.
- `delete_note`: Handles the deletion of a specific note based on its ID.

#### Set up URLs

In `django_notes/urls.py`:

```python
from django.urls import path
from notes import views

urlpatterns = [
    path('', views.list_notes, name='list_notes'),
    path('create/', views.create_note, name='create_note'),
    path('delete/<int:note_id>/', views.delete_note, name='delete_note'),
]
```

We define the URL patterns for the project. The default route displays the list of notes, `/create/` serves the note creation form, and `/delete/<note_id>/` handles the deletion of a specific note.

#### Create templates

Create `notes/templates/notes/base.html`:

```html
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>{% block title %}Django Notes{% endblock %}</title>
    <style>
      body {
        font-family: Arial, sans-serif;
        line-height: 1.6;
        margin: 0;
        padding: 20px;
      }
      .container {
        max-width: 800px;
        margin: 0 auto;
      }
      h1 {
        color: #333;
      }
      form {
        margin-bottom: 20px;
      }
      input[type='text'],
      textarea {
        width: 100%;
        padding: 10px;
        margin-bottom: 10px;
      }
      button {
        background-color: #4caf50;
        color: white;
        padding: 10px 15px;
        border: none;
        cursor: pointer;
      }
      ul {
        list-style-type: none;
        padding: 0;
      }
      li {
        background: #f4f4f4;
        margin-bottom: 10px;
        padding: 10px;
      }
    </style>
  </head>
  <body>
    <div class="container">{% block content %}{% endblock %}</div>
  </body>
</html>
```

This is the base HTML template, which provides the layout and structure for the pages. Other templates will extend this layout, using the `block` tags to insert page-specific content.

Create `notes/templates/notes/create_note.html`:

```html
{% extends 'notes/base.html' %} {% block title %}Create Note{% endblock %} {% block content %}
<h1>Create a New Note</h1>
<form method="post">
  {% csrf_token %}
  <input type="text" name="title" placeholder="Title" required />
  <textarea name="content" placeholder="Content" required></textarea>
  <button type="submit">Create Note</button>
</form>
<a href="{% url 'list_notes' %}">Back to Notes</a>
{% endblock %}
```

This template displays the form to create a new note. It extends the base template and includes a form with fields for the title and content. Upon submission, the form sends a `POST` request to the server.

Create `notes/templates/notes/list_notes.html`:

```html
{% extends 'notes/base.html' %} {% block title %}Notes List{% endblock %} {% block content %}
<h1>Notes</h1>
<a href="{% url 'create_note' %}">Create New Note</a>
<ul>
  {% for note in notes %}
  <li>
    <h3>{{ note.title }}</h3>
    <p>{{ note.content }}</p>
    <small>Created at: {{ note.created_at }}</small>
    <div style="display: flex; justify-content: flex-end;">
      <form method="post" action="{% url 'delete_note' note.id %}" style="display: inline;">
        {% csrf_token %}
        <button
          type="submit"
          onclick="return confirm('Are you sure you want to delete this note?');"
        >
          Delete
        </button>
      </form>
    </div>
  </li>
  {% empty %}
  <li>No notes yet.</li>
  {% endfor %}
</ul>
{% endblock %}
```

This template displays a list of all notes. Each note shows its title, content, and creation time. A delete button is also provided next to each note, allowing for easy deletion.

#### Run migrations and start the server

```bash
python manage.py makemigrations
python manage.py migrate
python manage.py runserver
```

These commands generate and apply the database migrations for the Note model and start the development server, allowing you to access the app in your browser.

Visit `http://localhost:8000` to test the note-taking app.

![Django Notes App](/docs/guides/django_notes_app.png)

### Part 2: Use a read replica for read-only operations

#### Create a read replica on Neon

To create a read replica:

1. In the Neon Console, select **Branches**.
2. Select the branch where your database resides.
3. Click **Add Read Replica**.
4. On the **Add new compute** dialog, select **Read replica** as the **Compute type**.
5. Specify the **Compute size settings** options. You can configure a **Fixed Size** compute with a specific amount of vCPU and RAM (the default) or enable autoscaling by configuring a minimum and maximum compute size. You can also configure the **Suspend compute after inactivity** setting, which is the amount of idle time after which your read replica compute is automatically suspended. The default setting is 5 minutes.
   <Admonition type="note">
   The compute size configuration determines the processing power of your database. More vCPU and memory means more processing power but also higher compute costs. For information about compute costs, see [Billing metrics](/docs/introduction/billing).
   </Admonition>
6. When you finish making selections, click **Create**.

Your read replica compute is provisioned and appears on the **Computes** tab of the **Branches** page.

Navigate to the **Dashboard** page, select the branch where the read replica compute was provisioned, and set the compute option to **Replica** to obtain the read replica connection string:

![Read replica connection string](/docs/guides/read_replica_connection_string.png)

#### Set up database routing for read replicas

Create a new file `notes/db_router.py`:

```python
class PrimaryReplicaRouter:
    def db_for_read(self, model, **hints):
        return 'replica'

    def db_for_write(self, model, **hints):
        return 'default'

    def allow_relation(self, obj1, obj2, **hints):
        return True

    def allow_migrate(self, db, app_label, model_name=None, **hints):
        return True
```

This `PrimaryReplicaRouter` class defines the routing logic for database operations. The `db_for_read method` routes all read operations to the 'replica' database, while `db_for_write` directs write operations to the 'default' database. The `allow_relation` and `allow_migrate` methods are set to return `True`, allowing all relations and migrations across databases.

Update `django_notes/settings.py`:

```python
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'your_database_name',
        'USER': 'your_username',
        'PASSWORD': 'your_password',
        'HOST': 'your_primary_host',
        'PORT': '5432',
    },
    'replica': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'your_database_name',
        'USER': 'your_username',
        'PASSWORD': 'your_password',
        'HOST': 'your_read_replica_host',
        'PORT': '5432',
    }
}

DATABASE_ROUTERS = ['notes.db_router.PrimaryReplicaRouter']
```

In the `settings.py` file, we define two database connections: `'default'` for the primary database and `'replica'` for the read replica. Both use the PostgreSQL engine and share the same database name, but have different host addresses. The `DATABASE_ROUTERS` setting tells Django to use our custom `PrimaryReplicaRouter` for database routing decisions.

With these configurations in place, Django will automatically route read queries to the read replica and write queries to the primary database, effectively distributing the database load and potentially improving your application's performance.

## Conclusion

By leveraging Neon's read replicas in your Django application, you can significantly improve your application's performance and scalability. Django's database router makes it easy to set up and use read replicas without having to manually manage multiple database connections in your application code.

This setup allows you to distribute your read load across one or more read replicas while ensuring that all write operations are performed on the primary database. Monitor your application's performance and adjust the number of read replicas as needed to handle your specific load requirements.

You can find the source code for this application on GitHub:

<DetailIconCards>
<a href="https://github.com/dhanushreddy291/neon-read-replica-django" description="
Learn how to scale Django applications with Neon Postgres Read Replicas" icon="github">Use read replicas with Django</a>
</DetailIconCards>

<NeedHelp/>


# Scale your Next.js application with Drizzle ORM and Neon Postgres Read Replicas

---
title: Scale your Next.js application with Drizzle ORM and Neon Postgres Read Replicas
subtitle: Learn how to scale Next.js applications with Drizzle ORM's withReplicas() function and Neon Postgres Read Replicas
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2024-10-14T00:00:00.000Z'
updatedOn: '2024-10-14T00:00:00.000Z'
---

[Neon read replicas](https://neon.tech/docs/introduction/read-replicas) are independent read-only compute instances that can significantly enhance database performance and scalability. By distributing read operations across these replicas, you can reduce latency and improve overall system responsiveness, especially for read-heavy applications.

A key advantage of Neon's architecture is that adding a read replica doesn't require additional storage, making it a highly efficient scaling solution. This cost-effective approach is ideal for businesses of all sizes that need to improve database performance without increasing storage costs.

This guide demonstrates how to leverage Neon read replicas to efficiently scale Next.js applications using Drizzle ORM. You'll learn how to configure your Drizzle database client to work with read replicas, enabling you to optimize your database operations and improve overall application performance.

## Prerequisites

- A Neon account and a Project. If you don't have one, you can sign up for a Neon account and create a project by following the [Getting Started guide](https://neon.tech/docs/get-started-with-neon/signing-up).
- Basic knowledge of [Next.js](https://nextjs.org/docs) and TypeScript
- [Node.js](https://nodejs.org/en/download/package-manager) and npm installed on your local machine

## Build the Polling app

To demonstrate how to use Neon read replicas with Drizzle in Next.js, we'll build a simple Polling application that uses a Neon database. We'll then update the application to use a read replica for read operations, improving the application's performance and scalability.

### Part 1: Build the initial Polling app with a single database

#### Set up the project

Create a new Next.js project with all the default options:

```bash
npx create-next-app@latest polling-app
cd polling-app
```

#### Install required packages

Install Drizzle ORM and the PostgreSQL driver:

```bash
npm install drizzle-orm pg dotenv
npm install -D @types/pg drizzle-kit
```

#### Create the database schema

Create a new file `db/schema.ts`:

```typescript
import { pgTable, serial, text } from 'drizzle-orm/pg-core';

export const pollTable = pgTable('votes', {
  id: serial('id').primaryKey(),
  option: text('option').notNull(),
  ipAddress: text('ip_address').notNull(),
});
```

This code defines the schema for our `votes` table. It has an auto-incrementing `id`, an `option` field for the vote choice, and an `ipAddress` field to track unique voters.

#### Set up the Drizzle client

Create a new file `db/drizzle.ts`:

```typescript
import { drizzle } from 'drizzle-orm/node-postgres';
import { Pool } from 'pg';
import 'dotenv/config';

const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
});

export const db = drizzle(pool);
```

This code sets up the Drizzle ORM client. It creates a connection pool using the `DATABASE_URL` from our environment variables and initializes Drizzle with this pool.

#### Update `.env`

Add the Neon database connection string:

```
DATABASE_URL=postgres://your-username:your-password@your-neon-host/your-database
```

This environment variable stores the connection string for your Neon database.

#### Set up migrations and create tables

Add the following scripts to your `package.json`:

```json
{
  "scripts": {
    ...
    "migration:generate": "drizzle-kit generate",
    "migration:migrate": "drizzle-kit migrate"
  }
}
```

These scripts allow you to generate and run database migrations using [Drizzle Kit](https://www.npmjs.com/package/drizzle-kit).

Create a new file `drizzle.config.ts` in the root of your project:

```typescript
import type { Config } from 'drizzle-kit';
import 'dotenv/config';

export default {
  schema: './db/schema.ts',
  out: './db/migrations',
  dialect: 'postgresql',
  dbCredentials: {
    url: process.env.DATABASE_URL!,
  },
} satisfies Config;
```

This configuration file tells Drizzle Kit where to find your schema, where to output migrations, and which database to connect to.

To generate your first migration based on your schema, run:

```bash
npm run migration:generate
```

To apply the migration and create the table in your Neon database, run:

```bash
npm run migration:migrate
```

#### Create the API routes

Create a new file `app/api/vote/route.ts`:

```typescript
import { db } from '@/db/drizzle';
import { pollTable } from '@/db/schema';
import { NextRequest, NextResponse } from 'next/server';
import { eq, sql } from 'drizzle-orm';

export async function POST(req: NextRequest) {
  const ipAddress = req.headers.get('x-forwarded-for');

  if (ipAddress == null) {
    return NextResponse.json({ message: 'IP address not found!' });
  }

  const { option } = await req.json();

  const existingVote = await db
    .select()
    .from(pollTable)
    .where(eq(pollTable.ipAddress, ipAddress))
    .execute();

  if (existingVote.length > 0) {
    return NextResponse.json({ message: 'You have already voted!' });
  }

  // Insert a new vote
  await db.insert(pollTable).values({
    option: option,
    ipAddress: ipAddress,
  });

  return NextResponse.json({ message: 'Vote submitted successfully!' });
}

export async function GET() {
  const options = await db
    .select({
      count: sql<number>`cast(count(*) as int)`,
      option: pollTable.option,
    })
    .from(pollTable)
    .groupBy(pollTable.option)
    .execute();
  return NextResponse.json(options);
}
```

This file defines two API routes:

- The POST route handles new votes, checking for existing votes from the same IP and inserting new votes.
- The GET route retrieves the current vote counts for each option.

#### Create the frontend

Add [shadcn-ui](https://ui.shadcn.com/) to the project for styling:

```bash
npx shadcn@latest init -d
npx shadcn@latest add button card
```

Update `app/page.tsx`:

```typescript
"use client"

import { useState, useEffect } from "react"
import { Button } from "@/components/ui/button"
import { Card, CardContent, CardHeader, CardTitle } from "@/components/ui/card"

export default function Component() {
  const [votes, setVotes] = useState({ Python: 0, JavaScript: 0 });
  const [loading, setLoading] = useState(false);

  useEffect(() => {
    // Fetch the current poll results when the component mounts
    const fetchPollResults = async () => {
      try {
        const response = await fetch("/api/vote");
        const data = await response.json();

        setVotes({
          Python: data.find((option: { option: string }) => option.option === "Python")?.count || 0,
          JavaScript: data.find((option: { option: string }) => option.option === "JavaScript")?.count || 0,
        });

      } catch (error) {
        console.error("Error fetching poll results:", error)
      }
    }

    fetchPollResults()
  }, [])

  const handleVote = async (option: "Python" | "JavaScript") => {
    setLoading(true)
    try {
      const response = await fetch("/api/vote", {
        method: "POST",
        headers: {
          "Content-Type": "application/JavaScripton",
        },
        body: JSON.stringify({ option }),
      })

      if (!response.ok) {
        throw new Error("Error submitting vote")
      }

      const data = await response.json()
      if (data.message === "Vote submitted successfully!") {
        setVotes((prevVotes) => ({
          ...prevVotes,
          [option]: prevVotes[option] + 1,
        }))
      } else {
        alert(data.message)
      }

    } catch (error) {
      console.error("Error submitting vote:", error)
    } finally {
      setLoading(false)
    }
  }

  const totalVotes = votes.Python + votes.JavaScript
  const PythonPercentage = totalVotes ? (votes.Python / totalVotes) * 100 : 0
  const JavaScriptPercentage = totalVotes ? (votes.JavaScript / totalVotes) * 100 : 0

  return (
    <div className="flex items-center justify-center min-h-screen">
      <Card className="w-full max-w-md mx-auto bg-gradient-to-br from-purple-50 to-indigo-50 dark:from-gray-900 dark:to-gray-800 shadow-lg">
        <CardHeader className="pb-2">
          <CardTitle className="text-3xl font-bold text-center text-purple-700 dark:text-purple-300">
            What is your favorite programming language?
          </CardTitle>
        </CardHeader>
        <CardContent className="space-y-6 p-6">
          <div className="space-y-4">
            <Button
              onClick={() => handleVote("Python")}
              className="w-full justify-between text-lg font-semibold py-6 bg-gradient-to-r from-purple-500 to-indigo-500 hover:from-purple-600 hover:to-indigo-600 transition-all duration-300"
              disabled={loading}
            >
              <span>Python</span>
              <span className="bg-white text-purple-700 px-3 py-1 rounded-full text-sm">
                {votes.Python}
              </span>
            </Button>
            <div className="h-3 bg-gray-200 rounded-full overflow-hidden dark:bg-gray-700">
              <div
                className="h-full bg-gradient-to-r from-purple-500 to-indigo-500 transition-all duration-300 ease-in-out"
                style={{ width: `${PythonPercentage}%` }}
              />
            </div>
          </div>
          <div className="space-y-4">
            <Button
              onClick={() => handleVote("JavaScript")}
              className="w-full justify-between text-lg font-semibold py-6 bg-gradient-to-r from-indigo-500 to-purple-500 hover:from-indigo-600 hover:to-purple-600 transition-all duration-300"
              disabled={loading}
            >
              <span>JavaScript</span>
              <span className="bg-white text-indigo-700 px-3 py-1 rounded-full text-sm">
                {votes.JavaScript}
              </span>
            </Button>
            <div className="h-3 bg-gray-200 rounded-full overflow-hidden dark:bg-gray-700">
              <div
                className="h-full bg-gradient-to-r from-indigo-500 to-purple-500 transition-all duration-300 ease-in-out"
                style={{ width: `${JavaScriptPercentage}%` }}
              />
            </div>
          </div>
          <p className="text-center text-sm font-medium text-gray-600 dark:text-gray-300 mt-4">
            Total votes: {totalVotes}
          </p>
        </CardContent>
      </Card>
    </div>
  )
}
```

This component creates a user interface for the polling app. It includes:

- State management for votes and loading status
- An effect hook to fetch initial poll results
- A function to handle new votes
- A UI with buttons for voting and progress bars to show results

#### Run the application

```bash
npm run dev
```

![Polling app](/docs/guides/drizzle_polling_demo_app.png)

Visit [`http://localhost:3000`](http://localhost:3000) to test the polling app.

### Part 2: Use a read replica for read-only operations

#### Create a read replica on Neon

To create a read replica:

1. In the Neon Console, select **Branches**.
2. Select the branch where your database resides.
3. Click **Add Read Replica**.
4. On the **Add new compute** dialog, select **Read replica** as the **Compute type**.
5. Specify the **Compute size settings** options. You can configure a **Fixed Size** compute with a specific amount of vCPU and RAM (the default) or enable autoscaling by configuring a minimum and maximum compute size. You can also configure the **Suspend compute after inactivity** setting, which is the amount of idle time after which your read replica compute is automatically suspended. The default setting is 5 minutes.
   <Admonition type="note">
   The compute size configuration determines the processing power of your database. More vCPU and memory means more processing power but also higher compute costs. For information about compute costs, see [Billing metrics](/docs/introduction/billing).
   </Admonition>
6. When you finish making selections, click **Create**.

Your read replica compute is provisioned and appears on the **Computes** tab of the **Branches** page.

Navigate to the **Dashboard** page, select the branch where the read replica compute was provisioned, and set the compute option to **Replica** to obtain the read replica connection string:

![Read replica connection string](/docs/guides/read_replica_connection_string.png)

#### Update the Drizzle client

Modify `db/drizzle.ts`:

```typescript
import { drizzle } from 'drizzle-orm/node-postgres';
import { withReplicas } from 'drizzle-orm/pg-core';
import { Pool } from 'pg';
import 'dotenv/config';

const primaryDb = drizzle(
  new Pool({
    connectionString: process.env.DATABASE_URL!,
  })
);
const read = drizzle(
  new Pool({
    connectionString: process.env.READ_REPLICA_URL!,
  })
);

export const db = withReplicas(primaryDb, [read]);
```

This setup uses Drizzle's [`withReplicas`](https://orm.drizzle.team/docs/read-replicas) function to create a single database client that can handle both primary and read replica connections. It automatically routes read queries to the read replica and write queries to the primary database.

#### Update `.env`

Add the read replica connection string:

```
DATABASE_URL=postgres://your-username:your-password@your-neon-primary-host/your-database
READ_REPLICA_URL=postgres://your-username:your-password@your-neon-read-replica-host/your-database
```

These environment variables store the connection strings for both your primary database and the read replica.

<Admonition type="note">
   You can also pass an array of read replica connection strings if you want to use multiple read replicas. Neon supports adding multiple read replicas to a database branch.

```javascript
const primaryDb = drizzle(
  new Pool({
    connectionString: process.env.DATABASE_URL,
  })
);
const read1 = drizzle(
  new Pool({
    connectionString: process.env.READ_REPLICA_URL_1,
  })
);
const read2 = drizzle(
  new Pool({
    connectionString: process.env.READ_REPLICA_URL_2,
  })
);
const db = withReplicas(primaryDb, [read1, read2]);
```

  </Admonition>

If you want to read from the primary compute and bypass read replicas, you can use the `$primary()` key:

```javascript
const posts = await db.$primary().post.findMany();
```

You can find the source code for the application described in this guide on GitHub.

<DetailIconCards>
<a href="https://github.com/dhanushreddy291/neon-read-replica-drizzle" description="Use read replicas with Drizzle and Next.js" icon="github">Use read replicas with Drizzle</a>
</DetailIconCards>

## Conclusion

By leveraging Neon's read replicas with Drizzle in your Next.js application, you can significantly improve your application's performance and scalability. Drizzle makes it easy to set up and use read replicas without having to manually manage multiple database connections in your application code.

This setup allows you to distribute your read load across one or more read replicas while ensuring that all write operations are performed on the primary database. Monitor your application's performance and adjust the number of read replicas as needed to handle your specific load requirements.

<NeedHelp/>


# Scale your .NET application with Entity Framework and Neon Postgres Read Replicas

---
title: Scale your .NET application with Entity Framework and Neon Postgres Read Replicas
subtitle: Learn how to scale .NET applications with Entity Framework's DbContext and Neon Postgres Read Replicas
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2024-10-13T00:00:00.000Z'
updatedOn: '2024-10-13T00:00:00.000Z'
---

[Neon read replicas](https://neon.tech/docs/introduction/read-replicas) are independent read-only compute instances that perform read operations on the same data as your primary read-write compute. A key advantage of Neon's architecture is that adding a read replica to a Neon project doesn't require additional storage, making it an efficient scaling solution.

This guide demonstrates how to leverage Neon read replicas to efficiently scale .NET applications using Entity Framework Core. You'll learn how to configure your DbContext to work with read replicas, enabling you to optimize your database operations and improve overall application performance.

## Prerequisites

- A Neon account and a Project. If you don't have one, you can sign up for a Neon account and create a project by following the [Getting Started guide](/docs/get-started-with-neon/signing-up).
- Basic knowledge of .NET Core
- Dotnet SDK installed on your local machine. You can download it from the [official .NET website](https://dotnet.microsoft.com/download).
- Dotnet Entity Framework Core CLI tools installed. You can install them by running the following command:

  ```bash
  dotnet tool install --global dotnet-ef
  ```

## Build the Todo app

To demonstrate how to use Neon read replicas with Entity Framework Core, we'll build a simple Todo application that uses a Neon database. We'll then update the application to use a read replica for read operations, improving the application's performance and scalability. This is just a simple example to demonstrate the concept, and you can apply the same principles to more complex applications.

### Part 1: Build the initial Todo app with a single database

#### Set up the project

Create a new .NET Core Web API project using the following commands:

```bash
dotnet new webapi -n TodoApi
cd TodoApi
```

#### Delete the WeatherForecast files

Delete the files `WeatherForecast.cs` and `Controllers/WeatherForecastController.cs` as we won't be using them:

```bash
rm WeatherForecast.cs Controllers/WeatherForecastController.cs
```

#### Install required packages

Install Entity Framework Core Design and Npgsql packages:

<Admonition type="tip" title="Best Practice">
Ensure you install package versions that match your .NET version. You can verify your .NET version at any time by running `dotnet --version`.
</Admonition>

```bash
dotnet add package Microsoft.EntityFrameworkCore.Design --version 6.0.4
dotnet add package Npgsql.EntityFrameworkCore.PostgreSQL --version 6.0.4
```

#### Create the Todo model

Create a new file `Models/Todo.cs`:

```csharp
namespace TodoApi.Models
{
    public class Todo
    {
        public int Id { get; set; }
        public string? Name { get; set; }
        public bool IsComplete { get; set; }
    }
}
```

#### Create the database context

Create a new file `Data/TodoDbContext.cs`:

```csharp
using Microsoft.EntityFrameworkCore;
using TodoApi.Models;

namespace TodoApi.Data
{
    public class TodoDbContext : DbContext
    {
        public TodoDbContext(DbContextOptions<TodoDbContext> options) : base(options) { }
        public DbSet<Todo> Todos => Set<Todo>();
    }
}
```

#### Update `appsettings.json` / `appsettings.Development.json`:

Add the connection string:

```json
{
  "ConnectionStrings": {
    "TodoDbConnection": "Host=your-neon-host;Database=your-db;Username=your-username;Password=your-password"
  }
}
```

#### Create the TodoController

Create a new file `Controllers/TodoController.cs`:

```csharp
using Microsoft.AspNetCore.Mvc;
using Microsoft.EntityFrameworkCore;
using TodoApi.Data;
using TodoApi.Models;

namespace TodoApi.Controllers
{
    [ApiController]
    [Route("api/[controller]")]
    public class TodoController : ControllerBase
    {
        private readonly TodoDbContext _context;

        public TodoController(TodoDbContext context)
        {
            _context = context;
        }

        [HttpGet]
        public async Task<ActionResult<IEnumerable<Todo>>> GetTodos()
        {
            return await _context.Todos.ToListAsync();
        }

        [HttpGet("{id}")]
        public async Task<ActionResult<Todo>> GetTodo(int id)
        {
            var todo = await _context.Todos.FindAsync(id);
            if (todo == null)
            {
                return NotFound();
            }
            return todo;
        }

        [HttpPost]
        public async Task<ActionResult<Todo>> PostTodo(Todo todo)
        {
            _context.Todos.Add(todo);
            await _context.SaveChangesAsync();
            return CreatedAtAction(nameof(GetTodo), new { id = todo.Id }, todo);
        }

        [HttpPut("{id}")]
        public async Task<IActionResult> PutTodo(int id, Todo todo)
        {
            if (id != todo.Id)
            {
                return BadRequest();
            }
            _context.Entry(todo).State = EntityState.Modified;
            await _context.SaveChangesAsync();
            return NoContent();
        }

        [HttpDelete("{id}")]
        public async Task<IActionResult> DeleteTodo(int id)
        {
            var todo = await _context.Todos.FindAsync(id);
            if (todo == null)
            {
                return NotFound();
            }
            _context.Todos.Remove(todo);
            await _context.SaveChangesAsync();
            return NoContent();
        }
    }
}
```

This controller defines CRUD operations (Create, Read, Update, Delete) for Todo items using HTTP requests. It uses `TodoDbContext` to interact with the database.

#### Update `Program.cs` with the following content:

```csharp
using Microsoft.EntityFrameworkCore;
using TodoApi.Data;

var builder = WebApplication.CreateBuilder(args);

builder.Services.AddControllers();

builder.Services.AddDbContext<TodoDbContext>(opt =>
    opt.UseNpgsql(builder.Configuration.GetConnectionString("TodoDbConnection")));

builder.Services.AddEndpointsApiExplorer();
builder.Services.AddSwaggerGen();

var app = builder.Build();

if (app.Environment.IsDevelopment())
{
    app.UseSwagger();
    app.UseSwaggerUI();
}

app.UseAuthorization();
app.MapControllers();

if (app.Environment.IsDevelopment())
{
    app.Run("http://localhost:5001");
}
else
{
    app.UseHttpsRedirection();
    app.Run();
}
```

This code configures the application to use Entity Framework Core with a PostgreSQL database. It registers `TodoDbContext` with the application's services and sets up the database connection using the connection string from `appsettings.json` / `appsettings.Development.json`.

#### Create Migrations

Run the following commands to create and apply the initial migration:

```bash
dotnet ef migrations add InitialCreate
dotnet ef database update
```

#### Run the application

Start the application:

```bash
dotnet run
```

Visit the Swagger UI at [`http://localhost:5001/swagger`](http://localhost:5001/swagger) to test the API.

![Swagger UI](/docs/guides/dotnet_ef_todo_swagger_demo.png)

### Part 2: Use a read replica for read-only operations

#### Create a read replica on Neon

To create a read replica:

1. In the Neon Console, select **Branches**.
2. Select the branch where your database resides.
3. Click **Add Read Replica**.
4. On the **Add new compute** dialog, select **Read replica** as the **Compute type**.
5. Specify the **Compute size settings** options. You can configure a **Fixed Size** compute with a specific amount of vCPU and RAM (the default) or enable autoscaling by configuring a minimum and maximum compute size. You can also configure the **Suspend compute after inactivity** setting, which is the amount of idle time after which your read replica compute is automatically suspended. The default setting is 5 minutes.
   <Admonition type="note">
   The compute size configuration determines the processing power of your database. More vCPU and memory means more processing power but also higher compute costs. For information about compute costs, see [Billing metrics](/docs/introduction/billing).
   </Admonition>
6. When you finish making selections, click **Create**.

Your read replica compute is provisioned and appears on the **Computes** tab of the **Branches** page.

Navigate to the **Dashboard** page, select the branch where the read replica compute was provisioned, and set the compute option to **Replica** to obtain the read replica connection string:

![Read replica connection string](/docs/guides/read_replica_connection_string.png)

#### Update the TodoDbContext

Modify `Data/TodoDbContext.cs` to include separate read and write contexts:

```csharp
using Microsoft.EntityFrameworkCore;
using TodoApi.Models;

namespace TodoApi.Data
{
    public class TodoDbContext : DbContext
    {
        public TodoDbContext(DbContextOptions<TodoDbContext> options) : base(options) { }
        public DbSet<Todo> Todos => Set<Todo>();
    }

    public class TodoDbReadContext : DbContext
    {
        public TodoDbReadContext(DbContextOptions<TodoDbReadContext> options) : base(options) { }
        public DbSet<Todo> Todos => Set<Todo>();
    }
}
```

#### Update Program.cs

Modify `Program.cs` to include both read and write contexts:

```csharp
using Microsoft.EntityFrameworkCore;
using TodoApi.Data;

var builder = WebApplication.CreateBuilder(args);

builder.Services.AddControllers();

builder.Services.AddDbContext<TodoDbContext>(opt =>
    opt.UseNpgsql(builder.Configuration.GetConnectionString("TodoDbConnection")));
builder.Services.AddDbContext<TodoDbReadContext>(opt =>
    opt.UseNpgsql(builder.Configuration.GetConnectionString("TodoDbConnectionRead")));

builder.Services.AddEndpointsApiExplorer();
builder.Services.AddSwaggerGen();

var app = builder.Build();

if (app.Environment.IsDevelopment())
{
    app.UseSwagger();
    app.UseSwaggerUI();
}

app.UseAuthorization();
app.MapControllers();

if (app.Environment.IsDevelopment())
{
    app.Run("http://localhost:5001");
}
else
{
    app.UseHttpsRedirection();
    app.Run();
}
```

#### Update `appsettings.json` / `appsettings.Development.json`

Add the read replica connection string:

```json
{
  "ConnectionStrings": {
    "TodoDbConnection": "Host=your-neon-primary-host;Database=your-db;Username=your-username;Password=your-password",
    "TodoDbConnectionRead": "Host=your-neon-read-replica-host;Database=your-db;Username=your-username;Password=your-password"
  }
}
```

#### Update the TodoController

Modify `Controllers/TodoController.cs` to use separate read and write contexts:

```csharp
using Microsoft.AspNetCore.Mvc;
using Microsoft.EntityFrameworkCore;
using TodoApi.Data;
using TodoApi.Models;

namespace TodoApi.Controllers
{
    [ApiController]
    [Route("api/[controller]")]
    public class TodoController : ControllerBase
    {
        private readonly TodoDbContext _writeContext;
        private readonly TodoDbReadContext _readContext;

        public TodoController(TodoDbContext writeContext, TodoDbReadContext readContext)
        {
            _writeContext = writeContext;
            _readContext = readContext;
        }

        [HttpGet]
        public async Task<ActionResult<IEnumerable<Todo>>> GetTodos()
        {
            return await _readContext.Todos.ToListAsync();
        }

        [HttpGet("{id}")]
        public async Task<ActionResult<Todo>> GetTodo(int id)
        {
            var todo = await _readContext.Todos.FindAsync(id);
            if (todo == null)
            {
                return NotFound();
            }
            return todo;
        }

        [HttpPost]
        public async Task<ActionResult<Todo>> PostTodo(Todo todo)
        {
            _writeContext.Todos.Add(todo);
            await _writeContext.SaveChangesAsync();
            return CreatedAtAction(nameof(GetTodo), new { id = todo.Id }, todo);
        }

        [HttpPut("{id}")]
        public async Task<IActionResult> PutTodo(int id, Todo todo)
        {
            if (id != todo.Id)
            {
                return BadRequest();
            }
            _writeContext.Entry(todo).State = EntityState.Modified;
            await _writeContext.SaveChangesAsync();
            return NoContent();
        }

        [HttpDelete("{id}")]
        public async Task<IActionResult> DeleteTodo(int id)
        {
            var todo = await _writeContext.Todos.FindAsync(id);
            if (todo == null)
            {
                return NotFound();
            }
            _writeContext.Todos.Remove(todo);
            await _writeContext.SaveChangesAsync();
            return NoContent();
        }
    }
}
```

<Admonition type="tip" title="Did you know?">
You can use dotnet-ef migrations even with multiple db contexts. You can specify the context to use by passing the `--context` option to the `dotnet ef` command.
</Admonition>

The Todo API is now set up to use separate read and write contexts, leveraging Neon's read replica feature. Read operations (`GET` requests) will use the read replica, while write operations (`POST`, `PUT`, `DELETE`) will use the primary database.

You can find the source code for the application described in this guide on GitHub.

<DetailIconCards>
<a href="https://github.com/dhanushreddy291/neon-read-replica-entity-framework" description="Use read replicas with Entity Framework Core" icon="github">Read Replicas in .NET EF</a>
</DetailIconCards>

## Conclusion

This setup allows you to distribute your read load across one or more read replicas while ensuring that all write operations are performed on the primary database. Monitor your application's performance and adjust the number of read replicas as needed to handle your specific load requirements. With Neon, you can quickly scale out with as many read replicas as you need.

<NeedHelp/>


# Scale your Laravel application with Neon Postgres Read Replicas

---
title: Scale your Laravel application with Neon Postgres Read Replicas
subtitle: Learn how to scale Laravel applications with Neon Postgres Read Replicas
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2024-10-20T00:00:00.000Z'
updatedOn: '2024-10-20T00:00:00.000Z'
---

## Introduction

[Neon read replicas](https://neon.tech/docs/introduction/read-replicas) are independent read-only compute instances that can significantly enhance database performance and scalability. By distributing read operations across these replicas, you can reduce latency and improve overall system responsiveness, especially for read-heavy applications.

A key advantage of Neon's architecture is that adding a read replica doesn't require additional storage, making it a highly efficient scaling solution. This cost-effective approach is ideal for businesses of all sizes that need to improve database performance without increasing storage costs.

This guide demonstrates how to leverage Neon read replicas to efficiently scale Laravel applications. You'll learn how to configure your Laravel database connections to work with read replicas, enabling you to optimize your database operations and improve overall application performance. We'll use a simple URL shortener application as an example.

## Prerequisites

- A Neon account and a Project. If you don't have one, you can sign up for a Neon account and create a project by following the [Getting Started guide](https://neon.tech/docs/get-started-with-neon/signing-up).
- Basic knowledge of [Laravel](https://laravel.com/docs) and PHP
- [Composer](https://getcomposer.org/) installed on your local machine
- [PHP](https://www.php.net/manual/en/install.php) installed on your local machine

## Build the URL Shortener app

To demonstrate how to use Neon read replicas with Laravel, we'll build a simple URL shortener application that uses a Neon database. We'll then update the application to use a read replica for read operations, improving the application's performance and scalability.

### Part 1: Build the initial URL Shortener app with a single database

#### Set up the project

Create a new Laravel project:

```bash
laravel new url-shortener

 ┌ Would you like to install a starter kit? ────────────────────┐
 │ No starter kit                                               │
 └──────────────────────────────────────────────────────────────┘

 ┌ Which testing framework do you prefer? ──────────────────────┐
 │ Pest                                                         │
 └──────────────────────────────────────────────────────────────┘

 ┌ Would you like to initialize a Git repository? ────────┐
 │ Yes                                                    │
 └────────────────────────────────────────────────────────┘

 ┌ Which database will your application use? ────────────┐
 │ PostgreSQL                                            │
 └───────────────────────────────────────────────────────┘

 ┌ Default database updated. Would you like to run the default database migrations? ┐
 │ No                                                                               │
 └─────────────────────────────────────────────────────────────────────────────────┘
 cd url-shortener
```

#### Configure the database connection

Update your `.env` file with your Neon database credentials:

```
DB_CONNECTION=pgsql
DB_HOST=your-neon-host
DB_PORT=5432
DB_DATABASE=your-database-name
DB_USERNAME=your-username
DB_PASSWORD=your-password
```

#### Create the database schema

Create a new migration for the `urls` table:

```bash
php artisan make:migration create_urls_table
```

Edit the migration file in `database/migrations`:

```php
<?php

use Illuminate\Database\Migrations\Migration;
use Illuminate\Database\Schema\Blueprint;
use Illuminate\Support\Facades\Schema;

class CreateUrlsTable extends Migration
{
    public function up()
    {
        Schema::create('urls', function (Blueprint $table) {
            $table->id();
            $table->string('original_url');
            $table->string('short_code')->unique();
            $table->timestamps();
        });
    }

    public function down()
    {
        Schema::dropIfExists('urls');
    }
}
```

Run the migration:

```bash
php artisan migrate
```

<Admonition type="important">
Neon supports both direct and pooled database connection strings, which can be copied from the **Connection Details** widget on your Neon Project Dashboard. A pooled connection string connects your application to the database via a PgBouncer connection pool, allowing for a higher number of concurrent connections. However, using a pooled connection string for migrations can be prone to errors. For this reason, we recommend using a direct (non-pooled) connection when performing migrations. For more information about direct and pooled connections, see [Connection pooling](/docs/connect/connection-pooling).
</Admonition>

#### Create the model

Create a new model for the URL:

```bash
php artisan make:model Url
```

Edit `app/Models/Url.php`:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class Url extends Model
{
    use HasFactory;

    protected $fillable = ['original_url', 'short_code'];
}
```

#### Create the controller

Create a new controller for handling URL operations:

```bash
php artisan make:controller UrlController
```

Edit `app/Http/Controllers/UrlController.php`:

```php
<?php

namespace App\Http\Controllers;

use App\Models\Url;
use Illuminate\Http\Request;
use Illuminate\Support\Str;

class UrlController extends Controller
{
    public function shorten(Request $request)
    {
        $request->validate([ 'url' => 'required|url' ]);

        $url = Url::create([
            'original_url' => $request->url,
            'short_code' => Str::random(6),
        ]);

        return response()->json([ 'short_url' => url($url->short_code) ], 201);
    }

    public function redirect($shortCode)
    {
        $url = Url::where('short_code', $shortCode)->firstOrFail();
        return redirect($url->original_url);
    }
}
```

#### Set up the routes

Edit `routes/web.php`:

```php
<?php

use App\Http\Controllers\UrlController;
use Illuminate\Support\Facades\Route;

Route::get('/', function () { return view('home'); });
Route::post('/shorten', [UrlController::class, 'shorten']);
Route::get('/{shortCode}', [UrlController::class, 'redirect']);
```

#### Create a simple frontend

Create a new blade template `resources/views/home.blade.php`:

```html
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>URL Shortener</title>
    <script src="https://cdn.tailwindcss.com"></script>
  </head>
  <body class="bg-gray-100 flex min-h-screen items-center justify-center">
    <div class="w-96 rounded-lg bg-white p-8 shadow-md">
      <h1 class="mb-4 text-2xl font-bold">URL Shortener</h1>
      <form id="urlForm" class="mb-4">
        <input
          type="url"
          id="urlInput"
          placeholder="Enter URL to shorten"
          required
          class="mb-2 w-full rounded border p-2"
        />
        <button type="submit" class="bg-blue-500 hover:bg-blue-600 w-full rounded p-2 text-white">
          Shorten URL
        </button>
      </form>
      <div id="result" class="hidden">
        <p>
          Shortened URL:
          <a id="shortUrl" href="#" target="_blank" class="text-blue-500"></a>
        </p>
      </div>
    </div>

    <script>
      document.getElementById('urlForm').addEventListener('submit', async (e) => {
        e.preventDefault();
        const url = document.getElementById('urlInput').value;
        const response = await fetch('/shorten', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'X-CSRF-TOKEN': '{{ csrf_token() }}',
          },
          body: JSON.stringify({ url }),
        });
        const data = await response.json();
        document.getElementById('shortUrl').href = data.short_url;
        document.getElementById('shortUrl').textContent = data.short_url;
        document.getElementById('result').classList.remove('hidden');
      });
    </script>
  </body>
</html>
```

#### Run the application

Start the Laravel development server:

```bash
php artisan serve
```

Visit `http://localhost:8000` to test the URL shortener app.

![Laravel URL Shortener](/docs/guides/laravel_url_shortener.png)

### Part 2: Use a read replica for read-only operations

#### Create a read replica on Neon

To create a read replica:

1. In the Neon Console, select **Branches**.
2. Select the branch where your database resides.
3. Click **Add Read Replica**.
4. On the **Add new compute** dialog, select **Read replica** as the **Compute type**.
5. Specify the **Compute size settings** options. You can configure a **Fixed Size** compute with a specific amount of vCPU and RAM (the default) or enable autoscaling by configuring a minimum and maximum compute size. You can also configure the **Suspend compute after inactivity** setting, which is the amount of idle time after which your read replica compute is automatically suspended. The default setting is 5 minutes.
   <Admonition type="note">
   The compute size configuration determines the processing power of your database. More vCPU and memory means more processing power but also higher compute costs. For information about compute costs, see [Billing metrics](/docs/introduction/billing).
   </Admonition>
6. When you finish making selections, click **Create**.

Your read replica compute is provisioned and appears on the **Computes** tab of the **Branches** page.

Navigate to the **Dashboard** page, select the branch where the read replica compute was provisioned, and set the compute option to **Replica** to obtain the read replica connection string:

![Read replica connection string](/docs/guides/read_replica_connection_string.png)

#### Update the database configuration

Edit `config/database.php` to add the read replica configuration:

```php
'pgsql' => [
    'driver' => 'pgsql',
    'read' => [
        'host' => env('DB_READ_HOST'),
    ],
    'write' => [
        'host' => env('DB_WRITE_HOST'),
    ],
    'sticky'    => true,
    'port' => env('DB_PORT', '5432'),
    'database' => env('DB_DATABASE', 'laravel'),
    'username' => env('DB_USERNAME', 'root'),
    'password' => env('DB_PASSWORD', ''),
    'charset' => env('DB_CHARSET', 'utf8'),
    'prefix' => '',
    'prefix_indexes' => true,
    'search_path' => 'public',
    'sslmode' => 'prefer',
]
```

<Admonition type="info">
Now that you've completed the database migrations, you can leverage the pooled connection string for both read and write operations. This streamlines your database interactions. However, it's worth noting that for future migrations, it's best practice to use the direct connection string. This approach helps avoid potential complications that might arise during the migration process.
</Admonition>

Update your `.env` file with the read replica host:

```
DB_READ_HOST=your-neon-read-replica-host
DB_WRITE_HOST=your-neon-primary-host
```

## Automatic Query Routing with Eloquent

One of the great features of [Laravel's Eloquent ORM](https://laravel.com/docs/11.x/eloquent) is that it automatically routes queries to the appropriate database connection based on the type of query. This means that after configuring your read replica, you don't need to make any changes to your existing controller or model code.
Here's how Eloquent handles different types of queries:

- Read Operations: `SELECT` queries are automatically routed to the read replica.
- Write Operations: `INSERT`, `UPDATE`, and `DELETE` queries are sent to the primary (write) database.

This automatic routing happens transparently, allowing you to scale your application without modifying your application logic.

<Admonition type="tip">
  Laravel offers powerful flexibility in managing database connections. While automatic query routing is convenient, you can easily override it when needed. For instance, to explicitly use the write connection for querying the urls table, you can do the following:
  ```php
  <?php
  use Illuminate\Support\Facades\DB;
  ...
  
  $url = DB::connection('pgsql::write')->table('urls')->where('short_code', $shortCode)->first();
  ```
</Admonition>

You can find the source code for the application described in this guide on GitHub.
<DetailIconCards>
<a href="https://github.com/dhanushreddy291/neon-read-replica-laravel" description="
Learn how to scale Laravel applications with Neon Postgres Read Replicas" icon="github">Use read replicas with Laravel</a>
</DetailIconCards>

## Conclusion

By leveraging Neon's read replicas with Laravel, you can significantly improve your application's performance and scalability. Laravel's database configuration makes it easy to set up and use read replicas without having to manually manage multiple database connections in your application code.

This setup allows you to distribute your read load across one or more read replicas while ensuring that all write operations are performed on the primary database. Monitor your application's performance and adjust the number of read replicas as needed to handle your specific load requirements.

By implementing read replicas in your Laravel application, you're taking a significant step toward building a more scalable and performant system that can handle increased traffic and data loads.

<NeedHelp/>


# Run your own analytics with Umami, Fly.io and Neon

---
title: Run your own analytics with Umami, Fly.io and Neon
subtitle: Self host your Umami analytics on Fly.io and powered by Neon Postgres
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-06-05T00:00:00.000Z'
updatedOn: '2024-06-05T00:00:00.000Z'
---

In this guide, you will learn how to self host your Umami analytics instance on Fly.io and powered by Neon Postgres as the serverless database.

## Prerequisites

To follow along and deploy the application in this guide, you will need the following:

- [flyctl](https://fly.io/docs/getting-started/installing-flyctl/) – A command-line utility that lets you work with the Fly.io platform. You will also need [a fly.io account](https://fly.io/docs/hands-on/sign-up/).
- [A Neon account](https://console.neon.tech/signup) – The self-hosted Umami analytics instance will connect to a Neon serverless Postgres database 🚀

## Steps

- [What is Umami?](#what-is-umami)
- [Provisioning a Postgres database using Neon](#provisioning-a-postgres-database-using-neon)
- [Set up an Umami instance for Fly.io](#set-up-an-umami-instance-for-flyio)
- [Configure Neon Postgres as serverless database for self-hosted Umami analytics](#set-neon-postgres-as-serverless-database-for-self-hosted-umami-analytics)
- [Deploy to Fly.io](#deploy-to-flyio)

## What is Umami?

![Umami Analytics Preview](/guides/images/self-hosting-umami-neon/umami.jpeg)

Umami is a simple, fast, privacy-focused, open-source analytics solution. Umami is a better alternative to Google Analytics because it gives you total control of your data and does not violate the privacy of your users. <sup>[[1](https://umami.is/docs)]</sup>

## Provisioning a Postgres Database using Neon

Using a serverless Postgres database powered by Neon allows you to scale down to zero when the database is not being used, which saves on compute costs.
.

To get started, go to the [Neon Console](https://console.neon.tech/app/projects) and enter a name for your project.

You will be presented with a dialog that provides a connection string of your database. Click on the **Pooled connection** option and the connecting string automatically changes to a pooled connection string.

![](/guides/images/self-hosting-umami-neon/1689d44f-4c5d-4b2a-8d13-32407f9c8781.png)

All Neon connection strings have the following format:

```bash
postgres://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>
```

- `user` is the database user.
- `password` is the database user’s password.
- `endpoint_hostname` is the host with neon.tech as the [top level domain (TLD)](https://www.cloudflare.com/en-gb/learning/dns/top-level-domain/).
- `port` is the Neon port number. The default port number is 5432.
- `dbname` is the name of the database. “neondb” is the default database created with each Neon project if you don't specify your own database name.
- `?sslmode=require` is an optional query parameter that enforces the [SSL](https://www.cloudflare.com/en-gb/learning/ssl/what-is-ssl/) mode for better security when connecting to the Postgres instance.

Please save the connection string somewhere safe. Later, you will use it to configure the `DATABASE_URL` variable.

## Setup Umami analytics instance for Fly.io

To self host your Umami analytics instance, you'll use [Umami's pre-built Docker container for Postgres](https://github.com/umami-software/umami/pkgs/container/umami/157800125?tag=postgresql-latest). This will allow you to self host an Umami analytics instance on Fly.io with a single `fly.toml` file.

In your terminal window, execute the following commands to create a new directory and `cd` to it:

```bash
mkdir self-host-umami-neon
cd self-host-umami-neon
```

In the directory `self-host-umami-neon`, create a file named `fly.toml` with the following content:

```toml
# File: fly.toml

kill_signal = "SIGINT"
kill_timeout = "5s"

[experimental]
    auto_rollback = true

[build]
    image = "ghcr.io/umami-software/umami:postgresql-latest"

[[services]]
    protocol = "tcp"
    internal_port = 3000
    processes = ["app"]

[[services.ports]]
    port = 80
    handlers = ["http"]
    force_https = true

[[services.ports]]
    port = 443
    handlers = ["tls", "http"]

[services.concurrency]
    type = "connections"
    hard_limit = 25
    soft_limit = 20

[[services.tcp_checks]]
    interval = "15s"
    timeout = "2s"
    grace_period = "1s"
```

In the `build` property named `image`, you will see that it's pointing to the latest Postgres compatible pre-built Docker image of Umami.

Next, you need to create an app on Fly.io using the configuration present in `fly.toml` file. In your terminal window, execute the following command to launch a Fly.io app:

```bash
fly launch
```

When prompted by the CLI to allow copying of the existing configuration into a new app, answer with a `y`:

```bash
An existing fly.toml file was found
? Would you like to copy its configuration to the new app? Yes
```

Optional: When asked if you want to tweak the default settings, answer with a `y`:

```bash
Using build strategies '[the "ghcr.io/umami-software/umami:postgresql-latest" docker image]'. Remove [build] from fly.toml to force a rescan
Creating app in /Users/rishi/Desktop/test
We're about to launch your app on Fly.io. Here's what you're getting:

Organization: Rishi Raj Jain         (fly launch defaults to the personal org)
Name:         test                   (derived from your directory name)
Region:       Mumbai, India          (this is the fastest region for you)
App Machines: shared-cpu-1x, 1GB RAM (most apps need about 1GB of RAM)
Postgres:     <none>                 (not requested)
Redis:        <none>                 (not requested)

? Do you want to tweak these settings before proceeding? Yes
Opening https://fly.io/cli/launch/641f1a1d67950614e4e92820ba484310 ...
```

flyctl will then automatically take you to a web page, which allows you to visually edit the default settings. For example, you can change the app name to `self-host-umami-neon`, and change the region to say `ams`.

![Fly.io Deployment Setting](/guides/images/self-hosting-umami-neon/307247099-acca8350-75c8-4007-b486-42c4102dfe40.png)

Click on `Confirm Settings` to set this configuration, and go back to your terminal window. In your terminal window, you should now see output similar to the following:

```bash
Waiting for launch data... Done
Created app 'self-host-umami-neon' in organization 'personal'
Admin URL: https://fly.io/apps/self-host-umami-neon
Hostname: self-host-umami-neon.fly.dev
Wrote config file fly.toml
Validating /Users/rishi/Desktop/test/fly.toml
✓ Configuration is valid
==> Building image
Searching for image 'ghcr.io/umami-software/umami:postgresql-latest' remotely...
image found: img_8rlxp2mjm9g43jqo

Watch your deployment at https://fly.io/apps/self-host-umami-neon/monitoring

Provisioning ips for self-host-umami-neon
  Dedicated ipv6: 2a09:8280:1::2b:b52c:0
  Shared ipv4: 66.241.124.197
  Add a dedicated ipv4 with: fly ips allocate-v4

This deployment will:
 * create 2 "app" machines
```

Once the deployment is ready, you are left with just one step &#8212; to set the `DATABASE_URL` environment variable that we obtained in the previous section. We'll do that in the next section.

## Configure Neon Postgres as serverless database for self-hosted Umami analytics

In your Fly.io [Dashboard > Apps](https://fly.io/dashboard), click on your app name, and you will be taken to the overview of your app on Fly.io.

![](/guides/images/self-hosting-umami-neon/307262264-17f870e2-379f-4d80-b37e-6dac9075174c.png)

Click on `Secrets` in the left sidebar, and then click on `New Secret` on the top right corner to start creating an environment variable for your app.

![](/guides/images/self-hosting-umami-neon/307263442-b835f320-8ae8-4a8a-a4f0-dbeb9e07530f.png)

In the modal, set the name of the secret as `DATABASE_URL`, and set the `Secret` value to be the one that we obtained in the previous section. Click **Set secret** to save the environment variable.

![](/guides/images/self-hosting-umami-neon/307263972-75bef039-f4d1-4b7d-a66d-dd312290a6d1.png)

Great! With that done, you have succesfully ensured that each deployment of your app on Fly.io will have the database URL pointing to the Neon Postgres instance. Let's trigger a deploy to see it all in action.

## Deploy To Fly.io

You can now deploy your app to Fly by running the following command:

```bash
flyctl deploy
```

Once deployed, you will be able to log into your self hosted Umami analytics instance with the default credentials, i.e.; **admin** as the username & **umami** as the password. You will then be able to create new websites and analyze the traffic to those sites.

## Summary

In this guide, you learned how to run your own Umami analytics instance for analytics with Fly.io, powered by Neon postgres as your database.

<NeedHelp />


# Efficiently Syncing 60 Million Rows from Snowflake to Postgres

---
title: Efficiently Syncing 60 Million Rows from Snowflake to Postgres
subtitle: A comprehensive guide on optimizing data transfer from Snowflake to Postgres using chunking and upsert strategies.
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-11-26T00:00:00.000Z'
updatedOn: '2024-11-26T00:00:00.000Z'
---

Transferring large datasets can be challenging, especially when dealing with memory constraints and the need for atomic operations. This guide will provide a structured approach to efficiently sync data from Snowflake to Postgres, ensuring minimal memory usage and the ability to rollback in case of errors. In this guide, we will explore an efficient method to sync a large dataset (60 million rows) from Snowflake to Postgres on a nightly basis. We will discuss the challenges faced with the traditional `COPY INTO` method and provide a robust solution using chunking and upsert strategies.

A common scenario for synchronizing data from Snowflake to Postgres involves integrating third-party datasets sourced from the Snowflake Marketplace that necessitate transformation. This data can subsequently be loaded into Neon, facilitating the creation of relationships between the Snowflake tables and the Postgres tables. Such an integration enables application-facing data to be queried through a unified Object-Relational Mapping (ORM) framework.

## Table of Contents

- [Challenges with the easiest approach](#challenges-with-the-easiest-approach)
- [Proposed Solution](#proposed-solution)
- [When to Upsert or Copy](#when-to-upsert-or-copy)
- [Implementation steps for Chunking](#implementation-steps-for-chunking)

## Challenges with the easiest approach

The easiest method involves saving data from Snowflake to a local CSV file and then performing a massive `COPY INTO` operation to Postgres. This approach has several drawbacks, some of them being:

- **High Memory Usage**: Loading 60 million rows at once can lead to significant memory consumption.
- **Performance Issues**: The single transaction approach can lead to performance bottlenecks.

## Proposed Solution

To address these challenges, we recommend the following strategies:

1. **Chunking**: Split the data into smaller chunks (e.g., 5 million rows) to reduce memory usage.
2. **Upsert Operations**: Instead of truncating and copying, use upsert queries to handle both new and existing records efficiently.
3. **Automated Rollback**: Implement a rollback mechanism to ensure data integrity in case of errors.

## When to Upsert or Copy

If the data from Snowflake contains a mix of new rows and rows that need updates in Postgres, you should opt for the upsert method. This approach allows you to efficiently handle both new and existing records. However, if you choose to use the upsert method, ensure that you create relevant indexes to improve lookup speed during updates and conflict resolution via a key.

On the other hand, if your dataset consists solely of new records, the traditional truncate and copy approach may be more appropriate, as it simplifies the process and can be more performant in such cases.

## Implementation steps for Chunking

### 1. Chunking the Data

Use the `split` command to divide the CSV file into manageable chunks. For example, to split a large CSV file of 60 million rows into chunks of 5 million rows, use the following command:

```shell
split -l 5000000 large_data.csv chunk_
```

### 2. Python Script for Data Transfer

Below is a Python script that connects to Neon and processes each chunk (of 5 million rows). It uses the `psycopg2` library to handle database operations with automatic rollback in case of any errors.

```python
# File: sync_script.py

import glob
import psycopg2
from psycopg2 import sql, DatabaseError

# Database connection parameters
db_params = {
    "dbname": "neondb",
    "user": "neondb_owner",
    "password": "...",
    "host": "ep-...us-east-2.aws.neon.tech",
    "port": 5432
}

tableName = "my_table"

# Read all files that have chunk_ in the present directory
chunk_files = glob.glob("chunk_*")

try:
    # Connect to the database
    conn = psycopg2.connect(**db_params)
    conn.autocommit = False  # Enable manual transaction management
    cur = conn.cursor()

    for chunk in chunk_files:
        with open(chunk, 'r') as f:
            print(f"Processing {chunk}...")
            try:
                cur.copy_expert(sql.SQL("COPY {} FROM STDIN WITH CSV").format(sql.Identifier(tableName)), f)
                # Commit after successfully processing the chunk
                conn.commit()
                print(f"Successfully loaded {chunk}")
            except Exception as e:
                # Rollback all changes if any chunk fails
                conn.rollback()
                print(f"Error processing {chunk}: {e}")
                break  # Stop processing on first error

    cur.close()
    conn.close()
    print("All chunks processed.")

except DatabaseError as db_err:
    print(f"Database connection error: {db_err}")
    if conn:
        conn.rollback()
        conn.close()
```

The script above does the following:

- Uses glob to read all files in the current directory that match the pattern "chunk\_\*".
- Establishes a connection to the PostgreSQL database with manual transaction management enabled.
- Iterates over each chunk file, opening it for reading, and then uses the COPY command to load data from each chunk file into the specified table in the database.
- Commits the transaction after successfully processing each chunk; if an error occurs, it rolls back the transaction and stops further processing.
- Closes the database cursor and connection after processing all chunks or upon encountering an error.

### 3. Running the Script

To execute the script, run the following command in your terminal:

```shell
python3 sync_script.py
```

### 4. Maintenance

After the data transfer, consider running a `VACUUM` command to clean up unnecessary storage and reclaim space:

```sql
VACUUM ANALYZE table_name;
```

## Conclusion

By implementing chunking and upsert strategies, you can efficiently sync large datasets from Snowflake to Neon while minimizing memory usage and ensuring data integrity. This approach not only improves performance but also provides a robust error handling mechanism.

With these strategies in place, you can confidently manage your nightly data syncs without the risk of overwhelming your system resources.

<NeedHelp />


# Database Migrations in Spring Boot with Flyway and Neon

---
title: Database Migrations in Spring Boot with Flyway and Neon
subtitle: Learn how to manage database schema changes in a Spring Boot application using Flyway with Neon Postgres.
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-09-07T00:00:00.000Z'
updatedOn: '2024-09-07T00:00:00.000Z'
---

Database schema management is an essential part of every application development and maintenance process.

As your application grows, you need a reliable way to manage database changes across different environments.

This guide will walk you through setting up and using [Flyway](https://github.com/flyway/flyway) for database migrations in a [Spring Boot](https://github.com/spring-projects/spring-boot) application with Neon Postgres.

## Prerequisites

Before we begin, ensure you have:

- Java Development Kit installed
- [Maven](https://maven.apache.org/) for dependency management
- A [Neon](https://console.neon.tech/signup) account for serverless Postgres
- Basic familiarity with Spring Boot and SQL

Instead of Maven, you can use Gradle for dependency management. The steps will be similar but for this guide, we'll use Maven.

## Setting up the Project

1. Let's create a new Spring Boot project using [Spring Initializr](https://start.spring.io/) with the following dependencies:

   - Spring Web
   - Spring Data JPA
   - PostgreSQL Driver
   - Flyway Migration

   ![](https://imgur.com/KRACyq7.png)

   Once you've selected the dependencies, click "Generate" to download the project. Then, extract the ZIP file and open it in your favorite IDE.

2. If you're using Maven, your `pom.xml` should include these dependencies:

   ```xml
   <dependencies>
       <dependency>
           <groupId>org.springframework.boot</groupId>
           <artifactId>spring-boot-starter-data-jpa</artifactId>
       </dependency>
       <dependency>
           <groupId>org.springframework.boot</groupId>
           <artifactId>spring-boot-starter-web</artifactId>
       </dependency>
       <dependency>
           <groupId>org.flywaydb</groupId>
           <artifactId>flyway-core</artifactId>
       </dependency>
       <dependency>
           <groupId>org.postgresql</groupId>
           <artifactId>postgresql</artifactId>
           <scope>runtime</scope>
       </dependency>
   </dependencies>
   ```

## Configuring the Database Connection

Now that we have our project set up, let's configure the database connection.

To configure your Neon database connection details, open the `application.properties` file in `src/main/resources` and add the following properties:

```properties
spring.datasource.url=jdbc:postgresql://<your-neon-hostname>/<your-database-name>
spring.datasource.username=<your-username>
spring.datasource.password=<your-password>

spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.PostgreSQLDialect
spring.jpa.hibernate.ddl-auto=validate

spring.flyway.enabled=true
spring.flyway.locations=classpath:db/migration
```

Replace the placeholders with your actual Neon database credentials.

Note that we set `spring.jpa.hibernate.ddl-auto=validate` to prevent Hibernate from automatically modifying the schema. Flyway will handle all schema changes.

To learn more about managing your database schema using Hibernate, refer to the [Database Schema Changes with Hibernate, Spring Boot, and Neon](/guides/spring-boot-hibernate) guide.

## Creating Migration Scripts

Flyway uses SQL scripts for migrations. These scripts should be placed in the `src/main/resources/db/migration` directory.

Unlike other migration tools, Flyway uses a version-based naming convention for migration scripts so that it can track the order in which they should be applied. This ensures that migrations are applied in the correct order and only once, but you need to be careful when renaming or modifying existing scripts.

Naming convention for migration scripts:

- `V<VERSION>__<DESCRIPTION>.sql`
- Example: `V2__Create_users_table.sql`

We will start with `V2__` as the first migration script, as Flyway will use `V1__` for its internal schema history table.

Let's create our first migration script:

1. Create a file named `V2__Create_users_table.sql` in `src/main/resources/db/migration`:

   ```sql
   CREATE TABLE users (
       id SERIAL PRIMARY KEY,
       username VARCHAR(50) NOT NULL UNIQUE,
       email VARCHAR(100) NOT NULL UNIQUE,
       created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
   );
   ```

2. Create another file named `V3__Create_posts_table.sql`:

   ```sql
   CREATE TABLE posts (
       id SERIAL PRIMARY KEY,
       title VARCHAR(100) NOT NULL,
       content TEXT,
       user_id INTEGER NOT NULL,
       created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
       FOREIGN KEY (user_id) REFERENCES users(id)
   );
   ```

These scripts will create the `users` and `posts` tables in your Neon database when you run the migrations.

## Running Migrations

Now that we've configured Flyway with our Spring Boot application and Neon database, we can proceed to run the database migrations.

There are two primary methods to execute Flyway migrations: using the Flyway Maven plugin or programmatically through the Flyway API. Let's explore both approaches in detail.

### 1. Using the Flyway Maven Plugin

The Flyway Maven plugin allows you to run migrations directly from the command line, which can be useful for CI/CD pipelines or local development.

First, add the Flyway Maven plugin to your `pom.xml` file:

```xml
<build>
    <plugins>
        <plugin>
            <groupId>org.flywaydb</groupId>
            <artifactId>flyway-maven-plugin</artifactId>
            <version>8.0.0</version>
            <configuration>
				<url>jdbc:postgresql://<your_neon_hostname>/neondb?sslmode=require</url>
                <user>${spring.datasource.username}</user>
                <password>${spring.datasource.password}</password>
                <locations>
                    <location>classpath:db/migration</location>
                </locations>
            </configuration>
        </plugin>
    </plugins>
</build>
```

Next, run the following command to create the schema history table in your database:

```bash
mvn flyway:baseline
```

This will create the `flyway_schema_history` table if it doesn't already exist. The table is used by Flyway to track the applied migrations.

Now, you can run the following command to apply pending migrations:

```bash
mvn flyway:migrate
```

This command will execute all pending migrations in the order defined by their version numbers.

Additional useful Flyway Maven plugin commands include:

- `mvn flyway:info`: Displays the status of all migrations. This includes the version, description, type, and state of each migration.
- `mvn flyway:validate`: Validates the applied migrations against the available ones. This ensures that the schema history table is correct and that all migrations were applied successfully.
- `mvn flyway:repair`: Repairs the schema history table. This command is useful if you manually modify the schema history table or if a migration fails.

### 2. Using the Flyway API Programmatically

For more fine-grained control or to integrate migration execution within your application lifecycle, you can use the Flyway API programmatically.

Start by creating a configuration class to set up the Flyway bean:

```java
package com.example.neon;

import org.flywaydb.core.Flyway;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import javax.sql.DataSource;

@Configuration
public class FlywayConfig {

    @Bean(initMethod = "migrate")
    public Flyway flyway(DataSource dataSource) {
        return Flyway.configure()
                .dataSource(dataSource)
                .locations("classpath:db/migration")
                .baselineOnMigrate(true)
                .load();
    }
}
```

This configuration automatically triggers the migration when the application starts. You can now run your Spring Boot application to apply the migrations:

```bash
mvn spring-boot:run
```

Alternatively, if you want more control over when migrations run, you can remove the `initMethod = "migrate"` and call the `migrate()` method manually:

```java
@Service
public class DatabaseMigrationService {

    private final Flyway flyway;

    @Autowired
    public DatabaseMigrationService(Flyway flyway) {
        this.flyway = flyway;
    }

    public void migrateDatabase() {
        flyway.migrate();
    }
}
```

You can then inject this service and call the `migrateDatabase()` method when appropriate, such as during application startup or as part of a maintenance routine.

## Handling Schema Changes

As your application evolves, you'll need to make changes to your database schema. Here's how to handle common scenarios:

### Adding a New Column

To add a new column to an existing table, you will just need to create a new migration script with the `ALTER TABLE` statement.

Create a new migration script, e.g., `V4__Add_user_role.sql`:

```sql
ALTER TABLE users ADD COLUMN role VARCHAR(20);
```

After adding the new migration script, you can run the migration using the Flyway Maven plugin or programmatically by starting the Spring Boot application depending on your preferred method.

### Modifying an Existing Column

To modify an existing column, you can create a new migration script with the `ALTER TABLE` statement, e.g., `V5__Modify_user_role.sql`:

```sql
ALTER TABLE users ALTER COLUMN role SET NOT NULL;
```

After adding the new migration script, you can check the status of your migrations using the Flyway Maven plugin:

```bash
mvn flyway:info
```

You should see the new migration in the list with a `Pending` state indicating that it hasn't been applied yet:

```sql
+-----------+---------+-----------------------+----------+---------------------+----------+
| Category  | Version | Description           | Type     | Installed On        | State    |
+-----------+---------+-----------------------+----------+---------------------+----------+
|           | 1       | << Flyway Baseline >> | BASELINE | 2024-09-07 16:27:26 | Baseline |
| Versioned | 2       | Create users table    | SQL      | 2024-09-07 16:34:20 | Success  |
| Versioned | 3       | Create posts table    | SQL      | 2024-09-07 16:34:23 | Success  |
| Versioned | 4       | Add user role         | SQL      | 2024-09-07 16:40:03 | Success  |
| Versioned | 5       | Modify user role      | SQL      |                     | Pending  |
+-----------+---------+-----------------------+----------+---------------------+----------+
```

Then run the migration using your preferred method to apply the changes.

### Creating a New Table

To create a new table, you would just add a new migration script with the `CREATE TABLE` statement, e.g., `V6__Create_comments_table.sql`:

```sql
CREATE TABLE comments (
    id SERIAL PRIMARY KEY,
    post_id INTEGER NOT NULL,
    user_id INTEGER NOT NULL,
    content TEXT NOT NULL,
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (post_id) REFERENCES posts(id),
    FOREIGN KEY (user_id) REFERENCES users(id)
);
```

## Handling Rollbacks

The Flyway Community Edition doesn't support automatic rollbacks. When you need to roll back a migration, you can create a new migration script to undo the changes. This script should be named with a higher version number than the original migration.

Create a file named `V7__Remove_user_role.sql`:

```sql
ALTER TABLE users DROP COLUMN role;
```

Then run the migration as usual to apply the rollback. This will remove the `role` column from the `users` table as defined in the script.

The Flyway Pro and Enterprise Editions offer additional features like `undo` and `repair` commands for automatic rollback and fixing failed migrations. You can explore these options if you require more advanced rollback capabilities.

## Best Practices

There are several things to keep in mind when managing database migrations:

1. Always keep your migration scripts in version control along with your application code.

2. Make sure that your migrations can be applied multiple times without changing the result beyond the initial application.

3. When possible, write migrations that are backward compatible with the previous version of your application. This will make it easier to roll back changes if needed.

4. Test your migrations thoroughly in a non-production environment before applying them to production. A great way to do this is by using the Neon branching feature to create a separate environment for testing with your production data without affecting the live environment.

5. Once a migration has been applied to any environment, avoid modifying it. Instead, create a new migration to make further changes.

## Conclusion

Using Flyway with Spring Boot and Neon Postgres provides a production ready solution for managing database schema changes. By following these practices, you can ensure that your database schema evolves safely and consistently across all environments.

Remember to always test your migrations thoroughly and have a solid backup and rollback strategy in place. Neon's features like branching and point-in-time recovery can be a great addition to your already existing lifecycle of your database schema.

## Additional Resources

- [Flyway Documentation](https://flywaydb.org/documentation/)
- [Spring Boot Flyway Integration](https://docs.spring.io/spring-boot/docs/current/reference/html/howto.html#howto.data-initialization.migration-tool.flyway)
- [Neon Documentation](/docs)


# Database Schema Changes with Hibernate, Spring Boot, and Neon

---
title: Database Schema Changes with Hibernate, Spring Boot, and Neon
subtitle: Learn how to manage database schema changes with Hibernate, Spring Boot, and Neon Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-09-07T00:00:00.000Z'
updatedOn: '2024-09-07T00:00:00.000Z'
---

Managing database schema changes is an important aspect of any application development lifecycle.

When using Hibernate ORM with Spring Boot and Neon Postgres, you have several options for handling schema evolution.

This guide will explore different approaches, their pros and cons, and best practices for managing database schema changes.

## Prerequisites

Before we begin, ensure you have:

- Java Development Kit (JDK) 11 or later
- Maven or Gradle for dependency management
- A [Neon](https://console.neon.tech/signup) account for serverless Postgres
- Basic familiarity with Spring Boot, Hibernate, and JPA concepts

## Setting up the Project

1. Create a new Spring Boot project using [Spring Initializr](https://start.spring.io/) with the following dependencies:

   - Spring Web
   - Spring Data JPA
   - PostgreSQL Driver

2. If you're using Maven, your `pom.xml` should include these dependencies:

   ```xml
   <dependencies>
       <dependency>
           <groupId>org.springframework.boot</groupId>
           <artifactId>spring-boot-starter-data-jpa</artifactId>
       </dependency>
       <dependency>
           <groupId>org.springframework.boot</groupId>
           <artifactId>spring-boot-starter-web</artifactId>
       </dependency>
       <dependency>
           <groupId>org.postgresql</groupId>
           <artifactId>postgresql</artifactId>
           <scope>runtime</scope>
       </dependency>
   </dependencies>
   ```

3. Extract the project and open it in your favorite IDE.

## Configuring the Database Connection

Next, configure your application to connect to a Neon Postgres database. To do that define your Neon database connection in `application.properties`:

```properties
spring.datasource.url=jdbc:postgresql://<your-neon-hostname>/<your-database-name>
spring.datasource.username=<your-username>
spring.datasource.password=<your-password>
```

Replace the placeholders with your actual Neon database credentials.

While modifying the `application.properties` file, you can also configure Hibernate's DDL behavior and other properties:

```
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.PostgreSQLDialect
spring.jpa.hibernate.ddl-auto=update
```

We will revisit the `spring.jpa.hibernate.ddl-auto` property later in this guide.

## Approaches to Schema Management

Before we dive into specific schema changes, let's explore different approaches to managing schema changes with Hibernate and Spring Boot.

### 1. Hibernate Auto DDL (Development Only)

The `spring.jpa.hibernate.ddl-auto` property controls Hibernate's schema generation behavior.

The different possible values are:

- `create`: Drops and recreates the schema on each startup
- `create-drop`: Creates the schema on startup and drops it on shutdown
- `update`: Updates the schema if necessary and doesn't drop existing tables
- `validate`: Validates the schema but makes no changes
- `none`: Disables DDL handling

For development, you might use the `update` strategy in your `application.properties`:

```properties
spring.jpa.hibernate.ddl-auto=update
```

This allows Hibernate to automatically update the schema based on your entity classes.

**Pros:**

- Easy to use during development
- Automatically reflects changes in entity classes

**Cons:**

- Not suitable for production use due to potential data loss or corruption
- Doesn't provide fine-grained control over schema changes

The `update` strategy is useful for development but should be avoided in production environments due to the risk of data loss or corruption.

Another option is to use `validate` in production to prevent accidental schema changes. Or you can disable auto DDL and manage schema changes manually or programmatically.

A handy option is to use `create-drop` for integration tests to recreate the schema before each test run. This ensures a clean database state for each test, however you should not use this in production as it will drop the database on shutdown.

### 2. Schema Generation Scripts

Hibernate can generate schema creation and update scripts based on your entity mappings.

Add these properties to `application.properties`:

```properties
spring.jpa.properties.javax.persistence.schema-generation.scripts.action=create
spring.jpa.properties.javax.persistence.schema-generation.scripts.create-target=create.sql
spring.jpa.properties.javax.persistence.schema-generation.scripts.create-source=metadata
```

This generates a `create.sql` file in your project root, which you can then manually review and apply to your database.

**Pros:**

- Provides a SQL script that you can review and modify
- Allows for version control of schema changes

**Cons:**

- Requires manual application of scripts
- Doesn't handle incremental updates well

### 3. Programmatic Schema Management

You can use Hibernate's `SchemaManagementTool` for more control over schema updates. This allows you to programmatically create, update, or validate the schema.

You can call this method on application startup or trigger it manually when needed.

**Pros:**

- Provides programmatic control over schema updates
- Can be integrated into your application's lifecycle

**Cons:**

- Requires careful management to avoid unintended schema changes
- May not handle all types of schema changes smoothly

### 4. Using a Migration Tool (Recommended for Production)

For production environments, it's recommended to use a dedicated migration tool like Flyway or Liquibase. These tools provide better control, versioning, and rollback capabilities.

To use Flyway, add the dependency to your `pom.xml`:

```xml
<dependency>
    <groupId>org.flywaydb</groupId>
    <artifactId>flyway-core</artifactId>
</dependency>
```

Then create migration scripts in `src/main/resources/db/migration` following Flyway's naming convention (e.g., `V1__Create_user_table.sql`).

**Pros:**

- Provides fine-grained control over schema changes
- Supports versioning and rollbacks
- Works well in production environments

**Cons:**

- Requires manual creation of migration scripts
- Adds complexity to the development process

For more information on using Flyway with Spring Boot, refer to the [Database Migrations in Spring Boot with Flyway and Neon](/guides/spring-boot-flyway) guide.

## Using Hibernate auto DDL

Now that we've covered different approaches to schema management, let's look at how to handle specific schema changes using Hibernate and Spring Boot with Neon Postgres.

As we pointed out earlier, Hibernate's auto DDL feature is convenient for development but not recommended for production use. Let's see how it works and how to handle common schema changes.

Start by setting `spring.jpa.hibernate.ddl-auto=update` in your `application.properties` file and then follow the examples below.

### Creating a New Entity

Once you've set the Hibernate auto DDL property to `update`, Hibernate will automatically create tables based on your entity classes. Start by creating a new entity class in your project called `Product`:

```java
@Entity
public class Product {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(nullable = false)
    private String name;

    @Column(nullable = false)
    private BigDecimal price;
}
```

When you run your Spring Boot application, Hibernate will create the `products` table automatically with the `id`, `name`, and `price` columns based on your entity class.

### Adding a New Column

Now that you have the `Product` entity, let's add a new column to the `products` table.

1. Add the new field to your entity class:

   ```java
   @Entity
   public class Product {
       // ...existing fields

       @Column(name = "description")
       private String description;
   }
   ```

2. Run your application, and Hibernate will automatically add the `description` column to the `products` table. Quite convenient for development!

### Renaming and Dropping Columns

When using `update`, Hibernate doesn't handle column rename or drop operations automatically. You'll need to use migration scripts for these changes.

1. Update the `@Column` annotation in your entity:

   ```java
   @Column(name = "new_column_name")
   private String oldColumnName;
   ```

2. Create a migration script to rename the column:

   ```sql
   ALTER TABLE users RENAME COLUMN old_column_name TO new_column_name;
   ```

The same applies to dropping columns, where you'll need to create a migration script to drop the column from the table:

    ```sql
    ALTER TABLE users DROP COLUMN column_name;
    ```

For those types of changes, you should use a migration tool like Flyway or Liquibase to manage the schema changes.

An alternative approach here is to, use `create` or `create-drop` for `spring.jpa.hibernate.ddl-auto`. This will recreate the schema on each startup, which can be useful for development but can not be used in production as it will lead to data loss.

## Best Practices

With the various approaches to schema management in mind, here are some best practices to follow when managing database schema changes with Hibernate, Spring Boot, and Neon Postgres:

1. While Hibernate's auto DDL is convenient for development, use a dedicated migration tool like [Flyway](/guides/spring-boot-flyway) for production environments.

2. Keep your entity classes and migration scripts in version control.

3. Always test schema changes in a non-production environment before applying them to production. A great way to do this is by using Neon's [branching feature](/docs/introduction/branching).

4. When possible, make schema changes that are backward compatible with the previous version of your application.

5. Make small, incremental changes rather than large, sweeping changes to your schema.

6. When using Hibernate's schema generation, always review the generated SQL before applying it to your database.

## Conclusion

Managing database schema changes with Hibernate, Spring Boot, and Neon requires careful consideration of your development workflow and production requirements.

While Hibernate's auto DDL feature is convenient for development, a more controlled approach using migration tools is recommended for production environments.

Always test your schema changes thoroughly in a non-production environment before applying them to your production database. With careful planning and the right tools, you can maintain a flexible and evolving database schema that supports your application's growth.

## Additional Resources

- [Hibernate ORM Documentation](https://hibernate.org/orm/documentation/5.4/)
- [Spring Boot JPA Documentation](https://docs.spring.io/spring-boot/docs/current/reference/html/data.html#data.sql.jpa-and-spring-data)
- [Flyway Documentation](https://flywaydb.org/documentation/)
- [Neon Documentation](/docs)


# Using Strapi CMS with Neon Postgres and Astro to build a blog

---
title: Using Strapi CMS with Neon Postgres and Astro to build a blog
subtitle: A step-by-step guide for building your own blog in an Astro application with Strapi CMS and Postgres powered by Neon
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-06-06T00:00:00.000Z'
updatedOn: '2024-06-06T00:00:00.000Z'
---

In this guide, you will learn how to set up a serverless Postgres database with Neon, configure Strapi CMS with Postgres, define a blog schema, and author content using Strapi CMS. The guide also covers configuring API read permissions and building a dynamic frontend with Astro to display blog pages based on Strapi content.

## Prerequisites

To follow the steps in this guide, you will need the following:

- [Node.js 18](https://nodejs.org/en) or later
- A [Neon](https://console.neon.tech/signup) account

## Steps

- [Provisioning a serverless Postgres database powered by Neon](#provisioning-a-serverless-postgres-database-powered-by-neon)
- [Setting up Strapi locally with Postgres](#setting-up-strapi-locally-with-postgres)
- [Configure a blog schema in Strapi CMS](#configure-a-blog-schema-in-strapi-cms)
- [Configure API read permissions in Strapi CMS](#configure-api-read-permissions-in-strapi-cms)
- [Create a new Astro application](#create-a-new-astro-application)
- [Integrate Tailwind CSS in your Astro application](#integrate-tailwind-css-in-your-astro-application)
- [Create dynamic blog routes in Astro](#create-dynamic-blog-routes-in-astro)
- [Build and test your Astro application locally](#build-and-test-your-astro-application-locally)
- [Scale-to-zero with Postgres (powered by Neon)](#scale-to-zero-with-postgres-powered-by-neon)

## Provisioning a serverless Postgres database powered by Neon

Using a serverless Postgres database powered by Neon lets you scale compute resources down to zero, which helps you save on compute costs.

To get started, go to the [Neon console](https://console.neon.tech/app/projects) and create a project.

You will then be presented with a dialog that provides a connection string of your database. Click on **Pooled connection** option and the connection string automatically updates to a pooled connection string.

![Neon Connection Details](/guides/images/strapi-cms/20b94d5f-aff4-4594-b60b-3a65d4fc884c.png)

All Neon connection strings have the following format:

```bash
postgres://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>?sslmode=require
```

- `<user>` is the database user.
- `<password>` is the database user’s password.
- `<endpoint_hostname>.neon.tech` is the host with `neon.tech` as the [top-level domain (TLD)](https://www.cloudflare.com/en-gb/learning/dns/top-level-domain/).
- `<port>` is the Neon port number. The default port number is 5432.
- `<dbname>` is the name of the database. **neondb** is the default database created with each Neon project if you do not define your own.
- `?sslmode=require` is an optional query parameter that enforces [SSL](https://www.cloudflare.com/en-gb/learning/ssl/what-is-ssl/) mode for better security when connecting to the Postgres instance.

Each of the above values (except `sslmode`) is used in the next step &#8212; creating a local instance of the Strapi CMS application with Postgres.

## Setting up Strapi locally with Postgres

Let's begin with creating a Strapi CMS backend to serve the content for blog posts. Open your terminal and run the following command:

```bash
npx create-strapi-app@latest blog-api
```

`npx create-strapi-app` is the recommended way to scaffold a Strapi CMS project quickly.

When prompted, choose the following:

![Create Strapi CMS project](/guides/images/strapi-cms/bbdb6810-5336-4a10-9feb-c63b27874a80.png)

- `Custom (manual) settings` as the installation type.
- `TypeScript` as the preferred language.
- `postgres` as the default database client.
- `neondb` as the database name.
- `<endpoint_hostname>.neon.tech` as the host.
- `5432` as the port.
- `neondb_owner` as the username.
- `<password>` as the password.
- `y` to enable an SSL connection.

Once that’s done, change to the project directory and start the app:

```bash
cd blog-api
yarn develop
```

The command `strapi develop` runs, which takes care of creating the minimal schema required by Strapi CMS in the Neon Postgres database. When the setup is complete, you will be taken to `http://localhost:1337/admin` automatically. You will need to create an account for your locally hosted Strapi CMS instance. Strapi CMS makes sure to store the credentials (and all other data) in your Neon Postgres database.

![](/guides/images/strapi-cms/52fbde59-04bd-4af0-9e85-5bb33694d7a5.png)

To proceed, click the **Let's start** button to access the admin dashboard. Now, let's learn how to create the blog schema in Strapi CMS.

## Configure a blog schema in Strapi CMS

Once you have logged into the admin dashboard, it will by default, take you to the **Content-Type Builder** section. Here's where you can start to create the Blog schema. Click on **+ Create new collection type** to get started.

![Content-Type Builder](/guides/images/strapi-cms/55407bd1-68d4-4251-af02-571ef400f943.png)

Enter the **Display name** for your blog schema. For example, name it **Blog**, and click **Continue**.

![Create a collection type](/guides/images/strapi-cms/3f6b865d-100c-46ca-980d-c70ecefb4e86.png)

The application now asks you to create the first field inside the schema. Let's start with the **Title** of the blog first. Select **Text**, and it will automatically take you to the field detail section.

![Select a field](/guides/images/strapi-cms/6118918b-e0f2-493a-a80f-366b1577246c.png)

Enter **Title** as the Name of the first field and click **Finish**.

![Add new title field](/guides/images/strapi-cms/c4570e47-7a3e-4dfe-b855-6c461fe94748.png)

Once that's done, you are taken to an overview of the newly created **Blog** schema. Let's add two more fields. Click on **+ Add another field to this collection type**.

![Add another field](/guides/images/strapi-cms/e7437391-5fdf-4cbd-a922-fd500ab1f6d2.png)

First, you will add an `image` to be associated with the blog. Select **Media** and you are automatically taken to the asset detail section.

![Add media](/guides/images/strapi-cms/9a96ebf1-a6c7-4805-a219-c5b56f7a5b30.png)

Enter **Image** as the Name of the image field and click **Finish**.

![Add image field](/guides/images/strapi-cms/45f62587-a717-4b39-a47e-4d57b4e83517.png)

Next, you will add a `markdown` to be associated with the blog. Select **Rich Text (Markdown)**. You are automatically taken to the field detail section.

![Add Markdown field](/guides/images/strapi-cms/b5a13e5d-139a-44bd-9a36-751785aec8f8.png)

Enter **text** as the Name of the markdown field and click **Finish**.

![Enter markdown field name](/guides/images/strapi-cms/76b789ff-a15f-48ba-84a4-0cfe2cb37eb7.png)

Great! Click **Save** to save the present configuration.

![Configuration saved](/guides/images/strapi-cms/32a0b6ca-e7fd-4f6e-9942-4d63cc66d02b.png)

Click on the **Content Manager** button in the sidebar to start adding your first blog content. Click **+ Create new entry** to get started.

![Content Manager](/guides/images/strapi-cms/c735741c-1b1d-49f0-8f61-4ba210f799a6.png)

Enter the **Title**, select the **Image**, and input the markdown associated with the blog. You are now done writing your first post in the local Strapi CMS instance. All the data is synchronized in Postgres (powered by Neon). To finish off the content creation process, click **Save** and **Publish**.

![Create an entry](/guides/images/strapi-cms/045777e9-a898-490a-bf2b-271858e7ba6a.png)

With that done, let's move on to configuring read permissions for connected clients to access the data corresponding to the blog schema via an API.

## Configure API Read Permissions in Strapi CMS

To be able to fetch the data authored in your local Strapi CMS instance, you will need to configure what is readable and writeable using APIs. Navigate to **Settings > API Tokens** in your admin dashboard. Click on **Create new API Token** to start creating a new API token.

![API tokens](/guides/images/strapi-cms/846d9a45-138c-4474-b34d-33ddc8027e03.png)

Enter a Name to be associated with the token and set **Token Duration** to **unlimited** for the sake of this example. Finally, click **Save** to obtain the API token.

![Create API token](/guides/images/strapi-cms/10f85000-9b9b-428e-ab03-3faaa54cbaf3.png)

Copy the API token and store it somewhere safe as **STRAPI_API_TOKEN**.

Now, let's move on to creating an Astro application to create dynamic blog pages based on blog data that's accessible via your locally hosted instance of Strapi CMS.

## Create a new Astro application

Let’s get started by creating a new Astro project. Open your terminal and run the following command:

```bash
npm create astro@latest blog-ui
```

`npm create astro` is the recommended way to scaffold an Astro project quickly.

When prompted, choose:

- `Empty` when prompted on how to start the new project.
- `Yes` when prompted if plan to write Typescript.
- `Strict` when prompted how strict Typescript should be.
- `Yes` when prompted to install dependencies.
- `Yes` when prompted to initialize a git repository.

Once that’s done, change to the project directory and start the app:

```bash
cd blog-ui
npm run dev
```

The app should be running on [localhost:4321](http://localhost:4321/). Let's close the development server for now.

Next, execute the following command to install the necessary libraries and packages for building the application:

```bash
npm install dotenv marked @tailwindcss/typography
npm install -D @types/node
```

The commands above install the packages, with the `-D` flag specifying the libraries intended for development purposes only.

The libraries installed include:

- [dotenv](https://npmjs.com/package/dotenv): A library for handling environment variables.
- [marked](https://npmjs.com/package/marked): A markdown parser and compiler.
- [@tailwindcss/typography](https://npmjs.com/package/@tailwindcss/typography): A set of `prose` classes for HTML rendered from Markdown or pulled from a CMS.

The development-specific libraries include:

- [@types/node](https://npmjs.com/package/@types/node): Type definitions for node.

Then, add the following lines to your `tsconfig.json` file to make relative imports within the project easier:

```diff
{
  "extends": "astro/tsconfigs/strict",
  "compilerOptions": { // [!code ++]
    "baseUrl": ".", // [!code ++]
    "paths": { // [!code ++]
      "@/*": ["src/*"] // [!code ++]
    } // [!code ++]
  } // [!code ++]
}
```

Now, create a `.env` file. You are going to add the API token obtained earlier.

The `.env` file should contain the following keys:

```bash
# .env

STRAPI_API_TOKEN="..."
```

## Integrate Tailwind CSS in your Astro application

For styling the app, you will use Tailwind CSS. Install and set up Tailwind at the root of your project's directory by running:

```bash
npx astro add tailwind
```

When prompted, choose:

- `Yes` when prompted to install the Tailwind dependencies.
- `Yes` when prompted to generate a minimal `tailwind.config.mjs` file.
- `Yes` when prompted to make changes to the Astro configuration file.

The command finishes integrating TailwindCSS into your Astro project and installs the following dependencies:

- `tailwindcss`: TailwindCSS as a package to scan your project files to generate corresponding styles.
- `@astrojs/tailwind`: The adapter that brings Tailwind's utility CSS classes to every `.astro` file and framework component in your project.

To load pre-configured styles for your HTML (rendered from markdown), update your `tailwind.config.mjs` as follows:

```tsx
/** @type {import('tailwindcss').Config} */
export default {
  content: ['./src/**/*.{astro,html,js,jsx,md,mdx,svelte,ts,tsx,vue}'],
  theme: {
    extend: {},
  },
  plugins: [require('@tailwindcss/typography')], // [!code ++]
};
```

## Create dynamic blog routes in Astro

To programmatically create pages as you keep authoring more content in your locally hosted Strapi CMS, you are going to use [dynamic routes](https://docs.astro.build/en/guides/routing/#dynamic-routes) in Astro. With dynamic routes, you create a single file with a name like `[slug].astro`, where slug represents a [unique and dynamic variable](https://docs.astro.build/en/reference/api-reference/#contextparams) for each blog. Using [getStaticPaths](https://docs.astro.build/en/reference/api-reference/#getstaticpaths), you can programmatically create multiple blog pages with custom data using Strapi CMS as your data source. Let's see this in action. Create a file named `[slug].astro` in the `src/pages` directory with the following code:

```astro
---
// File: src/pages/[slug].astro

import "dotenv/config";
import { marked } from 'marked';

export async function getStaticPaths() {
  const response = await fetch(`http://127.0.0.1:1337/api/blogs?populate=Image`, {
    headers: {
      "Content-Type": "application/json",
      Authorization: `Bearer ${process.env.STRAPI_API_TOKEN}`,
    },
  });
  const {data} = await response.json();
  return data.map(({ id, attributes }) => ({
    params: { slug: id },
    props: attributes,
  }));
}

const { Title, text, Image } = Astro.props

const markdown = marked.parse(text)
---

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" type="image/svg+xml" href="/favicon.svg" />
    <meta name="viewport" content="width=device-width" />
    <meta name="generator" content={Astro.generator} />
    <title>Astro</title>
  </head>
  <body>
    <h1>{Title}</h1>
    <img src={`http://127.0.0.1:1337${Image.data[0].attributes.url}`} />
    <article set:html={markdown} />
  </body>
</html>
```

Let's understand the code above in two parts:

- Inside `getStaticPaths` function, a fetch call is made to the locally hosted Strapi CMS API to get all the blogs with their **Title**, **Image** and **text** values. Looping over each blog item, an array is created that passes all the data obtained as the [**props**](https://docs.astro.build/en/reference/api-reference/#contextprops), and its **id** as the unique variable to be associated with each blog.

- The HTML section represents the content of a particular blog page. The blog data attributes such as Title, Image URL, and markdown are obtained from `Astro.props` (passed in `getStaticPaths` as props). Further, the markdown is parsed into HTML using the `marked` library, and injected into the DOM using `set:html` template directive of Astro.

## Build and Test your Astro application locally

To test the Astro application in action, prepare a build and run the preview server using the following command:

```bash
npm run build && npm run preview
```

## Scale-to-zero with Postgres (powered by Neon)

Interestingly, during the entire process of building this application, you have used Neon's **Scale-to-zero** feature which places your Postgres compute endpoint into an idle state after 5 minutes of inactivity. Click the **Operations** button in your Neon console sidebar to see when the compute was started and automatically suspended to reduce compute usage.

![Neon Monitoring page](/guides/images/strapi-cms/ee753f7d-3da8-4a4c-84c5-be7b6cdce486.png)

## Summary

In this guide, you learned how to build a blog in an Astro application using Strapi CMS and a serverless Postgres database (powered by Neon). Additionally, you learned how to create content collections in Strapi CMS and dynamic blog routes in an Astro application.

<NeedHelp />


# Building a High-Performance Sensor Data API with FastAPI and Postgres' TimescaleDB Extension

---
title: Building a High-Performance Sensor Data API with FastAPI and Postgres' TimescaleDB Extension
subtitle: Create an  API for streaming, storing, and querying sensor data using Postgres TimescaleDB and FastAPI
author: sam-harri
enableTableOfContents: true
createdAt: '2024-10-12T00:00:00.000Z'
updatedOn: '2024-10-12T00:00:00.000Z'
---

In this guide, you'll build a high-performance API for streaming, storing, and querying sensor data using FastAPI and TimescaleDB for efficient time-series data storage.
By combining FastAPI with TimescaleDB's advanced time-series features, you'll be able to maintain low latency queries even at the petabyte scale, making it perfect for things like IoT systems that generate large volumes of sensor data.

## Prerequisites

Before starting, ensure you have the following tools and services ready:

- `pip`: Required for installing and managing Python packages, including [uv](https://docs.astral.sh/uv/) for creating virtual environments. You can check if `pip` is installed by running the following command:
  ```bash
  pip --version
  ```
- Neon serverless Postgres : you will need a Neon account for provisioning and scaling your `PostgreSQL` database. If you don't have an account yet, [sign up here](https://console.neon.tech/signup).

## Setting up the Project

Follow these steps to set up your project and virtual environment:

1.  Create a `uv` project

    If you don't already have uv installed, you can install it with:

    ```bash
    pip install uv
    ```

    Once `uv` is installed, create a new project:

    ```bash
    uv init timescale_fastapi
    ```

    This will create a new project directory called `timescale_fastapi`. Open this directory in your code editor of your choice.

2.  Set up the virtual environment.

        You will now create and activate a virtual environment in which your project's dependencies will beinstalled.

        <CodeTabs labels={["Linux/macOS", "Windows"]}>

            ```bash
            uv venv
            source .venv/bin/activate
            ```

            ```bash
            uv venv
            .venv\Scripts\activate
            ```

        </CodeTabs>

        You should see `(timescale_fastapi)` in your terminal now, this means that your virtual environment is activated.

3.  Install dependencies.

    Next, add all the necessary dependencies for your project:

    ```bash
    uv add python-dotenv asyncpg loguru fastapi uvicorn requests
    ```

    where each package does the following:

    - `FastAPI`: A Web / API framework
    - `AsyncPG`: An asynchronous PostgreSQL client
    - `Uvicorn`: An ASGI server for our app
    - `Loguru`: A logging library
    - `Python-dotenv`: To load environment variables from a .env file

4.  Create the project structure.

    Create the following directory structure to organize your project files:

    ```md
    timescale_fastapi
    ├── src/
    │ ├── database/
    │ │ └── postgres.py
    │ ├── models/
    │ │ └── product_models.py
    │ ├── routes/
    │ │ └── product_routes.py
    │ └── main.py
    ├── .env  
    ├── .python-version
    ├── README.md  
    ├── pyproject.toml  
    └── uv.lock
    ```

## Setting up your Database

In this section, you will set up the `TimescaleDB` extension using Neon's console, add the database's schema, and create the database connection pool and lifecycle management logic in FastAPI. Optionally, you can also add some mock data to test your API endpoints.

Given TimescaleDB is an extension on top of vanilla Postgres, you must first add the extension by running the following SQL in the `SQL Editor` tab of the Neon console.

```sql
CREATE EXTENSION IF NOT EXISTS timescaledb;
```

Next, you will add the necessary tables to your database with:

```sql
CREATE TABLE IF NOT EXISTS sensors (
    sensor_id SERIAL PRIMARY KEY,
    sensor_type VARCHAR(50) NOT NULL,
    description VARCHAR(255),
    location VARCHAR(255)
);


CREATE TABLE IF NOT EXISTS sensor_data (
    sensor_id INT REFERENCES sensors(sensor_id),
    value FLOAT NOT NULL,
    time TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    PRIMARY KEY(sensor_id, time)
);
```

One of TimescaleDB's core features is `Hypertables`, which is an optimized abstraction for handling large time-series data. It partitions your data into chunks based on time, allowing efficient storage, querying, and performance at scale. By converting the sensor_data table into a hypertable, TimescaleDB will manage the underlying chunking and indexing automatically.

To convert the `sensor_data` table into a hypertable, use the following command:

```sql
SELECT create_hypertable('sensor_data', 'time');
```

Now that the schema is ready, you can optionally populate the database with some sample sensor data. First, insert the metadata for two sensors:

```sql
INSERT INTO sensors (sensor_type, description, location)
VALUES
    ('temperature', 'Outdoor temperature sensor', 'Backyard'),
    ('humidity', 'Indoor humidity sensor', 'Living Room');
```

Next, generate time-series data for the past 14 days with one-minute intervals for both sensors. Here's how you can insert random data for each sensor using Timescales `generate_series()` feature.

```sql
INSERT INTO sensor_data (sensor_id, value, time)
SELECT 1 as sensor_id,
       15 + random() * 15 AS value,
       generate_series(
           now() - INTERVAL '14 days',
           now(),
           INTERVAL '1 minute'
       ) AS time;

INSERT INTO sensor_data (sensor_id, value, time)
SELECT 2 as sensor_id,
       40 + random() * 20 AS value,
       generate_series(
           now() - INTERVAL '14 days',
           now(),
           INTERVAL '1 minute'
       ) AS time;
```

With your schema and sample data in place, you're now ready to connect to your database in the FastAPI application. To do this you must create a `.env` file in the root of the project to hold environment-specific variables, such as the connection string to your Neon PostgreSQL database.

```bash
DATABASE_URL=postgres://user:password@your-neon-hostname.neon.tech/neondb?sslmode=require
```

Make sure to replace the placeholders (user, password, your-neon-hostname, etc.) with your actual Neon database credentials, which are available in the console.

In your project, the `database.py` file manages the connection to `PostgreSQL` using `asyncpg` and its connection pool, which is a mechanism for managing and reusing database connections efficiently. With this, you can use asynchronous queries, allowing the application to handle multiple requests concurrently.

```python
import os
import asyncpg
import dotenv
from loguru import logger
from typing import Optional

dotenv.load_dotenv()

conn_pool: Optional[asyncpg.Pool] = None


async def init_postgres() -> None:
    """
    Initialize the PostgreSQL connection pool and create the products table if it doesn't exist.
    """
    global conn_pool
    try:
        logger.info("Initializing PostgreSQL connection pool...")

        conn_pool = await asyncpg.create_pool(
            dsn=os.getenv("DATABASE_URL"), min_size=1, max_size=10
        )
        logger.info("PostgreSQL connection pool created successfully.")

    except Exception as e:
        logger.error(f"Error initializing PostgreSQL connection pool: {e}")
        raise


async def get_postgres() -> asyncpg.Pool:
    """
    Get a reference to the PostgreSQL connection pool.

    Returns
    -------
    asyncpg.Pool
        The connection pool object to the PostgreSQL database.
    """
    global conn_pool
    if conn_pool is None:
        logger.error("Connection pool is not initialized.")
        raise ConnectionError("PostgreSQL connection pool is not initialized.")
    try:
        return conn_pool
    except Exception as e:
        logger.error(f"Failed to return PostgreSQL connection pool: {e}")
        raise


async def close_postgres() -> None:
    """
    Close the PostgreSQL connection pool.
    """
    global conn_pool
    if conn_pool is not None:
        try:
            logger.info("Closing PostgreSQL connection pool...")
            await conn_pool.close()
            logger.info("PostgreSQL connection pool closed successfully.")
        except Exception as e:
            logger.error(f"Error closing PostgreSQL connection pool: {e}")
            raise
    else:
        logger.warning("PostgreSQL connection pool was not initialized.")

```

`init_postgres` is responsible for opening the connection pool to the `PostgreSQL` database and `close_postgres` is responsible for gracefully closing all connections in the pool when the `FastAPI` app shuts down to properly manage the lifecycle of the database.

Throughout your API you will also need access to the pool to get connection instances and run queries. `get_postgres` returns the active connection pool. If the pool is not initialized, an error is raised.

## Defining the Pydantic Models

Now, you will create `Pydantic` models to define the structure of the data your API expects and returns, automatically validating incoming requests and responses versus the defined format.

```python
from pydantic import BaseModel
from typing import List
from datetime import datetime, date


class SensorData(BaseModel):
    value: float
    timestamp: datetime


class SensorDataBatch(BaseModel):
    data: List[SensorData]


class SensorCreate(BaseModel):
    sensor_type: str
    description: str
    location: str


class SensorDailyStatsResponse(BaseModel):
    day: date
    sensor_id: int
    avg_value: float
    min_value: float
    max_value: float
    reading_count: int
    median_value: float
    iqr_value: float
```

Each of the models represent the following:

- `SensorData`: A single sensor reading, including the value recorded and the timestamp when the reading occurred
- `SensorDataBatch`: A batch of data points, to support batch streaming in your API
- `SensorCreate`: The fields for creating a new sensor
- `SensorDailyStatsResponse`: The daily sensor statistics

## Creating the API Endpoints

In this section, you will define the FastAPI endpoints that allow you to manage sensor data. These endpoints handle tasks like creating new sensors, streaming sensor data (both single points and batches), and querying daily statistics for a specific sensor. With these endpoints, you can efficiently manage and analyze sensor data using TimescaleDB’s time-series capabilities.

```python
from fastapi import HTTPException, Path, Body, APIRouter, Depends
from database.postgres import get_postgres
from typing import Union, List
from asyncpg import Pool
from loguru import logger
from models.sensor_models import (
    SensorData,
    SensorDataBatch,
    SensorCreate,
    SensorDailyStatsResponse,
)

sensor_router = APIRouter()


@sensor_router.post("/sensors")
async def create_sensor(
    sensor: SensorCreate = Body(...), db: Pool = Depends(get_postgres)
):
    """
    Create a new sensor.

    Parameters
    ----------
    sensor : SensorCreate
        The sensor details (type, description, and location) to create.
    db : asyncpg.Pool
        Database connection pool injected by dependency.

    Returns
    -------
    dict
        A dictionary containing the newly created sensor ID and a success message.
    """
    insert_query = """
    INSERT INTO sensors (sensor_type, description, location)
    VALUES ($1, $2, $3)
    RETURNING sensor_id;
    """

    logger.info(
        f"Creating new sensor with type: {sensor.sensor_type}, location: {sensor.location}"
    )

    async with db.acquire() as conn:
        sensor_id = await conn.fetchval(
            insert_query, sensor.sensor_type, sensor.description, sensor.location
        )

    if sensor_id is None:
        logger.error("Failed to create sensor.")
        raise HTTPException(status_code=500, detail="Failed to create sensor")

    logger.info(f"Sensor created successfully with ID: {sensor_id}")
    return {"sensor_id": sensor_id, "message": "Sensor created successfully."}


@sensor_router.post("/sensor_data/{sensor_id}")
async def stream_sensor_data(
    sensor_id: int = Path(...),
    sensor_data: Union[SensorData, SensorDataBatch] = Body(...),
    db: Pool = Depends(get_postgres),
):
    """
    Stream sensor data (single or batch) for a specific sensor.

    Parameters
    ----------
    sensor_id : int
        The ID of the sensor to associate the data with.
    sensor_data : Union[SensorData, SensorDataBatch]
        The sensor data to stream, which can be either a single data point or a batch.
    db : asyncpg.Pool
        Database connection pool injected by dependency.

    Returns
    -------
    dict
        A success message once the data is streamed.
    """
    insert_query = """
    INSERT INTO sensor_data (sensor_id, value, time)
    VALUES ($1, $2, $3);
    """

    logger.info(f"Streaming data for sensor_id: {sensor_id}")

    async with db.acquire() as conn:
        async with conn.transaction():
            if isinstance(sensor_data, SensorDataBatch):
                for data in sensor_data.data:
                    logger.debug(f"Batch data: {data.value} at {data.timestamp}")
                    await conn.execute(
                        insert_query, sensor_id, data.value, data.timestamp
                    )
            elif isinstance(sensor_data, SensorData):
                logger.debug(
                    f"Single data: {sensor_data.value} at {sensor_data.timestamp}"
                )
                await conn.execute(
                    insert_query, sensor_id, sensor_data.value, sensor_data.timestamp
                )

    logger.info(f"Sensor data streamed successfully for sensor_id: {sensor_id}")
    return {"message": "Sensor data streamed successfully."}


@sensor_router.get(
    "/daily_avg/{sensor_id}", response_model=List[SensorDailyStatsResponse]
)
async def get_sensor_daily_avg(
    sensor_id: int = Path(..., description="The ID of the sensor"),
    db: Pool = Depends(get_postgres),
):
    """
    Query daily statistics (min, max, median, IQR) for a specific sensor over the last 7 days.

    Parameters
    ----------
    sensor_id : int
        The ID of the sensor.
    db : asyncpg.Pool
        Database connection pool injected by dependency.

    Returns
    -------
    List[SensorDailyStatsResponse]
        A list of daily sensor statistics (average, min, max, median, IQR).
    """

    query = """
    WITH sensor_stats AS (
        SELECT
            time_bucket('1 day', time) AS day,
            sensor_id,
            avg(value) AS avg_value,
            min(value) AS min_value,
            max(value) AS max_value,
            count(*) AS reading_count,
            percentile_cont(0.5) WITHIN GROUP (ORDER BY value) AS median_value,
            percentile_cont(0.75) WITHIN GROUP (ORDER BY value) -
            percentile_cont(0.25) WITHIN GROUP (ORDER BY value) AS iqr_value
        FROM sensor_data
        WHERE sensor_id = $1
        GROUP BY day, sensor_id
    )
    SELECT * FROM sensor_stats
    ORDER BY day DESC
    LIMIT 7;
    """

    async with db.acquire() as conn:
        rows = await conn.fetch(query, sensor_id)

    if not rows:
        raise HTTPException(status_code=404, detail="No data found for this sensor.")

    return [
        SensorDailyStatsResponse(
            day=row["day"],
            sensor_id=row["sensor_id"],
            avg_value=row["avg_value"],
            min_value=row["min_value"],
            max_value=row["max_value"],
            reading_count=row["reading_count"],
            median_value=row["median_value"],
            iqr_value=row["iqr_value"],
        )
        for row in rows
    ]

```

The code defines endpoints for:

- `POST /sensors`: This endpoint creates a new sensor by providing the sensor type, description, and location.

- `POST /sensor_data/{sensor_id}`: Streams sensor data for a specific sensor. The data can be a single point or a batch.

- `GET /daily_avg/{sensor_id}`: Retrieves daily statistics (average, min, max, median, IQR) for the given sensor over the last 7 days.

In the query for the sensor statistics, the data is able to be partitioned quickly with Timescale's `time_bucket()` function by using the indexes generated when you created the hypertable. Likewise, you can easily calculate things like the interquartile range (IQR) using Timescale-specific functions.

## Running the Application

After setting up the database, models, and API routes, the next step is to run the `FastAPI` application and test it out.

The `main.py` file defines the `FastAPI` application, manages the database lifecycle, and includes the routes you created above.

```python
from fastapi import FastAPI
from contextlib import asynccontextmanager
from database.postgres import init_postgres, close_postgres
from routes.sensor_routes import sensor_router


@asynccontextmanager
async def lifespan(app: FastAPI):
    await init_postgres()
    yield
    await close_postgres()


app: FastAPI = FastAPI(lifespan=lifespan, title="FastAPI TimescaleDB Sensor Data API")
app.include_router(sensor_router)
```

To run the application, use uvicorn CLI with the following command:

```bash
uvicorn main:app --host 0.0.0.0 --port 8080
```

Once the server is running, you can access the API documentation and test the endpoints directly in your browser:

- Interactive API Docs (Swagger UI):  
  Visit `http://127.0.0.1:8080/docs` to access the automatically generated API documentation where you can test the endpoints.
- Alternative Docs (ReDoc):  
  Visit `http://127.0.0.1:8080/redoc` for another style of API documentation.

## Testing the API

You can test your application using `HTTPie`, a command-line tool for making HTTP requests. The following steps will guide you through creating sensors, streaming data, and querying sensor statistics.

1. Retrieve sensor statistics for pre-generated data (optional).

   If you followed the optional data generation steps, you can retrieve daily statistics for the pre-generated sensors:

   ```bash
   http GET http://127.0.0.1:8080/daily_avg/1
   ```

   ```bash
   http GET http://127.0.0.1:8080/daily_avg/2
   ```

   These commands will return the daily statistics (average, min, max, median, and IQR) for the pre-generated temperature and humidity sensors over the last 7 days.

2. Create a new sensor.

   Start by creating a new sensor (e.g., a temperature sensor for the living room):

   ```bash shouldWrap
   http POST http://127.0.0.1:8080/sensors sensor_type="temperature" description="Living room temperature sensor" location="Living Room"
   ```

   You should see a response confirming the creation of the sensor with a unique ID:

   ```json
   {
     "sensor_id": 3,
     "message": "Sensor created successfully."
   }
   ```

3. Stream a single sensor data point.

   Stream a single data point for the newly created sensor (`sensor_id = 3`):

   ```bash shouldWrap
   http POST http://127.0.0.1:8080/sensor_data/3 value:=23.5 timestamp="2024-10-12T14:29:00"
   ```

   You should get a response indicating success:

   ```json
   {
     "message": "Sensor data streamed successfully."
   }
   ```

4. Stream a batch of sensor data.

   You can also stream multiple sensor data points in a batch for the same sensor:

   ```bash shouldWrap
   http POST http://127.0.0.1:8080/sensor_data/3 data:='[{"value": 22.5, "timestamp": "2024-10-12T14:30:00"}, {"value": 22.7, "timestamp": "2024-10-12T14:31:00"}]'
   ```

   This will send two data points to the sensor. The response will confirm successful streaming of the batch data:

   ```json
   {
     "message": "Sensor data streamed successfully."
   }
   ```

5. Retrieve daily statistics for the new sensor.

   After streaming the sensor data, you can retrieve the daily statistics for the new sensor (`sensor_id = 3`):

   ```
   http GET http://127.0.0.1:8080/daily_avg/3
   ```

   This will return daily statistics (average, min, max, median, and IQR) for the new sensor over the last 7 days:

   ```json
   [
     {
       "day": "2024-10-12",
       "sensor_id": 3,
       "avg_value": 22.6,
       "min_value": 22.5,
       "max_value": 22.7,
       "reading_count": 2,
       "median_value": 22.6,
       "iqr_value": 0.2
     }
   ]
   ```

By following these steps, you can easily create sensors, stream sensor data, and query statistics from your API. For sensors with pre-generated data, you can retrieve the statistics immediately. For new sensors, you can stream data and retrieve their daily stats dynamically.

## Conclusion

Now, you have created and tested an API for managing, streaming, and querying sensor data into `TimescaleDB` using `FastAPI`. By leveraging TimescaleDB for time-series data storage, you now have a high-performance solution for handling sensor data at scale.

As a next step, you can look into streaming data into the database using a distributed event platform like `Kafka` or `Red Panda`, or using `Timescale` to monitor the sensor data with `Apache Superset` or `Grafana`.


# Migrate from Vercel Postgres SDK to the Neon serverless driver

---
title: Migrate from Vercel Postgres SDK to the Neon serverless driver
subtitle: Learn how to smoothly transition your application from using Vercel Postgres SDK to the Neon serverless driver
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2024-10-28T00:00:00.000Z'
updatedAt: '2024-10-28T00:00:00.000Z'
---

With Vercel Postgres transitioning to Neon's native integration in the [Vercel Marketplace](https://vercel.com/blog/introducing-the-vercel-marketplace), now is the perfect time to migrate from the Vercel Postgres SDK [(@vercel/postgres)](https://vercel.com/docs/storage/vercel-postgres/sdk) to the [Neon serverless driver](https://github.com/neondatabase/serverless).

## Why migrate?

Switching to the Neon serverless driver provides several advantages. It offers greater flexibility by allowing the choice between HTTP for single queries or WebSockets for transactions and full [node-postgres](https://node-postgres.com/) compatibility. Additionally, it enhances maintainability by relying on Neon's actively maintained, native database driver.

## Prerequisites

To begin, you’ll need:

- An existing application using the Vercel Postgres SDK
- A [Neon account](https://neon.tech/docs/get-started-with-neon/signing-up) (your Vercel Postgres database will automatically migrate to Neon)

## Migration Steps

### 1. Install the Neon serverless driver

Start by installing the Neon serverless driver in your project:

```bash
npm install @neondatabase/serverless
```

<Admonition type="important">
To ensure proper configuration, set your environment variable to `DATABASE_URL` when referencing the database URL in your code, especially if you're following this guide.
</Admonition>

### 2. Update your database connection

Replace your Vercel Postgres SDK imports and connection setup with the Neon serverless driver. You have two options:

#### Option A: Using HTTP (Recommended for simple queries)

```diff
import { sql } from '@vercel/postgres'; // [!code --]

import { neon } from '@neondatabase/serverless'; // [!code ++]
const sql = neon(process.env.DATABASE_URL!); // [!code ++]
```

#### Option B: Using WebSockets (Recommended for transactions)

```diff
import { db } from '@vercel/postgres'; // [!code --]

import ws from 'ws'; // [!code ++]
import { Pool, neonConfig } from '@neondatabase/serverless'; // [!code ++]

const pool = new Pool({ connectionString: process.env.DATABASE_URL }); // [!code ++]
neonConfig.webSocketConstructor = ws; // [!code ++]
```

### 3. Update your queries

Here are common query patterns and how to migrate them:

#### Simple Queries

```diff
# Vercel Postgres SDK
const { rows } = await sql`SELECT * FROM users WHERE id = ${userId}`; // [!code --]

# Neon HTTP
const rows = await sql`SELECT * FROM users WHERE id = ${userId}`; // [!code ++]

# Neon WebSockets
const { rows } = await pool.query('SELECT * FROM users WHERE id = $1', [userId]); // [!code ++]
```

#### Transactions

```diff
 import { db } from '@vercel/postgres'; // [!code --]

async function transferFunds(fromId: number, toId: number, amount: number) { // [!code --]
  const client = await db.connect(); // [!code --]
  try { // [!code --]
    await client.query('BEGIN'); // [!code --]
    await client.query('UPDATE accounts SET balance = balance - $1 WHERE id = $2', [ // [!code --]
      amount, // [!code --]
      fromId, // [!code --]
    ]); // [!code --]
    await client.query('UPDATE accounts SET balance = balance + $1 WHERE id = $2', [amount, toId]); // [!code --]
    await client.query('COMMIT'); // [!code --]
  } catch (e) { // [!code --]
    await client.query('ROLLBACK'); // [!code --]
    throw e; // [!code --]
  } finally { // [!code --]
    client.release(); // [!code --]
  } // [!code --]
} // [!code --]

import { Pool } from '@neondatabase/serverless'; // [!code ++]

async function transferFunds(fromId: number, toId: number, amount: number) { // [!code ++]
  const pool = new Pool({ connectionString: process.env.DATABASE_URL }); // [!code ++]
  try { // [!code ++]
    await pool.query('BEGIN'); // [!code ++]
    await pool.query('UPDATE accounts SET balance = balance - $1 WHERE id = $2', [amount, fromId]); // [!code ++]
    await pool.query('UPDATE accounts SET balance = balance + $1 WHERE id = $2', [amount, toId]); // [!code ++]
    await pool.query('COMMIT'); // [!code ++]
  } catch (e) { // [!code ++]
    await pool.query('ROLLBACK'); // [!code ++]
    throw e; // [!code ++]
  } finally { // [!code ++]
    await pool.end(); // [!code ++]
  } // [!code ++]
} // [!code ++]
```

## Best practices

1.  **Choose the right connection method**:

    - Use HTTP (`neon()`) for single queries and simple transactions.
    - Use WebSockets (`Pool`) for complex transactions and session-based operations.

2.  **Connection management**:

    - For HTTP queries, reuse the `sql` query function.
    - For WebSocket connections in serverless environments, always close connections:

    ```typescript
    const pool = new Pool({ connectionString: process.env.DATABASE_URL });
    try {
      // Your queries here
    } finally {
      await pool.end();
    }
    ```

3.  **Error Handling**:
    ```typescript
    try {
      const result = await sql`SELECT * FROM users`;
      return result;
    } catch (error) {
      console.error('Database error:', error);
      throw new Error('Failed to fetch users');
    }
    ```

## Working with ORMs

Neon's serverless driver is compatible with popular ORMs like Prisma and Drizzle ORM. Check out the following guides to learn more:

<DetailIconCards>

<a href="/docs/guides/prisma" description="Learn how to connect to Neon from Prisma" icon="prisma">Prisma</a>

<a href="https://orm.drizzle.team/docs/tutorials/drizzle-with-neon" description="Learn how to connect to Neon from Drizzle ORM" icon="drizzle">Drizzle ORM</a>

</DetailIconCards>

## Advanced Configuration

For most cases, using neon serverless driver is straightforward without needing advanced configuration. However, for custom setups or troubleshooting, here are the key options:

- **poolQueryViaFetch**: Setting `poolQueryViaFetch` to true sends `Pool.query()` calls as low-latency `HTTP` fetch requests (currently defaults to false).

- **wsProxy**: This option is for connecting via a WebSocket proxy deployed in front of your your own Postgres instance, which allows you to use the Neon serverless driver with a local development environment.

For more information about these options, see [Advanced configuration](https://github.com/neondatabase/serverless/blob/main/CONFIG.md#advanced-configuration).

<NeedHelp/>


