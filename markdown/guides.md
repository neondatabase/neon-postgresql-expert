# ...

---
updatedOn: '2024-05-13T13:24:36.612Z'
---

# Guides

Welcome to Neon guides! This folder contains the source code of the [Neon guides](/guides/).

## Basic information

1. Every single Markdown file in this folder will be turned into a guides page.
2. Folder and file names should follow kebab-case.
3. `slug` is generated based on the folder structure and file names inside this folder. In order to see page `slug`, you can [start](../../README.md#run-the-website) and build the project with `npm run build` command that will display all generated pages.
4. Page `path` is generated by combining [`GUIDES_BASE_PATH`](../../src/constants/guides.js) and page `slug`.
5. There is no need to add `h1` to the page since it will be displayed automatically with the value from `title` field.

## Fields

Right now Markdown files accept the following fields:

1. `title` — title of the page (required)
2. `subtitle` — subtitle of the page.
3. `author` — your author ID.
4. `redirectFrom` — array of strings with paths to redirect from to the page, should start and end with a slash, e.g. `/guides/old-path/`
5. `isDraft` — flag that says the page is not ready yet. It won't appear in production but will appear in the development mode.
6. `enableTableOfContents` — flag that turns on the display of the outline for the page. The outline gets built out of second and third-level headings ([`h2`, `h3`]), thus appears as two-level nested max.
7. `ogImage` - the social preview image of the page.

> ⚠️ Please note that the project won't build if at least one of the Markdown files is missing a required field.

## Code blocks

All available languages for code blocks can be found [here](https://shiki.matsu.io/languages).

You can use fenced code blocks with three backticks (```) on the lines before and after the code block. And display code with options

- enable highlighting single lines, multiple lines, and ranges of code lines

  Examples:
  - Single line highlight

    ````md
    ```c++ {1}
    #include <iostream>

    int main() {
        std::cout << "Hello World";
        return 0;
    }
    ```
    ````

  - Multiple lines

    ````md
    ```c++ {1,2,5}
    #include <iostream>

    int main() {
        std::cout << "Hello World";
        return 0;
    }
    ```
    ````

  - Range of code lines

    ````md
    ```c++ {1-3,5}
    #include <iostream>

    int main() {
        std::cout << "Hello World";
        return 0;
    }
    ```
    ````

- use `[!code highlight]` to highlight a line.

  ```ts
  export function foo() {
    console.log('Highlighted'); // [!code highlight]
  }
  ```

- use `[!code word:xxx]` to highlight a word.

  ```ts
  export function foo() {
    // [!code word:Hello]
    const msg = 'Hello World';
    console.log(msg); // prints Hello World
  }
  ```

- `showLineNumbers` - flag to show on the line numbers in the code block.

  Example:

  ````md
  ```c++ showLineNumbers
  #include <iostream>

  int main() {
      std::cout << "Hello World";
      return 0;
  }
  ```
  ````

- `shouldWrap` - flag to enable code wrapping in the code block.

  Example:

  ````md
  ```powershell shouldWrap
  powershell -Command "Start-Process -FilePath powershell -Verb RunAs -ArgumentList '-NoProfile','-InputFormat None','-ExecutionPolicy Bypass','-Command ""iex (iwr -UseBasicParsing https://cli.configu.com/install.ps1)""'"
  ```
  ````

## Code Tabs

To display code tabs, wrap all pieces of code with `<CodeTabs></CodeTabs>` and write labels of code tabs in order:

````md
<CodeTabs labels={["Shell", "C++", "C#", "Java"]}>

```bash {2-4}
#!/bin/bash
STR="Hello World!"
echo $STR
```

```c++
#include <iostream>

int main() {
    std::cout << "Hello World";
    return 0;
}
```

```csharp
namespace HelloWorld
{
    class Hello {
        static void Main(string[] args)
        {
            System.Console.WriteLine("Hello World");
        }
    }
}
```

```java
import java.io.*;

class GFG {
    public static void main (String[] args) {
       System.out.println("Hello World");
    }
}
```

</CodeTabs>
````

<details>
<summary>Examples</summary>

![Code tabs example](../docs/code-tabs-example.jpg)

</details>

## Tabs

To display the tabs with content as image, video, code block, .etc, wrap the `TabItem` with `Tabs`

````md
<Tabs labels={["Content", "CLI"]}>

<TabItem>
In your config v3 project, head to the `/metadata/databases/databases.yaml` file and add the database configuration as below.

```bash showLineNumbers
- name: <db_name>
  kind: postgres
  configuration:
    connection_info:
      database_url:
        from_env: <DB_URL_ENV_VAR>
    pool_settings:
      idle_timeout: 180
      max_connections: 50
      retries: 1
  tables: []
  functions: []
```

Apply the Metadata by running:

```bash
hasura metadata apply
```

If you've spun up the Hasura Engine with Docker, you can access the Hasura Console by accessing it in a browser at the URL of your Hasura Engine instance, usually http://localhost:8080.

<Admonition type="note">
To access the Hasura Console via the URL the HASURA_GRAPHQL_ENABLE_CONSOLE environment variable or the `--enable-console` flag must be set to true.
</Admonition>

</TabItem>

<TabItem>
Alternatively, you can create read replicas using the Neon API or Neon CLI.

```bash
curl --request POST \
     --url https://console.neon.tech/api/v2/projects/late-bar-27572981/endpoints \
     --header 'Accept: application/json' \
     --header "Authorization: Bearer $NEON_API_KEY" \
     --header 'Content-Type: application/json' \
     --data '
{
  "endpoint": {
    "type": "read_only",
    "branch_id": "br-young-fire-15282225"
  }
}
' | jq
```

</TabItem>

</Tabs>
````

## Admonition

To improve the guides readability, one can leverage an Admonition custom component. Just wrap your piece of text with `<Admonition></Admonition>` and pass the type.

There are 6 types of Admonition: `note`, `important`, `tip`, `warning`, `info`, `comingSoon`; the default is `note`.

You may also specify an optional title with prop `title`.

Example:

```md
<Admonition type="note" title="Your title">
  The branch creation process does not increase load on the originating project. You can create a branch at any time without worrying about downtime or performance degradation.
</Admonition>

<Admonition type="info">
  The branch creation process does not increase load on the originating project. You can create a branch at any time without worrying about downtime or performance degradation.
</Admonition>
```

<details>
<summary>Examples</summary>

![Admonition example](../docs/admonition-example.jpg)

</details>

## CTA

This is a simple block with title, description text and one CTA button that accomplish certain actions.

```md
<CTA />
```

Check the example for default data of CTA block

<details>
<summary>Example</summary>

![CTA example](../docs/cta-example.jpg)

</details>

To change text in CTA block, you can pass to the component props `title`, `description`, `buttonText`, `buttonUrl`:

```md
<CTA title="Try it on Neon!" description="Neon is Serverless Postgres built for the cloud. Explore Postgres features and functions in our user-friendly SQL editor. Sign up for a free account to get started." buttonText="Sign Up" buttonUrl="https://console.neon.tech/signup" />
```

## Images

The images should be sourced in `public/guides` directory and be used in `.md` with the relative path, that begins with a `/` slash

Example file structure:

```md
├── public
│ ├── guides
│ │ ├── conceptual-guides
│ │ ├── neon_architecture_2.png // put images in a directory with the same name as the .md file
├── content
│ ├── guides
│ │ ├── conceptual-guides
│ │ ├── architecture-overview.md
```

To display images using Markdown syntax, use the following syntax: `![alt text](image url)`. Example content in `architecture-overview.md`:

```md
![Neon architecture diagram](/guides/images/conceptual-guides/neon_architecture_2.png)
```

If you need an image without border to show an annotated piece of UI, use the `"no-border"` attribute as in the example below:

```md
![Neon architecture diagram](/guides/images/conceptual-guides/neon_architecture_2.png 'no-border')
```

With this approach, all images on your guide pages will be displayed both on the production and GitHub preview.

## Definition list

Custom `mdx` component that makes possible using [extended markdown syntax for descriptions lists](https://www.markdownguide.org/extended-syntax/#definition-lists). Fully [WCAG-compliant](https://www.w3.org/TR/WCAG20-TECHS/H40.html). It provides an accessible way to make term lists, and it's a generally good way to add structure to a text when a writer needs more than bullets and less than headings.

The usage is pretty [straightforward](https://github.com/neondatabase/website/pull/231/commits/8f795eaf700c31794a2267fc5978c22bfc649a0c):

```md
[comment]: <> (other content here)

<DefinitionList>
[comment]: <> (required new line)
Scenario executor
: First definition
: Second definition

Soak test
: First and only definition

Smoke test
Another term for smoke test
: First definition for both terms
: Second definition for both terms
: ...n definition for both terms

[Stress test](/)
: First and **only** definition for both terms with additional markup <br/> Read more: [link](/)

[comment]: <> (other content here)
</DefinitionList>

[comment]: <> (other content here)
```

### Acceptable markup for term

- `*italic*`
- `[link](/)`
- `**strong**` - but that doesn't make sense, by default terms appearance is already bold
- `inlineCode` - but it doesn't alter it's change in this context

### Constraints

- using emojis in `dt` is prohibited, as it potentially can mess up with `id` attribute, and `href` at anchor. We can not be sure which range will be used to display a particular symbol (depends on editor OS) and if it is going to be stripped.
- if there are multiple terms for a given set of descriptions, only the first one will have an `id` and an `anchor`
- make absolutely sure your `dt` text content is unique across the page to avoid `id` collisions

### Acceptable markup for description

- everything for term
- emojis
- any inline html
- line breaks `<br/>` (recommended way to separate visually something inside a single description)

<details>
<summary>Examples</summary>

![Definition list example](../docs/definition-list-example.jpg)

</details>

## Detail Icon Cards

`DetailIconCards` is a custom MDX component that displays data in a card format. Each card contains icon, title, href and description. This layout is especially useful for presenting grouped information in a visually pleasing and easy-to-understand way.

```md
<DetailIconCards>

<a href="https://api-guides.neon.tech/reference/getting-started-with-neon-api" description="Collaborate on open-source projects" icon="github">Headless vector search</a>

<a href="https://api-guides.neon.tech/reference/getting-started-with-neon-api" description="Collaborate on open-source projects" icon="github">Open AI completions</a>

</DetailIconCards>
```

## Shared MDX components

Create a [markdown file](https://github.com/neondatabase/website/blob/main/content/docs/shared-content/need-help.md) in folder `content/docs/shared-content/`, add to `sharedMdxComponents` the name of component and the path to component.

```js
const sharedMdxComponents = {
  // ConponentName: 'shared-content/component-filename'
  NeedHelp: 'shared-content/need-help',
};

export default sharedMdxComponents;
```

Insert a shared markdown and render inline.

```md
## Resources

- [Open AI tiktoken source code on GitHub](https://github.com/openai/tiktoken)
- [pg_tiktoken source code on GitHub](https://github.com/kelvich/pg_tiktoken)

<NeedHelp/>
```

You can pass props to the shared component:

```md
<ComponentWithProps text="The pgvector extension" />
```

`component-with-props.md`

```md
<Admonition type="note" title="Test component with props">
  {text}
</Admonition>
```

## Author data

Your author's data should be sourced in `content/guides/authors/data.json` file.

You should start with your name as the ID for the object in kebab-case. Then you can use these fields inside the object:

1. `name` — your name (required).
2. `position` — your position.
3. `bio` — your bio.
4. `link` — link to one of your social media accounts. Each link should have `title` and `url` fields.

Then you can add your photo to `public/guides/authors/` folder with the filename as your ID.

Please make sure that your ID is the same in your author's data, in `content/guides/authors/data.json`, and the photo filename.

Example usage:

```md
├── content
│ ├── guides
│ ├── next-server-actions.md // your guide
│ ├── authors
│ ├── data.json // your data here
├── public
│ ├── guides
│ ├── authors
│ ├── rishi-raj-jain.jpg // your photo
```

`content/guides/next-server-actions.md`:

```md
---
title: ...
author: rishi-raj-jain
---
```

`content/guides/authors/data.json`:

```json
{
  "rishi-raj-jain": {
    "name": "Rishi Raj Jain",
    "position": "Software Engineer",
    "bio": "Technical Writer",
    "link": { "title": "GitHub", "url": "https://github.com/rishi-raj-jain" }
  }
}
```

With this approach, all your data will be displayed in guide's author section.

## Contributing

For small changes and spelling fixes, we recommend using the GitHub UI because Markdown files are relatively easy to edit.

For larger contributions, consider [running the project locally](../../README.md#getting-started) to see how changes look like before making a pull request.


# Building AI Agents with AgentStack and Neon

---
title: Building AI Agents with AgentStack and Neon
subtitle: Build a Web scraper AI Agent in minutes with AgentStack, Neon, and Firecrawl
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-02-04T00:00:00.000Z'
updatedOn: '2025-02-04T00:00:00.000Z'
---

The rapid evolution of AI agents has created a key challenge: how to build and deploy agents quickly and efficiently. Imagine creating intelligent agents that can not only perform complex tasks but also interact easily with your data infrastructure, without adding unnecessary complexity to the code.

This guide introduces [**AgentStack**](https://docs.agentstack.sh/introduction), a rapid development framework, and Neon, the serverless Postgres database, and shows how they can be used together to build powerful AI agents with database integration. We'll walk through building a **Web Scraper AI Agent** using AgentStack's CLI and tool integrations.

With AgentStack and Neon, you can generate complete agent workflows, create new agents and tasks with simple commands, and integrate them directly with your data layer. Let's get started.

## What you will build

This example will show you how to:

- Set up an **AgentStack** project.
- Use the **AgentStack CLI** to generate agents and tasks.
- Equip your agents with **tools** like **Neon** for data storage and **Firecrawl** for web scraping.
- Run your agent crew to scrape the [neon.tech/guides](/guides) page, extract blog post metadata (titles, authors, dates) from it, and store it in a Neon Postgres database.
- Use **AgentOps** for observability of your agent's execution.

## Prerequisites

Before you start building your Web Scraper Agent, ensure you have the following prerequisites in place:

- **Python 3.10 or higher:** Download from [python.org](https://www.python.org/downloads/).
- **uv package installer (or Poetry):** Recommended for faster dependency installation. Install `uv` as described here: [astral-sh/uv](https://github.com/astral-sh/uv?tab=readme-ov-file#installation). Alternatively, use `poetry`.
- **AgentStack CLI:** Install the AgentStack Command Line Interface (CLI). Follow the [Getting started with AgentStack](https://docs.agentstack.sh/installation) guide.
- **Accounts and API Keys:** You will need accounts and API keys for these services:
  - **OpenAI API key**: We will use OpenAI's `gpt-4o-mini` model to power AI agents. Get an OpenAI API key at [platform.openai.com](https://platform.openai.com).
  - **Neon account**: Sign up for a free Neon account at [neon.tech](https://console.neon.tech/signup). You will need a Neon API key to connect to your Neon database.
  - **Firecrawl account**: Sign up for a Firecrawl account at [firecrawl.dev](https://firecrawl.dev). You will need a Firecrawl API key to use the web scraping tool.
  - **AgentOps account**: Sign up for an AgentOps account at [agentops.ai](https://agentops.ai) to leverage agent observability features. You will need an AgentOps API key.

## Building the Web Scraper agent

Let's start building our agent using AgentStack, Neon, and Firecrawl. We'll walk through each step, from initializing the project to running the agent crew.

### Project setup with AgentStack CLI

AgentStack simplifies project setup with its CLI. Let's start by initializing a new AgentStack project.

- **Initialize AgentStack Project:**

  Open your terminal and run the following command to initialize a new AgentStack project named `web_scraper`:

  ```bash
  agentstack init web_scraper
  ```

  AgentStack CLI will prompt you to select a template. Choose the **Empty Project** template for this guide.

  ![AgentStack Project Template](/docs/guides/agentstack-default-template.png)

- **Navigate to your project directory and activate the virtual environment:**

  ```bash
  cd web_scraper && source .venv/bin/activate
  ```

### Project structure

After initialization, your project directory `web_scraper` will have the following structure:

```
web_scraper/
├── agentstack.json     # AgentStack project configuration
├── pyproject.toml      # Python dependencies
├── .env                # environment variables file
├── src/
│   ├── crew.py         # Defines your agent crew and workflow
│   ├── main.py         # Main script to run your agent
│   ├── config/         # Configuration files
│       ├── agents.yaml   # Agent configurations
│       └── tasks.yaml    # Task configurations
```

### Generating agents with AgentStack CLI

We will create 3 agents for our Web Scraper crew:

- **`web_scraper`:** Responsible for web scraping and markdown extraction.
- **`data_extractor`:** Specialized in extracting structured data from web content.
- **`content_storer`:** Manages storing extracted data in a Neon Postgres database.

Now, let's generate the agents using the AgentStack CLI.

1. **Generate `web_scraper` agent:**

   ```bash
   agentstack generate agent web_scraper
   ```

   When creating the first agent, `agentstack` will ask you to choose a default Large Language Model (LLM). We recommend `openai/gpt-4o-mini` for this guide, as it offers a good balance between performance and cost. You will need to enter this model name manually, as it's not included in the default list.

   ![AgentStack LLM Model](/docs/guides/agentstack-llm-model.png)

2. **Generate `data_extractor` agent:**

   ```bash
   agentstack generate agent data_extractor
   ```

3. **Generate `content_storer` agent:**

   ```bash
   agentstack generate agent content_storer
   ```

   AgentStack CLI uses simple commands to generate the basic structure for your agents and tasks, significantly speeding up the development process.

### Configuring agents in `agents.yaml`

Open `src/config/agents.yaml` and configure the agents as follows:

```yaml shouldWrap
web_scraper:
  role: >-
    Web scraper specializing in markdown extraction.
  goal: >-
    Visit a website and accurately return its content in markdown format.
  backstory: >-
    You are a meticulous data entry employee with expertise in web scraping and markdown formatting. Your task is to retrieve website content and present it clearly in markdown.
  llm: openai/gpt-4o-mini
data_extractor:
  role: >-
    Data extraction expert for web content analysis.
  goal: >-
    Analyze web page content and extract structured information
  backstory: >-
    You are an expert data analyst skilled in extracting key information from web pages. You are adept at identifying and listing key details such as blog post titles, author names, and publication dates from website content.
  llm: openai/gpt-4o-mini
content_storer:
  role: >-
    Database engineer
  goal: >-
    Store structured web content in a Postgres database, create relevant tables, insert data, and formulate SQL queries to retrieve stored data.
  backstory: >-
    You are an expert database engineer. You are skilled in database design, data insertion, and writing efficient SQL queries for data retrieval.
  llm: openai/gpt-4o-mini
```

AgentStack uses YAML configuration files to define the roles, goals, and backstories of our agents. This configuration-driven approach makes it easy to modify and extend our agent crew without changing code.

### Generating tasks with AgentStack CLI

Now that we have our agents defined, we need to create tasks for them. Tasks define the specific actions each agent will perform within the agent crew's workflow.

For this example the task for the AI agent is simple: get the list of blog post titles, author names, and publication dates from our [guides page](/guides) and store it in a Postgres database.

We will need three tasks for our Web Scraper crew:

1. **Generate `scrape_site` task:**

   ```bash
   agentstack generate task scrape_site
   ```

2. **Generate `extract` task:**

   ```bash
   agentstack generate task extract
   ```

3. **Generate `store` task:**

   ```bash
   agentstack generate task store
   ```

### Configuring tasks in `tasks.yaml`

Open `src/config/tasks.yaml` and configure the tasks as follows:

```yaml shouldWrap
scrape_site:
  description: >-
    Fetch the content of https://neon.com/guides in markdown format. Ensure accurate and complete retrieval of website content.
  expected_output: >-
    The complete content of the website https://neon.com/guides, formatted in markdown.
  agent: >-
    web_scraper
extract:
  description: >-
    Analyze the provided website content and extract a structured list of blog post titles, author names, and publication dates. Limit the extraction to the first 20 blog posts.
  expected_output: >-
    A list of blog post titles, author names, and publication dates extracted from the website content.
  agent: >-
    data_extractor
store:
  description: >-
    Store the extracted blog post data into a Postgres database within Neon. Create a table named 'posts' and corresponding schema for the posts and insert them. After inserting the data, formulate and test an SQL query to retrieve all inserted data. Provide the tested SQL query as the output.
  expected_output: >-
    A valid and tested SQL query that retrieves all data inserted into the 'posts' table in the Neon database.
  agent: >-
    content_storer
```

Similar to agents, tasks are also configured via YAML, defining the description of the task, the expected output, and the agent assigned to perform it. This makes the workflow easily understandable and modifiable.

### Adding Firecrawl and Neon tools to the Crew

To enable web scraping and data storage capabilities, we will integrate **Firecrawl** and **Neon** tools into our agent crew. We will use Firecrawl for web scraping the `neon.tech/guides` page and Neon for storing the extracted data in a Postgres database.

- Add **Firecrawl** tool using the following command:

  ```bash
  agentstack tools add firecrawl
  ```

- Add **Neon** tool using the following command:

  ```bash
  agentstack tools add neon
  ```

The `agentstack tools add` command simplifies the integration of tools by automatically updating your project configuration and `crew.py` file to include the necessary tool classes.

#### Understanding Neon Tool actions

AgentStack's Neon tool integration equips the agents with a suite of pre-built actions to interact with Neon serverless Postgres databases. These actions are automatically available to any agent you equip with the Neon tool, like the `content_storer` agent in our example. The Neon tool provides the following actions:

- **`create_database`**: This action allows our agent to create a new Neon project and database on demand. It returns a connection URI, which is essential for subsequent database interactions. By default, it creates a database named `neondb` with the role `neondb_owner`. This is particularly useful for agents that need to manage their own isolated databases or when the database needs to be created as part of the agent workflow.

- **`execute_sql_ddl`**: Agents use this action to execute Data Definition Language (DDL) commands. DDL commands are used to define the database schema, such as creating, altering, or dropping tables. For instance, the `content_storer` agent uses this action to create the `posts` table in the Neon database.

- **`run_sql_query`**: This action enables agents to run Data Manipulation Language (DML) queries like `SELECT`, `INSERT`, `UPDATE`, and `DELETE`. In the example, the `content_storer` agent uses this action to insert the scraped blog post metadata into the `posts` table and to formulate and test a `SELECT` query to retrieve the data. The results from these queries are returned to the agent as formatted strings, allowing the agent to process and reason about the data.

These actions empower our agents to fully manage and utilize Neon databases within their workflows, from database creation and schema definition to data manipulation and retrieval, all without requiring manual coding of database interactions.

#### Understanding Firecrawl Tool actions

AgentStack's Firecrawl tool integration provides the agents with a set of actions to perform web scraping tasks. These actions are readily available to any agent equipped with the Firecrawl tool, like the `web_scraper` agent in our example. The Firecrawl tool offers the following actions:

- **`web_scrape`**: This action allows our agent to scrape the content of a single webpage and retrieve it in markdown format. It's designed for efficiently extracting content from individual URLs when we need the content of a specific page. The agent provides a URL, and Firecrawl returns the webpage's content as markdown text.

- **`web_crawl`**: For more extensive data gathering, the `web_crawl` action enables our agent to initiate a web crawl starting from a given URL. This action not only scrapes the initial URL but also explores and scrapes content from linked pages that are children of the starting URL. It's important to note that the crawl is limited to sublinks of the provided URL, preventing it from venturing to entirely separate sections of a website or different domains. This action is asynchronous and returns a `crawl_id`.

- **`retrieve_web_crawl`**: Since `web_crawl` is an asynchronous operation, we use the `retrieve_web_crawl` action to get the results of a crawl that was initiated previously using the `web_crawl` action. This action requires the `crawl_id` that was returned by the initial `web_crawl` action. It checks the status of the crawl and returns the scraped content once the crawl is complete. Agents can use this action in a loop or after a delay to check for and retrieve crawl results, allowing for more complex workflows where web crawling is part of a longer process.

These actions equip our agents with powerful web scraping capabilities, ranging from simple single-page content extraction to comprehensive crawling of website sections, enabling them to gather web data effectively as part of their tasks.

We can improve the efficiency of our agents by specifying their tool usage in crew.py. Since the `web_scraper` agent only needs firecrawl, the `content_storer` agent only needs neon, and the data_extractor agent needs no tool, assigning these tools directly in the configuration will reduce agent context memory and enable more focused task execution, mirroring human specialization.

Your `src/crew.py` file should now look like this, with the tools integrated into the respective agents:

```python shouldWrap
from crewai import Agent, Crew, Process, Task
from crewai.project import CrewBase, agent, crew, task
import agentstack


@CrewBase
class WebscraperCrew:
    """web_scraper crew"""

    @agent
    def web_scraper(self) -> Agent:
        return Agent(
            config=self.agents_config["web_scraper"],
            tools=[*agentstack.tools["firecrawl"]],
            verbose=True,
        )

    @agent
    def data_extractor(self) -> Agent:
        return Agent(
            config=self.agents_config["data_extractor"],
            tools=[],
            verbose=True,
        )

    @agent
    def content_storer(self) -> Agent:
        return Agent(
            config=self.agents_config["content_storer"],
            tools=[*agentstack.tools["neon"]],
            verbose=True,
        )

    @task
    def scrape_site(self) -> Task:
        return Task(
            config=self.tasks_config["scrape_site"],
        )

    @task
    def extract(self) -> Task:
        return Task(
            config=self.tasks_config["extract"],
        )

    @task
    def store(self) -> Task:
        return Task(
            config=self.tasks_config["store"],
        )

    @crew
    def crew(self) -> Crew:
        """Creates the Test crew"""
        return Crew(
            agents=self.agents,
            tasks=self.tasks,
            process=Process.sequential,
            verbose=True,
        )

```

### Configure API keys in `.env`

To authenticate with OpenAI, Neon, and Firecrawl, you need to configure API keys. Open the `.env` file and fill in your API keys obtained in the [Prerequisites](#prerequisites) section:

```env
OPENAI_API_KEY=YOUR_OPENAI_API_KEY
NEON_API_KEY=YOUR_NEON_API_KEY
FIRECRAWL_API_KEY=YOUR_FIRECRAWL_API_KEY
AGENTOPS_API_KEY=YOUR_AGENTOPS_API_KEY
```

### Running the Web Scraper agent

Now that we have set up our agents, tasks, and tools, let's run the agent crew to scrape the `neon.tech/guides` page, extract blog post metadata, and store it in a Neon Postgres database.

```bash
agentstack run
```

This command will:

- Initialize the AgentStack environment.
- Load agent and task configurations.
- Instantiate the agent crew defined in `src/crew.py`.
- Execute the tasks in sequence.
- Utilize the `neon` and `firecrawl` tools within the agents' tasks as defined in `src/crew.py`.
- Print the final output to your terminal.

You should see the agent's execution logs and the final output, including the final SQL query generated by the `content_storer` agent.

![Web Scraper Agent Output 1](/docs/guides/agentstack-neon-example-output-1.png)
![Web Scraper Agent Output 2](/docs/guides/agentstack-neon-example-output-2.png)

### Verifying the output

After the agent run completes, check your terminal for the output. It should display the SQL query generated by the `content_storer` agent.

You can verify that the data has been stored in your Neon database by:

- Logging into your Neon account at [console.neon.tech](https://console.neon.tech).
- Navigating to your project and database.
- Clicking on the `Tables` tab to view the `posts` table created by the agent.

![Neon SQL Editor](/docs/guides/agentstack-neon-database-data.png)

<Admonition type="info" title="AgentOps: Gain Observability into your AI Agents">

AgentStack integrates with [**AgentOps**](https://www.agentops.ai) by default to provide full observability for your AI agent development. Built by the same team behind AgentStack, AgentOps allows you to:

- **Visualize agent execution:** See step-by-step execution graphs of your agents, making it easy to understand workflows and debug issues.
- **Track LLM costs:** Monitor your spending on LLM providers to manage costs effectively.
- **Benchmark Agent performance:** Evaluate agent performance and ensure quality.
- **Enhance security:** Detect potential vulnerabilities like prompt injection.

![AgentOps Visualization](/docs/guides/agentops-visualization.png)

To use AgentOps, make sure you have your `AGENTOPS_API_KEY` configured in your `.env` file (as covered in the [Prerequisites](#prerequisites)). AgentOps automatically starts tracking your agent executions when you run `agentstack run` because of the `agentops.init()` call in `src/main.py`.

Use AgentOps to gain insights into your agents' behavior and performance, allowing for continuous improvement and optimization.

With a total run cost of only $0.01 in OpenAI credits (as seen in the AgentOps dashboard), this AI agent runs efficiently while requiring no custom code. It avoids complex programming for tasks like web-scraping and SQL queries, making it widely applicable.
</Admonition>

**Congratulations!** You have successfully built and run a Web Scraper agent using AgentStack, Neon, and Firecrawl, demonstrating how to automate web data extraction and storage into a serverless Postgres database with minimal effort!

## Next Steps

- **Explore more tools:** Browse AgentStack's [community tools](https://docs.agentstack.sh/tools/community), including Perplexity (knowledge retrieval), Composio (platform connections), and Stripe (payments).
- **Expand your crew:** Add more agents and tasks to handle complex workflows.
- **Monitor performance:** Use [AgentOps](https://docs.agentops.ai/) to track and optimize execution and costs.

You can find the source code for the application described in this guide on GitHub.

<DetailIconCards>
    <a href="https://github.com/neondatabase-labs/neon-agenstack-example" description="AgentStack + Neon Example" icon="github">Building AI Agents with AgentStack and Neon</a>
</DetailIconCards>

## Resources

- [AgentStack Documentation](https://docs.agentstack.sh/introduction)
- [AgentOps Documentation](https://docs.agentops.ai/)
- [Firecrawl Documentation](https://docs.firecrawl.dev/introduction)
- [Firecrawl AgentStack Tool](https://docs.agentstack.sh/tools/tool/firecrawl)
- [CrewAI Documentation](https://docs.crewai.com/introduction)
- [Neon AgentStack Tool](https://docs.agentstack.sh/tools/tool/neon)
- [Neon API Reference](https://api-docs.neon.tech/reference/getting-started-with-neon-api)
- [Neon API keys](/docs/manage/api-keys#creating-api-keys)

<NeedHelp/>


# Building Intelligent Search with AI Embeddings, Neon, and pgvector

---
title: Building Intelligent Search with AI Embeddings, Neon, and pgvector
subtitle: Learn how to create a semantic search system using AI embeddings, Neon, and pgvector.
author: bobbyiliev
enableTableOfContents: true
createdAt: '2025-05-17T00:00:00.000Z'
updatedOn: '2025-05-17T00:00:00.000Z'
---

Traditional text search relies on exact keyword matches, which often misses the semantic meaning behind queries.

When someone searches for "car maintenance," they might also be interested in results about "vehicle servicing" or "auto repair", but keyword-based search won't make these connections.

AI embeddings solve this problem by converting text into high-dimensional vectors that capture semantic meaning. Words and phrases with similar meanings cluster together in this vector space, enabling search systems that understand context and intent rather than just matching exact words.

The pgvector extension brings vector similarity search directly into PostgreSQL, letting you store embeddings alongside your regular data and perform complex semantic searches with simple SQL queries. Combined with Neon's serverless PostgreSQL, you can build intelligent search systems that scale automatically with your application's needs.

In this guide, you'll learn how to build a semantic search system that can power document search and content recommendations using OpenAI embeddings stored in Neon with pgvector.

## What you'll build

By the end of this guide, you'll have:

- An understanding of AI embeddings and how they improve search
- A Neon database configured with the pgvector extension
- An embedding generation service powered by OpenAI's API
- A document search system that understands semantic similarity

## Prerequisites

To follow along with this guide, you'll need:

- A [Neon account](https://console.neon.tech/signup) with a project
- An [OpenAI API key](https://platform.openai.com/api-keys)
- Node.js 20.x or later installed
- Basic familiarity with SQL and REST APIs
- Understanding of JavaScript promises and async/await

## Understanding AI embeddings and vector search

Before diving into the implementation, let's understand how AI embeddings work and why they're great for search.

### What are embeddings?

Embeddings are numerical representations of text that capture semantic meaning in high-dimensional space. When you send text to OpenAI's embedding API, it returns an array of floating-point numbers (typically 1,536 dimensions for the `text-embedding-3-small` model) that represents the "meaning" of that text.

Here's a simplified example of how embeddings work:

```javascript
// These texts have similar meanings
const text1 = "The cat jumped over the fence";
const text2 = "A feline leaped across a barrier";

// Their embeddings would be mathematically similar
const embedding1 = [-0.02, 0.15, -0.08, ...]; // 1,536 numbers
const embedding2 = [-0.01, 0.16, -0.09, ...]; // Similar values
```

The key insight is that semantically similar texts produce similar embedding vectors. This enables search systems that can find relevant content even when it doesn't share exact keywords with the query.

### Why pgvector?

pgvector extends PostgreSQL with vector data types and similarity search operations. Instead of moving your data to specialized vector databases, you can store embeddings alongside your existing relational data and perform vector similarity searches with SQL:

```sql
-- Find documents most similar to a query embedding
SELECT title, content,
       embedding <-> $1 as distance
FROM documents
ORDER BY embedding <-> $1
LIMIT 5;
```

The `<->` operator calculates the distance between vectors, with smaller distances indicating higher similarity.

Now let's build a system that puts these concepts to work.

## Setting up pgvector on Neon

We'll start by enabling pgvector on your Neon database and creating the necessary tables for our search system.

First, create a new Neon project optimized for vector operations:

1. Navigate to the [Neon Console](https://console.neon.tech)
2. Click "New Project"
3. Name your project "semantic-search-system"
4. Choose a region close to your users
5. Select at least 1 CU for compute size (vector operations can be CPU-intensive)

Once your project is created, we need to enable the pgvector extension. Connect to your database and run this SQL:

```sql
-- Enable the pgvector extension
CREATE EXTENSION IF NOT EXISTS vector;

-- Verify the extension is working
SELECT extversion FROM pg_extension WHERE extname = 'vector';
```

This command adds vector data types and similarity functions to your PostgreSQL database. You should see confirmation that the vector extension is now available.

Next, we'll create the database schema for our semantic search system. This includes tables for documents and their embeddings, along with indexes for fast vector similarity search.

```sql
-- Table for storing documents with their embeddings
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    title TEXT NOT NULL,
    content TEXT NOT NULL,
    category VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    embedding vector(1536) -- OpenAI's text-embedding-3-small produces 1536-dimensional vectors
);

-- Create indexes for fast vector similarity search
CREATE INDEX documents_embedding_idx ON documents USING ivfflat (embedding vector_cosine_ops);
```

The `vector(1536)` data type stores 1,536-dimensional vectors, matching OpenAI's embedding size. The `ivfflat` indexes enable fast approximate nearest neighbor searches using cosine similarity, this is crucial for performance when searching through thousands of embeddings.

## Building an embedding generation service

Now we'll create a Node.js service that handles the complexities of generating embeddings with [OpenAI's API](https://platform.openai.com/docs/guides/embeddings). This service will be the bridge between your text content and the vector representations stored in the database.

Let's set up the project structure and install the necessary dependencies:

```bash
mkdir semantic-search-service
cd semantic-search-service
npm init -y
npm install openai pg dotenv express cors
```

Create a `.env` file to securely store your API credentials:

```bash
OPENAI_API_KEY=your_openai_api_key_here
DATABASE_URL=postgresql://user:password@ep-abc123.region.aws.neon.tech/neondb?sslmode=require&channel_binding=require
```

Now let's create an embedding service that handles OpenAI API interactions. This service will preprocess text, generate embeddings, and handle errors gracefully:

```javascript
// embedding.js
const OpenAI = require('openai');
require('dotenv').config();

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

class EmbeddingService {
  constructor() {
    this.model = 'text-embedding-3-small'; // Cost-effective and performant
    this.maxTokens = 8191; // Maximum tokens for this model
  }

  async generateEmbedding(text) {
    try {
      // Clean and prepare text for embedding generation
      const cleanText = this.preprocessText(text);

      const response = await openai.embeddings.create({
        model: this.model,
        input: cleanText,
      });

      return response.data[0].embedding;
    } catch (error) {
      console.error('Error generating embedding:', error);
      throw new Error(`Failed to generate embedding: ${error.message}`);
    }
  }

  preprocessText(text) {
    // Remove extra whitespace and normalize the text
    let cleaned = text.trim().replace(/\s+/g, ' ');

    // Truncate if too long (rough estimate: 1 token ≈ 4 characters)
    const maxChars = this.maxTokens * 3; // Conservative estimate
    if (cleaned.length > maxChars) {
      cleaned = cleaned.substring(0, maxChars) + '...';
    }

    return cleaned;
  }

  // Create optimized text for embedding generation
  createDocumentText(title, content, category = '') {
    return `Title: ${title}\nCategory: ${category}\nContent: ${content}`;
  }
}

module.exports = EmbeddingService;
```

This service handles a few important tasks: it preprocesses text to ensure it fits within OpenAI's token limits, combines document fields into optimized text for better embeddings, and provides error handling for API calls. The `createDocumentText` method is particularly important because it structures the input text in a way that produces better semantic embeddings.

As of the time of writing, OpenAI's `text-embedding-3-small` model is the newest and most cost-effective for generating embeddings.

## Creating a document management system

Now we'll build a service that manages documents in our database and automatically generates embeddings for each document. This service will handle both individual documents and batch operations.

Let's create the document service that ties together our database and embedding generation:

```javascript
// document-service.js
const { Pool } = require('pg');
const EmbeddingService = require('./embedding');
require('dotenv').config();

class DocumentService {
  constructor() {
    this.pool = new Pool({
      connectionString: process.env.DATABASE_URL,
      ssl: true,
      max: 20,
    });
    this.embeddingService = new EmbeddingService();
  }

  async addDocument(title, content, category = null) {
    const client = await this.pool.connect();

    try {
      await client.query('BEGIN');

      // Combine document fields into optimized text for embedding
      const fullText = this.embeddingService.createDocumentText(title, content, category || '');

      // Generate embedding using OpenAI API
      console.log(`Generating embedding for document: ${title}`);
      const embedding = await this.embeddingService.generateEmbedding(fullText);
      const formattedEmbedding = `[${embedding.join(',')}]`;

      // Store document with its embedding in the database
      const result = await client.query(
        `
        INSERT INTO documents (title, content, category, embedding)
        VALUES ($1, $2, $3, $4)
        RETURNING id, title, created_at
      `,
        [title, content, category, formattedEmbedding]
      );

      await client.query('COMMIT'); // Complete transaction

      console.log(`Document added successfully: ${result.rows[0].id}`);
      return result.rows[0];
    } catch (error) {
      await client.query('ROLLBACK'); // Undo changes on error
      console.error('Error adding document:', error);
      throw error;
    } finally {
      client.release(); // Return connection to pool
    }
  }

  async searchDocuments(query, limit = 10, category = null) {
    try {
      // First, convert the search query into an embedding
      const fullText = this.embeddingService.createDocumentText(query, '', category || '');
      const queryEmbedding = await this.embeddingService.generateEmbedding(fullText);

      // Format embedding as a pgvector-compatible string
      const formattedEmbedding = `[${queryEmbedding.join(',')}]`;

      // Start building the SQL query
      let sql = `
            SELECT 
                id,
                title,
                content,
                category,
                created_at,
                1 - (embedding <=> $1) as similarity_score
            FROM documents
            WHERE 1 - (embedding <=> $1) > 0.3
            `;

      const params = [formattedEmbedding];

      // Add category filter if needed
      if (category) {
        sql += ` WHERE category = $2`;
        params.push(category);
      }

      // Append ORDER BY and LIMIT clauses
      sql += ` ORDER BY embedding <=> $1 LIMIT $${params.length + 1}`;
      params.push(limit);

      const result = await this.pool.query(sql, params);

      // Format results
      return result.rows.map((row) => ({
        ...row,
        similarity_score: parseFloat(row.similarity_score.toFixed(4)),
        preview: row.content.length > 200 ? row.content.substring(0, 200) + '...' : row.content,
      }));
    } catch (error) {
      console.error('Error searching documents:', error);
      throw error;
    }
  }

  async getDocumentById(id) {
    try {
      const result = await this.pool.query(
        `
        SELECT id, title, content, category, created_at
        FROM documents 
        WHERE id = $1
      `,
        [id]
      );

      return result.rows[0] || null;
    } catch (error) {
      console.error('Error getting document:', error);
      throw error;
    }
  }
}

module.exports = DocumentService;
```

This service provides the core functionality for our semantic search system. The `addDocument` method generates embeddings and stores them alongside the document data, while `searchDocuments` performs the actual semantic search by converting queries to embeddings and finding the most similar documents using pgvector's distance operators.

The key insight here is the `<=>` operator in the SQL query, this calculates cosine distance between vectors, with smaller values indicating higher similarity. We convert this to a similarity score between 0 and 1 for easier interpretation.

The `getDocumentById` method retrieves a specific document by its ID, allowing applications to fetch full content when needed.

## Building the search API

With our document service in place, let's create an Express API that exposes our document search capabilities to applications.

This API will provide endpoints for adding documents and performing semantic searches.

```javascript
// server.js
const express = require('express');
const cors = require('cors');
const DocumentService = require('./document-service');
require('dotenv').config();

const app = express();
app.use(express.json());
app.use(cors());

const documentService = new DocumentService();

// Add a single document with automatic embedding generation
app.post('/documents', async (req, res) => {
  try {
    const { title, content, category } = req.body;

    // Validate required fields
    if (!title || !content) {
      return res.status(400).json({
        error: 'Title and content are required',
      });
    }

    // Add document and generate embedding
    const result = await documentService.addDocument(title, content, category);
    res.status(201).json(result);
  } catch (error) {
    console.error('Error adding document:', error);
    res.status(500).json({
      error: 'Failed to add document',
      message: error.message,
    });
  }
});

// Perform semantic search across all documents
app.post('/search', async (req, res) => {
  try {
    const { query, limit = 10, category } = req.body;

    if (!query) {
      return res.status(400).json({
        error: 'Search query is required',
      });
    }

    const startTime = Date.now();

    // Execute semantic search using vector similarity
    const results = await documentService.searchDocuments(query, limit, category);

    const searchTime = Date.now() - startTime;

    res.json({
      query,
      results,
      count: results.length,
      search_time_ms: searchTime,
    });
  } catch (error) {
    console.error('Error searching documents:', error);
    res.status(500).json({
      error: 'Search failed',
      message: error.message,
    });
  }
});

// Get a specific document by ID
app.get('/documents/:id', async (req, res) => {
  try {
    const { id } = req.params;
    const document = await documentService.getDocumentById(parseInt(id));

    if (!document) {
      return res.status(404).json({ error: 'Document not found' });
    }

    res.json(document);
  } catch (error) {
    console.error('Error getting document:', error);
    res.status(500).json({
      error: 'Failed to get document',
      message: error.message,
    });
  }
});

// Health check endpoint
app.get('/health', async (req, res) => {
  try {
    // Test database connection
    await documentService.pool.query('SELECT 1');
    res.json({ status: 'healthy', database: 'connected' });
  } catch (error) {
    res.status(500).json({ status: 'unhealthy', database: 'disconnected' });
  }
});

const PORT = process.env.PORT || 3000;
app.listen(PORT, () => {
  console.log(`Semantic search service running on port ${PORT}`);
});
```

This API provides clean, RESTful endpoints for our semantic search functionality. The `/search` endpoint is the heart of the system, it takes natural language queries and returns semantically relevant documents, even when there are no exact keyword matches.

Notice how we measure and return the search time. This helps you monitor performance as your document collection grows.

## Testing the semantic search system

With our API in place, now let's test our semantic search system to see how it finds relevant documents based on meaning rather than exact keywords. We'll add sample documents and run various search queries.

Create a test script to populate your database with sample content and test the search functionality:

```javascript
// test-search.js
const axios = require('axios');

const baseURL = 'http://localhost:3000';

async function testSemanticSearch() {
  try {
    console.log('Adding sample documents to test semantic search...\n');

    // Add diverse sample documents across different topics
    const sampleDocuments = [
      {
        title: 'Getting Started with Machine Learning',
        content:
          'Machine learning is a subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed. This guide covers the basics of supervised learning, unsupervised learning, and neural networks.',
        category: 'technology',
      },
      {
        title: 'Healthy Cooking Tips',
        content:
          "Eating nutritious meals doesn't have to be complicated. Focus on fresh ingredients, reduce processed foods, and try cooking methods like steaming and grilling. Meal prep can save time and help maintain a balanced diet.",
        category: 'health',
      },
      {
        title: 'Remote Work Best Practices',
        content:
          'Working from home requires discipline and good habits. Set up a dedicated workspace, maintain regular hours, and use collaboration tools effectively. Communication with team members is crucial for remote success.',
        category: 'productivity',
      },
      {
        title: 'Understanding Neural Networks',
        content:
          'Neural networks are computing systems inspired by biological neural networks. They consist of layers of interconnected nodes that process information. Deep learning uses multi-layered neural networks to solve complex problems.',
        category: 'technology',
      },
    ];

    // Add each document to the system
    for (const doc of sampleDocuments) {
      await axios.post(`${baseURL}/documents`, doc);
      console.log(`✓ Added: ${doc.title}`);
    }

    console.log('\nDocuments added successfully! Now testing semantic search...\n');

    // Test semantic searches that don't use exact keywords
    const testQueries = [
      'artificial intelligence and computers', // Should find ML and neural network docs
      'how to stay healthy while eating', // Should find cooking tips
      'working from home effectively', // Should find remote work practices
      'deep learning algorithms', // Should find both AI-related documents
    ];

    // Run each test query and display results
    for (const query of testQueries) {
      console.log(`🔍 Searching for: "${query}"`);

      const response = await axios.post(`${baseURL}/search`, {
        query,
        limit: 3,
      });

      const { results, search_time_ms } = response.data;

      console.log(`   Found ${results.length} results in ${search_time_ms}ms`);

      // Display the most relevant results
      results.forEach((doc, index) => {
        console.log(`   ${index + 1}. ${doc.title} (similarity: ${doc.similarity_score})`);
        console.log(`      Category: ${doc.category}`);
        console.log(`      Preview: ${doc.preview}\n`);
      });

      console.log('---\n');
    }
  } catch (error) {
    console.error('Test failed:', error.response?.data || error.message);
  }
}

// Start the server and run tests
console.log('Make sure your server is running with: node server.js\n');
testSemanticSearch();
```

Install `axios` for the test script and run it:

```bash
npm install axios

# Start your server in one terminal
node server.js

# Run the test in another terminal
node test-search.js
```

You should see output showing how the semantic search finds relevant documents even when the search terms don't appear exactly in the document text.

For example, searching for "artificial intelligence and computers" should return documents about machine learning and neural networks, demonstrating the power of semantic understanding.

The similarity scores help you understand how closely each result matches the query - scores closer to 1.0 indicate higher semantic similarity.

## Performance optimization

As your document collection grows, you'll want to optimize performance.

First, let's properly configure the pgvector indexes. The default index settings work for small datasets, but you'll need to tune them for larger collections:

```sql
-- Drop existing indexes to recreate with optimal settings
DROP INDEX IF EXISTS documents_embedding_idx;

-- Create optimized indexes based on your data size
-- Rule of thumb: lists = rows / 1000, with minimum of 10
CREATE INDEX documents_embedding_idx ON documents
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);  -- Adjust based on your document count

-- Update table statistics for optimal query planning
ANALYZE documents;
```

These indexes dramatically improve query speed by creating approximate nearest neighbor searches instead of comparing every vector.

The `lists` parameter should be adjusted based on your data size, more documents need more lists for optimal performance.

## Conclusion

You now should have a solid foundation for a semantic search system that uses AI embeddings alongside `pgvector` in Neon.

You've built a simple semantic search system that demonstrates the power of AI embeddings with Neon and `pgvector`.

The foundation you've built can be extended with features like real-time search suggestions or multilingual support. The principles of semantic search and vector similarity will enable you to create intelligent applications that understand user intent and context.

## Additional Resources

- [pgvector Documentation](https://github.com/pgvector/pgvector)
- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)
- [Neon Serverless PostgreSQL](/docs)
- [Vector Similarity Search Best Practices](https://github.com/pgvector/pgvector#best-practices)

<NeedHelp />


# Automate Database Branching with Alchemy and Neon Postgres

---
title: Automate Database Branching with Alchemy and Neon Postgres
subtitle: Learn how to use Alchemy Infrastructure-as-Code to programmatically create and manage Neon database branches
author: bobbyiliev
enableTableOfContents: true
createdAt: '2025-05-10T00:00:00.000Z'
updatedOn: '2025-05-10T00:00:00.000Z'
---

Database branching is one of Neon's most powerful features, letting you create isolated database copies in seconds.

[Alchemy](https://github.com/sam-goodwin/alchemy) is a TypeScript-native Infrastructure-as-Code tool that lets you automate cloud resources with simple async functions. Unlike traditional IaC tools like Terraform that require learning new languages and complex state management, Alchemy lets you manage infrastructure using the TypeScript you already know.

In this guide, you'll learn how to use Alchemy to automatically create, manage, and clean up Neon database branches as part of your development workflow. Think of it as "Git for your database infrastructure", you'll define what branches you want in code, and Alchemy will make sure they exist.

## Prerequisites

To follow along with this guide, you'll need:

- [Node.js 20](https://nodejs.org/) or later installed
- A [Neon account](https://console.neon.tech/signup) with a project
- A [Neon API key](/docs/manage/api-keys) (you'll need this to manage branches programmatically)
- Basic familiarity with TypeScript and REST APIs

## What you'll build

By the end of this guide, you'll have:

- Learn what Alchemy is and how it differs from traditional Infrastructure-as-Code tools
- Create custom Alchemy resources for managing Neon branches
- Automated branch creation for feature development
- Branch cleanup when features are merged

## Understanding Alchemy basics

Before we dive into the implementation, let's understand what makes Alchemy different from traditional Infrastructure-as-Code tools and why it's particularly well-suited for managing Neon database branches.

### What is Alchemy?

Traditional IaC tools like Terraform require you to learn new languages and manage complex state files. Alchemy takes a different approach: you write regular TypeScript functions that describe your infrastructure.

Here's how a typical resource looks in Alchemy:

```typescript
const MyResource = Resource('my::Resource', async function (id, props) {
  if (this.phase === 'delete') {
    // Clean up the resource
    await deleteFromAPI(this.state.resourceId);
    return this.destroy();
  }

  // Create or update the resource
  const result = await createOrUpdateAPI(props);
  return { ...props, resourceId: result.id };
});
```

This function handles the entire lifecycle of a resource. When you run your code, Alchemy calls this function for each resource you've defined, automatically handling creation, updates, and deletion.

### Declarative with imperative code

Even though you write regular functions, the end result is declarative. You define what you want, and Alchemy figures out how to get there:

```typescript
// Define what infrastructure you want
const app = await alchemy('my-app');

const database = await Database('prod-db', {
  name: 'production',
  size: 'large',
});

const testDb = await Database('test-db', {
  name: 'testing',
  size: 'small',
});

await app.finalize();
```

If you run this code twice, Alchemy won't create duplicate resources. If you remove the `testDb` and run again, Alchemy will automatically delete it. This automatic cleanup is what makes Alchemy particularly powerful for managing database branches.

### Why this works well for database branching

Database branches often need dynamic configuration based on feature names, environments, or developer preferences. With Alchemy, you can use regular programming constructs:

```typescript
// Create branches for active pull requests
const pullRequests = await github.getPullRequests();

for (const pr of pullRequests) {
  await NeonBranch(`pr-${pr.number}`, {
    name: `pr-${pr.number}-${pr.title.toLowerCase().replace(/\s+/g, '-')}`,
    parentBranch: pr.targetBranch === 'main' ? 'production' : 'staging',
  });
}
```

When pull requests are merged or closed, the corresponding branches get cleaned up automatically on the next run.

## Security and Encryption with Alchemy

Before we dive into building our Neon branch automation, it's crucial to understand how Alchemy handles sensitive data like API keys and connection strings.

### Why Secret Management Matters

When working with database infrastructure, you'll be dealing with sensitive information like:

- Neon API keys
- Database connection strings
- Authentication credentials

Without proper handling, these values would be stored as plain text in Alchemy's state files, creating a security risk.

### How Alchemy Secrets Work

Alchemy provides built-in encryption for sensitive data through the `alchemy.secret()` function. When you wrap a value with this function, Alchemy automatically encrypts it before storing it in state files.

Here's what happens behind the scenes:

- **Plain text in code**: `alchemy.secret(process.env.API_KEY)`
- **Encrypted in state**: `{"@secret": "Tgz3e/WAscu4U1oanm5S4YXH..."}`

### Encryption Password

Secrets are encrypted using a password that you provide when initializing your Alchemy app. This password is used to encrypt and decrypt all secret values in your application.

**Important**: Always store your encryption password securely and never commit it to source control.

```typescript
const app = await alchemy('my-app', {
  password: process.env.ENCRYPTION_PASSWORD,
});
```

We'll see how to implement this properly in the next section.

## Setting up your project

Let's start by creating a new project and installing the necessary dependencies. We'll build this step by step so you can see exactly how everything fits together.

First, create a new directory for our project. This will be a standalone project that you can later integrate into your existing applications:

```bash
mkdir neon-alchemy-demo
cd neon-alchemy-demo
npm init -y
```

The `npm init -y` command creates a basic `package.json` file with default settings. You'll see it creates a simple Node.js project structure.

Next, let's install all the dependencies we'll need. Alchemy is designed to work seamlessly with TypeScript, so we'll use TypeScript throughout this guide:

```bash
npm install alchemy dotenv
npm install --save-dev typescript @types/node tsx
```

After running this, you'll have Alchemy installed along with TypeScript support and the tools we need to run our code. Alchemy itself has zero dependencies, which means it won't bloat your project or conflict with other tools you're using.

Now let's configure our project for TypeScript and ESM modules. First, update your `package.json` to define the module type and add a start script:

```json
{
  "type": "module",
  "scripts": {
    "start": "tsx main.ts"
  }
}
```

This tells Node.js to treat your project as an ESM module, which is required for Alchemy to work properly. The `tsx` package allows you to run TypeScript files directly without needing to compile them first.

Create a `tsconfig.json` file that tells TypeScript how to compile our code:

```json
{
  "compilerOptions": {
    "target": "ES2022",
    "module": "ESNext",
    "moduleResolution": "bundler",
    "allowSyntheticDefaultImports": true,
    "esModuleInterop": true,
    "strict": true,
    "skipLibCheck": true
  }
}
```

This configuration sets up modern TypeScript with ESM modules, which Alchemy requires. The settings ensure compatibility with Alchemy's async-native design.

Finally, we need to store our credentials securely. Create a `.env` file in your project root:

```bash
NEON_API_KEY=your_neon_api_key_here
NEON_PROJECT_ID=your_project_id_here
ENCRYPTION_PASSWORD=your_strong_encryption_password_here
```

Replace the values with:

- `your_neon_api_key_here`: Your actual Neon API key
- `your_project_id_here`: Your project ID from the Neon Console
- `your_strong_encryption_password_here`: A strong password for encrypting secrets (generate this securely)

> **Security note**: The `ENCRYPTION_PASSWORD` is used by Alchemy to encrypt sensitive data like API keys and connection strings. Choose a strong, unique password and store it securely. Never commit this to source control.

## Creating a Neon Branch resource

Here's where Alchemy gets really handy. Alchemy resources are just async functions that create, update, or delete cloud resources. We're going to create a custom resource that knows how to manage Neon branches through the Neon API.

Think of this resource as a "blueprint" that Alchemy can use to create as many Neon branches as you need. Once we write this code, we can reuse it throughout our application.

Create a file called `neon-branch.ts`:

```typescript
import alchemy, { Resource } from 'alchemy';

interface NeonBranchProps {
  name: string;
  projectId: string;
  parentBranchId?: string;
  apiKey: ReturnType<typeof alchemy.secret>;
}

interface NeonBranchState extends Omit<NeonBranchProps, 'apiKey'> {
  branchId: string;
  connectionString: ReturnType<typeof alchemy.secret>;
  createdAt: string;
  apiKey: ReturnType<typeof alchemy.secret>;
}

export const NeonBranch = Resource<NeonBranchState, NeonBranchProps>(
  'neon::Branch',
  async function (id: string, props: NeonBranchProps) {
    const { name, projectId, parentBranchId, apiKey } = props;

    if (this.phase === 'delete') {
      const branchId = this.state?.branchId ?? (this.output as any)?.branchId;

      if (branchId) {
        try {
          const deleteResponse = await fetch(
            `https://console.neon.tech/api/v2/projects/${projectId}/branches/${branchId}`,
            {
              method: 'DELETE',
              headers: {
                Authorization: `Bearer ${apiKey}`,
                'Content-Type': 'application/json',
              },
            }
          );

          if (!deleteResponse.ok) {
            console.warn(`Failed to delete branch: ${deleteResponse.statusText}`);
          }
        } catch (err) {
          console.warn(`Error during branch deletion: ${(err as Error).message}`);
        }
      } else {
        console.warn('No branchId found in state or output; skipping deletion.');
      }

      return this.destroy();
    }

    // Check if branch already exists
    if (this.state?.branchId) {
      // Branch exists, return current state
      return this.state;
    }

    // Create a new branch
    const createBranchResponse = await fetch(
      `https://console.neon.tech/api/v2/projects/${projectId}/branches`,
      {
        method: 'POST',
        headers: {
          Authorization: `Bearer ${apiKey}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          name,
          parent_id: parentBranchId,
        }),
      }
    );

    if (!createBranchResponse.ok) {
      const errorText = await createBranchResponse.text();
      throw new Error(
        `Failed to create branch: ${createBranchResponse.status} ${createBranchResponse.statusText} - ${errorText}`
      );
    }

    const branchResult = await createBranchResponse.json();
    const branch = branchResult.branch;

    // Get the project's main endpoint to build connection string
    const endpointsResponse = await fetch(
      `https://console.neon.tech/api/v2/projects/${projectId}/endpoints`,
      {
        headers: {
          Authorization: `Bearer ${apiKey}`,
        },
      }
    );

    if (!endpointsResponse.ok) {
      throw new Error(`Failed to get endpoints: ${endpointsResponse.statusText}`);
    }

    const endpointsResult = await endpointsResponse.json();

    // Use the primary endpoint (usually the first one or the one marked as primary)
    const primaryEndpoint =
      endpointsResult.endpoints.find((ep: any) => ep.branch_id === parentBranchId) ||
      endpointsResult.endpoints[0];

    if (!primaryEndpoint) {
      throw new Error('No endpoints found for project');
    }

    // Get project details to find the database name
    const projectResponse = await fetch(`https://console.neon.tech/api/v2/projects/${projectId}`, {
      headers: {
        Authorization: `Bearer ${apiKey}`,
      },
    });

    if (!projectResponse.ok) {
      throw new Error(`Failed to get project details: ${projectResponse.statusText}`);
    }

    const projectResult = await projectResponse.json();
    const databaseName = 'neondb';

    const connectionString = `postgres://${primaryEndpoint.host}/${databaseName}?sslmode=require&channel_binding=require&options=project%3D${projectId}`;

    return {
      ...props,
      branchId: branch.id,
      // Encrypt the connection string
      connectionString: alchemy.secret(connectionString),
      createdAt: branch.created_at,
    };
  }
);
```

Let's break down what this code does:

- The interfaces define the shape of our data. `NeonBranchProps` is what you pass in (the branch name, project ID, etc.), and `NeonBranchState` is what gets stored (including the generated branch ID and connection string).
- The Resource function is where Alchemy manages the lifecycle. When Alchemy runs, it calls this function for each branch resource you've defined. The function checks what phase it's in:
  - Delete phase: If you've removed a branch from your code, Alchemy calls the Neon API to delete it
  - Create/Update phase: If the branch doesn't exist, it creates it. If it already exists, it returns the current state
- The API calls use the standard Neon REST API to create branches and get connection information. This is the same API you could call manually with curl, but now it's automated.
- Notice how we use `ReturnType<typeof alchemy.secret>` to type sensitive values like API keys and connection strings. This ensures they're properly encrypted when stored in Alchemy's state files. When we create the connection string, we wrap it with `alchemy.secret()` to encrypt it before storage.

The great thing about this approach is that you define the resource once, and then Alchemy handles all the complexity of tracking state, making API calls, and cleaning up resources while keeping your sensitive data secure.

## Using the Neon Branch resource

Now let's put our resource to work. We'll create a simple script that demonstrates how to use it. This script will create a couple of branches and show you how Alchemy manages them.

Create a file called `main.ts`:

```typescript
import alchemy from 'alchemy';
import { NeonBranch } from './neon-branch.js';
import dotenv from 'dotenv';
dotenv.config();

// Initialize Alchemy with encryption password
const app = await alchemy('neon-demo', {
  password: process.env.ENCRYPTION_PASSWORD!,
});

// Create secrets from environment variables
const apiKey = alchemy.secret(process.env.NEON_API_KEY!);

// Create a feature branch
const featureBranch = await NeonBranch('feature-auth-system', {
  name: 'feature/auth-system',
  projectId: process.env.NEON_PROJECT_ID!,
  apiKey: apiKey,
});

console.log('Feature branch created!');
console.log('Branch ID:', featureBranch.branchId);
// Note: Connection string is encrypted, so we can't log it directly
console.log('Connection string: [encrypted]');

// Create a testing branch from the feature branch
const testingBranch = await NeonBranch('testing-auth-system', {
  name: 'test/auth-system',
  projectId: process.env.NEON_PROJECT_ID!,
  parentBranchId: featureBranch.branchId,
  apiKey: apiKey,
});

console.log('Testing branch created!');
console.log('Testing branch ID:', testingBranch.branchId);

await app.finalize();
```

Here's what this script does step by step:

- First, we initialize an Alchemy app with the name 'neon-demo' and provide an encryption password. This creates a local state file where Alchemy tracks what resources exist and encrypts sensitive data.
- Next, we create a secret from our Neon API key using `alchemy.secret()`, ensuring it's encrypted before being passed to resources.
- Then, we create a feature branch using our `NeonBranch` resource. The first parameter (`'feature-auth-system'`) is a unique ID that Alchemy uses to track this specific branch. The second parameter contains the branch configuration.
- We create a testing branch that branches off from our feature branch. Notice how we pass `featureBranch.branchId` as the `parentBranchId` - this creates a branch hierarchy just like you'd have with git branches.
- Finally, we call `app.finalize()` to tell Alchemy we're done. This is when Alchemy checks if there are any resources that need to be cleaned up.

The connection strings returned by our resource are encrypted secrets. In a real application, you'd decrypt them when needed using Alchemy's mechanisms.

Let's run this script and see it in action. Make sure you have your Neon API key and project ID set in your environment variables, then run:

```bash
npx tsx main.ts
```

When you run this command, you should see output similar to:

```
Feature branch created!
Branch ID: br_cool_darkness_12345
Connection string: [encrypted]
Testing branch created!
Testing branch ID: br_wispy_cloud_67890
```

If you check your Neon Console now, you'll see two new branches have appeared in your project. The connection strings are stored securely in Alchemy's encrypted state, but can be used by your application when needed.

## Automating branch cleanup

Here's where Alchemy really can help compared to manual branch management. With Alchemy, cleanup is automatic. When you remove a resource from your code, Alchemy automatically deletes it on the next run. Let's see this in action with a more practical example.

Create a file called `feature-workflow.ts`:

```typescript
import alchemy from 'alchemy';
import { NeonBranch } from './neon-branch.js';
import dotenv from 'dotenv';
dotenv.config();

// Initialize with encryption password
const app = await alchemy('feature-workflow', {
  password: process.env.ENCRYPTION_PASSWORD!,
});

// Create encrypted API key
const apiKey = alchemy.secret(process.env.NEON_API_KEY!);

// Simulate different features being worked on
const activeFeatures = [
  'user-authentication',
  'payment-integration',
  // 'email-notifications', // Simulate a completed feature by commenting this out
];

// Create branches for active features
for (const feature of activeFeatures) {
  const branch = await NeonBranch(`feature-${feature}`, {
    name: `feature/${feature}`,
    projectId: process.env.NEON_PROJECT_ID!,
    apiKey: apiKey,
  });

  console.log(`Branch created for ${feature}: ${branch.branchId}`);
}

await app.finalize();
```

This script simulates a development workflow where you have multiple features being worked on simultaneously. Notice the commented-out line for 'email-notifications' - this represents a feature that was completed and merged in our fictitious project.

Here's what happens when you run this script:

- First run: Alchemy creates branches for `user-authentication` and `payment-integration`. If the `email-notifications` branch existed from a previous run, it gets deleted because it's no longer in the active features list.

- Subsequent runs: If you add new features to the array, Alchemy creates branches for them. If you remove features (like commenting out lines), Alchemy automatically deletes the corresponding branches.

Run the script to see this in action:

```bash
npx tsx feature-workflow.ts
```

You'll see output showing which branches were created. If you run it again, the existing branches won't be recreated (Alchemy is smart about this), but any branches that are no longer defined in your code will be deleted.

This automatic cleanup means you never have to remember to delete old branches manually. Your database infrastructure stays clean, and you only pay for what you're actually using.

## Troubleshooting

While using Alchemy, you might encounter some common issues. Here are some tips to help you troubleshoot:

### Authentication errors

If you see errors about authentication or permissions, first test your API key manually to make sure it works:

```bash
curl -H "Authorization: Bearer YOUR_API_KEY" \
  https://console.neon.tech/api/v2/projects
```

Replace `YOUR_API_KEY` with your actual API key. If this doesn't return a list of your projects, the problem is with your API key. Make sure you created it correctly and that it has the right permissions.

### Rate limiting

Most APIs have rate limits to prevent abuse. For example, if you're creating many branches quickly, you might hit these limits. You can add small delays between branch creations to avoid this:

```typescript
// Add a small delay between branch creations
for (const feature of features) {
  await NeonBranch(`feature-${feature}`, {
    /* ... */
  });

  // Wait 1 second before creating the next branch
  await new Promise((resolve) => setTimeout(resolve, 1000));
}
```

This is usually only necessary if you're creating a large number of resources in a short time. For normal development workflows, you shouldn't run into this issue.

### Branch name conflicts

Neon branch names must be unique within a project. If you get errors about duplicate names, you can use prefixes or timestamps to make them unique:

```typescript
const uniqueName = `feature-${Date.now()}-${featureName}`;
```

Or use a more readable format:

```typescript
const uniqueName = `feature-${featureName}-${new Date().toISOString().split('T')[0]}`;
```

### State file issues

Alchemy stores its state in a local file (usually `.alchemy/state.json`). If this file gets corrupted or deleted, Alchemy might lose track of your resources. You can usually fix this by:

1. Deleting the `.alchemy` directory
2. Running your script again (Alchemy will detect existing resources)

Or by manually cleaning up resources in the Neon Console and starting fresh.

## Summary

You've learned how to use Alchemy to automate Neon database branching with proper security practices.

The combination of Alchemy's simple TypeScript approach and Neon's branching makes database infrastructure management almost invisible. You focus on building features, and the database branches you need just exist.

<NeedHelp />


# Querying Neon Postgres with Natural Language via Amazon Q Business

---
title: Querying Neon Postgres with Natural Language via Amazon Q Business
subtitle: Learn how to set up Amazon Q Business to query your Neon Postgres database using natural language
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-11-23T00:00:00.000Z'
updatedOn: '2024-11-23T00:00:00.000Z'
---

Amazon Q Business enables you to build an interactive chat application that combines your Neon Postgres data with large language model capabilities.

Rather than querying your database directly, Amazon Q Business creates an index of your data which users can then interact with through a natural language interface.

In this guide, you'll learn how to set up Amazon Q Business to query your Neon Postgres database using natural language. We'll cover the following topics:

## Prerequisites

Before starting, you'll need:

- An [AWS Account](https://aws.amazon.com/q/) with Amazon Q Business subscription (Pro or Lite plan)
- A [Neon](https://console.neon.tech/signup) account and project
- [AWS IAM Identity Center](https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/idc-setup.html) configured (recommended) or Identity Federation through IAM
- Basic understanding of databases and SQL

## Setting Up Your Environment

In order to use Amazon Q, you will need to have an AWS Organization set up with the IAM Identity Center enabled. This will allow you to manage your users and permissions across multiple AWS accounts.

### Configure IAM Identity Center

To begin setting up the AWS Q Business application, you will have to configure the application environment to enable end-user access by integrating AWS IAM Identity Center for streamlined user management.

If you haven't set up IAM Identity Center yet, complete the setup process first following AWS's [detailed guide](https://docs.aws.amazon.com/singlesignon/latest/userguide/get-set-up-for-idc.html).

The exact steps may vary depending on your organization's requirements, existing identity sources, security policies, and other factors. It's recommended to work with your organization's IT or security team to ensure a smooth setup.

You can follow the [IAM Identity Center setup guide](https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/create-application.html) for detailed steps.

### Create Your Application Environment

Once IAM Identity Center is configured, next you will have to create your Amazon Q Business application:

1. Navigate to the [Amazon Q Business Console](https://us-east-1.console.aws.amazon.com/amazonq/business/welcome) and choose "Create application". You must add, assign, and subscribe at least one user to your Amazon Q Business application environment for it to work as intended.

2. Configure basic settings:
   - Name: Pick a unique name for your application
   - Outcome: Choose 'Web experience', this will allow you to access Q as a managed web experience
   - For Access management method select 'IAM Identity Center' or 'IAM Identity Provider' depending on your setup
   - Under the 'Quick start user' section, select the user that you've created in the IAM Identity Center or click the 'Add new users and groups' button to create a new user if you haven't done so already
   - Next to the selected user, make sure to select subscription plan (Pro or Lite) and assign the user to the application

3. Once ready, click the create button. Amazon Q Business will automatically create a web experience unless you opt out. For detailed steps, follow the [application creation guide](https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/create-app.html).

Now that your environment is set up, you can proceed to connecting your Neon database as a data source and configuring the natural language interface.

## Database Setup

If you already have data in your Neon Postgres database, you can skip this section. Otherwise, follow these steps to create a sample database and set up data syncing with Amazon Q Business.

Let's set up a sample database with customer data:

```sql
-- Create customers table
CREATE TABLE customers (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    subscription_tier VARCHAR(50),
    monthly_spend DECIMAL(10,2),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Insert sample data
INSERT INTO customers (name, email, subscription_tier, monthly_spend) VALUES
('TechWave Solutions', 'contact@example.com', 'Premium', 9500.00),
('BlueSky Innovations', 'hello@example.com', 'Growth', 2200.00),
('Greenfield Consulting', 'info@example.com', 'Standard', 600.00),
('BrightFuture Partners', 'support@example.com', 'Premium', 12000.00),
('UrbanTech Solutions', 'services@example.com', 'Basic', 300.00);
```

The above SQL script creates a `customers` table. The table stores customer information which we will use demonstrate data syncing with Amazon Q Business and querying through natural language.

## Configuring Data Source

Once your Neon database is set up, you can configure it as a data source in Amazon Q Business.

Head to the [Amazon Q Business Console](https://us-east-1.console.aws.amazon.com/amazonq/business/applications), click on the application you created, and then follow these steps:

1. Navigate to "Data sources" and click "Add data source"
2. Search for the "PostgreSQL" connector and click "Add"
3. Configure connection details:

   Under '**Source**' add the following details:
   - Data source name: Neon Database
   - Host: your-neon-hostname.neon.tech
   - Port: 5432
   - Instance: `your_neon_database`
   - Check the '**Enable SSL Certificate Location**' box but leave the '**SSL Certificate Location**' field empty

   Under '**Authentication**', click on the '**Create and add a secret**' button and add the following details:
   - Secret name: Neon Database Secret
   - Username: `your_neon_database_username`
   - Password: `your_neon_database_password`

   Click '**Create secret**' and then select the secret that you've just created from the dropdown.

   Under '**IAM role**', create a new role or select an existing one with the necessary permissions for Amazon Q Business to access your Neon credentials through the secret manager. You can click the '**Create a new service role**' button to create a new role if you don't have one already.

4. After setting up your database connection, you'll need to configure how Amazon Q Business should sync your data. Here's how to configure the sync scope:

   Enter your SQL query that defines what data to sync. For example:

   ```sql
   SELECT
       id,
       name,
       CONCAT(subscription_tier, ': ', monthly_spend) AS body
   FROM customers
   ```

   > Note: SQL queries must be less than 32 KB in size and must only use DQL (Data Query Language) operations.
   - Define the required columns:
     - **Primary key column**: `id`
     - **Title column**: `name`
     - **Body column**: `body`

   Amazon Q Business offers additional configuration options like change-detecting columns, user IDs, groups, timestamps, and more. These settings help you fine-tune how the sync process works and when data should be updated. For most use cases, the default settings will work well.

5. Once you've configured the sync scope, you can set up the '**Sync Mode**' and '**Sync Schedule**'. The sync mode determines how Amazon Q Business should handle data changes, while the sync schedule defines how often the sync job should run.
   - **Sync Mode**: Choose between '**Full Sync**' or '**New and Modified Content Sync**'. Full Sync will reindex all data each time, while New and Modified Content Sync will only update new or modified data.
   - **Sync Schedule**: Define the sync schedule. You can choose from options like on-demand, hourly, daily, weekly, monthly, or a custom cron expression. For most applications, a daily sync schedule is sufficient to keep data up-to-date unless you have a high amount of data changes.

Once you've configured your data source, click the '**Sync**' button and Amazon Q Business will start syncing your Neon database data. You can monitor the sync status and view the indexed data in the Amazon Q Business Console.

It can take from a few minutes to a few hours depending on the size of your database and the sync frequency you've set.

## Web Experience Setup

Once your data source is synced, you can access and share the Amazon Q Business web interface. The web experience provides a chat interface out of the box where users can query your Neon database using natural language.

### Finding Your Web Experience URL

The web experience URL is automatically generated by Amazon Q Business. To find it:

1. Navigate to your application in the Amazon Q Business Console
2. Look for the '**Web experience settings**' section
3. Find the '**Deployed URL**' - this is the URL you'll share with your authorized users

Once you have the URL, visit it in your browser to access the Amazon Q Business web interface. You will be prompted to log in using your IAM Identity Center credentials and then you can start querying your Neon database using natural language.

### Customizing the Experience

Before sharing with users, you can customize the web interface:

1. Click '**Customize web experience**' in your application settings
2. Configure the following:
   - Title: Your Database Assistant
   - Subtitle: Query your database using natural language
   - Welcome message: Ask questions about your data in plain English

### Example Queries

Once users access the web experience, they can select the '**Company Knowledge**' tab and ask questions like:

- "Show me all enterprise customers"
- "What's our total monthly spend by subscription tier?"
- "Who are our top customers by monthly spend?"

### Access Control

Only users who have been granted access through IAM Identity Center can access the web experience. Make sure to add users to the appropriate groups and configure permissions correctly based on your organization's policies.

After users click the web experience URL, they'll be prompted to log in using their IAM Identity Center credentials. Once authenticated, they can start querying your Neon database using natural language.

Keep in mind that Amazon Q Business is optimized for English language queries, so instruct your users to formulate their questions in English for best results.

## Conclusion

Amazon Q provides a natural language interface to query your Neon Postgres database, making it easier for team members who aren't SQL experts to access data.

Amazon Q Business also provides several methods to add data to your application beyond your primary Neon database connection, you can upload files like documentation, guides, or data dictionaries directly through the AWS Q console. Or use a Web Crawler to index web pages and documents directly from your website or the internet.

## Additional Resources

- [Neon Documentation](/docs)
- [Amazon Q Business Documentation](https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/what-is.html))
- [AWS IAM Identity Center Guide](https://docs.aws.amazon.com/singlesignon/latest/userguide/what-is.html)

<NeedHelp />


# Building a RESTful API with ASP.NET Core, Swagger, and Neon

---
title: Building a RESTful API with ASP.NET Core, Swagger, and Neon
subtitle: Learn how to connect your .NET applications to Neon's serverless Postgres database
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-11-03T00:00:00.000Z'
updatedOn: '2024-11-03T00:00:00.000Z'
---

In this guide, we'll walk through the process of developing a RESTful API using ASP.NET Core, connecting it to a Neon Postgres database. We will cover CRUD operations using Entity Framework Core (EF Core), generate interactive API documentation with Swagger, and explore best practices for testing your API endpoints. As a bonus, we'll also implement JWT authentication to secure your endpoints.

## Prerequisites

Before we start, make sure you have the following:

- [.NET SDK 8.0](https://dotnet.microsoft.com/download/dotnet/8.0)
- [Neon account](https://neon.tech) for setting up your Postgres database
- [Postman](https://www.postman.com/downloads/) for API testing
- Basic knowledge of C# and ASP.NET Core
- Familiarity with Entity Framework Core

## Setting Up Your ASP.NET Core Project

First, create a new ASP.NET Core Web API project:

```bash
dotnet new webapi -n NeonApi
cd NeonApi
```

Install the required NuGet packages using the `dotnet add package` command:

```bash
dotnet add package Microsoft.EntityFrameworkCore
dotnet add package Npgsql.EntityFrameworkCore.PostgreSQL
dotnet add package Swashbuckle.AspNetCore
dotnet add package Microsoft.AspNetCore.Authentication.JwtBearer
dotnet add package Microsoft.EntityFrameworkCore.Design
```

The above packages include:

- `Microsoft.EntityFrameworkCore` - Entity Framework Core for database operations
- `Npgsql.EntityFrameworkCore.PostgreSQL` - PostgreSQL provider for EF Core
- `Swashbuckle.AspNetCore` - Swagger for API documentation
- `Microsoft.AspNetCore.Authentication.JwtBearer` - JWT authentication for securing endpoints
- `Microsoft.EntityFrameworkCore.Design` - EF Core design tools for migrations

### Configuring the Neon Database

Head over to your [Neon Dashboard](https://neon.tech) and create a new project.

Once done, grab your database connection string and add it to your `appsettings.json`:

```json
"ConnectionStrings": {
  "NeonDb": "Host=<your-host>;Database=<your-database>;Username=<your-username>;Password=<your-password>;Port=5432"
}
```

While you're in `appsettings.json`, add a section for JWT authentication after the connection string:

```json
"Jwt": {
"SecretKey": "your-very-secure-secret-key"
}
```

We will cover JWT authentication in more detail later in this guide, but for now, let's focus on setting up the API.

Next, update your `Program.cs` file to include the database context, Swagger, and JWT authentication:

```csharp
using NeonApi.Data;
using Microsoft.AspNetCore.Authentication.JwtBearer;
using Microsoft.EntityFrameworkCore;
using Microsoft.IdentityModel.Tokens;
using System.Text;

var builder = WebApplication.CreateBuilder(args);
var configuration = builder.Configuration;

// Add the Neon database context using Npgsql provider
builder.Services.AddDbContext<NeonDbContext>(options =>
    options.UseNpgsql(configuration.GetConnectionString("NeonDb")));

// Register controllers for handling incoming HTTP requests
builder.Services.AddControllers();

// Enable API endpoint exploration and Swagger documentation
builder.Services.AddEndpointsApiExplorer();
builder.Services.AddSwaggerGen();

// Set up JWT authentication to secure API endpoints
builder.Services.AddAuthentication(JwtBearerDefaults.AuthenticationScheme)
    .AddJwtBearer(options =>
    {
        options.TokenValidationParameters = new TokenValidationParameters
        {
            // Validate that the token is signed with the specified key
            ValidateIssuerSigningKey = true,
            IssuerSigningKey = new SymmetricSecurityKey(Encoding.UTF8.GetBytes(configuration["Jwt:SecretKey"])),

            // Disable issuer and audience validation for testing purposes
            ValidateIssuer = false,
            ValidateAudience = false
        };
    });

var app = builder.Build();

// Configure the middleware pipeline
if (app.Environment.IsDevelopment())
{
    app.UseSwagger();
    app.UseSwaggerUI();
}

app.UseHttpsRedirection();
app.UseAuthentication();
app.UseAuthorization();
app.MapControllers();

app.Run();
```

In the above, we configure the necessary services directly within `Program.cs` to connect our ASP.NET Core API to Neon and secure it with JWT authentication:

1. We use `AddDbContext` to set up `NeonDbContext` with the Npgsql provider, connecting to the Neon database using the connection string defined in `appsettings.json`. Make sure to update `"NeonDb"` with your actual connection string key if it's named differently.

2. We register controllers with `AddControllers()`, which allows the application to handle incoming API requests and map them to their respective endpoints.

3. By adding `EndpointsApiExplorer` and `SwaggerGen`, we enable automatic generation of API documentation. This provides a user-friendly interface to interact with your API endpoints, accessible at `/swagger`.

4. For the JWT authentication setup, we are implementing the following:
   - Here, we set up JWT authentication to protect your API routes. We use `AddJwtBearer` to validate tokens sent by clients.
   - The `TokenValidationParameters` section allows us to make sure that the token is signed with the specified key. Replace `"your-secret-key"` in `appsettings.json` with a secure key unique to your application.
   - For simplicity, `ValidateIssuer` and `ValidateAudience` are set to `false`, which means the API won't check who issued the token or its intended audience. This is useful for local development but should be tightened for production environments.

To avoid hardcoding sensitive information like the secret key, consider using environment variables or a configuration management system to securely store secrets.

## Creating the Entity Framework Core Models

Data models define the structure of your database tables and the relationships between them. Here, we'll create a simple `Product` model to represent products in our Neon database.

In the `Models` folder, create a `Product.cs` file:

```csharp
namespace NeonApi.Models
{
    public class Product
    {
        public int Id { get; set; }
        public string Name { get; set; }
        public decimal Price { get; set; }
        public string Description { get; set; }
    }
}
```

Here, we define a simple `Product` model with four properties:

- `Id`: This is the primary key, which will auto-increment.
- `Name`: Stores the product's name.
- `Price`: Holds the product's price as a decimal value.
- `Description`: Provides additional details about the product.

Each property corresponds to a column in the database table that Entity Framework will generate for us.

### Creating the Database Context

Next, we need to create a database context class, which serves as a bridge between our C# code and the Neon database.

Create a new folder named `Data` and add a `NeonDbContext.cs` file:

```csharp
namespace NeonApi.Data
{
    public class NeonDbContext : DbContext
    {
        public NeonDbContext(DbContextOptions<NeonDbContext> options) : base(options) { }

        // This DbSet represents the Products table in the Neon database
        public DbSet<Product> Products { get; set; }
    }
}
```

The above code snippet does the following:

- The `NeonDbContext` class inherits from `DbContext`, which is part of Entity Framework Core.
- We pass `DbContextOptions` to the constructor to configure the connection to our Neon database.
- The `DbSet<Product>` property represents the `Products` table. This allows us to perform CRUD operations on the `Product` model directly through this context.

### Running Migrations to Create the Database Schema

Now that we have defined our model and context, let's generate the database schema using migrations. Open your terminal and run the following commands:

```bash
dotnet ef migrations add InitialCreate
```

The above command generates a migration file based on the changes made to the database schema. The migration file contains instructions to create the `Products` table.

Next, apply the migration to your Neon database:

```bash
dotnet ef database update
```

The `dotnet ef database update` command applies the migration to your Neon database, creating the `Products` table and any other necessary schema changes.

> **Note**: Make sure your database connection string in `appsettings.json` is correctly configured before running the migrations. That way the changes are applied to your Neon database instance.

At this point, your database is set up and ready to store product data!

## Building the API Endpoints

With the database schema in place, let's create the API endpoints to perform CRUD operations on the `Products` table.

In the `Controllers` folder, create a `ProductsController.cs` file with the following content:

```csharp
using System.Collections.Generic;
using System.Threading.Tasks;
using Microsoft.AspNetCore.Mvc;
using Microsoft.EntityFrameworkCore;
using NeonApi.Data;
using NeonApi.Models;

[Route("api/[controller]")]
[ApiController]
public class ProductsController : ControllerBase
{
    private readonly NeonDbContext _context;

    public ProductsController(NeonDbContext context)
    {
        _context = context;
    }

    [HttpGet]
    public async Task<ActionResult<IEnumerable<Product>>> GetProducts()
    {
        // Retrieve all products from the database
        return await _context.Products.ToListAsync();
    }

    [HttpGet("{id}")]
    public async Task<ActionResult<Product>> GetProduct(int id)
    {
        // Retrieve a single product by ID
        var product = await _context.Products.FindAsync(id);
        if (product == null) return NotFound(); // Return 404 if not found
        return product;
    }

    [HttpPost]
    public async Task<ActionResult<Product>> CreateProduct(Product product)
    {
        // Add a new product to the database
        _context.Products.Add(product);
        await _context.SaveChangesAsync();

        // Return 201 Created status with the newly created product
        return CreatedAtAction(nameof(GetProduct), new { id = product.Id }, product);
    }

    [HttpPut("{id}")]
    public async Task<IActionResult> UpdateProduct(int id, Product product)
    {
        // Ensure the ID in the URL matches the ID of the provided product
        if (id != product.Id) return BadRequest();

        // Mark the product as modified
        _context.Entry(product).State = EntityState.Modified;
        await _context.SaveChangesAsync();

        // Return 204 No Content status after a successful update
        return NoContent();
    }

    [HttpDelete("{id}")]
    public async Task<IActionResult> DeleteProduct(int id)
    {
        // Find the product by ID
        var product = await _context.Products.FindAsync(id);
        if (product == null) return NotFound(); // Return 404 if not found

        // Remove the product from the database
        _context.Products.Remove(product);
        await _context.SaveChangesAsync();

        // Return 204 No Content status after successful deletion
        return NoContent();
    }
}
```

In the code above, we define a `ProductsController` to handle all CRUD operations for our `Product` model. Here's a breakdown of how each endpoint works:

1. The `GetProducts` method handles `GET /api/products` requests, fetching all products stored in the Neon database.

2. The `GetProduct` method handles `GET /api/products/{id}` requests to retrieve a single product by its unique ID. If no product with the given ID is found, it responds with a `404 Not Found`. This ensures the client is notified when attempting to access a non-existent product.

3. The `CreateProduct` method handles `POST /api/products` requests to add a new product. The response uses `CreatedAtAction` to include a link to the newly created resource, following REST best practices.

4. The `UpdateProduct` method handles `PUT /api/products/{id}` requests to modify an existing product. Once the update is successful, it responds with a `204 No Content`, indicating the operation was successful without returning any additional data.

5. The `DeleteProduct` method handles `DELETE /api/products/{id}` requests to remove a product by its ID. If the product doesn't exist, it returns a `404 Not Found` response.

Each endpoint is fully asynchronous and interacts with the Neon database through the `NeonDbContext` context.

## Setting Up Swagger for API Documentation

To document our API and provide an interactive interface for testing, we'll integrate Swagger into our ASP.NET Core project.

Swagger automatically generates OpenAPI documentation, making it easy to explore your API and test its endpoints directly from your browser.

### Enabling Swagger in `Program.cs`

To set up Swagger, add the following code in the `Configure` method of your `Program.cs` file:

```csharp
app.UseSwagger();
app.UseSwaggerUI(c =>
{
    // Configure Swagger UI at the root URL
    c.SwaggerEndpoint("/swagger/v1/swagger.json", "Neon API V1");
    c.RoutePrefix = string.Empty;
});
```

The `UseSwagger` middleware generates the OpenAPI documentation for your API, while `UseSwaggerUI` sets up the Swagger interface for interacting with your endpoints.

### Running Your Application

Now that Swagger is set up, start your application using:

```bash
dotnet run
```

Once the application is running, you will see the port where your API is hosted (usually `https://localhost:5229`), eg.:

```
Building...
info: Microsoft.Hosting.Lifetime[14]
      Now listening on: http://localhost:5229
```

Visit `https://localhost:5229/swagger` in your browser to access the Swagger UI.

The Swagger UI will appear, displaying all your API endpoints with detailed documentation. You can test the endpoints by sending requests directly from the UI and viewing the responses, making it easy to verify that everything works as expected.

## Testing Your API with Postman

Now that your API is running, you can use Postman or any other API testing tool to interact with your endpoints. You can even use the Swagger UI to test the endpoints, but Postman provides a more robust environment for testing complex scenarios.

In this section, we'll walk through testing the CRUD operations using Postman but feel free to use any tool you're comfortable with like Insomnia or cURL.

Download and launch [Postman](https://www.postman.com/downloads/). Create a new request to interact with your API.

Open Postman and create the following requests:

1. `GET`: `/api/products`
   - **Description**: Fetches all products.
   - Set to `GET`, enter `https://localhost:5001/api/products`, and click **Send**.
   - You should receive a `200 OK` response with a list of products.

2. `POST`: `/api/products`
   - **Description**: Creates a new product.
   - Set to `POST`, enter `https://localhost:5001/api/products`, and go to **Body** → **raw** → **JSON**.
   - Add:
     ```json
     {
       "name": "New Product",
       "price": 19.99,
       "description": "Sample product"
     }
     ```
   - Click **Send**. Expect a `201 Created` response.

3. `PUT`: `/api/products/{id}`
   - **Description**: Updates a product.
   - Set to `PUT`, enter `https://localhost:5001/api/products/1`.
   - Add:
     ```json
     {
       "id": 1,
       "name": "Updated Product",
       "price": 29.99,
       "description": "Updated description"
     }
     ```
   - Click **Send**. You should receive a `204 No Content`.

4. `DELETE`: `/api/products/{id}`
   - **Description**: Deletes a product.
   - Set to `DELETE`, enter `https://localhost:5001/api/products/1`, and click **Send**.
   - Expect a `204 No Content`.

After testing, check that all changes are reflected in your Neon database. Use both Postman and Swagger UI to confirm the endpoints are functioning correctly.

## Securing Your API with JWT Authentication (Bonus)

To protect your API endpoints, we’ll use [JWT](https://jwt.io/) (JSON Web Token) authentication. By adding the `[Authorize]` attribute to specific controller actions, you can ensure that only authenticated users have access. Here’s how to secure the `GetProducts` endpoint:

```csharp
[Authorize]
[HttpGet]
public async Task<ActionResult<IEnumerable<Product>>> GetProducts()
{
    return await _context.Products.ToListAsync();
}
```

Now, any requests to `GetProducts` will require a valid JWT token, if you try to access the endpoint without a token, you will receive a `401 Unauthorized` response.

### Generating and Using JWT Tokens

When a user successfully logs in, the server generates a JWT token containing the user's authentication details. This token typically includes claims such as the user's ID, email, and a unique identifier. The token is then signed with a secret key to ensure its integrity and prevent tampering. For example:

```csharp
public string GenerateJwtToken(User user)
{
    var claims = new[]
    {
        new Claim(JwtRegisteredClaimNames.Sub, user.Id.ToString()),
        new Claim(JwtRegisteredClaimNames.Email, user.Email),
        new Claim(JwtRegisteredClaimNames.Jti, Guid.NewGuid().ToString())
    };

    var key = new SymmetricSecurityKey(Encoding.UTF8.GetBytes(configuration["Jwt:SecretKey"]));
    var creds = new SigningCredentials(key, SecurityAlgorithms.HmacSha256);

    var token = new JwtSecurityToken(
        issuer: null,
        audience: null,
        claims: claims,
        expires: DateTime.UtcNow.AddHours(1),
        signingCredentials: creds);

    return new JwtSecurityTokenHandler().WriteToken(token);
}
```

The token looks like this:

```
eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwiZW1haWwiOiJhZG1pbkBleGFtcGxlLmNvbSIsImp0aSI6IjMzYjQwMzIwLWUwZjItNDIwZi1iZjIwLWUwZjIwZmJmMjAwMCIsImV4cCI6MTY0MzUwNzQwMH0.7
```

If you were to go to [jwt.io](https://jwt.io/), you could paste the token and see its decoded contents.

Once the token is generated, the client stores it in local storage or session storage and includes it in the `Authorization` header for all subsequent requests to secured endpoints. For instance:

```
Authorization: Bearer <your-jwt-token>
```

With this header in place, the server can authenticate the user without requiring them to log in again for each request.

## Conclusion

In this guide, we covered the process of building a RESTful API with ASP.NET Core, connecting it to a Neon Postgres database, and securing it with JWT authentication. We explored CRUD operations using Entity Framework Core, generated interactive API documentation with Swagger, and tested our endpoints using Postman.

As a next step, consider expanding your API with additional features, such as pagination, filtering, or sorting. You can also explore adding testing frameworks like xUnit or NUnit to write unit tests for your API endpoints.

For more information, check out:

- [Neon Documentation](/docs)
- [Npgsql Documentation](https://www.npgsql.org/doc/index.html)
- [Entity Framework Core Documentation](https://docs.microsoft.com/en-us/ef/core/)

<NeedHelp />


# Authentication and Authorization in ASP.NET Core with ASP.NET Identity and Neon

---
title: Authentication and Authorization in ASP.NET Core with ASP.NET Identity and Neon
subtitle: Learn how to implement secure user authentication and authorization in ASP.NET Core applications using ASP.NET Identity with Neon Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-11-03T00:00:00.000Z'
updatedOn: '2024-11-03T00:00:00.000Z'
---

In this guide, we'll explore how to implement secure authentication and authorization in an ASP.NET Core application using ASP.NET Core Identity with Neon Postgres as the database backend. We'll cover user management, role-based authorization, and JWT token generation for secure API access.

## Prerequisites

Before we begin, ensure you have:

- .NET 8.0 or later installed
- A [Neon account](https://console.neon.tech/signup)
- Basic familiarity with ASP.NET Core and Entity Framework Core

## Project Setup

First, create a new ASP.NET Core Web API project with authentication:

```bash
dotnet new webapi -n NeonApi
cd NeonApi
```

With the project created, install the necessary packages:

```bash
dotnet add package Microsoft.AspNetCore.Identity.EntityFrameworkCore
dotnet add package Microsoft.AspNetCore.Authentication.JwtBearer
dotnet add package Npgsql.EntityFrameworkCore.PostgreSQL
dotnet add package Microsoft.EntityFrameworkCore.Design
dotnet add package Microsoft.EntityFrameworkCore
dotnet add package Microsoft.IdentityModel.Tokens
dotnet add package System.IdentityModel.Tokens.Jwt
```

The above packages provide support for ASP.NET Identity, JWT authentication, and PostgreSQL database integration.

### Configuring the Neon Database

Head over to your [Neon Dashboard](https://neon.tech) and create a new project.

Once done, grab your database connection string and add it to your `appsettings.json`:

```json
"ConnectionStrings": {
  "NeonConnection": "Host=<your-host>;Database=<your-database>;Username=<your-username>;Password=<your-password>;Port=5432"
}
```

The `appsettings.json` file is great for local development, but you should use environment variables or a secure vault for production.

While editing `appsettings.json`, add JWT configuration as well right below the connection string:

```json
"Jwt": {
  "Key": "your-very-secure-and-long-secret-key",
  "Issuer": "https://your-app.com",
  "Audience": "https://your-app.com"
}
```

The `Key` is a secret key used to sign and verify JWT tokens, while `Issuer` and `Audience` are used to validate the token's origin and intended recipient.

## Configuring ASP.NET Identity with Neon

In order to store additional information about users, we need to create a custom user class that extends the default Identity user provided by ASP.NET Core Identity.

This will allow us to add new properties, like `FirstName` and `LastName`, that are not included in the default `IdentityUser` class.

### Custom User Model

Let's create a new file named `ApplicationUser.cs` inside the `Models` folder with the following content:

```csharp
using Microsoft.AspNetCore.Identity;

public class ApplicationUser : IdentityUser
{
    public string FirstName { get; set; }
    public string LastName { get; set; }
    public DateTime CreatedAt { get; set; } = DateTime.UtcNow;
}
```

In the code above:

1. By inheriting from the `IdentityUser` class, `ApplicationUser` gets all the built-in properties like `UserName`, `Email`, `PasswordHash`, and more. This means we don't have to rewrite any of the existing authentication logic.

2. We added three new fields: `FirstName` and `LastName` allow us to store the user's personal details. And `CreatedAt` captures the date and time when the user was created, which can be helpful for tracking new sign-ups.

Why extend the `IdentityUser`? Well, the default user model is quite limited, and many real-world applications need to store more information than just usernames and emails. By creating a custom `ApplicationUser`, you can add more fields as you need to fit your application's requirements.

### Database Context Configuration

A database context is a class that represents a session with the database, allowing us to query and save data. In our case, we're setting up a context specifically for handling ASP.NET Identity and our custom `ApplicationUser` model.

Create the database context in `Data/ApplicationDbContext.cs`, inheriting from `IdentityDbContext<ApplicationUser>`:

```csharp
using Microsoft.AspNetCore.Identity;
using Microsoft.AspNetCore.Identity.EntityFrameworkCore;
using Microsoft.EntityFrameworkCore;

public class ApplicationDbContext : IdentityDbContext<ApplicationUser>
{
    public ApplicationDbContext(DbContextOptions<ApplicationDbContext> options) : base(options) { }

    protected override void OnModelCreating(ModelBuilder builder)
    {
        base.OnModelCreating(builder);

        // Custom configurations for Identity tables
        builder.Entity<ApplicationUser>(entity =>
        {
            entity.ToTable(name: "Users");
        });

        builder.Entity<IdentityRole>(entity =>
        {
            entity.ToTable(name: "Roles");
        });
    }
}
```

In this example, we create `ApplicationDbContext` by inheriting from `IdentityDbContext<ApplicationUser>`. This base class, `IdentityDbContext`, already includes all the necessary tables for ASP.NET Identity, such as tables for users, roles, and user claims. By specifying `ApplicationUser` as the type, we're telling ASP.NET Identity to use our custom user model.

The `OnModelCreating` method provides us with a chance to further configure the database schema. Here, we customize the names of the tables used by Identity to be more straightforward:

- **Users Table**: We rename the default user table to simply `Users` for clarity.
- **Roles Table**: Similarly, we rename the default roles table to `Roles`.

These configurations allow us to have simpler, more intuitive table names in the database, while still retaining all the built-in functionality of ASP.NET Identity.

This is not a requirement, but it can be helpful for keeping your database schema organized and easy to understand.

### Registering Services

Now that we have our `ApplicationDbContext` and user model set up, it's time to configure our application's services in `Program.cs` to enable Identity and authentication.

Open `Program.cs` and update it as follows:

```csharp
var builder = WebApplication.CreateBuilder(args);

// Add Neon database context
builder.Services.AddDbContext<ApplicationDbContext>(options =>
    options.UseNpgsql(builder.Configuration.GetConnectionString("NeonConnection")));

builder.Services.AddControllers();

// Configure Identity
builder.Services.AddIdentity<ApplicationUser, IdentityRole>(options =>
{
    // Password settings
    options.Password.RequireDigit = true;
    options.Password.RequireLowercase = true;
    options.Password.RequireUppercase = true;
    options.Password.RequireNonAlphanumeric = true;
    options.Password.RequiredLength = 8;

    // Lockout settings
    options.Lockout.DefaultLockoutTimeSpan = TimeSpan.FromMinutes(5);
    options.Lockout.MaxFailedAccessAttempts = 5;

    // User settings
    options.User.RequireUniqueEmail = true;
})
.AddEntityFrameworkStores<ApplicationDbContext>()
.AddDefaultTokenProviders();

// Configure JWT authentication
builder.Services.AddAuthentication(options =>
{
    options.DefaultAuthenticateScheme = JwtBearerDefaults.AuthenticationScheme;
    options.DefaultChallengeScheme = JwtBearerDefaults.AuthenticationScheme;
})
.AddJwtBearer(options =>
{
    options.TokenValidationParameters = new TokenValidationParameters
    {
        ValidateIssuer = true,
        ValidateAudience = true,
        ValidateLifetime = true,
        ValidateIssuerSigningKey = true,
        ValidIssuer = builder.Configuration["Jwt:Issuer"],
        ValidAudience = builder.Configuration["Jwt:Audience"],
        IssuerSigningKey = new SymmetricSecurityKey(
            Encoding.UTF8.GetBytes(builder.Configuration["Jwt:Key"]))
    };
});

// Add authorization
builder.Services.AddAuthorization(options =>
{
    options.AddPolicy("RequireAdminRole", policy =>
        policy.RequireRole("Admin"));
});

var app = builder.Build();

// Add roles on startup
using (var scope = app.Services.CreateScope())
{
    var roleManager = scope.ServiceProvider.GetRequiredService<RoleManager<IdentityRole>>();
    await RoleHelper.EnsureRolesCreated(roleManager);
}

// Map controllers
app.MapControllers();

// Configure middleware
app.UseAuthentication();
app.UseAuthorization();
```

We are doing quite a few things here:

1. First, we configure our database context using the connection string we defined earlier in `appsettings.json`. This connects our application to the Neon Postgres database, allowing it to store and retrieve user data.
1. Next, we set up ASP.NET Identity to manage user accounts which includes:
   - We enforce strong passwords.
   - We configure lockout settings to protect against brute force attacks.
   - We require users to have unique emails.
1. After setting up Identity, we configure the JWT authentication. This is where the `JWT` token configuration from the `appsettings.json` file comes into play as well. This allows our API to issue tokens to authenticated users, which can then be used to access secured endpoints.
1. Additionally, we define an authorization policy called `RequireAdminRole`, which restricts certain actions to users with the "Admin" role.
1. To make sure our application has the necessary roles, we include a piece of code that runs on startup to create roles like "Admin" and "User" if they don't already exist. This is done using a scoped service to access the `RoleManager`.
1. Finally, we map our controllers to handle HTTP requests and add the necessary middleware for authentication and authorization.

## Implementing Authentication Controllers

Now that we have configured our database and Identity services, let's create a controller to manage user registration and login. This controller will handle the core authentication flow for our application.

You can create this in a new file, `Controllers/AuthController.cs` with the following content:

```csharp
using System;
using System.IdentityModel.Tokens.Jwt;
using System.Security.Claims;
using System.Text;
using System.Threading.Tasks;
using Microsoft.AspNetCore.Authorization;
using Microsoft.AspNetCore.Identity;
using Microsoft.AspNetCore.Mvc;
using Microsoft.Extensions.Configuration;
using Microsoft.IdentityModel.Tokens;

[ApiController]
[Route("api/[controller]")]
public class AuthController : ControllerBase
{
    private readonly UserManager<ApplicationUser> _userManager;
    private readonly SignInManager<ApplicationUser> _signInManager;
    private readonly IConfiguration _configuration;

    public AuthController(
        UserManager<ApplicationUser> userManager,
        SignInManager<ApplicationUser> signInManager,
        IConfiguration configuration)
    {
        _userManager = userManager;
        _signInManager = signInManager;
        _configuration = configuration;
    }

    [HttpPost("register")]
    public async Task<IActionResult> Register(RegisterDto model)
    {
        var user = new ApplicationUser
        {
            UserName = model.Email,
            Email = model.Email,
            FirstName = model.FirstName,
            LastName = model.LastName
        };

        var result = await _userManager.CreateAsync(user, model.Password);
        if (result.Succeeded)
        {
            // Assign default role
            await _userManager.AddToRoleAsync(user, "User");
            return Ok(new { message = "Registration successful" });
        }

        return BadRequest(result.Errors);
    }

    [HttpPost("login")]
    public async Task<IActionResult> Login(LoginDto model)
    {
        var user = await _userManager.FindByEmailAsync(model.Email);
        if (user == null)
        {
            return BadRequest("Invalid credentials");
        }

        var result = await _signInManager.CheckPasswordSignInAsync(
            user, model.Password, lockoutOnFailure: false);

        if (result.Succeeded)
        {
            var token = GenerateJwtToken(user);
            return Ok(new { token });
        }

        return BadRequest("Invalid credentials");
    }

    private string GenerateJwtToken(ApplicationUser user)
    {
        var claims = new[]
        {
            new Claim(JwtRegisteredClaimNames.Sub, user.Email),
            new Claim(JwtRegisteredClaimNames.Jti, Guid.NewGuid().ToString()),
            new Claim(ClaimTypes.NameIdentifier, user.Id)
        };

        var key = new SymmetricSecurityKey(Encoding.UTF8.GetBytes(_configuration["Jwt:Key"]));
        var creds = new SigningCredentials(key, SecurityAlgorithms.HmacSha256);

        var token = new JwtSecurityToken(
            issuer: _configuration["Jwt:Issuer"],
            audience: _configuration["Jwt:Audience"],
            claims: claims,
            expires: DateTime.Now.AddHours(3),
            signingCredentials: creds);

        return new JwtSecurityTokenHandler().WriteToken(token);
    }
}
```

To begin, we create a new `AuthController` class that will handle user authentication. This includes two primary actions: user registration and user login. By using this controller, users will be able to create accounts and log in to receive a JWT, which they can use to access protected endpoints.

Here's how it works:

- The `AuthController` uses `UserManager` for user operations, `SignInManager` for handling sign-ins, and `IConfiguration` for accessing JWT settings.
- The `Register` method creates a new user with the provided details. Once created, the user is assigned the "User" role. If registration succeeds, an `Ok` response is returned; otherwise, a `BadRequest` with errors is sent.
- The `Login` method verifies if the email exists and checks the password. On success, a JWT token is generated and returned for authenticated access.
- The `GenerateJwtToken` method creates a token with the user's ID and email as claims. It signs the token using the secret key from `appsettings.json` and sets it to expire in 3 hours.

## Implementing Role-Based Authorization

To manage user roles effectively, we'll create a helper class that checks if the necessary roles are set up in your system. This is useful when you want to predefine certain roles like "Admin" or "User" and make them available as soon as the application starts instead of manually creating them.

You can create this in a new file, `Helpers/RoleHelper.cs` as follows:

```csharp
using Microsoft.AspNetCore.Identity;
using System.Threading.Tasks;

public static class RoleHelper
{
    public static async Task EnsureRolesCreated(RoleManager<IdentityRole> roleManager)
    {
        string[] roles = { "Admin", "User", "Manager" };

        foreach (var role in roles)
        {
            if (!await roleManager.RoleExistsAsync(role))
            {
                await roleManager.CreateAsync(new IdentityRole(role));
            }
        }
    }
}
```

In the `RoleHelper` class, we define a method called `EnsureRolesCreated` which:

- Accepts a `RoleManager` instance to interact with the roles in the database.
- Defines an array of roles we want to set up ("Admin", "User", and "Manager").
- For each role, it checks if the role already exists using `RoleExistsAsync()`. If the role doesn't exist, it creates the role with `CreateAsync()`.

This way, you only need to call this method once during application startup to ensure all required roles are available for assignment.

## Authorization Policies

In this section, we're adding authorization to protect certain API endpoints, so that only authenticated users or users with specific roles can access them.

Create a protected endpoint that requires authentication, and an admin-only endpoint. You can create this in a new file, `Controllers/SecureController.cs`:

```csharp
using Microsoft.AspNetCore.Authorization;
using Microsoft.AspNetCore.Mvc;

[Authorize]
[ApiController]
[Route("api/[controller]")]
public class SecureController : ControllerBase
{
    [HttpGet]
    public IActionResult Get()
    {
        return Ok(new { message = "This is a secure endpoint" });
    }

    [Authorize(Policy = "RequireAdminRole")]
    [HttpGet("admin")]
    public IActionResult AdminOnly()
    {
        return Ok(new { message = "This is an admin-only endpoint" });
    }
}
```

With the above, we created a new controller called `SecureController` with two endpoints:

1. General Protected Endpoint:
   - The `/api/secure` route is protected with `[Authorize]`, allowing access only to authenticated users with a valid JWT token.
   - If access is granted, it returns a confirmation message.

2. Admin-Only Endpoint:
   - The `/api/secure/admin` route is restricted to users with the "Admin" role using `[Authorize(Policy = "RequireAdminRole")]`.
   - Only "Admin" users can access this. Others will receive a `403 Forbidden` response.

Using the same approach, you can create additional policies for different roles or permissions. This allows you to control access to your API endpoints based on user roles.

## Database Migrations

To set up your database schema, we need to run migrations. Migrations help keep your database in sync with your data models, allowing you to make changes to your schema without losing data.

Run the following commands to create the database and apply migrations:

- Create the initial migration:

```bash
dotnet ef migrations add InitialCreate
```

- Apply the migration to the database:

```bash
dotnet ef database update
```

If you were to make changes to your data models in the future, you would create a new migration and apply it using the same commands. Via the Neon console, you will now see the tables created by ASP.NET Identity.

## Testing Authentication

You can test your authentication endpoints using Postman or `curl` to actually verify that everything is working correctly. Let's quickly do that using `curl`.

### 1. Register a New User

To create a new account, send a `POST` request to the `/api/auth/register` endpoint with the user details:

```json
curl -X POST http://localhost:5241/api/auth/register \
     -H "Content-Type: application/json" \
     -d '{
           "email": "test@example.com",
           "password": "SecurePass123!",
           "firstName": "John",
           "lastName": "Doe"
         }'
```

This should return a response confirming that the user was successfully registered. Make sure to use a strong password and valid email format as our password policy requires it.

### 2. Log In and Get a JWT Token

Once registered, log in using the credentials you just created. Send a `POST` request to the `/api/auth/login` endpoint:

```json
curl -X POST http://localhost:5241/api/auth/login \
     -H "Content-Type: application/json" \
     -d '{
           "email": "test@example.com",
           "password": "SecurePass123!"
         }'
```

If the login is successful, you'll receive a JSON response containing a JWT access token. Save this token securely, as it will be used to access protected routes.

### 3. Access a Protected Endpoint

With the JWT token from the previous step, you can now access secured endpoints. Send a `GET` request to `/api/secure` and include the token in the `Authorization` header:

```bash
curl -X GET http://localhost:5241/api/secure \
     -H "Authorization: Bearer <your-jwt-token>"
```

If the token is valid, you'll receive a response from the protected resource. If not, you'll get an authentication error, indicating the token has expired or is invalid.

## User Session Management with Refresh Tokens

As an optional step, to improve user session management, we'll implement a two-token authentication system using both access tokens and refresh tokens:

- An access token which is a short-lived JWT used to authenticate API requests
- A longer-lived tokens used to obtain new access tokens without requiring re-login

### Setting Up the Token System

First, create a model for refresh tokens in `Models/RefreshToken.cs` with an ID, token, and expiry date:

```csharp
public class RefreshToken
{
    public int Id { get; set; }
    public string Token { get; set; }
    public DateTime ExpiryDate { get; set; } = DateTime.UtcNow.AddDays(7);
}
```

Next, update the `ApplicationUser` model to store refresh tokens:

```csharp
public class ApplicationUser : IdentityUser
{
    public string FirstName { get; set; }
    public string LastName { get; set; }
    public List<RefreshToken> RefreshTokens { get; set; } = new List<RefreshToken>();
}
```

### Implementing Token Management

Modify the login endpoint to return both the access and the refresh tokens:

```csharp
[HttpPost("login")]
public async Task<IActionResult> Login(LoginDto model)
{
    var user = await _userManager.FindByEmailAsync(model.Email);
    if (user == null)
    {
        return BadRequest("Invalid credentials");
    }

    var result = await _signInManager.CheckPasswordSignInAsync(
        user, model.Password, lockoutOnFailure: false);

    if (result.Succeeded)
    {
        // Generate both tokens
        var accessToken = GenerateJwtToken(user);
        var refreshToken = GenerateRefreshToken();

        // Save refresh token to user
        user.RefreshTokens.Add(refreshToken);
        await _userManager.UpdateAsync(user);

        return Ok(new
        {
            accessToken = accessToken,
            refreshToken = refreshToken.Token
        });
    }

    return BadRequest("Invalid credentials");
}
```

Add the refresh token endpoint to obtain new tokens:

```csharp
[HttpPost("refresh-token")]
public async Task<IActionResult> RefreshToken([FromBody] RefreshTokenDto refreshTokenDto)
{
    var user = await _userManager.Users
        .SingleOrDefaultAsync(u => u.RefreshTokens
            .Any(t => t.Token == refreshTokenDto.Token &&
                     t.ExpiryDate > DateTime.UtcNow));

    if (user == null)
        return BadRequest("Invalid token");

    var newAccessToken = GenerateJwtToken(user);
    var newRefreshToken = GenerateRefreshToken();

    // Remove old refresh token
    user.RefreshTokens.RemoveAll(t => t.Token == refreshTokenDto.Token);
    user.RefreshTokens.Add(newRefreshToken);
    await _userManager.UpdateAsync(user);

    return Ok(new
    {
        accessToken = newAccessToken,
        refreshToken = newRefreshToken.Token
    });
}

private RefreshToken GenerateRefreshToken()
{
    return new RefreshToken
    {
        Token = Convert.ToBase64String(RandomNumberGenerator.GetBytes(64)),
        ExpiryDate = DateTime.UtcNow.AddDays(7)
    };
}

public class RefreshTokenDto
{
    [Required]
    public string Token { get; set; }
}
```

Here we've added a new endpoint `/api/auth/refresh-token` that accepts a refresh token and returns new access and refresh tokens. The refresh token is stored in the user's `RefreshTokens` list and used to generate new tokens when needed. The refresh token is valid for 7 days, after which the user will need to log in again.

### Using the Token System

Unlike the standard login flow, the new flow involves three steps:

1. User logs in with credentials:

   ```json
   curl -X POST http://localhost:5241/api/auth/login \
       -H "Content-Type: application/json" \
       -d '{
           "email": "user@example.com",
           "password": "password123"
           }'
   ```

   Response includes both tokens:

   ```json
   {
     "accessToken": "eyJhbG...",
     "refreshToken": "long-base64-string..."
   }
   ```

2. Use the access token for API requests:

   ```bash
   curl -X GET http://localhost:5241/api/protected-endpoint \
       -H "Authorization: Bearer eyJhbG..."
   ```

3. When the access token expires, use the refresh token to get new tokens:

   ```json
   curl -X POST http://localhost:5241/api/auth/refresh-token \
       -H "Content-Type: application/json" \
       -d '{
           "token": "long-base64-string..."
           }'
   ```

   This returns new access and refresh tokens:

   ```json
   {
     "accessToken": "new-eyJhbG...",
     "refreshToken": "new-long-base64-string..."
   }
   ```

As a security measure, store refresh tokens securely in your Neon database while also making sure that clients use secure methods like HTTP-only cookies. Also, keep access tokens short-lived, rotate refresh tokens on refresh, and implement token expiration and revocation to enhance security.

## Integrating Auth0 for Authentication and Authorization (Optional)

If you're looking to add an extra layer of security and use external identity providers, integrating your ASP.NET Core application with Auth0 is a good option. This allows your users to authenticate using social accounts (like Google, GitHub, etc.) or enterprise identity providers.

Auth0 offers a flexible platform for managing user authentication, with built-in JWT token support that integrates seamlessly with your existing ASP.NET Core application.

Let's quickly walk through setting up Auth0 with ASP.NET Core for secure authentication and authorization.

### Setting Up Auth0 with ASP.NET Core

To get started, follow these high-level steps:

1. Start by creating an Auth0 API:
   - Log in to your [Auth0 Dashboard](https://manage.auth0.com/).
   - Navigate to the "APIs" section and click **Create API**.
   - Provide a name and a unique identifier for your API (e.g., `https://your-app.com/api`). Keep the default signing algorithm as `RS256`.

2. In the API settings, you can define permissions (scopes) to control access to your API endpoints. For example, you can create a `read:messages` permission to restrict access to certain routes.

3. Open your `appsettings.json` and add the following configuration:

   ```json
   "Auth0": {
     "Domain": "your-auth0-domain",
     "Audience": "https://your-app.com/api"
   }
   ```

4. Make sure you have the required packages installed if you haven't already as in the previous steps:

   ```bash
   dotnet add package Microsoft.AspNetCore.Authentication.JwtBearer
   ```

5. Update the `Program.cs` file to add the authentication middleware:

   ```csharp
   var builder = WebApplication.CreateBuilder(args);
   var domain = $"https://{builder.Configuration["Auth0:Domain"]}/";

   builder.Services.AddAuthentication(JwtBearerDefaults.AuthenticationScheme)
       .AddJwtBearer(options =>
       {
           options.Authority = domain;
           options.Audience = builder.Configuration["Auth0:Audience"];
           options.TokenValidationParameters = new TokenValidationParameters
           {
               NameClaimType = ClaimTypes.NameIdentifier
           };
       });

   builder.Services.AddAuthorization(options =>
   {
       options.AddPolicy("read:messages", policy =>
           policy.Requirements.Add(new HasScopeRequirement("read:messages", domain)));
   });

   builder.Services.AddSingleton<IAuthorizationHandler, HasScopeHandler>();

   var app = builder.Build();
   app.UseAuthentication();
   app.UseAuthorization();
   app.MapControllers();
   ```

With all that in place, you can secure your API endpoints using Auth0, use the `[Authorize]` attribute:

```csharp
[ApiController]
[Route("api")]
public class ApiController : ControllerBase
{
    // Requires authentication
    [HttpGet("private")]
    [Authorize]
    public IActionResult PrivateEndpoint()
    {
        return Ok(new { Message = "Hello from a private endpoint!" });
    }

    // Requires specific scope
    [HttpGet("private-scoped")]
    [Authorize("read:messages")]
    public IActionResult ScopedEndpoint()
    {
        return Ok(new { Message = "Hello from a scoped endpoint!" });
    }
}
```

For a more information on integrating Auth0 with ASP.NET Core, refer to the [Auth0 Documentation](https://auth0.com/docs/quickstart/backend/aspnet-core-webapi/01-authorization). The documentation covers everything from setting up your Auth0 tenant to configuring scopes and securing your APIs.

## Conclusion

In this guide, we implemented a secure authentication and authorization system in an ASP.NET Core application using ASP.NET Identity with Neon Postgres as the backend. We walked through setting up user registration and login endpoints, securing API routes with JWT tokens, and implementing role-based authorization.

For more information, check out:

- [ASP.NET Core Identity Documentation](https://docs.microsoft.com/en-us/aspnet/core/security/authentication/identity)
- [JWT Authentication in ASP.NET Core](https://docs.microsoft.com/en-us/aspnet/core/security/authentication/jwt-bearer)
- [Neon Documentation](/docs)

<NeedHelp />


# Getting started with AutoGen + Neon

---
title: Getting started with AutoGen + Neon
subtitle: A step-by-step guide to building AI agents using AutoGen and Neon
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-02-12T00:00:00.000Z'
updatedOn: '2025-02-12T00:00:00.000Z'
---

This guide demonstrates how to integrate AutoGen with Neon. [AutoGen](https://microsoft.github.io/autogen/stable) is an open-source framework developed by Microsoft for building AI agents that can converse, plan, and interact with tools (APIs). Combining AutoGen with Neon allows AI agents to manage your database, execute SQL queries, and automate data-related tasks.

In this guide, we'll walk through building an AI agent with a practical example: creating a system that retrieves recent machine learning papers from arXiv and stores them in a Neon database. Following this example, you will learn how to:

- Create an AutoGen agent with Neon API integration.
- Implement database operations (like project creation and SQL queries) as agent tools.
- Set up a workflow where multiple agents work together to accomplish research tasks.

## Prerequisites

Before you begin, make sure you have the following prerequisites:

- **Python 3.10 or higher:** This guide requires Python 3.10 or a later version. If you don't have it installed, download it from [python.org](https://www.python.org/downloads/).

- **Neon account and API key:**
  - Sign up for a free Neon account at [neon.tech](https://console.neon.tech/signup).
  - After signing up, get your Neon API Key from the [Neon console](https://console.neon.tech/app/settings/profile). This API key is needed to authenticate your application with Neon.

- **OpenAI account and API key:**
  - This guide uses the `gpt-4o` model from OpenAI to power the AI agent. If you don't have an OpenAI account, sign up at [platform.openai.com](https://platform.openai.com/).
  - Generate a new API key from the [OpenAI Platform API keys section](https://platform.openai.com/api-keys). This key allows AutoGen to interact with OpenAI's models.

With these prerequisites in place, you are ready to build your AI agent.

## AutoGen basics

Before we start building your AI agent, let's understand some fundamental concepts of AutoGen.

### What is AutoGen?

AutoGen is a framework designed to simplify the development of applications using LLMs. It allows you to construct AI workflows by creating **conversational agents** that are capable of:

- **Conversation:** Engaging in multi-agent dialogues to solve tasks collaboratively.
- **Planning:** Developing and executing strategic plans to achieve goals.
- **Tool utilization:** Integrating with external tools and APIs to extend their capabilities beyond simple text generation, enabling real-world interactions.

### Key components of AutoGen

- **Agents:** The foundational building blocks in AutoGen. Agents are autonomous entities that can:
  - **Receive and process messages:** Accept and understand messages from users or other agents.
  - **Act autonomously:** Perform tasks, utilize tools, or generate responses based on their programmed logic and received messages.
  - **Agent types:** AutoGen offers various agent types, including:
    - **`AssistantAgent`:** A versatile agent powered by an LLM, capable of using tools and designed to be helpful and able to follow instructions. Ideal for general tasks and complex reasoning.
    - **`CodeExecutorAgent`:** A specialized agent designed to execute code snippets. Useful for tasks requiring script execution or interacting with system commands.
    - **`UserProxyAgent`:** An agent that serves as an interface for human users. It can relay communications between the user and other agents and can be configured to request human input at specific workflow stages.

- **Teams (Group chat):** AutoGen facilitates forming agent teams to tackle complex problems collaboratively. Key team configurations include:
  - **`RoundRobinGroupChat`:** A straightforward team setup where agents communicate in turns, following a round-robin approach to ensure balanced contribution from each member.
  - **`SelectorGroupChat`:** A more sophisticated team configuration enabling advanced agent selection mechanisms, including LLM-driven speaker selection for dynamic conversation flow.

- **Tools:** AutoGen agents can leverage tools to interact with external environments or perform specialized functions. Tools can be:
  - **Python functions:** Custom Python functions that agents can call to execute specific actions or computations.
  - **External APIs:** Integrations with external services, allowing agents to access a wide range of functionalities like web searching.

- **Code execution:** AutoGen equips agents with code execution capabilities, enabling them to perform tasks involving computation, data manipulation, or system interactions, enhancing their problem-solving abilities.

- **Termination conditions:** To effectively manage conversations and workflows, AutoGen allows defining termination conditions. These conditions specify criteria for ending a conversation or task, ensuring efficient resource use and task completion. Examples include:
  - **`TextMentionTermination`:** Ends the conversation when a predefined text or phrase is detected in the dialogue (e.g., "TERMINATE").
  - **`MaxMessageTermination`:** Automatically stops the conversation after a set number of messages have been exchanged, preventing infinite loops.

Utilizing these fundamental components, AutoGen provides a robust and adaptable framework for building a diverse array of AI applications, ranging from simple interactive chatbots to intricate, collaborative multi-agent systems.

## Why Neon for AI Agents?

Neon's architecture is particularly well-suited for AI agent development, offering several key advantages:

- **One-Second Provisioning:** Neon databases can be provisioned in about a second. This is _critical_ for AI agents that need to dynamically create databases. Traditional databases, with provisioning times often measured in minutes, create a significant bottleneck. Neon's speed keeps agents operating efficiently.

- **Scale-to-Zero and Serverless Pricing:** Neon's serverless architecture automatically scales databases down to zero when idle, and you only pay for active compute time. This is cost-effective for AI agent workflows, which often involve unpredictable workloads and many short-lived database instances. It enables "database-per-agent" or "database-per-session" patterns without incurring prohibitive costs.

- **Agent-Friendly API:** Neon provides a simple REST API for managing databases, roles, branches, and various other Neon platform operations. This API is easy for AI agents (and human developers) to interact with programmatically, allowing agents to manage their own database infrastructure without complex tooling.

## Building the AI agent

Let's start building your AI agent. First, create a new directory for your project. For example, you can name it `autogen-neon-example`. Open this new folder in your preferred code editor.

### Setting up a virtual environment

Creating a virtual environment is strongly recommended to manage project dependencies in isolation. Use `venv` to create a virtual environment within your project directory:

```bash
cd autogen-neon-example
python3 -m venv venv
source venv/bin/activate   # For macOS/Linux. On Windows, use `venv\Scripts\activate`
```

### Installing required libraries

Next, install the necessary Python libraries for this project. Create a file named `requirements.txt` in your project directory and add the following dependencies:

```
autogen-agentchat[openai]
autogen-ext[openai]
python-dotenv
neon-api
psycopg2-binary
```

<Admonition type="note">
`neon-api` is the [Python wrapper for Neon's API](https://github.com/neondatabase/neon-api-python).
</Admonition>

Install these libraries using pip:

```bash
pip install -r requirements.txt
```

### Configuring API keys in `.env`

For secure API key management, create a `.env` file in your project directory and add your API keys as environment variables:

```env
OPENAI_API_KEY=YOUR_OPENAI_API_KEY
NEON_API_KEY=YOUR_NEON_API_KEY
```

**Replace the placeholders** `YOUR_OPENAI_API_KEY` and `YOUR_NEON_API_KEY` with the actual API keys you obtained in the [Prerequisites](#prerequisites) section.

<Admonition type="note">
    It is crucial to add `.env` to your `.gitignore` file if you are using Git for version control. This prevents your API keys from being inadvertently exposed in your code repository.
</Admonition>

### Creating the `main.py` file

Create a Python file named `main.py` in your project's root directory and copy the following code into it:

```python
import asyncio
import os

import psycopg2
from autogen_agentchat.agents import AssistantAgent, CodeExecutorAgent
from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor
from autogen_ext.models.openai import OpenAIChatCompletionClient
from dotenv import load_dotenv
from neon_api import NeonAPI
from psycopg2.extras import RealDictCursor

load_dotenv()

neon_client = NeonAPI(
    api_key=os.environ["NEON_API_KEY"],
)


def create_database(project_name: str) -> str:
    """
    Creates a new Neon project. (this takes less than 500ms)
    Args:
        project_name: Name of the project to create
    Returns:
        the connection URI for the new project
    """
    try:
        project = neon_client.project_create(project={"name": project_name}).project
        connection_uri = neon_client.connection_uri(
            project_id=project.id, database_name="neondb", role_name="neondb_owner"
        ).uri

        return f"Project/database created, connection URI: {connection_uri}"
    except Exception as e:
        return f"Failed to create project: {str(e)}"


def run_sql_query(connection_uri: str, query: str) -> str:
    """
    Runs an SQL query in the Neon database.
    Args:
        connection_uri: The connection URI for the Neon database
        query: The SQL query to execute
    Returns:
        the result of the SQL query
    """
    conn = psycopg2.connect(connection_uri)
    cur = conn.cursor(cursor_factory=RealDictCursor)
    try:
        cur.execute(query)
        conn.commit()

        # Try to fetch results (for SELECT queries)
        try:
            records = cur.fetchall()
            return f"Query result: {records}"
        except psycopg2.ProgrammingError:
            # For INSERT/UPDATE/DELETE operations
            return f"Query executed successfully"
    except Exception as e:
        conn.rollback()
        return f"Failed to execute SQL query: {str(e)}"
    finally:
        cur.close()
        conn.close()


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o", temperature=0.6)

    assistant = AssistantAgent(
        name="assistant",
        system_message="""You are a helpful AI assistant.
Solve tasks using your coding and language skills.
You are working with two other agents:
1. 'code_executor': Use this agent for non-database coding tasks such as general-purpose scripts, file manipulation, and system commands.
2. 'db_admin': Use this agent for all database-related tasks.
Do NOT generate or suggest any SQL or database connection code yourself. Clearly mention what needs to be done and send the request to 'db_admin'.

In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.
1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.
2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.

Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.
When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can't modify your code. So do not suggest incomplete code which requires users to modify. Don't use a code block if it's not intended to be executed by the user.
If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don't include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use 'print' function for the output when relevant, try to add print statements while sharing code with the user so it will be used for debugging. Check the execution result returned by the user.
If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.
When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.
Reply 'TERMINATE' in the end when the task is completed by everyone.
""",
        model_client=model_client,
    )

    code_executor = CodeExecutorAgent(
        name="code_executor",
        code_executor=LocalCommandLineCodeExecutor(work_dir="coding"),
        sources=["assistant"],
    )

    db_admin = AssistantAgent(
        name="db_admin",
        system_message="""You are a helpful database admin assistant with access to the following tools:
1.  **Project Creation:** Create a new Neon project by providing a project name and receive the connection URI.
2.  **SQL Execution:** Run SQL queries within a Neon database.
Use these tools to fulfill user requests.  For each step, clearly describe the action taken and its result.  Include the tool output directly in the chat.  When multiple SQL queries are required, combine them into a single grouped query.  Present the output of each individual query within the grouped query's response.
""",
        model_client=model_client,
        tools=[create_database, run_sql_query],
    )

    # The termination condition is a combination of text termination and max message termination, either of which will cause the chat to terminate.
    termination = TextMentionTermination("TERMINATE") | MaxMessageTermination(20)

    # The group chat will alternate between the assistant and the code executor.
    group_chat = RoundRobinGroupChat(
        [assistant, code_executor, db_admin], termination_condition=termination
    )

    # `run_stream` returns an async generator to stream the intermediate messages.
    stream = group_chat.run_stream(
        task="Get the 10 most recent Machine Learning papers from arXiv. Print the titles and links to the papers in the chat. Save them in a database named 'arxiv_papers'",
    )
    await Console(stream)


if __name__ == "__main__":
    asyncio.run(main())
```

Let's take a closer look at the code. While it might seem lengthy at first, we'll break it down into sections to make it easier for you to understand.

### Import necessary libraries

```python
import asyncio
import os

import psycopg2
from autogen_agentchat.agents import AssistantAgent, CodeExecutorAgent
from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor
from autogen_ext.models.openai import OpenAIChatCompletionClient
from dotenv import load_dotenv
from neon_api import NeonAPI
from psycopg2.extras import RealDictCursor

load_dotenv()

neon_client = NeonAPI(
    api_key=os.environ["NEON_API_KEY"],
)
```

This section imports all the Python libraries required for the script. These include libraries for AutoGen agents, Neon API interaction, SQL execution, environment variable management, and asynchronous operations. It also initializes the Neon API client using your API key loaded from the `.env` file.

### Define the tools for Agent interaction

To enable agents to interact with the Neon database, we define specific tools. In this example, we create two primary tools: `create_database` and `run_sql_query`.

#### Define `create_database` tool

```python
def create_database(project_name: str) -> str:
    """
    Creates a new Neon project. (this takes less than 500ms)
    Args:
        project_name: Name of the project to create
    Returns:
        the connection URI for the new project
    """
    try:
        project = neon_client.project_create(project={"name": project_name}).project
        connection_uri = neon_client.connection_uri(
            project_id=project.id, database_name="neondb", role_name="neondb_owner"
        ).uri

        return f"Project/database created, connection URI: {connection_uri}"
    except Exception as e:
        return f"Failed to create project: {str(e)}"
```

This Python function defines a tool that allows agents to create new Neon projects programmatically using the `neon_api_client`.

- It accepts `project_name: str` as an argument, which specifies the name for the new Neon project.
- It utilizes `neon_client.project_create()` to send a request to the Neon API to create a new project.
- Upon successful project creation, it retrieves the connection URI for the newly created Neon database using `neon_client.connection_uri()`.
- It returns a formatted string that confirms the project and database creation and includes the connection URI, which is essential for connecting to the database.
- In case of any errors during project creation, it catches the exception and returns an error message, aiding in debugging and error handling.

#### Define `run_sql_query` tool

```python
def run_sql_query(connection_uri: str, query: str) -> str:
    """
    Runs an SQL query in the Neon database.
    Args:
        connection_uri: The connection URI for the Neon database
        query: The SQL query to execute
    Returns:
        the result of the SQL query
    """
    conn = psycopg2.connect(connection_uri)
    cur = conn.cursor(cursor_factory=RealDictCursor)
    try:
        cur.execute(query)
        conn.commit()

        # Try to fetch results (for SELECT queries)
        try:
            records = cur.fetchall()
            return f"Query result: {records}"
        except psycopg2.ProgrammingError:
            # For INSERT/UPDATE/DELETE operations
            return f"Query executed successfully"
    except Exception as e:
        conn.rollback()
        return f"Failed to execute SQL query: {str(e)}"
    finally:
        cur.close()
        conn.close()
```

This Python function is defined as a tool for agents to execute SQL queries directly against a Neon database.

- It takes two arguments: `connection_uri: str`, which is the URI string required to establish a database connection, and `query: str`, the SQL query intended for execution.
- It establishes a connection to the Neon database using `psycopg2.connect(connection_uri)`
- It creates a cursor object using `conn.cursor(cursor_factory=RealDictCursor)`. The `RealDictCursor` is specified to fetch query results as dictionaries, which is often more convenient for data manipulation in Python.
- It executes the provided SQL query using `cur.execute(query)`.
- It includes error handling for SQL query execution. If any exception occurs, it rolls back the transaction using `conn.rollback()` and returns an error message.

### Define `main` asynchronous function

```python
async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o", temperature=0.6)

    assistant = AssistantAgent(
        name="assistant",
        system_message="""... (system message content)""",
        model_client=model_client,
    )

    code_executor = CodeExecutorAgent(
        name="code_executor",
        code_executor=LocalCommandLineCodeExecutor(work_dir="coding"),
        sources=["assistant"],
    )

    db_admin = AssistantAgent(
        name="db_admin",
        system_message="""... (system message content)""",
        model_client=model_client,
        tools=[create_database, run_sql_query],
    )

    termination = TextMentionTermination("TERMINATE") | MaxMessageTermination(20)

    group_chat = RoundRobinGroupChat(
        [assistant, code_executor, db_admin], termination_condition=termination
    )

    stream = group_chat.run_stream(
        task="Get the 10 most recent Machine Learning papers from arXiv. Print the titles and links to the papers in the chat. Save them in a database named 'arxiv_papers'",
    )
    await Console(stream)
```

This `async def main() -> None:` function is the core of your script, where you set up and orchestrate the AutoGen agents to perform the desired task. Let's break down what happens inside:

- **Initialize model client:**

  ```python
  model_client = OpenAIChatCompletionClient(model="gpt-4o", temperature=0.6)
  ```

  This line initializes the OpenAI model client, specifying `gpt-4o` as the LLM to be used. `OpenAIChatCompletionClient` is configured to interact with OpenAI's API, using the API key you've set up.

- **Create `assistant` agent:**

  ```python
  assistant = AssistantAgent(
      name="assistant",
      system_message="""... (system message)""",
      model_client=model_client,
  )
  ```

  Here, we instantiate the primary agent, `assistant`, using `AssistantAgent`. This agent is designed to be the main problem solver. The `system_message` is a crucial part of its configuration, defining its role, capabilities, and instructions on how to interact with other agents and tools. It emphasizes task planning, delegation, and using the specialized `code_executor` and `db_admin` agents for specific sub-tasks.

- **Create `code_executor` agent:**

  ```python
  code_executor = CodeExecutorAgent(
      name="code_executor",
      code_executor=LocalCommandLineCodeExecutor(work_dir="coding"),
      sources=["assistant"],
  )
  ```

  We then create a `CodeExecutorAgent` named `code_executor`. This agent is specialized in executing code and is equipped with `LocalCommandLineCodeExecutor` to run code locally. The `sources=["assistant"]` configuration indicates that this agent is intended to execute code suggested by the `assistant` agent.

- **Create `db_admin` agent:**

  ```python
  db_admin = AssistantAgent(
      name="db_admin",
      system_message="""... (system message content)""",
      model_client=model_client,
      tools=[create_database, run_sql_query],
  )
  ```

  Next, you create another `AssistantAgent`, `db_admin`, which is specifically designed for database administration tasks. Critically, we equip this agent with the `tools=[create_database, run_sql_query]` we defined earlier. The `system_message` for `db_admin` instructs it on its role as a database admin assistant and how to use the provided tools.

- **Define termination conditions:**

  ```python
  termination = TextMentionTermination("TERMINATE") | MaxMessageTermination(20)
  ```

  This sets up termination conditions for your group chat. The conversation will end if either the phrase "TERMINATE" is mentioned by any agent (`TextMentionTermination`) or if the conversation reaches 20 messages (`MaxMessageTermination(20)`), whichever comes first. This is important to prevent conversations from running indefinitely.

- **Create `group_chat`:**

  ```python
  group_chat = RoundRobinGroupChat(
      [assistant, code_executor, db_admin], termination_condition=termination
  )
  ```

  We then assemble your agents into a team using `RoundRobinGroupChat`. This configuration ensures that the agents (`assistant`, `code_executor`, `db_admin`) will take turns speaking in a round-robin fashion. The `termination_condition=termination` applies the termination conditions we defined earlier to this chat.

- **Run the chat:**

  ```python
  stream = group_chat.run_stream(
      task="Get the 10 most recent Machine Learning papers from arXiv. Print the titles and links to the papers in the chat. Save them in a database named 'arxiv_papers'",
  )
  await Console(stream)
  ```

  Finally, we initiate and run the group chat using `group_chat.run_stream()`. We provide the initial `task` for the agents: to retrieve the 10 most recent Machine Learning papers from arXiv, display their titles and links, and then store this information in a database named `arxiv_papers`. `Console(stream)` is used to provide a real-time, streaming output of the conversation to the console, making it easy for you to follow along with the agent's interactions.

    <Admonition type="warning">
        This guide uses `LocalCommandLineCodeExecutor` for simplicity, which allows AI agents to execute commands directly on your local machine. **This setup is highly insecure and is strictly NOT recommended for production environments.**  Agents could potentially perform harmful actions on your system.

        For production deployments, we strongly advise using `DockerCommandLineCodeExecutor`. This executor runs code within isolated Docker containers, significantly enhancing security by limiting the agent's access to your system.

        Setting up `DockerCommandLineCodeExecutor` involves additional configuration steps, including Docker setup and image management, which are beyond the scope of this getting started guide.  Please refer to the [AutoGen documentation](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.code_executors.docker.html#autogen_ext.code_executors.docker.DockerCommandLineCodeExecutor) for detailed instructions on how to configure and use `DockerCommandLineCodeExecutor` securely.

    </Admonition>

### Running the example

With your project set up and the `main.py` code in place, you are ready to execute the example.

Open your terminal and run the script using:

```bash
python main.py
```

Executing this command will:

- Launch the `main.py` script, initiating the AutoGen agent team.
- Start the collaborative process as the agents begin to interact to achieve the defined task.
- Display a real-time, step-by-step conversation between the agents directly in your console.
- Showcase the `assistant` agent's role in planning and delegating sub-tasks to the `code_executor` (for coding needs) and `db_admin` (for database operations).
- Ultimately, lead to the retrieval of recent ML papers from arXiv and their storage in a Neon database named `arxiv_papers`, demonstrating a complete workflow.

### Expected output

Upon running `python main.py`, you will see a detailed, turn-based conversation unfold in your console. This output will illustrate the dynamic interaction between your agents.

![Autogen-Neon example output 1](/docs/guides/autogen-neon-output-1.png)
![Autogen-Neon example output 2](/docs/guides/autogen-neon-output-2.png)
![Autogen-Neon example output 3](/docs/guides/autogen-neon-output-3.png)

You can verify the successful completion of the task by checking the [Neon Console](https://console.neon.tech/). The `arxiv_papers` project should have been created, and the recent ML papers from arXiv should be stored in the database.

![Output in Neon console](/docs/guides/autogen-neon-console.png)

**Congratulations!** You have successfully built and run an AutoGen agent team that effectively interacts with Neon for database management! This example serves as a foundation for creating more complex AI agents and workflows, enabling you to automate a wide range of tasks and processes.

You can find the source code for the application described in this guide on GitHub.

<DetailIconCards>
    <a href="https://github.com/neondatabase-labs/autogen-neon-example" description="AutoGen + Neon AI agent example" icon="github">AI Agent with AutoGen and Neon</a>
</DetailIconCards>

## Resources

- [AutoGen documentation](https://microsoft.github.io/autogen/stable/)
- [Neon documentation](/docs)
- [neon_api: Python API wrapper for the Neon API](https://github.com/neondatabase/neon-api-python)
- [Neon API reference](https://api-docs.neon.tech/reference/getting-started-with-neon-api)
- [Neon API keys](/docs/manage/api-keys#creating-api-keys)
- [Postgres for AI Agents](/use-cases/ai-agents)

<NeedHelp/>


# Build your first AI Agent for Postgres on Azure

---
title: 'Build your first AI Agent for Postgres on Azure'
subtitle: 'Learn how to build an AI Agent for Postgres using Azure AI Agent Service and Neon'
author: boburmirzo
enableTableOfContents: true
createdAt: '2025-04-07T00:00:00.000Z'
updatedOn: '2025-04-07T00:00:00.000Z'
---

AI agents are getting a lot of attention lately, but getting started can be confusing. You may have heard about tools like [LangChain/LangGraph](https://python.langchain.com/v0.1/docs/modules/agents/), [Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/overview/), [AutoGen](https://microsoft.github.io/autogen/), or [LlamaIndex](https://docs.llamaindex.ai/en/stable/use_cases/agents/). They are powerful, but sometimes all you need is something simple that works.

[**Azure AI Agent Service**](https://learn.microsoft.com/en-us/azure/ai-services/agents/overview) lets you build AI agents that can use your own tools, such as a function to read data from a Postgres database. It's designed to help developers get started fast, without needing to understand chains, graphs, or complex frameworks.

In this tutorial, you’ll learn how to create an AI agent that can answer questions about data in a **Postgres** database using **Azure AI Agent Service**.

## Sample Use Case

Imagine you run a SaaS product and track tenant-level usage data for billing (like API calls, storage, user sessions, etc.) in a Postgres table. You want an AI assistant to:

- Analyze recent usage for a tenant
- Spot spikes in usage (possible billing anomalies)
- Explain what happened in natural language

For example, when a user asks questions about their invoice, the AI can query Neon for relevant usage logs, summarize usage patterns, and offer explanations before routing to a human. Or when you open your billing dashboard, the agent proactively explains any spikes or changes:

> "Your API usage increased by 250% on April 3rd due to increased traffic from users in the EU region."

We’ll build an AI agent that connects to your Postgres database and uses a simple Python function to fetch and analyze the data.

We’ll use [**Neon Serverless Postgres**](/) as our database. It’s a fully managed, cloud-native Postgres that’s free to start, scales automatically, and works great for [AI agents](/use-cases/ai-agents) that need to query data on demand without managing infrastructure.

### Prerequisites

- An Azure subscription - [Create one for free](https://azure.microsoft.com/free/cognitive-services).
- Make sure you have the **Azure AI Developer** [RBAC role](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/rbac-azure-ai-foundry) assigned
- Install [Python 3.11.x](https://www.python.org/downloads/).

## Create a Neon Database on Azure

Open the [new Neon Resource page](https://portal.azure.com/#view/Azure_Marketplace_Neon/NeonCreateResource.ReactView) on the Azure portal, and it brings up the form to create a Neon Serverless Postgres Resource. Fill out the form with the required fields and deploy it.

### Obtain Neon Database Credentials

1. After the resource is created on Azure, go to the Neon Serverless Postgres Organization service and click on the Portal URL. This brings you to the Neon Console
2. Click “New Project”
3. Choose an Azure region
4. Give your project a name (e.g., “Postgres AI Agent”)
5. Click “Create Project”
6. Once the project is created successfully, copy the Neon connection string and note it down. You can find the connection details in the Connection Details widget on the Neon Dashboard.

```bash
    postgresql://[user]:[password]@[neon_hostname]/[dbname]?sslmode=require&channel_binding=require
```

## Create an AI Foundry Project on Azure

Create a new hub and project in the Azure AI Foundry portal by [following the guide](https://learn.microsoft.com/en-us/azure/ai-services/agents/quickstart?pivots=ai-foundry-portal#create-a-hub-and-project-in-azure-ai-foundry-portal) in the Microsoft docs. You also need to [deploy a model](https://learn.microsoft.com/en-us/azure/ai-services/agents/quickstart?pivots=ai-foundry-portal#deploy-a-model) like GPT-4o.

You only need the **Project connection string** and **Model Deployment Name** from the Azure AI Foundry portal. You can also find your connection string in the **overview** for your project in the [**Azure AI Foundry portal**](https://ai.azure.com/), under **Project details** > **Project connection string**.

![Project connection string in Azure AI Foundry Portal](/docs/guides/azure-ai-agent-service/azure-ai-foundry-find-project-connection-string.png)

Once you have all three values on hand: **Neon connection string**, **Project connection string,** and **Model Deployment Name,** you are ready to set up the Python project to create an Agent.

All the code and sample data are available in this [GitHub repository](https://github.com/neondatabase-labs/neon-azure-ai-agent-service-get-started). You can clone or download the project.

## Project Environment Setup

Create a `.env` file with your credentials:

```bash
PROJECT_CONNECTION_STRING="<Your AI Foundry connection string>"
AZURE_OPENAI_DEPLOYMENT_NAME="gpt-4o"
NEON_DB_CONNECTION_STRING="<Your Neon connection string>"
```

Create and activate a virtual environment:

```bash
python -m venv .venv
source .venv/bin/activate  # on macOS/Linux
.venv\Scripts\activate    # on Windows
```

Install required Python libraries:

```
pip install -r requirements.txt
```

Example `requirements.txt`:

```bash
pandas
python-dotenv
sqlalchemy
psycopg2-binary
azure-ai-projects
azure-identity
```

## Load Sample Billing Usage Data

We will use a mock dataset for tenant usage, including computed percent change in API calls and storage usage in GB:

```
tenant_id   date        api_calls  storage_gb

tenant_456	2025-04-01	1000	     25.0
tenant_456	2025-03-31	950	         24.8
tenant_456	2025-03-30	2200	     26.0
```

Run `python load_usage_data.py` [Python script](https://github.com/neondatabase-labs/neon-azure-ai-agent-service-get-started/blob/main/load_usage_data.py) to create and populate the `usage_data` table in your Neon Serverless Postgres instance:

```python
# load_usage_data.py file

import os
from dotenv import load_dotenv
from sqlalchemy import (
    create_engine,
    MetaData,
    Table,
    Column,
    String,
    Date,
    Integer,
    Numeric,
)

# Load environment variables from .env
load_dotenv()

# Load connection string from environment variable
NEON_DB_URL = os.getenv("NEON_DB_CONNECTION_STRING")
engine = create_engine(NEON_DB_URL)

# Define metadata and table schema
metadata = MetaData()

usage_data = Table(
    "usage_data",
    metadata,
    Column("tenant_id", String, primary_key=True),
    Column("date", Date, primary_key=True),
    Column("api_calls", Integer),
    Column("storage_gb", Numeric),
)

# Create table
with engine.begin() as conn:
    metadata.create_all(conn)

    # Insert mock data
    conn.execute(
        usage_data.insert(),
        [
            {
                "tenant_id": "tenant_456",
                "date": "2025-03-27",
                "api_calls": 870,
                "storage_gb": 23.9,
            },
            {
                "tenant_id": "tenant_456",
                "date": "2025-03-28",
                "api_calls": 880,
                "storage_gb": 24.0,
            },
            {
                "tenant_id": "tenant_456",
                "date": "2025-03-29",
                "api_calls": 900,
                "storage_gb": 24.5,
            },
            {
                "tenant_id": "tenant_456",
                "date": "2025-03-30",
                "api_calls": 2200,
                "storage_gb": 26.0,
            },
            {
                "tenant_id": "tenant_456",
                "date": "2025-03-31",
                "api_calls": 950,
                "storage_gb": 24.8,
            },
            {
                "tenant_id": "tenant_456",
                "date": "2025-04-01",
                "api_calls": 1000,
                "storage_gb": 25.0,
            },
        ],
    )

print("✅ usage_data table created and mock data inserted.")
```

## Create a Postgres Tool for the Agent

Next, we configure an AI agent tool to retrieve data from Postgres. The Python script [`billing_agent_tools.py`](https://github.com/neondatabase-labs/neon-azure-ai-agent-service-get-started/blob/main/billing_agent_tools.py) contains:

- The function `billing_anomaly_summary()` that:
  - Pulls usage data from Neon.
  - Computes `% change` in `api_calls`.
  - Flags anomalies with a threshold of `> 1.5x` change.
- Exports `user_functions` list for the Azure AI Agent to use. You do not need to run it separately.

```python
# billing_agent_tools.py file

import os
import json
import pandas as pd
from sqlalchemy import create_engine
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Set up the database engine
NEON_DB_URL = os.getenv("NEON_DB_CONNECTION_STRING")
db_engine = create_engine(NEON_DB_URL)

# Define the billing anomaly detection function
def billing_anomaly_summary(
    tenant_id: str,
    start_date: str = "2025-03-27",
    end_date: str = "2025-04-01",
    limit: int = 10,
) -> str:
    """
    Fetches recent usage data for a SaaS tenant and detects potential billing anomalies.

    :param tenant_id: The tenant ID to analyze.
    :type tenant_id: str
    :param start_date: Start date for the usage window.
    :type start_date: str
    :param end_date: End date for the usage window.
    :type end_date: str
    :param limit: Maximum number of records to return.
    :type limit: int
    :return: A JSON string with usage records and anomaly flags.
    :rtype: str
    """
    query = """
        SELECT date, api_calls, storage_gb
        FROM usage_data
        WHERE tenant_id = %s AND date BETWEEN %s AND %s
        ORDER BY date DESC
        LIMIT %s;
    """

    df = pd.read_sql(query, db_engine, params=(tenant_id, start_date, end_date, limit))

    if df.empty:
        return json.dumps(
            {"message": "No usage data found for this tenant in the specified range."}
        )

    df.sort_values("date", inplace=True)
    df["pct_change_api"] = df["api_calls"].pct_change()
    df["anomaly"] = df["pct_change_api"].abs() > 1.5

    return df.to_json(orient="records")

# Register this in a list to be used by FunctionTool
user_functions = [billing_anomaly_summary]
```

## Create and Configure the AI Agent

Now we'll set up the AI agent and integrate it with our Neon Postgres tool using the **Azure AI Agent Service SDK.** The [Python script](https://github.com/neondatabase-labs/neon-azure-ai-agent-service-get-started/blob/main/billing_anomaly_agent.py) does the following:

- **Creates the agent**
  Instantiates an AI agent using the selected model (`gpt-4o`, for example), adds tool access, and sets instructions that tell the agent how to behave (e.g., “You are a helpful SaaS assistant…”).
- **Creates a conversation thread**
  A thread is started to hold a conversation between the user and the agent.
- **Posts a user message**
  Sends a question like “Why did my billing spike for tenant_456 this week?” to the agent.
- **Processes the request**
  The agent reads the message, determines that it should use the custom tool to retrieve usage data, and processes the query.
- **Displays the response**
  Prints the response from the agent with a natural language explanation based on the tool’s output.

```python
# billing_anomaly_agent.py

import os
from datetime import datetime
from azure.ai.projects import AIProjectClient
from azure.identity import DefaultAzureCredential
from azure.ai.projects.models import FunctionTool, ToolSet
from dotenv import load_dotenv
from pprint import pprint
from billing_agent_tools import user_functions  # Custom tool function module

# Load environment variables from .env file
load_dotenv()

# Create an Azure AI Project Client
project_client = AIProjectClient.from_connection_string(
    credential=DefaultAzureCredential(),
    conn_str=os.environ["PROJECT_CONNECTION_STRING"],
)

# Initialize toolset with our user-defined functions
functions = FunctionTool(user_functions)
toolset = ToolSet()
toolset.add(functions)

# Create the agent
agent = project_client.agents.create_agent(
    model=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
    name=f"billing-anomaly-agent-{datetime.now().strftime('%Y%m%d%H%M')}",
    description="Billing Anomaly Detection Agent",
    instructions=f"""
    You are a helpful SaaS financial assistant that retrieves and explains billing anomalies using usage data.
    The current date is {datetime.now().strftime("%Y-%m-%d")}.
    """,
    toolset=toolset,
)
print(f"Created agent, ID: {agent.id}")

# Create a communication thread
thread = project_client.agents.create_thread()
print(f"Created thread, ID: {thread.id}")

# Post a message to the agent thread
message = project_client.agents.create_message(
    thread_id=thread.id,
    role="user",
    content="Why did my billing spike for tenant_456 this week?",
)
print(f"Created message, ID: {message.id}")

# Run the agent and process the query
run = project_client.agents.create_and_process_run(
    thread_id=thread.id, agent_id=agent.id
)
print(f"Run finished with status: {run.status}")

if run.status == "failed":
    print(f"Run failed: {run.last_error}")

# Fetch and display the messages
messages = project_client.agents.list_messages(thread_id=thread.id)
print("Messages:")
pprint(messages["data"][0]["content"][0]["text"]["value"])

# Optional cleanup:
# project_client.agents.delete_agent(agent.id)
# print("Deleted agent")

```

## Run the agent

To run the agent, run the following command

```bash
python billing_anomaly_agent.py
```

Snippet of output from agent:

```bash
Messages:
('Based on the usage data for tenant `tenant_456` between April 1 and April 6, '
 '2025:\n'
 '\n'
 '- **API calls:** 1,000\n'
 '- **Storage usage:** 25 GB\n'
 '\n'
 'There are no notable anomalies detected. The percent change in API activity '
 'compared to previous usage is not reported (likely constant or not '
 "significant), and usage levels seem normal. Let me know if you'd like a "
 'deeper investigation into prior weeks or specific metrics!')
```

## Using the Azure AI Foundry agent playground

After running your agent using the Azure AI Agent SDK, it is saved within your Azure AI Foundry project. You can now experiment with it using the **Agent Playground**.

![Azure AI Foundry agent playground](/docs/guides/azure-ai-agent-service/azure-ai-foundry-portal-agents-view.png)

**To try it out:**

- Go to the **Agents** section in your [Azure AI Foundry](https://ai.azure.com/) workspace.
- Find your **billing anomaly agent** in the list and click to open it.
- Use the playground interface to test different financial or billing-related questions, such as:
  > “Did tenant_456 exceed their API usage quota this month?”
  >
  > “Explain recent storage usage changes for tenant_456.”

This is a great way to validate your agent's behavior without writing more code.

## Summary

You’ve now created a working AI agent that talks to your Postgres database, all using:

- A simple Python function
- Azure AI Agent Service
- A Neon Serverless Postgres backend

This approach is beginner-friendly, lightweight, and practical for real-world use.

Want to go further? You can:

- Add more tools to the agent
- Integrate with [vector search](/docs/extensions/pgvector) (e.g., detect anomaly reasons from logs using embeddings)

## Resources

- [Neon on Azure](/docs/manage/azure)
- [Build AI Agents with Azure AI Agent Service and Neon](/blog/build-ai-agents-with-azure-ai-agent-service-and-neon)
- [Multi-Agent AI Solution with Neon, Langchain, AutoGen and Azure OpenAI](/blogolution-with-neon-langchain-autogen-and-azure-openai)

<NeedHelp />


# Building AI-Powered Chatbots with Azure AI Studio and Neon

---
title: Building AI-Powered Chatbots with Azure AI Studio and Neon
subtitle: Learn how to create AI powered chatbot using Azure AI Studio with Neon Postgres as the backend database
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-11-24T00:00:00.000Z'
updatedOn: '2024-11-24T00:00:00.000Z'
---

In this guide, we'll walk through creating an AI-powered chatbot from scratch. We will be using Azure AI Studio, Neon Postgres as the backend database, React for the frontend interface and Express for the backend API.

We'll deploy a GPT-4 model to Azure AI Studio, which we will then use to build a support chatbot that can answer questions, store conversations, and learn from interactions over time.

## Prerequisites

Before we begin, make sure you have:

- An [Azure account](https://azure.microsoft.com/free/) with an active subscription
- A [Neon account](https://console.neon.tech/signup) and project
- Basic familiarity with SQL and JavaScript/TypeScript
- [Node.js](https://nodejs.org/) 18.x or later installed

## Setting up Your Development Environment

If you haven't already, follow these steps to set up your development environment:

### Create a Neon Project

1. Navigate to the [Neon Console](https://console.neon.tech)
2. Click "New Project"
3. Select Azure as your cloud provider
4. Choose East US 2 as your region
5. Give your project a name (e.g., "chatbot-db")
6. Click "Create Project"

Save your connection details - you'll need these to configure your chatbot's database connection.

### Create the Database Schema

A standard chatbot needs to store conversations and track how users interact with it. We'll create a database schema in Neon Postgres that stores messages, tracks user data, and helps us understand how well the chatbot is performing.

Our schema will include 4 tables:

- `users`: Stores user information
- `conversations`: Manages chat sessions
- `messages`: Stores the messages between users and the bot
- `feedback`: Records user ratings and comments

Connect to your Neon database and execute the following SQL statements to create the tables:

```sql
-- Create users table
CREATE TABLE users (
    user_id VARCHAR(255) PRIMARY KEY,
    first_name VARCHAR(100),
    last_name VARCHAR(100),
    email VARCHAR(255),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Create conversations table
CREATE TABLE conversations (
    conversation_id SERIAL PRIMARY KEY,
    user_id VARCHAR(255) REFERENCES users(user_id),
    started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_message_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    status VARCHAR(50) DEFAULT 'active' CHECK (status IN ('active', 'closed', 'archived'))
);

-- Create messages table
CREATE TABLE messages (
    message_id SERIAL PRIMARY KEY,
    conversation_id INTEGER REFERENCES conversations(conversation_id),
    sender_type VARCHAR(50) CHECK (sender_type IN ('user', 'bot')),
    content TEXT NOT NULL,
    sent_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    tokens_used INTEGER,
    completion_tokens INTEGER,
    total_tokens INTEGER
);

-- Create feedback table for message ratings
CREATE TABLE message_feedback (
    feedback_id SERIAL PRIMARY KEY,
    message_id INTEGER REFERENCES messages(message_id),
    user_id VARCHAR(255) REFERENCES users(user_id),
    rating INTEGER CHECK (rating >= 1 AND rating <= 5),
    comment TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

With our 4 tables in place, we have a schema which allows us to:

- Track user interactions and store user data
- Manage chat sessions and track when they started
- Store messages between users and the bot
- Collect feedback on messages to improve the chatbot

### Set Up Azure AI Studio Project

With your Neon database ready, let's set up Azure AI Studio to deploy our own GPT-4 model.

In order to access the Azure OpenAI Studio, you need to create an Azure OpenAI resource. Here's how you can do that:

1. Go to [the Azure OpenAI resources portal](https://oai.azure.com/portal)
1. Click the "Create new Azure OpenAI resource" button
1. Fill in the required fields like the subscription, resource group, region and name
1. Click the "Next" button
1. For the network settings, you can leave the default settings so that all networks can access the resource or you can restrict access to specific networks
1. Click "Next" and under "Review + create" click the "Create" button to create the resource

This will create a new Azure OpenAI resource for you. The deployment might take a few minutes to complete.

Once the deployment is completed, you can again visit the [Azure OpenAI portal](https://oai.azure.com/portal), and you should see your newly created resource there with type "OpenAI".

### Deploy the Azure OpenAI Model

With the Azure OpenAI resource set up, we can now deploy the GPT-4 model. To deploy the Azure OpenAI model:

1. Go to the [Azure OpenAI portal](https://oai.azure.com/portal) again
1. Click on your OpenAI resource that you created earlier
1. Click on the "Model catalog" tab
1. Find and click on the "gpt-4" model from the list
1. Click the "Deploy" button
1. Wait for deployment to complete - you'll receive an Endpoint URL and API key

There are other models available in the Azure OpenAI Studio, but for this guide, we'll use the GPT-4 model for our chatbot.

After deployment, click "Open in playground" to test the model. The playground is a web interface where you can:

1. Test your model by chatting with it directly
1. Add training data to help the model understand your specific needs
1. Adjust settings like:
   - Maximum response length (how long answers can be)
   - Temperature (higher = more creative, lower = more focused)
   - Top P and Presence Penalty (control response variety)

Feel free to experiment with these settings to see how they affect the model's responses.

#### Setting Up Model Instructions

You can give the model instructions about how it should behave. Think of this like training a new colleague - you're telling them:

- What they should do
- What they shouldn't do
- How they should talk to users
- What information they can access

For example, you might write:

```
You are a customer service agent for a tech company.
- Always be polite and professional
- Only answer questions about our products and services
- If you don't know something, say so
- Format prices as USD with two decimal places
- Include links to our documentation when relevant
```

These instructions will be included with every message to the model. The model will follow these instructions for all conversations.

#### Testing Your Instructions

After setting up instructions for the model, you can test them in the playground, for example:

1. Try different types of questions in the playground
1. Check if the model follows your guidelines
1. Adjust the instructions if needed
1. Save the instructions when you're happy with the responses

Additionally, you can add training data to help the model understand your specific needs. To learn more about training data, check the [Azure OpenAI Studio documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/fine-tuning).

## Building the Backend

With the Azure OpenAI model deployed, we can now build the backend API that will interact with the model and store chat data in our Neon database.

But before we start building our backend, let's quickly look at how to get the API code from Azure OpenAI Studio. This will help us make sure that we're using the correct API format.

### Getting the API Code from Azure OpenAI Studio

1. In the Azure OpenAI Studio playground, click "View code"
2. From the dropdown menu, select "JSON"
3. Under "Key authentication" you'll see a sample request like this:

   ```json
   {
     "messages": [
       {
         "role": "system",
         "content": [
           {
             "type": "text",
             "text": "You are a marketing writing assistant. You help come up with creative content ideas ...\"\n"
           }
         ]
       }
     ],
     "temperature": 0.7,
     "top_p": 0.95,
     "max_tokens": 800
   }
   ```

This shows us the exact format we need to use when making API calls to Azure OpenAI.

### Setting Up the Project

First, let's create a new Node.js project and install the dependencies that we'll need for our chatbot backend.

Create a new project folder and initialize a new Node.js project:

```bash
mkdir azure-neon-chatbot
cd azure-neon-chatbot
npm init -y
```

After that, install the required packages:

```bash
npm install express pg dotenv cors axios
```

The packages we're installing are:

- `express`: Web framework for building our API endpoints
- `pg`: PostgreSQL client for connecting to Neon
- `dotenv`: Environment variable management
- `cors`: Handles Cross-Origin Resource Sharing for our frontend
- `axios`: Makes HTTP requests to Azure OpenAI API

### Project Structure

Before we start, let's organize our project files in a way that makes our code easy to maintain and update. We'll use a standard Node.js project structure that separates our code into different directories based on functionality:

- `config`: Holds configuration files, including database connection settings
- `services`: Contains the core business logic for chat functionality and OpenAI integration
- `routes`: Manages API endpoints and request handling
- `utils`: Stores helper functions and shared utilities
- The `.env` file will store our sensitive configuration values like API keys

The project structure will look like this:

```sh
azure-neon-chatbot/
├── src/
│   ├── config/
│   │   └── database.js
│   ├── services/
│   │   ├── chatService.js
│   │   └── openaiService.js
│   ├── routes/
│   │   └── chatRoutes.js
│   └── utils/
│       └── logger.js
├── .env
└── server.js
```

This structure will help us keep our code organized and makes it easier for other developers to understand and work with the project.

### Environment Configuration

Before we start coding, let's set up our environment configuration.

Create a `.env` file in your project root with the following configuration:

```env
# Database Configuration
DATABASE_URL='postgresql://neondb_owner:<your_password>@<your_host>.eastus2.azure.neon.tech/neondb?sslmode=require&channel_binding=require'

# Azure OpenAI Configuration
AZURE_OPENAI_ENDPOINT=https://<your-resource-name>.openai.azure.com
AZURE_OPENAI_API_KEY=<your-api-key>
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4

# Server Configuration
PORT=3000
NODE_ENV=development
```

You'll need to replace `<your_password>`, `<your_host>`, `<your-resource-name>`, and `<your-api-key>` with your actual values.

You can get your Azure OpenAI API key from the Azure OpenAI Studio portal under the Chat playground.

### Database Configuration

Next, let's set up the database connection. We'll use the `pg` package to connect to our Neon Postgres database.

Create a `src/config/database.js` file with the following code:

```javascript
const { Pool } = require('pg');
require('dotenv').config();

const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  ssl: {
    rejectUnauthorized: false,
  },
});

pool.connect((err, client, release) => {
  if (err) {
    console.error('Error connecting to the database:', err);
    return;
  }
  console.log('Successfully connected to Neon database');
  release();
});

module.exports = {
  query: (text, params) => pool.query(text, params),
  pool,
};
```

This sets up a connection to the Neon Postgres database using the `pg` package. We use the `DATABASE_URL` environment variable to connect to the database.

### OpenAI Service

Next, let's create a service to interact with the Azure OpenAI API. This service will handle sending messages to the GPT-4 model that we deployed earlier.

Create a `src/services/openaiService.js` file with the following code:

```javascript
// src/services/openaiService.js
const axios = require('axios');
require('dotenv').config();

class OpenAIService {
  constructor() {
    if (!process.env.AZURE_OPENAI_API_KEY) {
      throw new Error('AZURE_OPENAI_API_KEY is required');
    }
    if (!process.env.AZURE_OPENAI_ENDPOINT) {
      throw new Error('AZURE_OPENAI_ENDPOINT is required');
    }
    if (!process.env.AZURE_OPENAI_DEPLOYMENT_NAME) {
      throw new Error('AZURE_OPENAI_DEPLOYMENT_NAME is required');
    }

    this.endpoint = process.env.AZURE_OPENAI_ENDPOINT;
    this.apiKey = process.env.AZURE_OPENAI_API_KEY;
    this.deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME;
  }

  async generateResponse(userMessage, conversationHistory = []) {
    try {
      const url = `${this.endpoint}/openai/deployments/${this.deploymentName}/chat/completions?api-version=2024-02-15-preview`;

      const payload = {
        messages: [
          {
            role: 'system',
            content: [
              {
                type: 'text',
                text: 'You are a marketing writing assistant. You help come up with creative content ideas and content like marketing emails, blog posts, tweets, ad copy and product descriptions. You write in a friendly yet professional tone but can tailor your writing style that best works for a user-specified audience. If you do not know the answer to a question, respond by saying "I do not know the answer to your question."\n',
              },
            ],
          },
          ...conversationHistory,
          {
            role: 'user',
            content: [
              {
                type: 'text',
                text: userMessage,
              },
            ],
          },
        ],
        temperature: 0.7,
        top_p: 0.95,
        max_tokens: 800,
      };

      const headers = {
        'Content-Type': 'application/json',
        'api-key': this.apiKey,
      };

      const response = await axios.post(url, payload, { headers });

      return response.data.choices[0].message.content;
    } catch (error) {
      console.error('Error calling Azure OpenAI:', error.response?.data || error.message);
      if (error.response?.status === 401) {
        console.error('Authentication failed. Please check your AZURE_OPENAI_API_KEY.');
      }
      throw error;
    }
  }
}

module.exports = new OpenAIService();
```

This creates a service that handles all communication with Azure OpenAI. It does two main things:

1. Checks that we have all the required Azure OpenAI settings (API key, endpoint, and deployment name) when the service starts up
2. Provides a `generateResponse` method that:
   - Takes a user's message and any previous conversation history
   - Sends it to our deployed GPT-4 model on Azure
   - Returns the model's response

The service includes the bot's base instructions (as a marketing assistant in this example) and error handling for common issues like authentication problems.

Feel free to adjust the instructions and settings to match your chatbot's needs.

### Chat Service

Next, let's create a service to manage chat interactions. This service will handle user messages, conversation history, and saving messages to the database.

Create `src/services/chatService.js` with the following code:

```javascript
// src/services/chatService.js
const db = require('../config/database');
const openai = require('./openaiService');

class ChatService {
  async ensureUserExists(userId) {
    // Check if user exists
    const existingUser = await db.query('SELECT user_id FROM users WHERE user_id = $1', [userId]);

    if (existingUser.rows.length === 0) {
      // Create new user if doesn't exist
      await db.query('INSERT INTO users (user_id, first_name, last_name) VALUES ($1, $2, $3)', [
        userId,
        'Anonymous',
        'User',
      ]);
    }
  }

  async saveMessage(conversationId, senderType, content) {
    const query = `
      INSERT INTO messages (conversation_id, sender_type, content)
      VALUES ($1, $2, $3)
      RETURNING *
    `;
    return db.query(query, [conversationId, senderType, content]);
  }

  async getConversationHistory(conversationId, limit = 10) {
    const query = `
      SELECT sender_type, content
      FROM messages
      WHERE conversation_id = $1
      ORDER BY sent_at DESC
      LIMIT $2
    `;
    return db.query(query, [conversationId, limit]);
  }

  async processMessage(userId, message) {
    try {
      await this.ensureUserExists(userId);

      await db.query('BEGIN');

      // Get or create conversation
      let conversationId;
      const existingConversation = await db.query(
        'SELECT conversation_id FROM conversations WHERE user_id = $1 ORDER BY started_at DESC LIMIT 1',
        [userId]
      );

      if (existingConversation.rows.length === 0) {
        const newConversation = await db.query(
          'INSERT INTO conversations (user_id) VALUES ($1) RETURNING conversation_id',
          [userId]
        );
        conversationId = newConversation.rows[0].conversation_id;
      } else {
        conversationId = existingConversation.rows[0].conversation_id;
      }

      // Save user message
      await this.saveMessage(conversationId, 'user', message);

      // Get conversation history
      const history = await this.getConversationHistory(conversationId);
      const formattedHistory = history.rows.map((msg) => ({
        role: msg.sender_type === 'user' ? 'user' : 'assistant',
        content: [{ type: 'text', text: msg.content }],
      }));

      // Generate AI response
      const aiResponse = await openai.generateResponse(message, formattedHistory);

      // Save AI response
      await this.saveMessage(conversationId, 'bot', aiResponse);

      // Commit transaction
      await db.query('COMMIT');

      return {
        conversationId,
        reply: aiResponse,
      };
    } catch (error) {
      // Rollback transaction on error
      await db.query('ROLLBACK');
      console.error('Error processing message:', error);
      throw error;
    }
  }

  async startConversation(userId) {
    try {
      await this.ensureUserExists(userId);

      // Create new conversation
      const result = await db.query(
        'INSERT INTO conversations (user_id) VALUES ($1) RETURNING conversation_id',
        [userId]
      );

      return {
        conversationId: result.rows[0].conversation_id,
        message: 'Conversation started successfully',
      };
    } catch (error) {
      console.error('Error starting conversation:', error);
      throw error;
    }
  }
}

module.exports = new ChatService();
```

This chat service manages all our conversations and messages. There is a lot going on in this service, so let's break it down:

1. Creates or finds users in our database
1. Message Handling:
   - Saves messages from both users and the bot
   - Retrieves conversation history
1. Conversation Flow:
   - Starts new conversations
   - Processes incoming messages
   - Gets responses from Azure OpenAI
   - Stores everything in our Neon database

We are also using database transactions to make sure that all related data (messages, user info, and conversations) is saved correctly, with rollback support if anything fails. This helps maintain data consistency in our chat application.

For this service you can think of it as the coordinator between our database, the Azure AI model, and our chat interface which we'll build next.

### Chat API Routes Implementation

With our services in place, let's create the API routes that will handle incoming requests from our chat interface.

Create a `src/routes/chatRoutes.js` file with the following:

```javascript
// src/routes/chatRoutes.js
const express = require('express');
const router = express.Router();
const chatService = require('../services/chatService');

// Initialize or continue a chat session
router.post('/start', async (req, res) => {
  const { userId } = req.body;
  try {
    const result = await chatService.startConversation(userId);
    res.json(result);
  } catch (error) {
    console.error('Error starting conversation:', error);
    res.status(500).json({ error: 'Failed to start conversation' });
  }
});

// Send a message and get a response
router.post('/message', async (req, res) => {
  const { userId, message } = req.body;
  try {
    const result = await chatService.processMessage(userId, message);
    res.json(result);
  } catch (error) {
    console.error('Error processing message:', error);
    res.status(500).json({ error: 'Failed to process message' });
  }
});

// Get conversation history
router.get('/history/:conversationId', async (req, res) => {
  const { conversationId } = req.params;
  try {
    const history = await chatService.getConversationHistory(conversationId);
    res.json(history.rows);
  } catch (error) {
    console.error('Error fetching history:', error);
    res.status(500).json({ error: 'Failed to fetch conversation history' });
  }
});

module.exports = router;
```

The above are our API endpoints that our chat interface will use to communicate with the backend. We set up three main routes:

1. `/start`: Creates a new conversation for a user
2. `/message`: Handles sending messages and getting responses from the bot
3. `/history`: Retrieves past messages from a conversation

Each route connects to our chat service to perform its specific task. We'll use these routes to build our chat interface in the next section.

### Server Setup

Finally, let's set up our Express server to run our chatbot API. We'll also add a health check endpoint and error handling middleware.

Create `server.js` in your project root with the following content:

```javascript
// server.js
const express = require('express');
const cors = require('cors');
const dotenv = require('dotenv');
const chatRoutes = require('./src/routes/chatRoutes');

// Load environment variables
dotenv.config();

const app = express();

// Middleware
app.use(cors());
app.use(express.json());

// Routes
app.use('/api/chat', chatRoutes);

// Health check endpoint
app.get('/health', (req, res) => {
  res.json({ status: 'ok', timestamp: new Date().toISOString() });
});

// Error handling middleware
app.use((err, req, res, next) => {
  console.error(err.stack);
  res.status(500).json({
    error: 'Internal Server Error',
    message: process.env.NODE_ENV === 'development' ? err.message : undefined,
  });
});

const PORT = process.env.PORT || 3000;
app.listen(PORT, () => {
  console.log(`Server running on port ${PORT}`);
  console.log(`Environment: ${process.env.NODE_ENV}`);
});
```

The above is our main application file that brings everything together. It sets up an Express server with:

- CORS support to allow frontend access
- JSON parsing for API requests
- Our chat routes at `/api/chat`
- A health check endpoint to monitor the server

We also include an error handling middleware to catch any unhandled exceptions and log them to the console for easier debugging.

### Running the Application

Starting the server is straightforward - just run `node server.js`. Once started, the server will:

- Connect to your Neon database
- Listen for chat requests
- Be ready to handle messages from the chat interface

You can now send requests to `http://localhost:3000/api/chat` (or whichever port you configured) to interact with your chatbot.

## Creating the React Frontend

With our backend API ready, let's create a React frontend for our chatbot using Tailwind CSS for styling. We'll use TypeScript for type safety and Vite for faster development.

### Create React Project

First, let's create a new React project using Vite:

```bash
npm create vite@latest chatbot-frontend -- --template react-ts
```

Then navigate to project directory:

```bash
cd chatbot-frontend
```

And install the dependencies for the project:

```bash
npm install
```

After that, let's install the necessary packages for our chatbot interface such as Tailwind CSS and Axios:

```bash
npm install -D tailwindcss postcss autoprefixer
```

Also, let's install some additional utilities for our chatbot interface like clsx, Heroicons, and date-fns:

```
npm install clsx @heroicons/react date-fns
```

The `clsx` package is used to conditionally apply CSS classes, `@heroicons/react` provides SVG icons, and `date-fns` helps with date formatting. Those are not required but will make our chat interface a bit more user-friendly.

### Configure Tailwind CSS

With Tailwind CSS installed, let's set it up in our project. Start by initializing a Tailwind CSS:

```bash
npx tailwindcss init -p
```

This will create a `tailwind.config.js` file in your project root. Update the file with the following configuration:

```javascript
/** @type {import('tailwindcss').Config} */
export default {
  content: ['./index.html', './src/**/*.{js,ts,jsx,tsx}'],
  theme: {
    extend: {
      colors: {
        chatbot: {
          primary: '#0EA5E9',
          secondary: '#0284C7',
          accent: '#38BDF8',
          background: '#F0F9FF',
        },
      },
    },
  },
  plugins: [],
};
```

The above configuration extends the default Tailwind theme with custom colors for our chatbot interface and also specifies the content files to process.

After that, add the Tailwind directives to the `src/index.css` file:

```css
@tailwind base;
@tailwind components;
@tailwind utilities;

@layer base {
  body {
    @apply bg-chatbot-background;
  }
}
```

This will apply the Tailwind CSS styles to our project, so we can use them in our components.

### Create Environment Configuration

Our chatbot interface will need to connect to the backend API to send and receive messages. Let's set up the API URL in our environment configuration.

Create `.env` file in project root:

```env
VITE_API_URL=http://localhost:3000/api
```

Make sure to replace the `VITE_API_URL` with the actual URL of your backend API. This will allow our chatbot interface to communicate with the backend application.

### Project Structure

For our chat interface, let's organize our React components into a maintainable structure:

- `components/Chat`: Contains all chat-related components like message bubbles and input fields
- `components/Layout`: Holds reusable layout components
- `hooks`: Stores custom React hooks for managing chat functionality
- `types`: Defines TypeScript interfaces for our chat data

This structure will allow us to separate our code into logical pieces, so that it will be easier to find and update specific parts of the application.

The project structure will look like this:

```
src/
├── components/
│   ├── Chat/
│   │   ├── ChatBubble.tsx
│   │   ├── ChatInput.tsx
│   │   └── ChatInterface.tsx
│   └── Layout/
│       └── Container.tsx
├── hooks/
│   └── useChat.ts
└── types/
    └── chat.ts
```

With our project structure in place, let's start building our chatbot interface. We will create the components for our chat interface starting with the types and basic components, then bringing it all together.

### 1. Define Message Types

First, let's define TypeScript types for our chat messages:

```typescript
// src/types/chat.ts
export interface Message {
  sender: 'user' | 'bot';
  content: string;
  timestamp?: Date;
}
```

This defines a `Message` interface that tracks:

- Who sent the message (`sender`)
- Message content (`content`)
- When it was sent (`timestamp`)

We'll use this type to manage chat messages in our application.

### 2. Create the Layout Container

Next, let's create a container component which will provide a consistent spacing and width for our chat interface:

```typescript
// src/components/Layout/Container.tsx
export const Container = ({ children }: { children: React.ReactNode }) => (
  <div className="max-w-4xl mx-auto px-4 py-8 min-h-screen">
    {children}
  </div>
);
```

The container is a simple component that wraps all our chat components in a centered, responsive layout.

### 3. Build the Message Bubble Component

Each chat message will be displayed as a bubble with different styles for user and bot messages:

```typescript
// src/components/Chat/ChatBubble.tsx
import { Message } from '../../types/chat';
import { format } from 'date-fns';
import clsx from 'clsx';

interface ChatBubbleProps {
  message: Message;
}

export const ChatBubble = ({ message }: ChatBubbleProps) => {
  const isUser = message.sender === 'user';

  return (
    <div
      className={clsx(
        'flex w-full mt-2 space-x-3 max-w-xs',
        isUser ? 'ml-auto justify-end' : ''
      )}
    >
      <div>
        <div
          className={clsx(
            'p-3 rounded-lg',
            isUser
              ? 'bg-chatbot-primary text-white'
              : 'bg-white text-gray-800 border border-gray-200'
          )}
        >
          <p className="text-sm">{message.content}</p>
        </div>
        {message.timestamp && (
          <span className="text-xs text-gray-500 leading-none">
            {format(message.timestamp, 'HH:mm')}
          </span>
        )}
      </div>
    </div>
  );
};
```

The chat bubble component:

- Takes a `message` prop with sender and content
- Uses different styles for user vs bot messages
- Shows message timestamp and aligns user messages to the right, bot messages to the left

We are going to use this component to render chat messages in the chat interface.

### 4. Create the Message Input Component

Next, let's build an input field for users to type messages and a submit button:

```typescript
// src/components/Chat/ChatInput.tsx
import { PaperAirplaneIcon } from '@heroicons/react/24/solid';
import clsx from 'clsx';

interface ChatInputProps {
  value: string;
  onChange: (value: string) => void;
  onSubmit: () => void;
  isLoading?: boolean;
}

export const ChatInput = ({ value, onChange, onSubmit, isLoading }: ChatInputProps) => {
  return (
    <div className="border-t bg-white p-4">
      <form
        className="flex space-x-4"
        onSubmit={(e) => {
          e.preventDefault();
          onSubmit();
        }}
      >
        <input
          type="text"
          value={value}
          onChange={(e) => onChange(e.target.value)}
          placeholder="Type your message..."
          className="flex-1 border border-gray-300 rounded-lg px-4 py-2 focus:outline-none focus:ring-2 focus:ring-chatbot-primary"
        />
        <button
          type="submit"
          disabled={isLoading}
          className={clsx(
            'rounded-lg px-4 py-2 text-white',
            isLoading
              ? 'bg-gray-400 cursor-not-allowed'
              : 'bg-chatbot-primary hover:bg-chatbot-secondary'
          )}
        >
          <PaperAirplaneIcon className="h-5 w-5" />
        </button>
      </form>
    </div>
  );
};
```

This component includes:

- A text input field for messages
- A submit button with loading state
- A simple form handling with `preventDefault()`

The input field will allow users to type messages and submit them to the chatbot over the Azure OpenAI API that we set up earlier.

### 5. Create the Chat Hook

After building the basic components, let's create a custom hook to manage chat state and interactions. This hook will handle sending messages, loading states, and API calls to the backend:

```typescript
// src/hooks/useChat.ts
import { useState, useCallback } from 'react';
import { Message } from '../types/chat';

export const useChat = () => {
  const [messages, setMessages] = useState<Message[]>([]);
  const [isLoading, setIsLoading] = useState(false);
  const userId = useState(() => 'user-' + Date.now())[0];

  const sendMessage = useCallback(
    async (content: string) => {
      try {
        setIsLoading(true);
        const userMessage: Message = {
          sender: 'user',
          content,
          timestamp: new Date(),
        };
        setMessages((prev) => [...prev, userMessage]);

        const response = await fetch(`${import.meta.env.VITE_API_URL}/chat/message`, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ userId, message: content }),
        });

        const data = await response.json();

        const botMessage: Message = {
          sender: 'bot',
          content: data.reply,
          timestamp: new Date(),
        };
        setMessages((prev) => [...prev, botMessage]);
      } catch (error) {
        console.error('Error sending message:', error);
        const errorMessage: Message = {
          sender: 'bot',
          content: 'Sorry, I encountered an error. Please try again.',
          timestamp: new Date(),
        };
        setMessages((prev) => [...prev, errorMessage]);
      } finally {
        setIsLoading(false);
      }
    },
    [userId]
  );

  return { messages, isLoading, sendMessage };
};
```

The hook handles:

- Message state management
- API calls to the backend
- Error handling with user feedback

This hook will be used in the main chat interface component to manage chat interactions and state updates.

### 6. Build the Main Chat Interface

Finally, let's combine everything into the main chat interface component:

```typescript
// src/components/Chat/ChatInterface.tsx
import { useState } from 'react';
import { ChatBubble } from './ChatBubble';
import { ChatInput } from './ChatInput';
import { useChat } from '../../hooks/useChat';

export const ChatInterface = () => {
  const [input, setInput] = useState('');
  const { messages, isLoading, sendMessage } = useChat();

  const handleSubmit = async () => {
    if (!input.trim()) return;
    const message = input;
    setInput('');
    await sendMessage(message);
  };

  return (
    <div className="flex flex-col h-[600px] bg-white rounded-lg shadow-lg">
      <div className="p-4 border-b bg-chatbot-primary text-white">
        <h2 className="text-xl font-bold">Neon AI Assistant</h2>
      </div>

      <div className="flex-1 overflow-y-auto p-4 space-y-4 bg-gray-50">
        {messages.map((message, idx) => (
          <ChatBubble key={idx} message={message} />
        ))}
      </div>

      <ChatInput
        value={input}
        onChange={setInput}
        onSubmit={handleSubmit}
        isLoading={isLoading}
      />
    </div>
  );
};
```

The main interface component:

- Uses our chat hook for state management
- Renders message history with `ChatBubble` components
- Handles message input with `ChatInput` component

This component will display the chat interface with message bubbles, input field, and submit button for users to interact with the chatbot.

### 7. Update App Component

Finally, we can update the main App component to use our chat interface and wrap it in a container for layout:

```typescript
// src/App.tsx
import { Container } from './components/Layout/Container';
import { ChatInterface } from './components/Chat/ChatInterface';

function App() {
  return (
    <Container>
      <ChatInterface />
    </Container>
  );
}

export default App;
```

This wraps our chat interface in the container component for proper layout and spacing.

You can now start the development server to see your chat interface:

```bash
npm run dev
```

Visit `http://localhost:5173` to test the chatbot interface. It will automatically connect to your backend API running on port 3000. Make sure your backend server is running before testing the chat interface.

## Conclusion

In this guide, we've built a simple AI-powered chatbot widget that combines Azure AI Studio with Neon's serverless Postgres database. This implementation works well for documentation websites and help systems, where the chatbot can be embedded as a widget to provide immediate assistance to users.

When the Azure OpenAI model is trained on your specific documentation or knowledge base, the chatbot can provide accurate, relevant responses about your product or service. This creates a seamless experience for anonymous users who can get quick answers without searching through documentation.

Also, by capturing chat interactions, user queries, bot responses, and feedback in your database, you can analyze where users face challenges and identify areas for documentation improvement.

As a next step, you can further train your Azure OpenAI model with more specific data to improve its accuracy and relevance and extend the chatbot's functionality to handle more complex queries and tasks.

## Additional Resources

- [Azure Bot Service Documentation](https://docs.microsoft.com/en-us/azure/bot-service/)
- [LUIS Documentation](https://docs.microsoft.com/en-us/azure/cognitive-services/luis/)
- [Neon Documentation](/docs)
- [Bot Framework SDK](https://learn.microsoft.com/en-us/azure/bot-service/index-bf-sdk)

<NeedHelp />


# Sentiment Analysis with Azure AI Services and Neon

---
title: Sentiment Analysis with Azure AI Services and Neon
subtitle: Learn how to analyze customer feedback using Azure AI Language and store results in Neon Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-11-30T00:00:00.000Z'
updatedOn: '2024-11-30T00:00:00.000Z'
---

Analyzing customer sentiment can help you understand your customer satisfaction and identify areas for improvement. The Azure AI Language Services provide tools for sentiment analysis, key phrase extraction, and language detection which can be used to analyze customer feedback and extract valuable insights.

In this guide, you'll learn how to use Azure AI Language Services to analyze customer feedback and save the results in Neon Postgres. We'll go through setting up your environment, creating a database to store feedback and analysis results, and running the analysis to get useful insights.

## Prerequisites

- An [Azure account](https://azure.microsoft.com/free/) with an active subscription
- A [Neon account](https://console.neon.tech/signup) and project
- Node.js 18.x or later
- Basic familiarity with SQL and JavaScript

## Setting Up Your Development Environment

If you haven't already, follow these steps to set up your development environment:

### Create a Neon Project

1. Navigate to the [Neon Console](https://console.neon.tech)
2. Click "New Project"
3. Select Azure as your cloud provider
4. Choose East US 2 as your region
5. Name your project (e.g., "sentiment-analysis")
6. Click "Create Project"

Save your connection details, you'll need them later to connect to your Neon database.

### Create Database Schema

Next, you'll set up the database tables to store customer feedback and sentiment analysis results. These tables will hold the feedback text, analysis scores, sentiment labels, key phrases, and timestamps.

Connect to your Neon database and create tables for storing our customer feedback and sentiment analysis results from the Azure AI Language service:

```sql
CREATE TABLE customer_feedback (
    feedback_id SERIAL PRIMARY KEY,
    customer_id VARCHAR(50),
    feedback_text TEXT NOT NULL,
    product_id VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE sentiment_results (
    result_id SERIAL PRIMARY KEY,
    feedback_id INTEGER REFERENCES customer_feedback(feedback_id),
    sentiment_score DECIMAL(4,3),
    sentiment_label VARCHAR(20),
    key_phrases TEXT[],
    language_code VARCHAR(10),
    analyzed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

With the tables created, let's move on to setting up the Azure AI Language service.

### Set Up Azure AI Language

The [Azure AI Language service](https://learn.microsoft.com/en-us/azure/ai-services/language-service/overview) provides a set of tools for text analytics, including sentiment analysis, key phrase extraction, and language detection. To use the service, you'll need to create a new Language Service resource in your Azure account.

1. Go to the [Azure portal](https://portal.azure.com/)
1. Search for "Azure AI Services" in the search bar
1. From the list of services, select "Language Service"
1. Click the "Create" button to create a new Language Service resource
1. Select your subscription and resource group
1. Choose a region (East US 2 for proximity to Neon)
1. Select a pricing tier (Free tier for testing)
1. Create the resource
1. Once created, copy the endpoint URL and access key

### Project Setup

For the sake of this guide, we'll create a Node.js script that analyzes existing feedback stored in the Neon database. In a real-world app, you could integrate this process directly so that whenever a user posts a review, a queued job or a scheduled task automatically analyzes the feedback right away.

Let's start by creating a new Node.js project:

```bash
mkdir sentiment-analysis
cd sentiment-analysis
npm init -y
```

After creating the project, install the required packages:

```bash
npm install @azure/ai-language-text pg dotenv
```

The packages we're using are:

- `@azure/ai-language-text`: [Azure AI Language client library for JavaScript](https://www.npmjs.com/package/@azure/ai-language-text), this will allow us to interact with the Azure AI Language service to analyze text instead of using the REST API directly.
- `pg`: A PostgreSQL client for Node.js, this will allow us to connect to the Neon database and store the analysis results.
- `dotenv`: A package for loading environment variables from a `.env` file.

With the packages installed, create a `.env` file in the project root and add your Azure AI Language service key and endpoint, as well as your Neon database connection URL:

```env
AZURE_LANGUAGE_KEY=your_key_here
AZURE_LANGUAGE_ENDPOINT=your_endpoint_here
DATABASE_URL=postgres://user:password@your-neon-host.cloud/dbname
```

Change the `DATABASE_URL` to match your Neon database connection details. Also, replace `your_key_here` and `your_endpoint_here` with your Azure AI Language service key and endpoint which you can find in the Azure portal under your Language Service resource.

## Implementation

With everything set up, let's start implementing the sentiment analysis script. We'll create separate modules for database connection, Azure AI Language client, analysis script, and report generation.

### Database Connection

In this step, we'll set up a connection to our Neon Postgres database using the `pg` package. This connection will allow you to read customer feedback and store sentiment analysis results whenever the analysis script runs.

We'll use a connection pool to manage multiple database connections efficiently, which is especially useful when running scripts that perform multiple queries.

Create a new file `src/database.js` and add the following code:

```javascript
const { Pool } = require('pg');
require('dotenv').config();

const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  ssl: true,
});

module.exports = pool;
```

The `pool` object will be used to connect to the database and execute queries. We'll get the connection details from the `.env` file using the `dotenv` package.

### Azure AI Language Client

Now let's set up the Azure AI Language client to perform our sentiment analysis and extract key phrases from customer feedback stored in our Neon database. Here is where we'll use the `@azure/ai-language-text` package to interact with the Azure AI Language service.

Create a new file `src/textAnalytics.js` and add the following code:

```javascript
const { TextAnalysisClient, AzureKeyCredential } = require('@azure/ai-language-text');
require('dotenv').config();

const client = new TextAnalysisClient(
  process.env.AZURE_LANGUAGE_ENDPOINT,
  new AzureKeyCredential(process.env.AZURE_LANGUAGE_KEY)
);

async function analyzeSentiment(text) {
  const actions = [
    {
      kind: 'SentimentAnalysis',
    },
  ];

  const [result] = await client.analyze('SentimentAnalysis', [text]);

  return {
    score: result.confidenceScores[result.sentiment],
    label: result.sentiment,
    language: result.language,
  };
}

async function extractKeyPhrases(text) {
  const [result] = await client.analyze('KeyPhraseExtraction', [text]);
  return result.keyPhrases;
}

module.exports = {
  analyzeSentiment,
  extractKeyPhrases,
};
```

A quick overview of what we've done here:

- **Azure Client Setup**: We create a `TextAnalysisClient` using the Azure endpoint and API key from the environment variables. This client handles communication with the Azure AI Language service.
- **`analyzeSentiment` Function**: Analyzes the sentiment of the provided text and returns the sentiment label (positive, negative, mixed, or neutral), the sentiment score, and the detected language.
- **`extractKeyPhrases` Function**: Extracts key phrases from the given text, helping identify the main topics or themes.

Now that the Azure AI Language client is ready, let's move on to the main analysis script.

### Main Analysis Script

Now that we have our database connection and Azure AI Language client set up, let's create a script to process customer feedback, analyze it, and store the results in our Neon database.

Create a new file `src/analyze.js` and add the following code:

```javascript
const db = require('./database');
const { analyzeSentiment, extractKeyPhrases } = require('./textAnalytics');

async function processFeedback() {
  const client = await db.connect();

  try {
    await client.query('BEGIN');

    // Get unanalyzed feedback
    const feedbackResult = await client.query(`
      SELECT f.feedback_id, f.feedback_text 
      FROM customer_feedback f
      LEFT JOIN sentiment_results s ON f.feedback_id = s.feedback_id
      WHERE s.feedback_id IS NULL
    `);

    for (const row of feedbackResult.rows) {
      // Analyze sentiment and extract key phrases
      const [sentiment, keyPhrases] = await Promise.all([
        analyzeSentiment(row.feedback_text),
        extractKeyPhrases(row.feedback_text),
      ]);

      // Store results in the sentiment_results table
      await client.query(
        `
        INSERT INTO sentiment_results 
        (feedback_id, sentiment_score, sentiment_label, key_phrases, language_code)
        VALUES ($1, $2, $3, $4, $5)
      `,
        [row.feedback_id, sentiment.score, sentiment.label, keyPhrases, sentiment.language]
      );
    }

    await client.query('COMMIT');
    console.log(`Processed ${feedbackResult.rows.length} feedback items`);
  } catch (err) {
    await client.query('ROLLBACK');
    throw err;
  } finally {
    client.release();
  }
}

module.exports = { processFeedback };
```

Here we've implemented the following:

1. We start by fetching the customer feedback that hasn't been analyzed yet. We do this by selecting feedback entries that don't have corresponding sentiment analysis results in the `sentiment_results` table.
1. Next, for each feedback entry, it uses the `analyzeSentiment` function to get the sentiment and the `extractKeyPhrases` function to identify key phrases. These operations are performed in parallel using `Promise.all` to speed up the process.
1. After that, we insert the sentiment score, sentiment label, key phrases, and detected language into the `sentiment_results` table.

It is worth mentioning that, we are also using a database transaction (`BEGIN`, `COMMIT`, and `ROLLBACK`) to ensure data integrity. If an error occurs, changes are rolled back.

We can use this script and run it periodically or triggered whenever new feedback is received to keep the sentiment analysis up-to-date.

### Analysis Reports

With the sentiment analysis results stored in the database, we can generate reports to extract insights from the data. Let's create a module to generate sentiment analysis reports based on the stored results.

Create a new file `src/reports.js` and add the following code:

```javascript
const db = require('./database');

async function generateSentimentReport() {
  const client = await db.connect();

  try {
    // Overall sentiment distribution
    const sentimentDist = await client.query(`
      SELECT
        sentiment_label,
        COUNT(*) as count,
        AVG(sentiment_score) as avg_score
      FROM sentiment_results
      GROUP BY sentiment_label
    `);

    // Trending negative feedback topics
    const negativeTopics = await client.query(`
      SELECT
        UNNEST(key_phrases) as topic,
        COUNT(*) as mentions
      FROM sentiment_results
      WHERE sentiment_label = 'negative'
      GROUP BY topic
      ORDER BY mentions DESC
      LIMIT 10
    `);

    return {
      sentimentDistribution: sentimentDist.rows,
      topNegativeTopics: negativeTopics.rows,
    };
  } finally {
    client.release();
  }
}

module.exports = { generateSentimentReport };
```

Here we've defined two main functions that generate two main reports:

- **Sentiment Distribution**: This report shows the count and average sentiment score for each sentiment label (positive, negative, mixed, neutral).
- **Top Negative Topics**: This report lists the most common key phrases in negative feedback, helping identify recurring issues or topics that need attention.

These reports can be used to track customer sentiment trends, identify common complaints, and prioritize areas for improvement. For example, you can set up alerts like sending an email or a Slack message whenever the sentiment score drops below a certain threshold or when a specific topic is mentioned frequently.

## Running the Analysis

To put it all together, we'll create a script that processes customer feedback, analyzes it using Azure AI Language, and generates reports to summarize the insights.

Create a new file `index.js` and add the following code:

```javascript
const { processFeedback } = require('./src/analyze');
const { generateSentimentReport } = require('./src/reports');

async function main() {
  try {
    // Process new feedback
    await processFeedback();

    // Generate reports
    const report = await generateSentimentReport();
    console.log('Sentiment Distribution:', report.sentimentDistribution);
    console.log('Top Negative Topics:', report.topNegativeTopics);
  } catch (err) {
    console.error('Error:', err);
    process.exit(1);
  }
}

main();
```

This script will run the sentiment analysis on the customer feedback stored in the Neon database and generate reports based on the analysis results. You can run this script manually or set up a scheduled job to run it periodically.

The script can be extended to include additional reports, alerts, or integrations with other services based on the sentiment analysis results but for now, let's focus on running the analysis.

## Testing the Analysis

As a final step, let's test the sentiment analysis script by adding some sample customer feedback to the database and running the analysis script.

If you haven't already, create a few sample feedback entries in the `customer_feedback` table which we can use for testing. Here's an example SQL script to insert sample feedback with different sentiment labels:

```sql
INSERT INTO customer_feedback (customer_id, feedback_text, product_id) VALUES
-- Positive feedback
('CUST001', 'The new dashboard design is fantastic! Much easier to find everything I need and the analytics features are exactly what we were looking for.', 'SAAS-DASH'),
('CUST002', 'Your customer support team is incredible. Had an issue with our API integration and they helped us resolve it within minutes.', 'SAAS-SUPPORT'),
('CUST003', 'The automated reporting feature saves me hours every week. Best investment we''ve made this year for our analytics stack!', 'SAAS-REPORT'),

-- Mixed feedback
('CUST004', 'Love most of the new features, but the export functionality is a bit limited. Would love to see more format options.', 'SAAS-EXPORT'),
('CUST005', 'Great platform overall, though it can be slow during peak hours. The UI is intuitive but some advanced features are hard to find.', 'SAAS-PERF'),
('CUST006', 'Good value for enterprise plan, but smaller teams might find it pricey. The collaboration features make it worth it for us.', 'SAAS-PRICE'),

-- Negative feedback
('CUST007', 'Been experiencing frequent timeouts for the past week. Our team''s productivity has taken a hit and support hasn''t provided a clear timeline for resolution.', 'SAAS-PERF'),
('CUST008', 'The recent UI update is a massive step backward. Can''t find basic features anymore and my team is frustrated.', 'SAAS-UI'),
('CUST009', 'Pricing increased by 40% with barely any notice. Considering switching to alternatives as this no longer fits our budget.', 'SAAS-PRICE'),

-- Detailed feedback
('CUST010', 'We''ve been using the platform for 6 months now. The API documentation is comprehensive and integration was smooth. The webhook reliability has been perfect, and the custom event tracking is powerful. Only wish the dashboard had more customization options.', 'SAAS-API'),
('CUST011', 'While the data visualization options are powerful, the learning curve is steep. Took our team weeks to fully understand all features. Once mastered though, the insights we get are invaluable.', 'SAAS-VIS'),
('CUST012', 'The collaboration features are game-changing for our remote team. Real-time editing, commenting, and version control work flawlessly. However, the mobile app needs improvement.', 'SAAS-COLLAB'),

-- Technical feedback
('CUST013', 'REST API rate limits are too restrictive on the growth plan. Had to upgrade just for the API limits even though we don''t need the other enterprise features.', 'SAAS-API'),
('CUST014', 'SSO integration was unnecessarily complicated. Documentation is outdated and support couldn''t help with our specific Azure AD setup.', 'SAAS-AUTH'),
('CUST015', 'The new GraphQL API is amazing! Much more efficient than the REST endpoints. Query performance improved our app''s load time significantly.', 'SAAS-API');
```

The feedback entries include a mix of positive, mixed, and negative sentiments, as well as some feedback to test the sentiment analysis and key phrase extraction.

After running the SQL script to insert the sample feedback, let's run the sentiment analysis script:

```bash
node index.js
```

Check the console output for sentiment analysis results and reports:

```shell
Processed 15 feedback items
Sentiment Distribution: [
  {
    sentiment_label: 'negative',
    count: '4',
    avg_score: '0.84500000000000000000'
  },
  {
    sentiment_label: 'positive',
    count: '9',
    avg_score: '0.96777777777777777778'
  },
  { sentiment_label: 'mixed', count: '1', avg_score: null },
  {
    sentiment_label: 'neutral',
    count: '1',
    avg_score: '0.88000000000000000000'
  }
]
Top Negative Topics: [
  { topic: 'support', mentions: '2' },
  { topic: 'team', mentions: '2' },
  { topic: 'recent UI update', mentions: '1' },
  { topic: 'past week', mentions: '1' },
  { topic: 'clear timeline', mentions: '1' },
  { topic: 'SSO integration', mentions: '1' },
  { topic: 'productivity', mentions: '1' },
  { topic: 'Pricing', mentions: '1' },
  { topic: 'resolution', mentions: '1' },
  { topic: 'hit', mentions: '1' }
]
```

This output gives you a snapshot of customer sentiment and highlights recurring issues which can help you identify areas for improvement.

## Automating the Analysis

While running the sentiment analysis manually is useful for testing, in a production environment you'll want to automate this process.

One option is to integrate the sentiment analysis code into your application, so it runs whenever new feedback is submitted.

Alternatively, you can use a scheduled task to process feedback at regular intervals. For example, you could create an Azure Function that runs every few hours to analyze the new feedback and generate reports.

1. If you haven't already, follow the [Azure Functions Quickstart guide](https://learn.microsoft.com/azure/azure-functions/functions-create-first-function) to set up your development environment.

2. Create a new Azure Function with a timer trigger. The schedule expression `0 0 */2 * * *` will run the function every two hours.

3. Replace the default function code with the following to process the feedback from your Neon Postgres database:

   ```javascript
   const { processFeedback } = require('./src/analyze');

   module.exports = async function (context, myTimer) {
     const timeStamp = new Date().toISOString();

     if (myTimer.isPastDue) {
       context.log('Timer function is running late!');
     }

     try {
       await processFeedback();
       context.log('Sentiment analysis completed successfully:', timeStamp);
     } catch (err) {
       context.log.error('Error processing feedback:', err);
       throw err;
     }
   };
   ```

The timer schedule is defined in the `function.json` file as follows:

```json
{
  "bindings": [
    {
      "name": "sentimentAnalysisTrigger",
      "type": "timerTrigger",
      "direction": "in",
      "schedule": "0 0 */2 * * *"
    }
  ]
}
```

This configuration makes sure that the function runs every two hours. You can adjust the schedule as needed using a [cron expression](https://learn.microsoft.com/en-gb/azure/azure-functions/functions-bindings-timer).

For more details on connecting Azure Functions to a Postgres database and deploying the function to Azure, see the [Building a Serverless Referral System with Neon Postgres and Azure Functions](/guides/azure-functions-referral-system) guide.

## Analyzing Results

Now that you've processed customer feedback and stored the sentiment analysis results, you can run SQL queries to extract insights from the data. Here are some example queries to get you started:

1. This query shows how sentiment varies over time, giving you a sense of customer satisfaction trends:

   ```sql
   SELECT
       DATE_TRUNC('day', cf.created_at) AS date,
       AVG(sr.sentiment_score) AS avg_sentiment,
       COUNT(*) AS feedback_count
   FROM customer_feedback cf
   JOIN sentiment_results sr ON cf.feedback_id = sr.feedback_id
   GROUP BY date
   ORDER BY date;
   ```

2. Identify the most mentioned topics in negative feedback to spot recurring issues:

   ```sql
   SELECT
       kp AS topic,
       COUNT(*) AS mentions
   FROM sentiment_results sr,
        UNNEST(key_phrases) kp
   WHERE sentiment_label = 'negative'
   GROUP BY kp
   ORDER BY mentions DESC
   LIMIT 10;
   ```

## Conclusion

In this guide, we covered how to analyze customer feedback using Azure AI Language Services and store the results in a Neon Postgres database. This setup is just a starting point. You can expand it by adding real-time triggers, building dashboards, or supporting multiple languages.

The Azure AI Language service also includes SDKs for other languages like [Python](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-textanalytics-readme), [Java](https://learn.microsoft.com/en-us/java/api/overview/azure/ai-textanalytics-readme), and [.NET](https://learn.microsoft.com/en-us/dotnet/api/overview/azure/ai.textanalytics-readme), so you can integrate sentiment analysis into your existing applications.

You can extend this system by adding more analysis, visualizations, or multi-language support based on your needs.

## Additional Resources

- [Azure AI Language Documentation](https://learn.microsoft.com/azure/ai-services/language-service/)
- [Neon Documentation](/docs)
- [Azure AI Language Client Library](https://learn.microsoft.com/javascript/api/overview/azure/ai-language-text-readme)

<NeedHelp />


# Full-Text Search with Neon and Azure AI Search

---
title: Full-Text Search with Neon and Azure AI Search
subtitle: Build a powerful hybrid search system for developer resources with Neon and Azure AI Search
author: bobbyiliev
enableTableOfContents: true
createdAt: '2025-01-05T00:00:00.000Z'
updatedOn: '2025-01-05T00:00:00.000Z'
---

In this guide you will learn how to implement a hybrid search functionality using Neon and Azure AI Search.

For the purpose of this guide, we will create a Node.js application that will allow you to search through your database. We will use the Azure SDK to interact with Azure AI Search. The application itself will represent a developer learning platform, which contains resources such as tutorials, cheat sheets, videos, and interactive code examples.

We will be using Neon for efficient full-text search and Azure AI Search for the AI-driven capabilities.

### What You'll Learn

- Configure Neon for full-text search
- Set up Azure AI Search and use the Azure SDK
- Implement hybrid search combining both systems
- Build a Node.js service to handle search requests

---

## Prerequisites

To follow this guide, ensure you have:

- A [Neon account](https://console.neon.tech/signup) with an active project.
- An [Azure account](https://azure.microsoft.com/free/) with Azure AI Search enabled.
- [Node.js](https://nodejs.org/) 18.x or later.
- Basic knowledge of SQL and JavaScript.

---

## Step 1: Set Up Neon for Full-Text Search

Let's start by creating our database schema and configuring Neon for [full-text search](/guides/full-text-search).

We'll create a schema to store and organize developer learning resources. The schema will include tables for technologies and tutorials, which will allow us to have an efficient categorization and search functionalities. We'll also set up a GIN index for full-text search.

```sql
-- Technologies Table
CREATE TABLE technologies (
    tech_id SERIAL PRIMARY KEY,
    name TEXT NOT NULL,
    category TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Tutorials Table
CREATE TABLE tutorials (
    tutorial_id SERIAL PRIMARY KEY,
    title TEXT NOT NULL,
    content TEXT NOT NULL,
    tech_id INTEGER REFERENCES technologies(tech_id),
    tags TEXT[],
    difficulty_level TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- GIN Index for Full-Text Search
CREATE INDEX idx_tutorials_search ON tutorials
    USING GIN (to_tsvector('english', title || ' ' || content));
```

With the above schema, we will be able to store technologies and tutorials. The `tags` column will be used for categorization, and the `difficulty_level` column will help users filter resources based on their skill level.

The index here is using the `to_tsvector` function to create a GIN index for efficient full-text search. This index will allow us to search through the `title` and `content` columns of the `tutorials` table. For more information on full-text search in PostgreSQL, check out the [Neon Full-Text Search guide](/guides/full-text-search).

Next, you can insert developer resources into the `technologies` and `tutorials` tables.

```sql
-- Insert more Technologies
INSERT INTO technologies (name, category) VALUES ('React', 'Frontend');
INSERT INTO technologies (name, category) VALUES ('Python', 'Backend');

-- Insert Tutorials
INSERT INTO tutorials (title, content, tech_id, tags, difficulty_level)
VALUES ('Getting Started with Node.js', 'Learn the basics of Node.js', 1, ARRAY['Node.js', 'Backend'], 'Beginner');

INSERT INTO tutorials (title, content, tech_id, tags, difficulty_level)
VALUES ('Building a React App', 'Step-by-step guide to building a React application', 1, ARRAY['React', 'Frontend'], 'Intermediate');

INSERT INTO tutorials (title, content, tech_id, tags, difficulty_level)
VALUES ('Python for Data Science', 'Introduction to using Python for data science', 2, ARRAY['Python', 'Data Science'], 'Advanced');
```

Feel free to add more resources to the tables to test the search functionality.

## Step 2: Configure Azure AI Search

With the database schema set up, let's configure Azure AI Search to index and search through our developer resources.

If you haven't already, create an Azure account and enable Azure AI Search. Also, make sure you have the [Azure CLI installed](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli).

### Setting Up Azure AI Search

1. Verify your Azure CLI installation and login to your Azure account:

   ```bash
   az --version
   az login
   ```

1. You can start by creating an Azure AI Search Resource.
   You can reference the [latest Azure documentation](https://learn.microsoft.com/en-us/azure/search/search-create-service-portal) to create a search service. Use the "Basic" SKU for testing or scale up for production depending on your requirements.

   ```bash
   az search service create \
       --name developer-search \
       --resource-group <your-resource-group> \
       --sku Basic \
       --location eastus2
   ```

   This command creates a new Azure AI Search service named `developer-search` in the `eastus2` region. You might have to choose a different name based on availability. Also, make sure to replace `<your-resource-group>` with your Azure resource group as per the Azure documentation above.

   Alternatively, you can create the search service using the [Azure Portal](https://learn.microsoft.com/en-us/azure/search/search-create-service-portal).

1. To get your service endpoint, you can the Azure portal and [find your search service](https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/CognitiveServicesHub/%7E/CognitiveSearch).
   - Under the Overview section, copy the URL and save it for a later step.
   - Also grab the API key of your Azure AI Search service from the Azure Portal. In the 'Settings' > 'Keys' section, copy and save an admin key for full rights to create and delete objects. There are two interchangeable primary and secondary keys. Choose either one.

### Creating an Index

In Azure AI Search, an index is your searchable data. It defines the structure of your data and the fields you can search on. You can learn more about [indexing in Azure AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-an-index) in the official documentation.

There are a few ways to create an index in Azure AI Search. You can use the Azure Portal, Azure CLI, or the Azure SDK.

For a quick start, you can use the Azure Portal, to create an index schema.

1. [Find your search service](https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/CognitiveServicesHub/%7E/CognitiveSearch) and navigate to the "Indexes" section to create a new index:

1. Here's an example schema for the developer resources:

   ```json
   {
     "name": "developer-index",
     "fields": [
       { "name": "id", "type": "Edm.String", "key": true },
       { "name": "title", "type": "Edm.String", "searchable": true, "sortable": true },
       { "name": "description", "type": "Edm.String", "searchable": true },
       { "name": "tags", "type": "Collection(Edm.String)", "facetable": true },
       { "name": "type", "type": "Edm.String", "searchable": false, "filterable": true },
       { "name": "content", "type": "Edm.String", "searchable": true }
     ],
     "suggesters": [
       {
         "name": "suggester",
         "searchMode": "analyzingInfixMatching",
         "sourceFields": ["title", "description"]
       }
     ]
   }
   ```

   The above schema defines an index named `developer-index` with fields for `id`, `title`, `description`, `tags`, `type`, and `content`. The `suggesters` section enables auto-suggestions based on the `title` and `description` fields.

   We will reference the `developer-index` index in our Node.js application to query the Azure AI Search service so if you choose a different name, make sure to update the application accordingly.

Rather than creating the index manually, you can also use the Azure SDK as well.

1. First, install the Azure SDK for JavaScript:

   ```bash
   npm install @azure/search-documents
   ```

1. Then, initialize the search client:

   ```js
   const { SearchIndexClient, AzureKeyCredential } = require('@azure/search-documents');

   const endpoint = 'YOUR_SEARCH_ENDPOINT';
   const apiKey = 'YOUR_ADMIN_API_KEY';
   const indexName = 'developer-index';

   const searchClient = new SearchIndexClient(endpoint, new AzureKeyCredential(apiKey));
   ```

1. Create an index schema that defines the structure of your searchable content:

   ```js
   const indexDefinition = {
     name: 'developer-index',
     fields: [
       {
         name: 'id',
         type: 'Edm.String',
         key: true,
         searchable: false,
       },
       {
         name: 'title',
         type: 'Edm.String',
         searchable: true,
         filterable: true,
         sortable: true,
       },
       {
         name: 'content',
         type: 'Edm.String',
         searchable: true,
         filterable: false,
       },
       {
         name: 'tags',
         type: 'Collection(Edm.String)',
         searchable: true,
         filterable: true,
         facetable: true,
       },
       {
         name: 'category',
         type: 'Edm.String',
         searchable: true,
         filterable: true,
         facetable: true,
       },
     ],
     suggesters: [
       {
         name: 'sg',
         searchMode: 'analyzingInfixMatching',
         sourceFields: ['title'],
       },
     ],
   };

   async function createSearchIndex() {
     try {
       await searchClient.createIndex(indexDefinition);
       console.log(`Index ${indexName} created successfully`);
     } catch (error) {
       console.error('Error creating index:', error);
     }
   }
   ```

You can use the `createSearchIndex` function to create the index schema in Azure AI Search. Note that you need to replace `YOUR_SEARCH_ENDPOINT` and `YOUR_ADMIN_API_KEY` with your Azure AI Search service endpoint and admin API key.

Run the function to create the index schema in Azure AI Search. You can verify the index creation in the Azure Portal under the "Indexes" section of your search service.

### Adding Data to the Index

As of the time of writing, Azure AI Search does not support direct indexing from PostgreSQL, but to populate your index, you can use the Azure Search REST API to upload data to Azure AI Search or use the Azure SDK as well.

Alternatively, you can export data from PostgreSQL to [Azure Blob Storage](https://learn.microsoft.com/en-us/azure/search/search-blob-storage-integration) and then add the blob storage as a data source to Azure AI Search. This method is useful for large datasets or when you need to sync data periodically.

Here's how you can add data to the index using the Azure SDK:

1. First, create a utility function to upload documents to your search index using the Azure SDK:

   ```javascript
   const { SearchClient, AzureKeyCredential } = require('@azure/search-documents');

   async function uploadToSearchIndex(documents, endpoint, indexName, apiKey) {
     const searchClient = new SearchClient(endpoint, indexName, new AzureKeyCredential(apiKey));

     try {
       const uploadResult = await searchClient.uploadDocuments(documents);
       console.log(`Uploaded ${uploadResult.results.length} documents`);

       // Check for any failed uploads
       const failedUploads = uploadResult.results.filter((r) => !r.succeeded);
       if (failedUploads.length > 0) {
         console.warn(`Failed to upload ${failedUploads.length} documents`);
         console.warn(failedUploads);
       }

       return uploadResult;
     } catch (error) {
       console.error('Error uploading documents:', error);
       throw error;
     }
   }
   ```

2. For testing purposes, let's create sample documents directly in the script itself:

   ```javascript
   const sampleDocuments = [
     {
       id: '1',
       title: 'Introduction to PostgreSQL',
       description: 'Learn the basics of PostgreSQL, an open-source relational database system.',
       tags: ['postgresql', 'database'],
       type: 'tutorial',
       content: 'PostgreSQL is a powerful, open-source relational database system.',
     },
     {
       id: '2',
       title: 'Advanced SQL Queries',
       description: 'Master complex SQL queries for data analysis.',
       tags: ['sql', 'data analysis'],
       type: 'tutorial',
       content: 'Learn about joins, subqueries, and window functions in SQL.',
     },
   ];

   // Upload the sample documents
   await uploadToSearchIndex(
     sampleDocuments,
     process.env.AZURE_SEARCH_ENDPOINT,
     'developer-index',
     process.env.AZURE_SEARCH_KEY
   );
   ```

3. Feel free to replace the sample documents with your own data and run the script to upload the documents to your Azure AI Search index.

4. After that, you can use the Azure portal or CLI to query the index and verify the data upload.

If you wanted to you could also directly use the REST API itself, you can find more information on the [Azure Search docs](https://learn.microsoft.com/en-us/rest/api/searchservice/create-data-source) about uploading data to Azure AI Search.

Now that you have your data indexed in Azure AI Search, let's move on to building the hybrid search service that combines Neon and Azure AI Search.

Make sure to go over the [data import strategies](https://learn.microsoft.com/en-us/azure/search/search-what-is-data-import) to understand how to import data into Azure AI Search efficiently.

## Step 3: Build the Hybrid Search Service

For this demo we will build a Node.js application, but Azure AI Search can be integrated with any programming language or framework.

For Node.js Azure provides an [official SDK](https://www.npmjs.com/package/@azure/search-documents) that you can use to interact with the Azure AI Search service. Besides JavaScript, Azure provides SDKs for different languages like Java, Python, and .NET to interact with the search service.

### Project Structure

First, let's set up a clear project structure that will help us organize our code:

```plaintext
hybrid-search/
├─ src/
│   ├─ config/
│   │   └─ database.js     # Database configuration
│   ├─ services/
│   │   ├─ neonService.js  # Neon search implementation
│   │   └─ azureService.js # Azure AI Search implementation
│   ├─ routes/
│   │   └─ searchRoutes.js # API endpoints
│   ├─ utils/
│   │   └─ searchUtils.js  # Shared utilities
│   └─ app.js             # Main application file
├─ .env                   # Environment variables
└─ package.json
```

### Setting Up Dependencies

First, install the required packages:

```bash
npm install @azure/search-documents pg express dotenv cors
```

We are using the following packages:

- `@azure/search-documents`: Official Azure AI Search client library
- `pg`: PostgreSQL client for Node.js
- `express`: Web framework for Node.js
- `dotenv`: Environment variable loader
- `cors`: Cross-origin resource sharing middleware

### Database Configuration

The database configuration module will handle our connection to Neon:

```javascript
// src/config/database.js
const { Pool } = require('pg');
require('dotenv').config();

const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  ssl: true,
  // Connection pool settings
  max: 20,
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000,
});

// Helper function to query the database
async function query(text, params) {
  const client = await pool.connect();
  try {
    const result = await client.query(text, params);
    return result;
  } finally {
    client.release();
  }
}

module.exports = { pool, query };
```

This configuration:

- Creates a connection pool with optimal settings for a web application
- Implements a helper function for executing queries
- Handles client release to prevent connection leaks

### Implementing Neon Search Service

The Neon service which we will create will handle full-text search using Postgres's built-in capabilities:

```javascript
// src/services/neonService.js
const { query } = require('../config/database');

class NeonSearchService {
  async search(searchQuery, filters = {}, limit = 10) {
    try {
      let sql = `
        WITH RankedResults AS (
          SELECT
            t.tutorial_id,
            t.title,
            t.content,
            t.difficulty_level,
            t.tags,
            tech.name as technology,
            ts_rank_cd(
              setweight(to_tsvector('english', title), 'A') ||
              setweight(to_tsvector('english', content), 'B'),
              plainto_tsquery('english', $1)
            ) as rank
          FROM tutorials t
          JOIN technologies tech ON t.tech_id = tech.tech_id
          WHERE to_tsvector('english', title || ' ' || content) @@ plainto_tsquery($1)
      `;

      const values = [searchQuery];
      let paramIndex = 2;

      // Add filters if provided
      if (filters.difficulty) {
        sql += ` AND difficulty_level = $${paramIndex}`;
        values.push(filters.difficulty);
        paramIndex++;
      }

      if (filters.technology) {
        sql += ` AND tech.name = $${paramIndex}`;
        values.push(filters.technology);
        paramIndex++;
      }

      if (filters.tags && filters.tags.length > 0) {
        sql += ` AND tags && $${paramIndex}`;
        values.push(filters.tags);
        paramIndex++;
      }

      sql += `
        )
        SELECT *,
          CASE
            WHEN title ILIKE '%' || $1 || '%' THEN rank * 2  -- Boost exact title matches
            ELSE rank
          END as final_rank
        FROM RankedResults
        ORDER BY final_rank DESC
        LIMIT $${paramIndex}
      `;
      values.push(limit);

      const result = await query(sql, values);
      return result.rows;
    } catch (error) {
      console.error('Neon search error:', error);
      throw error;
    }
  }
}

module.exports = new NeonSearchService();
```

This implementation includes several features:

- Uses a [CTE](/postgresql/postgresql-tutorial/postgresql-cte) for better query organization
- Implements weighted ranking for title and content
- Supports filtering by difficulty, technology, and tags
- Handles SQL injection through parameterized queries

### Implementing Azure AI Search Service

With the Neon service in place, let's create the Azure service to handle AI-powered search features:

```javascript
// src/services/azureService.js
const { SearchClient, AzureKeyCredential } = require('@azure/search-documents');
require('dotenv').config();

class AzureSearchService {
  constructor() {
    this.client = new SearchClient(
      process.env.AZURE_SEARCH_ENDPOINT,
      'developer-index',
      new AzureKeyCredential(process.env.AZURE_SEARCH_KEY)
    );
  }

  async search(searchQuery, filters = {}, limit = 10) {
    try {
      const searchOptions = {
        select: ['id', 'title', 'description', 'content', 'tags', 'type'],
        queryType: 'simple',
        semanticConfiguration: 'default',
        highlightFields: 'content',
        top: limit,
        includeTotalCount: true,
        captions: 'extractive',
        answers: 'extractive',
        filter: this.buildFilter(filters),
      };

      const searchResults = await this.client.search(searchQuery, searchOptions);
      return this.processResults(searchResults);
    } catch (error) {
      console.error('Azure search error:', error);
      throw error;
    }
  }

  buildFilter(filters) {
    const filterConditions = [];

    if (filters.difficulty) {
      filterConditions.push(`difficulty eq '${filters.difficulty}'`);
    }

    if (filters.type) {
      filterConditions.push(`type eq '${filters.type}'`);
    }

    if (filters.tags && filters.tags.length > 0) {
      const tagConditions = filters.tags.map((tag) => `tags/any(t: t eq '${tag}')`).join(' or ');
      filterConditions.push(`(${tagConditions})`);
    }

    return filterConditions.length > 0 ? filterConditions.join(' and ') : undefined;
  }

  async processResults(searchResults) {
    const results = [];
    for await (const result of searchResults.results) {
      results.push({
        id: result.document.id,
        title: result.document.title,
        description: result.document.description,
        content: result.document.content,
        tags: result.document.tags,
        highlights: result.highlights,
        captions: result.captions,
        score: result.score,
      });
    }
    return results;
  }
}

module.exports = new AzureSearchService();
```

This implementation includes:

- Azure AI search client initialization
- Result highlighting for better user experience
- Extractive captions for quick content preview
- Filtering options
- Score normalization for better integration with Neon results

For the complete list of search options and features, refer to the [Azure AI Search documentation](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search).

### Implementing the Search Routes

With both Neon and Azure services ready, let's create the routes module to handle search requests:

```javascript
// src/routes/searchRoutes.js
const express = require('express');
const neonSearch = require('../services/neonService');
const azureSearch = require('../services/azureService');
const { mergeResults } = require('../utils/searchUtils');

const router = express.Router();

router.post('/search', async (req, res) => {
  try {
    const { query, filters = {}, limit = 10, searchType = 'hybrid' } = req.body;

    // Validate the search query
    if (!query || query.trim().length === 0) {
      return res.status(400).json({
        error: 'Search query is required',
      });
    }

    // Determine search strategy
    let results;
    switch (searchType) {
      case 'hybrid':
        const [neonResults, azureResults] = await Promise.all([
          neonSearch.search(query, filters, limit),
          azureSearch.search(query, filters, limit),
        ]);
        results = mergeResults(neonResults, azureResults);
        break;

      case 'neon':
        results = await neonSearch.search(query, filters, limit);
        break;

      case 'azure':
        results = await azureSearch.search(query, filters, limit);
        break;

      default:
        return res.status(400).json({
          error: 'Invalid search type',
        });
    }

    res.json({
      query,
      results,
      metadata: {
        total: results.length,
        searchType,
        executionTime: process.hrtime()[1] / 1000000,
      },
    });
  } catch (error) {
    console.error('Search error:', error);
    res.status(500).json({
      error: 'An error occurred during search',
    });
  }
});

module.exports = router;
```

This implementation includes:

- Supports different search strategies (hybrid, Neon-only, Azure-only)
- Includes error handling and validation
- Provides execution metadata
- Handles result merging for hybrid searches depending on the search strategy

### Utility Functions

With everything in place, let's also create some utility functions to handle common operations:

```javascript
// src/utils/searchUtils.js

function mergeResults(neonResults, azureResults) {
  const merged = new Map();

  // Process Neon results
  neonResults.forEach((result) => {
    merged.set(result.tutorial_id.toString(), {
      ...result,
      neon_rank: result.rank,
      azure_score: 0,
      final_score: normalizeNeonScore(result.rank),
    });
  });

  // Process Azure results
  azureResults.forEach((result) => {
    const existingResult = merged.get(result.id);
    if (existingResult) {
      existingResult.azure_score = result.score;
      existingResult.highlights = result.highlights;
      existingResult.final_score = calculateHybridScore(existingResult.neon_rank, result.score);
    } else {
      merged.set(result.id, {
        ...result,
        neon_rank: 0,
        final_score: normalizeAzureScore(result.score),
      });
    }
  });

  // Sort by final score and convert to array
  return Array.from(merged.values()).sort((a, b) => b.final_score - a.final_score);
}

function normalizeNeonScore(rank) {
  return Math.min(rank, 1);
}

function normalizeAzureScore(score) {
  return score;
}

function calculateHybridScore(neonRank, azureScore) {
  const normalizedNeon = normalizeNeonScore(neonRank);
  const normalizedAzure = normalizeAzureScore(azureScore);

  const NEON_WEIGHT = 0.4;
  const AZURE_WEIGHT = 0.6;

  return normalizedNeon * NEON_WEIGHT + normalizedAzure * AZURE_WEIGHT;
}

module.exports = {
  mergeResults,
  calculateHybridScore,
};
```

These utilities handle:

- Result merging from both search engines
- Score normalization and weighting
- Proper sorting of combined results

### Main Application File

Finally, let's tie everything together in a main application file where we set up our Express server:

```javascript
// src/app.js
const express = require('express');
const cors = require('cors');
const searchRoutes = require('./routes/searchRoutes');

const app = express();

// Middleware
app.use(cors());
app.use(express.json());

// Routes
app.use('/api', searchRoutes);

// Error handling
app.use((err, req, res, next) => {
  console.error(err.stack);
  res.status(500).json({
    error: 'Something went wrong!',
    message: process.env.NODE_ENV === 'development' ? err.message : undefined,
  });
});

const PORT = process.env.PORT || 3000;
app.listen(PORT, () => {
  console.log(`Search service running on port ${PORT}`);
});

module.exports = app;
```

This main file:

- Sets up our middleware
- Configures the routes
- Adds error handling

Each component of this implementation is designed to be maintainable. You can easily extend the search service with additional features like data imports to Azure AI Search from PostgreSQL for example.

### Testing the Search Service

Create a `.env` file in the project root with the following environment variables:

```plaintext
DATABASE_URL=postgres://user:password@localhost:5432/developer_resources
AZURE_SEARCH_ENDPOINT=https://<developer-search>.search.windows.net
AZURE_SEARCH_KEY=your-azure-search-key
PORT=3000
```

Start the application by running:

```bash
node src/app.js
```

To test the search service, run the application and send a POST request to the `/api/search` endpoint with a search query:

```bash
curl -X POST \
  -H "Content-Type: application/json" \
  -d '{"query": "Node.js", "limit": 5, "searchType": "hybrid"}' \
  http://localhost:3000/api/search
```

This request will search for resources related to Node.js using the hybrid search strategy. You can also test the Neon and Azure search strategies by changing the `searchType` parameter.

Feel free to customize the search query, filters, and limit to test different scenarios. Also, consider adding more resources to the database and Azure AI Search index to see the hybrid search in action.

## Conclusion

In this guide, you learned how to build a hybrid search service using Neon and Azure AI Search.

As a next step, you can check out the [Full-Text Search guide](/guides/full-text-search) to learn more about Neon's capabilities and how to optimize your search queries. Additionally, you can explore the [Azure AI Search documentation](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search) to discover more advanced features and integrations.

## Additional Resources

- [Azure AI Language Documentation](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search)
- [Neon Documentation](/docs)
- [Azure AI Language Client Library](https://learn.microsoft.com/en-us/javascript/api/overview/azure/search-documents-readme)

<NeedHelp />


# Database Migrations with Entity Framework Core and Azure Pipelines for Neon

---
title: Database Migrations with Entity Framework Core and Azure Pipelines for Neon
subtitle: Automating schema changes with EF Core and Azure Pipelines in Neon Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2025-01-18T00:00:00.000Z'
updatedOn: '2025-01-18T00:00:00.000Z'
---

[Entity Framework Core](https://learn.microsoft.com/en-us/ef/core/) provides a great migration system for managing database schema changes in .NET applications. When combined with [Azure Pipelines](https://azure.microsoft.com/en-us/products/devops/pipelines#overview), you can automate database migrations as part of a CI/CD pipeline, ensuring that schema changes are safely applied to your Neon Postgres database.

In this guide, you'll learn how to use EF Core to create and apply database migrations in Neon and automate the process using Azure Pipelines.

## Prerequisites

To follow along, you'll need the following:

- A [Neon account](https://neon.tech) with an active project.
- [.NET 8.0](https://dotnet.microsoft.com/en-us/download/dotnet) installed.
- A [GitHub](https://github.com/) or [Azure DevOps](https://dev.azure.com/) repository for version control.
- An active [Azure DevOps](https://azure.microsoft.com/en-us/products/devops/) account with access to Azure Pipelines.

## Setting Up the Entity Framework Core Project

If you don't already have a .NET project, you can follow these steps to create a new one and set up EF Core for database migrations.

### Create a New .NET Project

Start by creating a new .NET project:

```bash
dotnet new webapi -n NeonMigrations
cd NeonMigrations
```

### Install Required Packages

Add the necessary EF Core and PostgreSQL packages:

```bash
dotnet add package Microsoft.EntityFrameworkCore
dotnet add package Microsoft.EntityFrameworkCore.Design
dotnet add package Npgsql.EntityFrameworkCore.PostgreSQL
dotnet add package dotenv.net
```

The packages that we are installing are:

- `Microsoft.EntityFrameworkCore`: The core EF package.
- `Microsoft.EntityFrameworkCore.Design`: Required for EF Core CLI tools.
- `Npgsql.EntityFrameworkCore.PostgreSQL`: The PostgreSQL provider for EF Core.
- `dotenv.net`: A library for loading environment variables from a `.env` file.

Install the EF Core CLI tools:

```bash
dotnet tool install --global dotnet-ef
```

### Configure the Database Connection

Retrieve your Neon database connection string from the [Neon Console](https://console.neon.tech) and store it in the `.env` file:

```bash
DATABASE_URL=Host=<your-host>;Database=<your-database>;Username=<your-username>;Password=<your-password>;SSLMode=Require
```

Then, configure your database context in `ApplicationDbContext.cs`:

```csharp
using Microsoft.EntityFrameworkCore;
using dotenv.net;

public class ApplicationDbContext : DbContext
{
    protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder)
    {
        DotEnv.Load();
        optionsBuilder.UseNpgsql(Environment.GetEnvironmentVariable("DATABASE_URL"));
    }

    public DbSet<Product> Products { get; set; }
}
```

## Creating and Applying Migrations

Migration files are used to define schema changes in your database. In this section, let's create a simple `Product` entity and apply a migration to your Neon database.

### Define the Data Model

Create a simple `Product` entity:

```csharp
public class Product
{
    public int Id { get; set; }
    public string Name { get; set; }
    public decimal Price { get; set; }
}
```

That represents a product table with an `Id`, `Name`, and `Price`.

### Generate a Migration

Run the following command to create a migration:

```bash
dotnet ef migrations add InitialCreate
```

### Apply the Migration

Run the following command to apply the migration to your Neon database:

```bash
dotnet ef database update
```

To learn more about migrations in Entity Framework Core, refer to the [Neon documentation](/docs/guides/entity-migrations) guide which provides a detailed explanation of the migration process.

At this point, your database schema is set up. Next, we'll automate this process using Azure Pipelines.

## Creating an Azure DevOps Project

If you don't already have a project set up, follow these steps:

1. Navigate to [Azure DevOps](https://dev.azure.com/) and sign in.
2. Click **New Project**, give it a name, select visibility (private or public), and choose Git as the version control option.
3. Once the project is created, go to **Repos** and initialize a new repository (or push an existing one).

For more details, refer to the official [Azure DevOps documentation](https://learn.microsoft.com/en-us/azure/devops/organizations/projects/create-project?view=azure-devops&tabs=browser) and the [Azure Repos guide](https://learn.microsoft.com/en-us/azure/devops/repos/get-started/?view=azure-devops).

## Automating Migrations with Azure Pipelines

With your migrations set up and the project in Azure DevOps, you can now automate the process using [Azure Pipelines](https://learn.microsoft.com/en-us/azure/devops/pipelines/?view=azure-devops). This way, any new migrations will be applied automatically when you push changes to your repository.

### Create an Azure Pipeline

Similar to GitHub Actions, Azure Pipelines use a YAML configuration file to define the build process.

In your repository, create a `azure-pipelines.yml` file with the following content:

```yaml
trigger:
  - main

variables:
  DATABASE_URL: $(DATABASE_URL)

steps:
  - task: UseDotNet@2
    inputs:
      packageType: 'sdk'
      version: '8.x'
      installationPath: $(Agent.ToolsDirectory)/dotnet

  - script: |
      dotnet restore
      dotnet build --configuration Release
    displayName: 'Build Application'

  - script: |
      dotnet tool install --global dotnet-ef
      dotnet ef database update
    displayName: 'Apply Database Migrations'
    env:
      DATABASE_URL: $(DATABASE_URL)
```

This pipeline configuration:

- Triggers on new commits to the `main` branch. You can adjust the trigger as needed. For a complete list of triggers, refer to the [Azure Pipelines documentation](https://learn.microsoft.com/en-us/azure/devops/pipelines/build/triggers?view=azure-devops).
- Sets the `DATABASE_URL` variable from the Azure DevOps pipeline.
- Installs the EF Core CLI tools and applies the migrations.

### Configure Pipeline Variables

To securely store your database connection string, create a variable group in Azure DevOps:

1. Go to **Pipelines** → **Library** → **+ Variable Group**.
1. Set the name to `NeonMigrations`.
1. Create a variable named `DATABASE_URL` and set it to your Neon database connection string.
1. Mark it as a **secret** to protect sensitive information.
1. Save the variable group.

### Run the Pipeline

Commit the `azure-pipelines.yml` file to your repository. The pipeline will trigger on new commits to `main`, applying any pending migrations automatically.

If you were to go to the Azure DevOps Pipelines section, you would see the pipeline running and applying the migrations. If the pipeline is not triggered automatically, you can manually run it from the Azure DevOps UI.

Note that if you've just created the pipeline, Azure DevOps might limit the number of parallel pipelines you can run. You can request additional pipelines parallelism via the form [here](https://aka.ms/azpipelines-parallelism-request) or upgrade your Azure DevOps plan.

## Handling Migration Conflicts

When working in a team, conflicts may arise due to multiple migration files being generated. You can check out the Entity [Framework Core documentation for some best practices](https://learn.microsoft.com/en-us/ef/core/managing-schemas/migrations/teams) but here are some general tips:

1. Before adding a new migration, apply any existing ones using:
   ```bash
   dotnet ef database update
   ```
1. If conflicting migration files exist, delete and regenerate them:
   ```bash
   dotnet ef migrations remove
   dotnet ef migrations add NewMigration
   ```

In addition, consider the following:

- When applying migrations, use a direct Neon connection instead of a [pooled one](/docs/connect/connection-pooling).
- Before applying changes to production, test them in a staging environment or using a [Neon branch](/docs/introduction/branching).

## Conclusion

By integrating Entity Framework Core with Azure Pipelines, you can simplify database migrations and ensure schema changes are consistently applied to your Neon Postgres database. Automating migrations reduces the risk of human error and helps maintain database integrity across environments.

As a next step, make sure to explore [Neon branches](/docs/introduction/branching), so you can test your migrations in a staging environment before deploying to production.

## Additional Resources

- [Entity Framework Core Migrations](https://learn.microsoft.com/en-us/ef/core/managing-schemas/migrations/?tabs=dotnet-core-cli)
- [Azure Pipelines Overview](https://azure.microsoft.com/en-us/products/devops/pipelines#overview)
- [Neon Documentation](/docs)

<NeedHelp />


# Building a Robust JSON API with TypeScript, Postgres, and Azure Functions

---
title: Building a Robust JSON API with TypeScript, Postgres, and Azure Functions
subtitle: Learn how to leverage TypeScript, Neon Postgres Databases, and Azure Functions for Next-Level API Performance
author: jess-chadwick
enableTableOfContents: true
createdAt: '2025-02-01T00:00:00.000Z'
updatedOn: '2025-02-01T00:00:00.000Z'
---

Creating scalable and maintainable APIs is a cornerstone of modern web development. In this post I will show you how to build a simple (but realistic) Recipes API using one of my favorite combinations of technologies: TypeScript for type safety, Postgres for database storage, and Azure Functions for serverless hosting.

Using this combination gives a great balance of development experience and deploying your application without having to worry about managing your own infrastructure.

> **Installing tooling**
>
> Before we get started, you'll need to have the following tools installed:
>
> - [Node.js](https://nodejs.org) _(specifically, the npm package manager)_
> - [Azure Functions Core Tools](https://learn.microsoft.com/en-us/azure/azure-functions/create-first-function-cli-typescript?pivots=nodejs-model-v4#install-the-azure-functions-core-tools) _(I'm using version 4.x)_

### Setting up the project

Because I're creating an application that will be deployed as an Azure Function, it makes sense to start by using the Azure Functions Core Tools to create a new project
since that configuration can get a little overwhelming to write out yourself.

I'll run the following command to create a new project folder and initialize it with everything I need to start building my API:

```bash
mkdir recipes-api
cd recipes-api
func init --typescript
```

That creates all of the configuration and file structure required to build and deploy an Azure Function.
With that in place the next thing to do is add a function to the project with the command:

```bash
func new --name RecipesApi --template "HTTP trigger" --authlevel "anonymous"
```

With all that completed, I have a new Azure Function project with a single function that will respond to HTTP requests at the `/api/RecipesApi` endpoint.

### Introducing an API Framework

Opening up `src/functions/RecipeApi.ts`, I see that it's been implemented using the Azure Functions API.
This is a great way to get started, but it's a little low-level for my taste, so I'd like to use an API framework to make my life easier.

The first question I have to answer is: which API framework will I use? I'll be honest: for most applications I build, I reach for Express.js.
It's a mature, flexible, easily extended, and it's got a dirt-simple API which makes it a great choice for long-running servers.
However, serverless functions are stateless and short-lived, making it important for your deployed code to be as small as possible so that it boots up quickly.
Not to mention, you generally pay for the amount of memory and the time your function is running so it (quite literally!) pays for your code to be small and execute as fast as possible.
So while, yes, _no_ Express.js is not particularly slow or gigantic... but there are better options.

Luckily, the creators of Express.js have created a lightweight alternative called [**Hono**](https://hono.dev/) - a fast, minimalist JavaScript web framework. It's got an API that's very similar to Express.js and (my favorite part), has first-class support for TypeScript, so that makes it the perfect choice for serverless functions.

I can install Hono into my existing project by running:

```bash
npm install hono
```

However, since I'm running in an Azure Functions project, I need to install the Azure Functions adapter for Hono as well:

```bash
npm install @marplex/hono-azurefunc-adapter
```

And now that I have all my dependencies, I'll update my function to use Hono.

First, I'll create a file to hold my application logic, `src/app.ts`:

```typescript
import { Hono } from 'hono';

const app = new Hono();

app.get('/api', (c) => c.text('Recipes API'));

export default app;
```

> **IMPORTANT:** note that I'm using the route `/api` since this is the
> prefix that Azure Functions by default for its HTTP-triggered functions.

Then, replace the contents of `src/functions/RecipeApi.ts` with the following:

```typescript
import { app } from '@azure/functions';
import { azureHonoHandler } from '@marplex/hono-azurefunc-adapter';
import honoApp from '../app';

app.http('RecipesApi', {
  methods: ['GET', 'POST', 'DELETE', 'PUT'],
  authLevel: 'anonymous',
  route: '{*proxy}',
  handler: azureHonoHandler(honoApp.fetch),
});
```

Now, I'm not going to go into detail on this snippet because [the Hono docs](https://hono.dev/docs/getting-started/azure-functions) do a pretty great job of it.
In short, this code replaces the original Azure Function configuration with one that simply hands off all requests to the Hono application.

With everything in place, let's run the app to test it out:

```bash
npm run start
```

If everything is working correctly, when you hit the URL provided (mine is `http://localhost:7071/api`) you should see output that looks like this:

```bash
> curl http://localhost:7071/api
Recipes API
```

Now that I've proven the base application is working, I can start building out the API logic.

### Defining Data Models with TypeScript and Zod

A little bit about me: I **adore** TypeScript. I adore it so much that I simply _refuse_ to write regular JavaScript anymore.

So, before I even write any logic, I'm going to define the data models for my API using TypeScript interfaces.

```typescript
// src/models.d.ts
export interface Ingredient {
  id: number;
  /** the name of the ingredient, e.g. "sugar" */
  name: string;
  /** the amount of the ingredient required */
  quantity_amount: number;
  /** e.g. 'g', 'ml', 'tbsp', 'cup' */
  quantity_type: string;
}

export interface Recipe {
  id: number;
  /** the name of the recipe */
  name: string;
  /** the description or  */
  description: string;
  /** (optional) the URL that this recipe originated from, if applicable */
  url: string;
  /** the list of ingredients required for this recipe */
  ingredients: Ingredient[];
  /** the list of steps required to prepare this recipe */
  preparation_steps: string[];
}

// A simple view of recipe, excluding things like ingredients and preparation steps
// e.g. for a list of recipes
export type RecipeOverview = Pick<Recipe, 'id' | 'name' | 'description'>;
```

Just two models: one for recipes and another to describe the ingredients of those recipes.
I've also defined a `RecipeOverview` type that's a subset of the `Recipe` type, which I can use when I want to display a list of recipes without all the details.
Simple enough, right? Now let's prep our database to store this data.

### Creating a Postgres Database with Neon

Postgres is my go-to database for most projects. And using a managed service like [Neon](https://neon.com/) makes it even easier to get up and running Serverless Progres databases on Azure.
So, I'm going to head over to my [Neon projects](https://console.neon.tech/app/projects) and create a new project with a Postgres database, then use the following schema to create a table to store my recipes:

```sql
CREATE TABLE recipes (
    id SERIAL PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    description TEXT NOT NULL,
    preparation_steps TEXT[] NOT NULL,
    url VARCHAR(255)
);

CREATE TABLE ingredients (
    id SERIAL PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    quantity_amount NUMERIC NOT NULL,
    quantity_type VARCHAR(255) NOT NULL,
    recipe_id INTEGER REFERENCES recipes(id) ON DELETE CASCADE
);
```

Then I'll populate it with some sample data (a delicious chocolate cake recipe!):

```sql
WITH inserted_recipe AS (
    INSERT INTO recipes (name, description, preparation_steps)
    VALUES (
        'The Best Chocolate Cake EVER',
        'A delicious chocolate cake',
        ARRAY[
            'Preheat oven to 350F',
            'In a large mixing bowl, mix flour, sugar, cocoa powder',
            'Add milk, vegetable oil, and eggs',
            'Bake for 30 minutes, or until a toothpick comes out clean'
        ]
    )
    RETURNING id
)

INSERT INTO ingredients (name, quantity_amount, quantity_type, recipe_id)
SELECT name, quantity_amount, quantity_type, (SELECT id FROM inserted_recipe)
FROM (VALUES
    ('flour', 2, 'cups'),
    ('sugar', 1, 'cup'),
    ('cocoa powder', 0.5, 'cup'),
    ('milk', 1, 'cup'),
    ('vegetable oil', 0.5, 'cup'),
    ('eggs', 2, 'large')
) AS ingredient_data(name, quantity_amount, quantity_type);
```

### Database Interaction

Neon databases are serverless, distributed, fully managed, and a whole bunch of other things, but most importantly, they're just Postgres databases.
So, I can use the `pg` package to interact with my database just like I would with any other Postgres database.

```bash
npm install pg
```

Unfortunately, the `pg` package does not include any types (at least not as of this writing),
so I'll also install the types for the `pg` package to give myself a better development experience:

```bash
npm install @types/pg
```

Then, I'll login to the [Neon web console](https://console.neon.tech/) and use the "Connect" button to grab my database connection string.
I'll paste that connection string into a new setting inside of the `local.settings.json` file in my project, like this:

```json
{
  "IsEncrypted": false,
  "Values": {
    "FUNCTIONS_WORKER_RUNTIME": "node",
    "DATABASE_URL": "postgresql://recipes_owner:secret_password@jchadwick-pooler.eastus2.azure.neon.tech/recipes?sslmode=require&channel_binding=require",
    "AzureWebJobsStorage": "UseDevelopmentStorage=true"
  }
}
```

Finally, it's time to write some code to interact with the database.
I'll create a new file, `src/lib/db.ts`, to hold my database connection logic:

```typescript
// src/lib/db.ts
import { Pool } from 'pg';

const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
});

export default pool;
```

### Querying the Database from an Azure Function

Now that I have my data models and database connection set up, I can start building out the API logic, which is simple enough.
For that, let's head back to `src/app.ts`.

First, I'll import the `db` connection from `src/lib/db.ts` and the `RecipeOverview` type from `src/models.d.ts`.

```typescript
import { Hono } from 'hono';
import db from './lib/db';
import type { RecipeOverview } from './models';
```

Then, I'll add an endpoint to query all recipes from the database (including their ingredients):

```typescript
app.get('/api/recipes', async (c) => {
  const { rows: recipes } = await db.query<RecipeOverview>(
    'SELECT id, name, description FROM recipes'
  );
  return c.json(recipes);
});
```

We can verify all this works by running the app, hitting the `/api/recipes` endpoint, and seeing the following output:

```bash
> curl http://localhost:7071/api/recipes

# output:
[{"id":1,"name":"The Best Chocolate Cake EVER","description":"A delicious chocolate cake"}]
```

Exciting stuff - I've now queried my database from an Azure Function using TypeScript!

Let's add another endpoint to get a single recipe by its ID, including its ingredients in the response:

```typescript
app.get('/api/recipes/:id', async (c) => {
  const recipeId = +c.req.param('id');

  const [recipeResults, ingredientsResults] = await Promise.all([
    db.query<Recipe>(
      `SELECT id, name, description, preparation_steps, url
       FROM recipes 
       WHERE id = $1
       LIMIT 1
     `,
      [recipeId]
    ),
    db.query<Ingredient>(
      `SELECT id, name, quantity_amount, quantity_type
       FROM ingredients
       WHERE recipe_id = $1
      `,
      [recipeId]
    ),
  ]);

  if (recipeResults.rowCount === 0) {
    // invalid recipe ID - return a 404 result
    return c.json(null, 404);
  }

  const recipe = {
    ...recipeResults.rows[0],
    ingredients: ingredientsResults.rows,
  };

  return c.json(recipe);
});
```

Now, when I hit the `/api/recipes/1` endpoint, I should see the following output:

```bash
> curl http://localhost:7071/api/recipes/1

# output:
{ "id": 1, "name": "The Best Chocolate Cake EVER", // ... }
```

And, finally, let's add an endpoint to _create_ a new recipe:

```typescript
app.post('/api/recipes', async (c) => {
  const { name, description, url, preparation_steps, ingredients } = await c.req.json<Recipe>();

  // create the recipe, retrieving the added recipe
  // so we can use its id to add ingredients below
  const {
    rows: [addedRecipe],
  } = await db.query<Recipe>(
    `INSERT INTO recipes (name, description, url, preparation_steps)
       VALUES ($1, $2, $3, $4)
       RETURNING id, name, description, url, preparation_steps
      `,
    [name, description, url, preparation_steps]
  );

  // bulk insert ingredients for the recipe
  const { rows: addedIngredients } = await db.query(
    `
    INSERT INTO ingredients (name, quantity_amount, quantity_type, recipe_id)
    VALUES ${ingredients
      .map(
        // produces a string like ($1, $2, $3, $4) to create placeholders
        // for each one of the ingredients, concatenating them all together
        // with commas to produce a single string like:
        // ($1, $2, $3, $4),($5, $6, $7, $8),($9, $10, $11, $12)
        (_, i) => `($${i * 4 + 1}, $${i * 4 + 2}, $${i * 4 + 3}, $${i * 4 + 4})`
      )
      .join(',')}
    RETURNING id, name, quantity_amount, quantity_type
    `,
    ingredients.flatMap((ingredient) => [
      ingredient.name,
      ingredient.quantity_amount,
      ingredient.quantity_type,
      addedRecipe.id,
    ])
  );

  addedRecipe.ingredients = addedIngredients;

  return c.json(addedRecipe, 201);
});
```

Although there is quite a bit more code (including some gnarly string replacements for the variable placeholders),
this endpoint expects a JSON payload with the recipe data and ingredients, and it will insert the new recipe into the database.

So when I hit the `/api/recipes` endpoint with a POST request, I should see the following output:

```bash
> curl -X POST \
  -H 'Content-Type: application/json' \
  -d '{
    "name": "Peanut butter and jelly sandwich",
    "description": "A classic sandwich, perfect for lunch or even just a filling snack",
    "preparation_steps": [
        "Smear peanut butter on one slice of bread",
        "Smear jelly on the other slice of bread",
        "Place the two slices of bread together, smeared sides facing each other"
    ],
    "ingredients": [
        { "name": "bread", "quantity_amount": 2, "quantity_type": "slice" },
        { "name": "peanut butter", "quantity_amount": 1, "quantity_type": "tbsp" },
        { "name": "fruit jelly (any flavor)", "quantity_amount": 1, "quantity_type": "tbsp" }
    ]
  }' \
  http://localhost:7071/api/recipes

# output:
{"id": 2, "name": "Peanut butter and jelly sandwich", "description": ... }
```

### Deploying to Azure

Now that I've got the API working how I want it, it's time to bundle it up and deploy it to Azure.
Of course, the first thing you'll need is an Azure account; if you don't already have one, sign up for a free account [here](https://azure.microsoft.com/en-us/free/).

The next prerequisite to creating an Azure Function App is a resource group for it to live in.
For this you can either choose an existing resource group or create a new one.
I like to use the Azure CLI to do everything, so I'll create a new resource group in my preferred location (`eastus2`) using the command:

```bash
> az group create --name recipes-api-rg --location eastus2
# output:
{
  "id": "/subscriptions/<subscription_id>/resourceGroups/recipes-api-rg",
  "location": "eastus2",
  "managedBy": null,
  "name": "recipes-api-rg",
  "properties": {
    "provisioningState": "Succeeded"
  },
  "tags": null,
  "type": "Microsoft.Resources/resourceGroups"
}
```

Next you'll need to create a storage account to hold your function's code.
I'll create a new storage account in the same resource group using the command
(ensuring that my storage account name is globally unique):

```bash
> az storage account create \
    --name recipesapistorage2000 \
    --resource-group recipes-api-rg \
    --location eastus2
```

> **NOTE:**
>
> If this command produces an error (like mine did at first), double-check that
> the Microsoft.Storage Resource Provider is registered for your subscription.
>
> You can do this through the Azure Portal:
>
> - Go to your subscription page
> - Navigate to the "Resource providers" blade
> - Find "Microsoft.Storage" in the list
> - Click "Register" if it's not already registered

Then, I'll create a new Function App in that resource group using the command below
(again, ensuring that my function app name is globally unique):

```bash
> az functionapp create \
    --resource-group recipes-api-rg \
    --consumption-plan-location eastus2 \
    --storage-account recipesapistorage2000 \
    --name recipes-api-2000 \
    --runtime node \
    --runtime-version 20
```

And, finally, I can deploy my function using this command to package and upload it to my newly-created Azure Function App:

```bash
func azure functionapp publish recipes-api-2000
```

My API is now live on Azure, and I can verify it's working by hitting the URL provided by the `func azure functionapp publish` command:

```bash
> curl https://recipes-api-2000.azurewebsites.net/api

# output:
Recipes API
```

### Configuring the Azure Function App

I've verified that my function app is deployed and working, so that's exciting!

However, if I hit any of the endpoints that access the Postgres database, I will get a failure response.

```bash
> curl https://recipes-api-2000.azurewebsites.net/api/recipes

# output:
500 Internal Server Error
```

These endpoints work fine locally because I've set the `DATABASE_URL` environment variable in my `local.settings.json` file,
however my deployed Function doesn't have this setting.

Luckily, Azure Functions makes it easy to set environment variables for your function app.
Just run the following command to set the `DATABASE_URL` environment variable to the connection string for my Neon database:

Define an environment variable in the Azure Function App settings to store the database connection string, using this command:

```bash
> az functionapp config appsettings set \
    --name recipes-api-2000 \
    --resource-group recipes-api-rg \
    --settings DATABASE_URL="postgresql://recipes_owner:9WAzoqh2NvYm@ep-black-bush-a8jqxdjf-pooler.eastus2.azure.neon.tech/recipes?sslmode=require&channel_binding=require"
```

Now when I hit the `/api/recipes` endpoint, I see the repsonse that I expect:

```bash
> curl https://recipes-api-2000.azurewebsites.net/api/recipes

# output:
[{"id":1,"name":"The Best Chocolate Cake EVER","description":"A delicious chocolate cake"}]
```

And that's it! My Recipes API is deployed and working in the cloud.

### Wrapping Up

The setup I've shown here provides a solid foundation for building a robust, type-safe, and scalable JSON API.
While this is where I'm going to end this post, there is still several things I've had to leave out.

Using what I've already shown in this article you should be able take care of some of these yourself, such as:

- adding new endpoints to update and delete recipes
- adding error handling
- introducing input validation to new recipes
- updating the `/recipes` endpoint with pagination and filtering of recipes

Perhaps the biggest thing I've left out is security.  
If you hadn't noticed, this API is completely open and doesn't require any authentication to access, meaning that anyone can come along and add recipes to the database.

Now, I could have introduced some basic HTTP authentication, but one of Neon's best features is its built-in support for JWT authentication and Row Level Security, so I've decided to create an entirely separate post to cover that.
Stay tuned for that post!

In the meantime, I hope you've found this post helpful and that it inspires you to build your own APIs combining the strengths of TypeScript, Postgres (via Neon), and Azure Functions to create efficient and maintainable backend services that are super easy to develop, deploy, and scale.
Good luck, and happy coding!

## Additional Resources

- [GitHub Repository for this article](https://github.com/jchadwick/neon-azure-api)
- [Neon Documentation](/docs)
- [Using Hono with Azure Functions](https://hono.dev/docs/getting-started/azure-functions)
- [Azure Functions Documentation](https://learn.microsoft.com/en-us/azure/azure-functions/)

<NeedHelp />


# Building a Serverless Referral System with Neon Postgres and Azure Functions

---
title: Building a Serverless Referral System with Neon Postgres and Azure Functions
subtitle: Learn how to create a serverless referral system using Neon Postgres and Azure Functions
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-11-24T00:00:00.000Z'
updatedOn: '2024-11-24T00:00:00.000Z'
---

Serverless computing makes it easier for developers to build and run apps without worrying about managing and scaling servers. By combining Azure Functions with Neon's serverless Postgres, you can create scalable and cost-effective applications that handle high volumes of traffic with ease.

In this guide, we'll explore how to create a serverless referral system using Azure Serverless Functions and Neon Postgres. We'll build a simple serverless referral system that allows users to create referral codes, track successful referrals, and award points to referrers.

## Prerequisites

Before we begin, make sure you have:

- [Node.js](https://nodejs.org/) 18.x or later installed
- [Visual Studio Code](https://code.visualstudio.com/) with the [Azure Functions extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurefunctions)
- An [Azure account](https://azure.microsoft.com/free/) with an active subscription
- A [Neon account](https://console.neon.tech/signup) and project
- [Azure Functions Core Tools version 4.x](https://learn.microsoft.com/en-gb/azure/azure-functions/create-first-function-vs-code-node?pivots=nodejs-model-v4#install-or-update-core-tools)

## Creating Your Neon Project

Neon is now available in Azure! You can create serverless Postgres databases that run on Azure infrastructure. To learn more about Neon's Azure launch, check out the [announcement post](/blog/neon-is-coming-to-azure).

To create your Neon project on Azure:

1. Navigate to the [Neon Console](https://console.neon.tech).
2. Click **Create Project**.
3. Give your project a name.
4. Under **Cloud Service Provider**, select **Azure**.
5. Select a **Region** for your project.
6. Select the **Compute size** for your Neon database.
7. Click **Create Project**.

Once created, save your connection details - you'll need these to configure your Azure Functions connection to Neon Postgres.

## Database Schema Design

Let's start by designing the database schema for our referral system. We'll need tables to track users, referral codes, and rewards.

1. Create the `users` table:

   ```sql
   CREATE TABLE users (
       user_id SERIAL PRIMARY KEY,
       email VARCHAR(255) UNIQUE NOT NULL,
       name VARCHAR(100),
       points INT DEFAULT 0,
       created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
   );
   ```

2. Create the `referral_codes` table:

   ```sql
   CREATE TABLE referral_codes (
       code_id SERIAL PRIMARY KEY,
       referrer_id INT REFERENCES users(user_id),
       code VARCHAR(10) UNIQUE NOT NULL,
       times_used INT DEFAULT 0,
       max_uses INT DEFAULT 10,
       points_per_referral INT DEFAULT 100,
       expires_at TIMESTAMP,
       created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
       CONSTRAINT valid_points CHECK (points_per_referral > 0)
   );
   ```

3. Create the `referrals` table to track successful user referrals:

   ```sql
   CREATE TABLE referrals (
       referral_id SERIAL PRIMARY KEY,
       referrer_id INT REFERENCES users(user_id),
       referred_id INT REFERENCES users(user_id),
       code_id INT REFERENCES referral_codes(code_id),
       points_awarded INT,
       status VARCHAR(20) DEFAULT 'pending',
       created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
       CONSTRAINT unique_referral UNIQUE(referred_id),
       CONSTRAINT valid_status CHECK (status IN ('pending', 'completed', 'rejected'))
   );
   ```

With the three tables in place, our database schema:

- Tracks users and their total points
- Manages referral codes with usage limits and expiration dates
- Records each referral transaction
- Enforces data integrity through constraints
- Prevents duplicate referrals for the same user

## Setting Up Your Development Environment

To start building serverless applications with Azure Functions you need to set up your development environment first.

### Installing Required Tools

If you don't have the required tools installed, you can follow these steps to set up your development environment:

1. Install the **Azure Functions** extension for VS Code (if you haven't already):
   - Open VS Code
   - Click the Extensions icon or press `Ctrl+Shift+X` or `Cmd+Shift+X`
   - Search for "Azure Functions"
   - Install the extension from Microsoft

2. Install **Azure Functions Core Tools version 4.x**:

   In Visual Studio Code, select `F1` to open the command palette, and then search for and run the command 'Azure Functions: Install or Update Core Tools'.

   The Core Tools provide a local development environment and CLI commands for creating, testing, and deploying Azure Functions.

### Project Setup

With the required Azure tools installed, you're ready to create your first Azure Functions project.

1. Open a terminal and navigate to the directory where you want to create your project.

   Create a new directory for your project:

   ```bash
   mkdir referral-system
   ```

   Change to the project directory:

   ```bash
   cd referral-system
   ```

   Initialize a new Azure Functions project:

   ```bash
   func init
   ```

   When prompted, select:
   - Node.js as the runtime
   - JavaScript as the language

   This might take a few moments to complete, and it creates a basic project structure with the following files:
   - `host.json`: Contains global configuration options
   - `local.settings.json`: Stores app settings and connection strings for local development
   - `package.json`: Manages project dependencies

2. Install the required dependencies:

   ```bash
   npm install pg uuid dotenv
   ```

   We're using:
   - `pg` for Postgres connection to Neon
   - `uuid` for generating unique referral codes
   - `dotenv` for environment variables management

3. Configure your database connection by creating a `.env` file:

   ```bash
   NEON_CONNECTION_STRING=postgres://user:password@ep-xyz.region.azure.neon.tech/neondb
   ```

   Replace `user`, `password`, `ep-xyz.region`, and `neondb` with your Neon connection details.

4. Create a database utility file `src/utils/db.js` for our connection management:

   ```js
   const { Pool } = require('pg');
   require('dotenv').config();

   const pool = new Pool({
     connectionString: process.env.NEON_CONNECTION_STRING,
     ssl: true,
   });

   // Wrapper for queries with automatic connection handling
   async function query(text, params) {
     const client = await pool.connect();
     try {
       const result = await client.query(text, params);
       return result;
     } finally {
       client.release();
     }
   }

   module.exports = {
     query,
     pool,
   };
   ```

   We will use this utility to execute queries against the Neon Postgres database across our Azure Functions.

With your project set up, you're ready to start building the core functions for your referral system.

## Creating the Core Functions

We'll create several Azure Functions to handle different aspects of the referral system.

Each function will be an HTTP-triggered function that interacts with the Neon Postgres database to generate referral codes, process referrals, and retrieve referral statistics.

Other types of triggers, such as queue or timer triggers, can also be used depending on your requirements, but for this guide, we'll focus on HTTP triggers.

### 1. Generate Referral Code

Let's start by creating a function to generate unique referral codes for users.

Run the following command to create a new HTTP-triggered function:

```bash
func new --name generateReferralCode --template "HTTP trigger"
```

This will create a new function in `src/functions/generateReferralCode.js`. Open the file and replace the contents with the following code:

```js
const { app } = require('@azure/functions');
const { v4: uuidv4 } = require('uuid');
const { query } = require('../utils/db');

app.http('generateReferralCode', {
  methods: ['POST'],
  authLevel: 'anonymous',
  handler: async (request, context) => {
    try {
      const { userId } = await request.json();

      // Generate unique 8-character code
      const code = uuidv4().split('-')[0];

      // Insert new referral code
      const result = await query(
        `INSERT INTO referral_codes
                (referrer_id, code, max_uses, points_per_referral, expires_at)
                VALUES ($1, $2, $3, $4, NOW() + INTERVAL '30 days')
                RETURNING *`,
        [userId, code, 10, 100]
      );

      return {
        status: 201,
        body: JSON.stringify({
          code: result.rows[0].code,
          expiresAt: result.rows[0].expires_at,
        }),
      };
    } catch (error) {
      context.log('Error generating referral code:', error);
      return {
        status: 500,
        body: 'Error generating referral code',
      };
    }
  },
});
```

This function:

- Generates an 8-character referral code using `uuidv4`
- Inserts the code into the `referral_codes` table with a 30-day expiration
- Returns the generated code and expiration date

The endpoint will be available at `http://localhost:7071/api/generateReferralCode` and expects a JSON payload with the `userId` of the referrer. We'll be testing this function shortly once all functions are in place.

### 2. Process Referral

With the referral code generation function in place, let's create a function to process referrals when new users sign up.

Create a new HTTP-triggered function, again using the Azure Functions CLI:

```bash
func new --name processReferral --template "HTTP trigger"
```

This will create a new function in `src/functions/processReferral.js`. Replace the contents with the following code:

```js
const { app } = require('@azure/functions');
const { query } = require('../utils/db');

app.http('processReferral', {
  methods: ['POST'],
  authLevel: 'anonymous',
  handler: async (request, context) => {
    try {
      const { referralCode, newUserEmail, newUserName } = await request.json();

      // Start transaction
      await query('BEGIN');

      // Verify referral code
      const codeResult = await query(
        `SELECT * FROM referral_codes
                WHERE code = $1
                AND times_used < max_uses
                AND expires_at > NOW()`,
        [referralCode]
      );

      if (codeResult.rows.length === 0) {
        await query('ROLLBACK');
        return {
          status: 400,
          body: 'Invalid or expired referral code',
        };
      }

      const code = codeResult.rows[0];

      // Create new user
      const newUserResult = await query(
        `INSERT INTO users (email, name)
                VALUES ($1, $2)
                RETURNING user_id`,
        [newUserEmail, newUserName]
      );

      // Record referral
      await query(
        `INSERT INTO referrals
                (referrer_id, referred_id, code_id, points_awarded, status)
                VALUES ($1, $2, $3, $4, 'completed')`,
        [code.referrer_id, newUserResult.rows[0].user_id, code.code_id, code.points_per_referral]
      );

      // Update referral code usage
      await query(
        `UPDATE referral_codes
                SET times_used = times_used + 1
                WHERE code_id = $1`,
        [code.code_id]
      );

      // Award points to referrer
      await query(
        `UPDATE users
                SET points = points + $1
                WHERE user_id = $2`,
        [code.points_per_referral, code.referrer_id]
      );

      await query('COMMIT');

      return {
        status: 200,
        body: JSON.stringify({
          message: 'Referral processed successfully',
        }),
      };
    } catch (error) {
      await query('ROLLBACK');
      context.log('Error processing referral:', error);
      return {
        status: 500,
        body: 'Error processing referral',
      };
    }
  },
});
```

There is a bit more going on in this function, so let's break it down:

- The function accepts `POST` requests with the referral code, new user email, and name
- It starts a transaction to ensure data integrity
- Verifies the referral code is valid and not expired
- Creates a new user record
- Records the referral transaction
- Updates the referral code usage and awards points to the referrer
- Commits the transaction if successful, otherwise rolls back

The endpoint will be available at `http://localhost:7071/api/processReferral` and expects a JSON payload with the referral code, new user email, and name.

### 3. Get Referral Stats

Finally, let's create a function to retrieve referral statistics for a given user.

Create a new HTTP-triggered function using the Azure Functions CLI:

```bash
func new --name getReferralStats --template "HTTP trigger"
```

Open the generated file `src/functions/getReferralStats.js` and replace the contents with the following code:

```js
const { app } = require('@azure/functions');
const { query } = require('../utils/db');

app.http('getReferralStats', {
  methods: ['GET'],
  authLevel: 'anonymous',
  handler: async (request, context) => {
    try {
      const userId = request.query.get('userId');

      const stats = await query(
        `SELECT
                    u.points as total_points,
                    COUNT(r.referral_id) as total_referrals,
                    COUNT(CASE WHEN r.status = 'completed' THEN 1 END) as successful_referrals,
                    COUNT(CASE WHEN r.created_at > NOW() - INTERVAL '30 days' THEN 1 END) as recent_referrals
                FROM users u
                LEFT JOIN referrals r ON u.user_id = r.referrer_id
                WHERE u.user_id = $1
                GROUP BY u.user_id, u.points`,
        [userId]
      );

      return {
        status: 200,
        body: JSON.stringify(stats.rows[0]),
      };
    } catch (error) {
      context.log('Error fetching referral stats:', error);
      return {
        status: 500,
        body: 'Error fetching referral stats',
      };
    }
  },
});
```

Let's again break down the function:

- The function accepts `GET` requests with a `userId` query parameter
- It retrieves the total points, total referrals, successful referrals, and recent referrals for the given user
- The query joins the `users` and `referrals` tables to calculate the statistics
- The results are returned as JSON

The endpoint will be available at `http://localhost:7071/api/getReferralStats?userId=1`, where `userId` is the ID of the user to fetch statistics for.

## Testing Your Functions Locally

With all the core functions in place, it's time to test them locally before deploying to Azure.

Let's test each function locally to ensure everything works as expected.

1. Start the Azure Functions runtime locally:

   ```bash
    func start
   ```

   You should see output indicating your functions are running, typically on `http://localhost:7071`. And you should see the URLs for each function:

   ```bash
   Functions:

    generateReferralCode: [POST] http://localhost:7071/api/generateReferralCode

    getReferralStats: [GET] http://localhost:7071/api/getReferralStats

    processReferral: [POST] http://localhost:7071/api/processReferral
   ```

2. To test the functions, you can use `curl` or a tool like Postman.

   First, create a test user in your database:

   ```sql
   INSERT INTO users (email, name)
   VALUES ('test@example.com', 'Test User')
   RETURNING user_id;
   ```

   Note the returned `user_id` (let's say it's 1) and use it to test the referral code generation:

   ```bash
   curl -X POST http://localhost:7071/api/generateReferralCode \
   -H "Content-Type: application/json" \
   -d '{"userId": 1}'
   ```

   You should receive a response like:

   ```json
   {
     "code": "a1b2c3d4",
     "expiresAt": "2024-12-29T14:30:00.000Z"
   }
   ```

   Check the response for the generated referral code and expiration date.

   Verify in the database:

   ```sql
   SELECT * FROM referral_codes WHERE referrer_id = 1;
   ```

3. With the referral code in hand, test the referral processing function.

   Using the referral code from the previous step:

   ```bash

   curl -X POST http://localhost:7071/api/processReferral \
    -H "Content-Type: application/json" \
    -d '{
        "referralCode": "a1b2c3d4",
        "newUserEmail": "friend@example.com",
        "newUserName": "Referred Friend"
    }'
   ```

   If the referral is successful, you should see:

   ```json
   {
     "message": "Referral processed successfully"
   }
   ```

   Otherwise, check the response for error messages from the function.

   Verify the changes in the database:

   ```sql
   -- Check the new user was created
   SELECT * FROM users WHERE email = 'friend@example.com';

   -- Check the referral was recorded
   SELECT * FROM referrals ORDER BY created_at DESC LIMIT 1;

   -- Verify points were awarded to the referrer
   SELECT points FROM users WHERE user_id = 1;

   -- Check referral code usage was incremented
   SELECT times_used FROM referral_codes WHERE code = 'a1b2c3d4';
   ```

4. Next, test the referral statistics function.

   Using the original user's ID:

   ```bash
   curl "http://localhost:7071/api/getReferralStats?userId=1"
   ```

   You should see something like:

   ```json
   {
     "total_points": 100,
     "total_referrals": 1,
     "successful_referrals": 1,
     "recent_referrals": 1
   }
   ```

5. You can also test error conditions to ensure your functions handle them correctly.

   Test invalid referral code:

   ```bash
   curl -X POST http://localhost:7071/api/processReferral \
    -H "Content-Type: application/json" \
    -d '{
        "referralCode": "invalid",
        "newUserEmail": "another@example.com",
        "newUserName": "Another Friend"
    }'
   ```

   Test duplicate referral by using the same email:

   ```bash
   curl -X POST http://localhost:7071/api/processReferral \
    -H "Content-Type: application/json" \
    -d '{
        "referralCode": "a1b2c3d4",
        "newUserEmail": "friend@example.com",
        "newUserName": "Duplicate User"
    }'
   ```

6. The Azure Functions runtime will output logs to your terminal.

   Watch for any errors or debugging information as you test:

   ```bash
   [2024-11-29T14:30:00.000Z] Executing 'Functions.generateReferralCode'
      (Reason='This function was programmatically called via the host APIs.', Id=...)
   ```

If you encounter any issues, you can check the function logs or query the database directly to understand what's happening. Common issues might include:

If you need to, you can reset the test data in your database state by running the following SQL commands to delete the test data:

```sql
-- Clean up test data
DELETE FROM referrals;
DELETE FROM referral_codes;
DELETE FROM users;
-- Reset sequences
ALTER SEQUENCE users_user_id_seq RESTART WITH 1;
ALTER SEQUENCE referral_codes_code_id_seq RESTART WITH 1;
ALTER SEQUENCE referrals_referral_id_seq RESTART WITH 1;
```

After testing your functions locally, you're ready to deploy them to Azure.

## Deploying to Azure

Deploying your referral system to Azure involves three main steps:

1. Create Azure Resources
2. Configure Application Settings
3. Deploy Your Functions

Let's walk through each step.

### Step 1: Create Azure Resources

First, you need to create the necessary resources in Azure: a resource group, a storage account, and a Function App.

1. Log in to Azure CLI:

   ```bash
   az login
   ```

   Follow the instructions to authenticate with your Azure account and select the appropriate subscription.

2. Create a resource group:

   ```bash
   az group create --name referral-system --location eastus2
   ```

   This command creates a new resource group named `referral-system` in the East US 2 region. You can choose a different region if needed. A resource group is a logical container for your Azure resources and helps you manage them together more efficiently for billing, access control, and monitoring.

3. Create a storage account:

   Azure Functions requires a storage account to store logs and runtime state.

   ```bash
   az storage account create \
     --name <referral-storage-unique-name> \
     --location eastus \
     --resource-group referral-system \
     --sku Standard_LRS
   ```

   Make sure to replace `referral-storage-unique-name` with a unique name for your storage account. Otherwise, the command will fail.

4. Create the Function App:

   Use the following command to create a Function App running Node.js 18 on the Consumption Plan:

   ```bash
   az functionapp create \
     --name referral-system \
     --storage-account <referral-storage-unique-name> \
     --consumption-plan-location eastus2 \
     --resource-group referral-system \
     --runtime node \
     --runtime-version 18 \
     --functions-version 4
   ```

   Replace `referral-system` with your desired Function App name and `referral-storage-unique-name` with your storage account name.

### Step 2: Configure Application Settings

After creating the Function App, you need to configure it to connect to your Neon database by setting environment variables.

1. Set the `NEON_CONNECTION_STRING` using the Azure CLI:

   ```bash
   az functionapp config appsettings set \
     --name referral-system \
     --resource-group referral-system \
     --settings NEON_CONNECTION_STRING="postgres://user:password@ep-xyz.region.azure.neon.tech/neondb?sslmode=require&channel_binding=require"
   ```

2. Alternative: Configure settings in the Azure Portal:
   - Go to your **Function App** in the Azure Portal.
   - Select **Configuration** under the **Settings** section.
   - Add a new application setting:
     - **Name:** `NEON_CONNECTION_STRING`
     - **Value:** `postgres://user:password@ep-xyz.region.azure.neon.tech/neondb`
   - Save your changes.

### Step 3: Deploy Your Functions

With everything set up, you can now deploy your functions to Azure directly from Visual Studio Code.

- Open **VS Code** and press `F1` to open the command palette.
- Search for and select **Azure sign in** to authenticate with your Azure account.
- Then again press `F1` to open the command palette.
- Search for and select **Azure Functions: Deploy to Function App...**.
- Select your Azure subscription.
- Choose the Function App (`referral-system`) you created earlier.
- Click **Deploy** when prompted.

You'll see a notification in VS Code once the deployment is successful but it may take a few minutes to complete.

### Step 4: Verify the Deployment

After deploying your functions, test them to ensure they're working correctly.

1. Retrieve the function URL:

   Use the Azure CLI to get the URL for your `generateReferralCode` function:

   ```bash
   az functionapp function show \
     --name referral-system \
     --resource-group referral-system \
     --function-name generateReferralCode \
     --query "invokeUrlTemplate"
   ```

2. Test the function:

   Replace `<function-url>` with the URL returned from the previous step:

   ```bash
   curl -X POST <function-url> \
     -H "Content-Type: application/json" \
     -d '{"userId": 1}'
   ```

## Performance Optimization

With your referral system deployed to Azure, you should consider some standard performance optimizations to make sure it scales well under load.

1. Add indexes to frequently queried columns:

   ```sql
   CREATE INDEX idx_referral_codes_code ON referral_codes(code);
   CREATE INDEX idx_referrals_referrer ON referrals(referrer_id);
   CREATE INDEX idx_referrals_status ON referrals(status);
   ```

   You can learn more about indexing in the [Neon documentation](/postgresql/postgresql-indexes).

2. Implement connection pooling in your database utility:

   ```js
   const pool = new Pool({
     connectionString: process.env.NEON_CONNECTION_STRING,
     ssl: true,
     max: 20, // maximum number of clients
     idleTimeoutMillis: 30000,
     connectionTimeoutMillis: 2000,
   });
   ```

   Alternatively, you can use the [Neon connection pool](/docs/connect/connection-pooling) feature to manage connections efficiently.

3. On the Azure Functions side, consider enabling [Azure Functions Premium Plan](https://azure.microsoft.com/en-us/pricing/details/functions/), which offers more control over scaling and performance.

### Cleaning Up Azure Resources (Optional)

If you no longer need the Azure resources created for this project, you can delete them to avoid incurring any charges.

1. Deleting the resource group will remove all associated resources, including the Function App, storage account, and any other resources within the group:

   ```bash
   az group delete --name referral-system --yes --no-wait
   ```

   - The `--yes` flag skips the confirmation prompt.
   - The `--no-wait` flag allows the command to run asynchronously.

2. To verify the deletion:
   - Log in to the [Azure Portal](https://portal.azure.com).
   - Navigate to **Resource Groups** and confirm that the `referral-system` group is no longer listed.

## Conclusion

You now have a scalable referral system built with Neon Postgres and Azure Functions. The system handles referral code generation, tracks successful referrals, awards points, and provides referral statistics.

As a next step, you can add more features to your referral system, such as authentication, email notifications, and user dashboards.

## Additional Resources

- [Neon Documentation](/docs)
- [Azure Functions Documentation](https://docs.microsoft.com/en-us/azure/azure-functions/)
- [Node-Postgres Documentation](https://node-postgres.com/)

<NeedHelp />


# Automating Workflows with Azure Logic Apps and Neon

---
title: Automating Workflows with Azure Logic Apps and Neon
subtitle: Learn how to automate database operations and processes by connecting Azure Logic Apps with Neon Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2025-01-26T00:00:00.000Z'
updatedOn: '2025-01-26T00:00:00.000Z'
---

Azure Logic Apps provides a way to build automated workflows that integrate apps, data, services, and systems.

When you combine it with Neon's serverless Postgres, you can create automation solutions that handle database operations based on various triggers and events.

In this guide, you'll learn how to create Logic Apps workflows that interact with Neon Postgres to automate common processes and database operations.

## Prerequisites

Before starting this guide, ensure you have:

- [Azure account](https://azure.microsoft.com/free/) with an active subscription
- [Neon account](https://console.neon.tech/signup) and project
- Basic familiarity with SQL and Azure services

## Understanding Azure Logic Apps

[Azure Logic Apps](https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-overview) is a cloud service that helps you schedule, automate, and orchestrate tasks, business processes, and workflows. With Logic Apps, you can:

- Trigger workflows based on events or schedules
- Connect to various services and APIs
- Process data using built-in operations and custom implementations
- Handle errors, implement retries, and monitor your workflow execution

## Setting Up Your Environment

Before you can start creating workflows with Azure Logic Apps and Neon, you need to set up your environment.

### Step 1: Create a Neon Project

1. Log in to the [Neon Console](https://console.neon.tech)
2. Click "New Project"
3. Select Azure as your cloud provider and pick a region
4. Name your project
5. Click "Create Project"

Save your connection details - you'll need them to connect Logic Apps to your database.

### Step 2: Create Database Schema

For the purpose of this guide, let's create a sample schema for tracking customer orders. This will help demonstrate various automation scenarios:

```sql
CREATE TABLE orders (
    customer_email VARCHAR(255) NOT NULL,
    product_name VARCHAR(100) NOT NULL,
    quantity INTEGER NOT NULL,
    status VARCHAR(50) DEFAULT 'pending',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

You can run the SQL statement in the Neon Console's SQL editor to create the schema. This table will store order details, including the customer's email, product name, quantity, status, and creation timestamp.

### Step 3: Set Up Azure Logic Apps

With your Neon project and database schema in place, you can now set up Azure Logic Apps.

1. Go to the [Azure Portal](https://portal.azure.com)
1. Search for "Logic Apps" and select it
1. Click "Add" to create a new Logic App
1. Select "Consumption" plan type
1. Fill in the basics:
   - Choose your subscription
   - Select or create a resource group
   - Name your Logic App
   - Choose the region, best to pick the same region as your Neon project
1. Click "Review + create" and then "Create"

This will take a few moments to deploy your Logic App. Once it's ready, you can start building workflows.

## Creating Your First Workflow

Before you start building your first workflow, let's understand the key components of an Azure Logic App:

- **Trigger**: An event that starts the workflow. It could be a timer, HTTP request, or a change in a service. Each workflow must have at least one trigger.
- **Action**: A step that performs a specific operation, such as sending an email, updating a database, or calling an API.

You can chain multiple actions together to create complex workflows that automate business processes.

Let's create a workflow that automatically processes new orders. For our fictional store the workflow will:

1. Trigger when a new order record is created
1. Update the order status in the database
1. Send a notification email to the customer

### Step 1: Configure a Trigger

Every Logic App workflow starts with a trigger and each workflow needs at least one trigger.

For our order processing workflow, we'll use an HTTP trigger that listens for new orders:

1. Open your Logic App in the Azure Portal
1. Go to the Logic App Designer, it is under "Development Tools"
1. Click "Add a trigger"
1. Search for "When an HTTP request is received" and select it
1. For the payload schema you can use the following example:
   ```json
   {
     "type": "object",
     "properties": {
       "customerEmail": {
         "type": "string"
       },
       "productName": {
         "type": "string"
       },
       "quantity": {
         "type": "integer"
       }
     },
     "required": ["customer_email", "product_name", "quantity"]
   }
   ```
1. Click "Save" which will generate a URL that you can use to trigger the workflow

### Step 2: Add a Postgres Connection

With the trigger in place, you can now add actions to the workflow. Let's start by connecting to your Neon Postgres database where you store order data:

1. While in the Logic App Designer, click "Add an action"
1. Search for "PostgreSQL" and select "Insert row"
1. Configure the connection:
   - Connection Name: "NeonConnection"
   - Server: Your Neon host (e.g., "ep-cool-smoke-123456.eastus2.azure.neon.tech")
   - Database name: Your database name
   - Username: Your Neon database username
   - Password: Your Neon database password
   - Encrypt connection: Yes
1. Click "Create"

### Step 3: Insert Order Data

One important limitation to note in the [Azure Logic Apps PostgreSQL connector](https://learn.microsoft.com/en-gb/connectors/postgresql/#insert-row) is that insert row operation requires you to provide explicit value for Primary Key column, even though default/autoincrement value is defined.

With the database connection set up, you can now insert the order data into the database using the trigger payload:

1. While still in the Logic App Designer, from the "Table name" dropdown, select the "orders" table you created earlier
1. For the "Columns" field, set the values:
   - customer_email: `@triggerBody()?['customerEmail']`
   - product_name: `@triggerBody()?['productName']`
   - quantity: `@triggerBody()?['quantity']`
1. If you were to check "Code View", you would see the following JSON:
   ```json
   {
     "type": "ApiConnection",
     "inputs": {
       "host": {
         "connection": {
           "referenceName": "postgresql"
         }
       },
       "method": "post",
       "body": {
         "customer_email": "@triggerBody()?['customerEmail']",
         "product_name": "@triggerBody()?['productName']",
         "quantity": "@triggerBody()?['quantity']"
       },
       "path": "/datasets/default/tables/@{encodeURIComponent(encodeURIComponent('[public].[orders]'))}/items"
     },
     "runAfter": {}
   }
   ```
1. After adding the database action, click "Save"

### Step 3: Test the Workflow

To test the workflow, let's use the Azure Logic Apps run interface:

1. Go to the Logic App Designer
1. Click the "Run" dropdown and select "Run with payload" to trigger the workflow
1. Under "Request body", enter sample order data:
   ```json
   {
     "customerEmail": "test@example.com",
     "productName": "Laptop",
     "quantity": 2
   }
   ```
1. Click "Run" to execute the workflow
1. Check the database to verify that the order was inserted

## Extending Your Workflow

Once your basic workflow is working, you can extend it by integrating additional actions. One common enhancement is sending an email notification when a new order is placed.

### Adding an Email Notification

You can use the built-in email actions in Azure Logic Apps to notify customers when their order is received:

1. In the Logic App Designer, click **Add an action** after the database insertion step.
2. Search for "Send an email" and select your preferred email service (e.g., **Outlook.com**, **Gmail**, or **SendGrid**).
3. Follow the prompts to connect your email provider.
4. Depending on the service you choose, you may need to authorize Azure Logic Apps to send emails on your behalf. For example, if you're using Gmail, you can follow [this guide](https://learn.microsoft.com/en-gb/connectors/gmail/) to set up the email action.
5. Click "Save".

Now, every time a new order is placed, the Logic App will automatically insert the order into your Neon Postgres database and send a confirmation email to the customer.

## Conclusion

Azure Logic Apps and Neon Postgres provide a straightforward way to automate workflows and database operations. Whether you need to process orders, send notifications, or connect different services, this approach allows you to set up reliable automation with minimal effort.

Thanks to the Azure Logic Apps PostgreSQL connector, you can easily integrate your Neon Postgres database into your workflows and build powerful automation solutions without writing code or managing infrastructure. Azure Logic Apps has a large number of connectors available, so you can easily integrate with other services and systems to create complex workflows.

## Additional Resources

- [Azure Logic Apps Documentation](https://docs.microsoft.com/azure/logic-apps/)
- [Neon Documentation](/docs)
- [PostgreSQL Connector Documentation](https://docs.microsoft.com/connectors/postgresql/)

<NeedHelp />


# Connect Azure services to Neon with Azure Service Connector

---
title: 'Connect Azure services to Neon with Azure Service Connector'
subtitle: 'Learn how to connect Azure compute services to Neon using Azure Service Connector'
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-04-04T00:00:00.000Z'
updatedOn: '2025-04-04T00:00:00.000Z'
---

Azure Service Connector lets you connect Azure compute services to backend services like Neon. It achieves this primarily by managing connection secrets (like database credentials using Azure Key Vault for secure storage) and configuring your application with the necessary connection details, typically via environment variables. Your application code then uses this injected configuration with standard database drivers and libraries to connect to Neon.

While Service Connector offers significant automation for some Azure-native services (like using Managed Identities), its integration with Neon currently relies on **connection string authentication**. This means you'll need to provide your Neon database credentials during setup, but Service Connector helps manage how these credentials are stored and exposed to your application.

This guide demonstrates connecting an Azure compute service (using **Azure App Service** as an example) to your Neon database using the Azure Service Connector via the Azure Portal.

## Prerequisites

- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/).
- An Azure compute service to connect to Neon. Azure Service Connector supports the following compute services:
  - Azure App Service
  - Azure Functions
  - Azure Container Apps
  - Azure Kubernetes Service (AKS)
  - Azure Spring Apps
- An existing Neon project created on Azure. If you don't have one, see [Get started with Neon Serverless Postgres on Azure](/guides/neon-azure-integration).
- Your Neon project's connection details readily available:
  - **Hostname (endpoint ID)**: e.g., `ep-abc-123.eastus2.azure.neon.tech`
  - **Database name**: e.g., `neondb`
  - **Role (username)**: e.g., `neondb_owner`
  - **Password**
  - You can find these in the **Connection to your database** widget on Neon Console. See [Connect from any application](/docs/connect/connect-from-any-app).

## Supported authentication for Neon

Currently, Azure Service Connector supports connecting to Neon **only via connection string**.

- You will provide your Neon database hostname, database name, username, and password during the Service Connector setup.
- Managed Identity and Service Principal authentication methods, which provide passwordless connections and automated credential rotation for some Azure services, are **not** currently supported when connecting to Neon via Service Connector.

## Connecting via the Azure portal

Follow these steps to create a service connection from your Azure compute service (e.g., App Service) to Neon.

1.  **Navigate to your Azure compute service:** In the Azure portal, locate and select the specific App Service, Function App, or other compute resource you want to connect to Neon.
2.  **Open service connector:** In the service's left-hand menu, scroll down to the **Settings** section and select **Service Connector**.

    ![Service connector menu item](/docs/guides/azure-service-connector/service-connector-menu-item.png)

3.  **Start connection creation:** Click the **+ Create** button on the Service Connector page.
4.  **Configure basics:**
    - **Service type:** Search for and select `Neon Serverless Postgres`.

      ![Select Neon service type](/docs/guides/azure-service-connector/service-type-selection.png)

    - **Connection name:** Assign a descriptive name for this connection within Azure (e.g., `neon_db_connection`), or accept the auto-generated name. This name is for Azure management purposes.
    - **Neon Postgres hostname:** **Manually enter** the full hostname (including the endpoint ID) from your Neon project's connection details. _(Service Connector currently cannot automatically discover existing Neon resources)._
    - **Neon Postgres database name:** **Manually enter** the name of the Neon database you wish to connect to. (e.g., `neondb`).
    - **Client type:** Select the primary programming language or framework your application uses (e.g., `.NET`, `Python`, `Java`, `Node.js`, `Go`, etc.). This choice influences the naming convention and format of the environment variables Service Connector creates.
    - Click **Next: Authentication**.

5.  **Configure authentication:**
    - The **Connection string** option will be pre-selected, as it's the only supported method for Neon.
    - You now need to provide your Neon **Username** and **Password**. Service Connector offers two ways to handle the _password_:
      1.  **Database credentials:**

          You can use database credentials for the first time connection to create a new Key Vault secret. For applications that already have a Key Vault secret, you can use the Key Vault option to reference the existing secret.

                  ![Select Authentication Tile](/docs/guides/azure-service-connector/authentication-type.png)

          - Select the **Database credentials** tile.
          - Enter your Neon database **Username**.
          - Enter your Neon database **Password**.

            <Admonition type="important" title="Important">
            Check the **Store Secret in Key Vault** box. This prompts you to select an existing Azure Key Vault or **Create new**.

                ![Select Store secret in Key Vault](/docs/guides/azure-service-connector/key-vault-selection.png)

            If you create new, Azure provisions a Key Vault instance and securely stores _the password_ as a secret within it.

            ![Store Secret in Key Vault Option](/docs/guides/azure-service-connector/key-vault-creation.png)

            Service Connector will then reference this secret. This is more secure than storing the password directly in App Service configuration.
            </Admonition>

      2.  **Key Vault reference (for pre-existing Key Vault secret):**
          - Select the **Key Vault** tile.
          - Choose the **Subscription** and **Key vault** containing your _pre-existing_ secret.
          - Select the **Secret** that holds your Neon database _password_.
          - Enter your Neon database **Username** manually in the provided field.

    - Review the **Configuration information** section. This previews the environment variables Service Connector will set based on your choices (e.g., `NEON_POSTGRESQL_CONNECTIONSTRING` or individual components).

      ![Advanced configuration information](/docs/guides/azure-service-connector/advanced-configuration.png)

      If your application uses a custom naming convention for environment variables that differs from the default ones provided by Service Connector, you can choose to modify the variable names accordingly by clicking the **Edit** icon next to the variable name.

    - Click **Next: Networking**.

6.  **Configure networking:**
    - For Neon connections via Service Connector in the portal, you can **skip** this step. Network access controls (like IP allow lists) are managed directly within your Neon project settings, not through Service Connector's network configuration options (Firewall, Service Endpoint, Private Endpoint) which apply primarily to Azure target services.
    - Refer to Neon's [IP Allow](/docs/introduction/ip-allow) documentation to configure network access if needed.
    - Click **Next: Review + Create**.

7.  **Review and create:**
    - Review the summary of the connection details. Verify the target service (Neon), compute service, authentication method, and especially the environment variables that will be created.
    - Click **Create**.

Azure will now provision the connection. This process might take a minute or two. Service Connector configures the necessary settings on your Azure compute service (primarily environment variables, potentially linking to Key Vault).

You can confirm the connection was created successfully by returning to the **Service Connector** page for your compute service. The new Neon connection should be listed.

    ![Service Connector Created Successfully](/docs/guides/azure-service-connector/service-connector-created.png)

## Using the connection in your application

After successful creation, Service Connector injects the connection details into your Azure compute service's environment. Your application code accesses these environment variables to connect to Neon using a standard PostgreSQL driver or library appropriate for your chosen language/framework.

How Service Connector exposes the details depends on the **Client type** you selected:

- **For most client types (.NET, Python, Go, Java, PHP, Ruby):** Service Connector creates a single environment variable named `NEON_POSTGRESQL_CONNECTIONSTRING` containing the full, formatted connection string.
- **For other client types (like Node.js, Django or other):** Service Connector creates individual environment variables like `NEON_POSTGRESQL_HOST`, `NEON_POSTGRESQL_PORT`, `NEON_POSTGRESQL_DATABASE`, `NEON_POSTGRESQL_USER`, and `NEON_POSTGRESQL_PASSWORD`.

You can view the configured environment variables in your App Service under **Settings** -> **Environment variables**.

    ![Neon environment variables in App Service](/docs/guides/azure-service-connector/environment-variables.png)

Here's an example of how to use the connection string in a Python application using the `psycopg2` library:

```python
import os
import psycopg2

connection_string = os.environ.get("NEON_POSTGRESQL_CONNECTIONSTRING")

if not connection_string:
    print("Error: NEON_POSTGRESQL_CONNECTIONSTRING environment variable not set.")
    # Handle error appropriately - raise exception, exit, etc.
else:
    conn = None
    try:
        conn = psycopg2.connect(connection_string)
        print("Successfully connected to Neon!")

        with conn.cursor() as cur:
            cur.execute("SELECT version()")
            print(f"PostgreSQL version: {cur.fetchone()[0]}")

    except (Exception, psycopg2.DatabaseError) as error:
        print(f"Error connecting to Neon: {error}")
    finally:
        if conn is not None:
            conn.close()
            print("Database connection closed.")
```

Adapt the code to fetch and use the environment variables according to your application's language, framework, and the specific variables created by Service Connector based on your "Client type" selection. Refer to the table below or the [Azure documentation](https://learn.microsoft.com/en-us/azure/service-connector/how-to-integrate-neon-postgres#default-environment-variable-names-or-application-properties-and-sample-code) for specific variable names.

**Environment variables and properties by client type**

| Client type             | Primary configuration           | Example variable / property                                                                                                                           | Notes                                                                                                                                                        |
| :---------------------- | :------------------------------ | :---------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------- |
| .NET                    | Env var: connection string      | `NEON_POSTGRESQL_CONNECTIONSTRING`                                                                                                                    | Standard Npgsql format. (eg, `Server=ep-still-mud-a12aa123.eastus2.azure.neon.tech;Database=<database-name>;Port=5432;Ssl Mode=Require;User Id=<username>`). |
| Java (JDBC)             | Env var: connection string      | `NEON_POSTGRESQL_CONNECTIONSTRING`                                                                                                                    | `jdbc:postgresql://...` format.                                                                                                                              |
| Java (Spring Boot JDBC) | Application Properties          | `spring.datasource.url`, `...username`, `...password`                                                                                                 | Service Connector sets corresponding env vars that Spring Boot picks up.                                                                                     |
| Python (psycopg2)       | Env var: connection string      | `NEON_POSTGRESQL_CONNECTIONSTRING`                                                                                                                    | Key-value format `dbname=... host=... user=... password=... port=... sslmode=require&channel_binding=require`                                                |
| Go (pg)                 | Env var: connection string      | `NEON_POSTGRESQL_CONNECTIONSTRING`                                                                                                                    | Similar key-value format as Python.                                                                                                                          |
| Node.js (pg)            | Env Vars: Individual Components | `NEON_POSTGRESQL_HOST`, `NEON_POSTGRESQL_USER`, `NEON_POSTGRESQL_PASSWORD`, `NEON_POSTGRESQL_DATABASE`, `NEON_POSTGRESQL_PORT`, `NEON_POSTGRESQL_SSL` | Construct connection object/string from parts.                                                                                                               |
| PHP                     | Env var: connection string      | `NEON_POSTGRESQL_CONNECTIONSTRING`                                                                                                                    | Key-value format.                                                                                                                                            |
| Ruby                    | Env var: connection string      | `NEON_POSTGRESQL_CONNECTIONSTRING`                                                                                                                    | Key-value format.                                                                                                                                            |
| Django                  | Env vars: individual components | `NEON_POSTGRESQL_NAME`, `NEON_POSTGRESQL_HOST`, `NEON_POSTGRESQL_USER`, `NEON_POSTGRESQL_PASSWORD`                                                    | Construct Django database settings from parts.                                                                                                               |

## Resources

- [Neon on Azure](/docs/manage/azure)
- [Official Azure Service Connector Documentation](https://learn.microsoft.com/en-us/azure/service-connector/)
- [Azure Docs: Integrate Neon Serverless Postgres with Service Connector](https://learn.microsoft.com/en-us/azure/service-connector/how-to-integrate-neon-postgres)
- [Neon Documentation](/docs)
- [Find your Neon connection details](/docs/connect/connect-from-any-app)
- [Neon IP Allowlisting](/docs/introduction/ip-allow)

<NeedHelp />


# Building Azure Static Web Apps with Neon

---
title: Building Azure Static Web Apps with Neon
subtitle: A step-by-step guide to creating and deploying static sites using Azure and Neon Postgres
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2024-12-14T00:00:00.000Z'
updatedOn: '2024-12-14T00:00:00.000Z'
---

If you’re looking for a modern way to deploy web applications without managing traditional server infrastructure, [Azure Static Web Apps](https://azure.microsoft.com/en-us/products/app-service/static) might be just what you need. It’s a service optimized for hosting static assets with global distribution, but its real strength lies in its integration with [Azure Functions](https://azure.microsoft.com/en-us/products/functions) for backend operations.

One of the most compelling features of Azure Static Web Apps is its built-in CI/CD pipeline powered by [GitHub Actions](https://github.com/features/actions). When you connect your repository, Azure configures the required GitHub workflows automatically. You only need to push your code, and GitHub Actions will build, optimize, and deploy your entire application across a global network.

In this guide, you’ll learn how to build a simple to-do application using Azure Static Web Apps. You’ll cover the basics of getting your website online and creating your first API endpoint with Azure Functions. By the end, you’ll understand how to combine static web content with dynamic features to build a fully functional web application.

## Prerequisites

Before you begin, make sure you have:

- [Node.js](https://nodejs.org/) 18.x or later installed
- [Visual Studio Code](https://code.visualstudio.com/)
- An [Azure account](https://azure.microsoft.com/free/) with an active subscription

## Create a Neon Project

Neon is now available in Azure! You can create serverless Postgres databases that run on Azure infrastructure. To learn more about Neon's Azure launch, check out the [announcement post](/blog/neon-is-coming-to-azure).

To create your Neon project on Azure, follow our [Getting Started with Neon on Azure guide](/guides/neon-azure-integration).

Once created, save your database connection string, which you'll need to connect to your Neon Postgres database from Azure Functions.

## Database Schema

For your todo application, you'll need a simple database schema to store todo items. You'll create a `todos` table with the following fields:

- `id`: Auto-incrementing unique identifier (Primary Key)
- `text`: Required text field to store the task description
- `completed`: Boolean field with a default value of false to track task completion status

\*You'll be creating the table via Azure Functions later in the guide.

## Setting up your development environment

To begin building your Azure Static Web App with Neon Postgres, you'll need to set up your development environment. This involves installing the required tools and configuring your project.

### Installing required tools

1. Install the **Azure Static Web Apps** and **Azure Functions** extensions for Visual Studio Code:
   - Open VS Code.
   - Click the Extensions icon or press `Ctrl+Shift+X` or `Cmd+Shift+X`.
   - Search for "Azure Static Web Apps" and "Azure Functions" extensions.
   - Install both of the extensions from Microsoft.

   ![Extensions to Download](/docs/guides/swa-extensions-to-download.png)

2. Install **Azure Functions Core Tools version 4.x**:

   In Visual Studio Code, select `F1` to open the command palette, and then search for and run the command 'Azure Functions: Install or Update Core Tools' and select **Azure Functions v4**.

3. Install the **Azure Static Web Apps CLI**:

   Open a terminal and run the following command to install the Azure Static Web Apps CLI:

   ```bash
   npm install -g @azure/static-web-apps-cli
   ```

   The CLI makes it easy to run your static web app and Azure Functions locally for testing and debugging.

### Project setup

With the Azure tools installed, you can now create your new Azure Static Web App.

Open a terminal and navigate to the directory where you want to create your project:

```bash
mkdir swa-todo
cd swa-todo && code .
```

## Creating the Static Web App

You'll start by creating the frontend of your todo application. The frontend will be a simple HTML, CSS, and JavaScript application that allows you to add, update, and delete todo items. For the backend, you'll use Azure Functions to handle API requests and interact with the Neon Postgres database.

Architecture overview:

- Frontend: A web application built with HTML, CSS, and JavaScript.
- Backend: Serverless API endpoints using Azure Functions.
- Hosting: Azure Static Web Apps for reliable and scalable web hosting.
- Database: Neon serverless Postgres for storing todo data.

Project structure:

```
swa-todo/
├── index.html     # The main HTML file for the todo app
├── styles.css     # The CSS file for styling the app
└── app.js         # The JavaScript file for handling user interactions
└── api/           # Azure Functions backend
```

### Building the Frontend

Create a new file named `index.html` in the root directory of your project and add the following content:

```html
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Simple Todo App</title>
    <link rel="stylesheet" href="styles.css" />
  </head>
  <body>
    <div class="todo-container">
      <h1>Todo List</h1>
      <div class="todo-input">
        <input type="text" id="todoInput" placeholder="Add a new todo" />
        <button onclick="addTodo()">Add</button>
      </div>
      <ul id="todoList" class="todo-list">
        <!-- Todos will be inserted here -->
      </ul>
      <div id="error" class="error"></div>
    </div>
    <script src="app.js"></script>
  </body>
</html>
```

The simple html file contains a form for adding new todos and a list to display existing todos. To display the todos, we'll use JavaScript to fetch the data from the backend API and update the DOM accordingly.

Create a new file named `styles.css` in the root directory of your project and add the following CSS styles:

```css
/* Base styles */
body {
  font-family: Arial, sans-serif;
  max-width: 600px;
  margin: 0 auto;
  padding: 20px;
  background-color: #f5f5f5;
}

/* Container styles */
.todo-container {
  background-color: white;
  padding: 20px;
  border-radius: 8px;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
}

/* Input area styles */
.todo-input {
  display: flex;
  gap: 10px;
  margin-bottom: 20px;
}

input[type='text'] {
  flex-grow: 1;
  padding: 8px;
  border: 1px solid #ddd;
  border-radius: 4px;
}

button {
  padding: 8px 16px;
  background-color: #4caf50;
  color: white;
  border: none;
  border-radius: 4px;
  cursor: pointer;
  transition: background-color 0.2s;
}

button:hover {
  background-color: #45a049;
}

/* Todo list styles */
.todo-list {
  list-style: none;
  padding: 0;
}

.todo-item {
  display: flex;
  align-items: center;
  gap: 10px;
  padding: 10px;
  border-bottom: 1px solid #eee;
}

.todo-item:last-child {
  border-bottom: none;
}

.todo-item.completed span {
  text-decoration: line-through;
  color: #888;
}

/* Delete button styles */
.delete-btn {
  background-color: #f44336;
  margin-left: auto;
}

.delete-btn:hover {
  background-color: #da190b;
}

/* Utility styles */
.loading {
  text-align: center;
  color: #666;
  padding: 20px;
}

.error {
  color: #f44336;
  margin-top: 10px;
}
```

The CSS file contains styles for the todo app, including the layout, input fields, buttons, and todo list items. You can customize the styles to match your design preferences. The example uses a simple design to help you get started.

Create a new file named `app.js` in the root directory of your project and add the following JavaScript code:

```javascript
// State management
let todos = [];

// Initialize app
document.addEventListener('DOMContentLoaded', loadTodos);

// Load todos from API
async function loadTodos() {
  try {
    showLoading();
    const response = await fetch('/api/todos');
    todos = await response.json();
    if (!response.ok) throw new Error('Failed to load todos');
  } catch (error) {
    showError(error.message);
  } finally {
    hideLoading();
    renderTodos();
  }
}

// Add new todo
async function addTodo() {
  const input = document.getElementById('todoInput');
  const text = input.value.trim();

  if (!text) return;

  try {
    const response = await fetch('/api/todos', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ text, completed: false }),
    });

    const todo = await response.json();
    if (!response.ok) throw new Error('Failed to add todo');
    const todoId = todo?.[0]?.id;

    todos.push({ id: todoId, text, completed: false });
    renderTodos();
    input.value = '';
  } catch (error) {
    showError(error.message);
  }
}

// Toggle todo completion
async function toggleTodo(id) {
  try {
    const todo = todos.find((t) => t.id === id);
    if (!todo) throw new Error('Todo not found');

    const response = await fetch('/api/todos', {
      method: 'PUT',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ id, completed: !todo.completed }),
    });

    await response.json();
    if (!response.ok) {
      throw new Error('Failed to update todo');
    }
    todos[todos.indexOf(todo)].completed = !todo.completed;
    renderTodos();
  } catch (error) {
    showError(error.message);
    await loadTodos();
  }
}

// Delete todo
async function deleteTodo(id) {
  try {
    const response = await fetch('/api/todos', {
      method: 'DELETE',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ id }),
    });

    await response.json();
    if (!response.ok) throw new Error('Failed to delete todo');

    todos = todos.filter((todo) => todo.id !== id);
    renderTodos();
  } catch (error) {
    showError(error.message);
    await loadTodos();
  }
}

// Render todos to DOM
function renderTodos() {
  const todoList = document.getElementById('todoList');
  if (!todoList) {
    console.error('Todo list container not found');
    return;
  }

  todoList.innerHTML = '';

  todos.forEach((todo) => {
    const li = document.createElement('li');
    li.className = `todo-item ${todo.completed ? 'completed' : ''}`;

    const checkbox = document.createElement('input');
    checkbox.type = 'checkbox';
    checkbox.checked = todo.completed;
    checkbox.addEventListener('change', () => toggleTodo(todo.id));

    const span = document.createElement('span');
    span.textContent = todo.text;

    const deleteButton = document.createElement('button');
    deleteButton.className = 'delete-btn';
    deleteButton.textContent = 'Delete';
    deleteButton.addEventListener('click', () => deleteTodo(todo.id));

    li.append(checkbox, span, deleteButton);
    todoList.appendChild(li);
  });
}

// Utility functions
function showLoading() {
  document.getElementById('todoList').innerHTML = '<div class="loading">Loading...</div>';
}

function hideLoading() {
  document.getElementById('todoList').innerHTML = '';
}

function showError(message) {
  const errorDiv = document.getElementById('error');
  errorDiv.textContent = message;
  setTimeout(() => (errorDiv.textContent = ''), 3000);
}
```

The JavaScript file contains the core functions for managing todos, namely loading todos from the backend API, adding new todos, toggling completion status, and deleting todos. It also includes utility functions for showing loading indicators and error messages to the user.

Let's break down the key functions and features of the todo app:

Core Functions:

1. `loadTodos()`
   - Fetches existing todos from the Azure Functions API endpoint via `GET` request
   - Handles loading states and error conditions
   - Automatically called when the DOM loads
   - Updates the UI with current todo items

2. `addTodo()`
   - Creates new todo items via `POST` request to the API
   - Validates input to prevent empty submissions
   - Updates local state and UI after successful creation
   - Includes error handling with user feedback

3. `toggleTodo(id)`
   - Updates todo completion status via `PUT` request
   - Includes error handling and state updates

4. `deleteTodo(id)`
   - Removes todos via `DELETE` request to the API
   - Updates local state after successful deletion

UI Management:

1. `renderTodos()`
   - Dynamically generates DOM elements for each todo.
   - Handles todo item styling based on completion status.
   - Creates interactive elements (checkboxes, delete buttons).

Utility Functions:

1. `showLoading()`
   - Displays loading indicator during API operations.
   - Provides visual feedback for better user experience.

2. `hideLoading()`
   - Removes loading indicator after operations complete.
   - Prepares UI for content display.

3. `showError(message)`
   - Displays error messages to users.
   - Implements auto-dismissing notifications (3-second timeout).
   - Provides clear feedback for error conditions.

State Management:

- All todo items are stored in the `todos` array.

Error Handling:

- Comprehensive try-catch blocks around API operations.
- Detailed error messages for debugging.

### Testing the Frontend locally

To test the frontend, in your terminal, run the following command to start a local server:

```bash
swa start
```

This will start a local server on [`http://localhost:4280`](http://localhost:4280) where you can access the todo app. Open the URL in your browser to be greeted with the initial todo list interface:

![Todo App Initial State](/docs/guides/swa-todo-app-intial.png)

\*Note that full functionality will be available once you set up the Azure Functions backend API.

### Creating Azure Functions

Azure Functions lets you write backend code that runs without managing any servers. We'll use it to create API endpoints that handle all our todo operations – creating, reading, updating, and deleting todos from our database. Think of these functions as small pieces of code that wake up when needed and automatically handle user requests.

Create a new directory named `api` in the root of your project:

```bash
mkdir api && cd api
```

Press F1 in VS Code to open the command palette and run the command `Azure Static Web Apps: Create HTTP Function`. Choose `JavaScript` as the language with the Model version `v4`. Name the function `todos` when prompted.

This will create a new Azure Function in the `api` directory, which will serve as the backend API for our todo application with an api endpoint `/api/todos`.

You'll need the [`@neondatabase/serverless`](https://www.npmjs.com/package/@neondatabase/serverless) package to connect to the Neon Postgres database. Install the package by running the following command in the `api` directory:

```bash
npm install @neondatabase/serverless
```

Edit the `api/src/functions/todos.js` file to add the following code:

```javascript
const { app } = require('@azure/functions');
const { neon } = require('@neondatabase/serverless');

const sql = neon(process.env.DATABASE_URL);

// Helper function to get todos and create table if it doesn't exist
const getTodos = async () => {
  try {
    const todos = await sql`SELECT * FROM todos`;
    return todos;
  } catch (error) {
    if (error.code === '42P01') {
      // Table does not exist, so create it
      await sql`
        CREATE TABLE todos (
          id SERIAL PRIMARY KEY,
          text TEXT NOT NULL,
          completed BOOLEAN NOT NULL
        )`;
      return [];
    }
    throw error;
  }
};

app.http('todos', {
  methods: ['GET', 'POST', 'PUT', 'DELETE'],
  authLevel: 'anonymous',
  handler: async (request, context) => {
    const method = request.method.toLowerCase();

    try {
      switch (method) {
        case 'get':
          context.log('Processing GET request for todos');
          const todos = await getTodos();
          return { status: 200, jsonBody: todos };

        case 'post':
          const newTodo = await request.json();
          context.log('Adding new todo:', newTodo);

          if (!newTodo.text) {
            return {
              status: 400,
              jsonBody: { error: 'Todo text is required' },
            };
          }

          const createdTodo = await sql`
            INSERT INTO todos (text, completed)
            VALUES (${newTodo.text}, ${newTodo.completed || false})
            RETURNING *
          `;
          return { status: 201, jsonBody: createdTodo };

        case 'put':
          const updatedTodo = await request.json();
          context.log('Updating todo:', updatedTodo);

          if (!updatedTodo.id) {
            return {
              status: 400,
              jsonBody: { error: 'Todo ID is required' },
            };
          }

          const todo = await sql`
            UPDATE todos
            SET completed = ${updatedTodo.completed}
            WHERE id = ${updatedTodo.id}
            RETURNING *
          `;

          if (todo.length === 0) {
            return {
              status: 404,
              jsonBody: { error: 'Todo not found' },
            };
          }
          return { status: 200, jsonBody: todo };

        case 'delete':
          const { id } = await request.json();
          context.log('Deleting todo:', id);

          if (!id) {
            return {
              status: 400,
              jsonBody: { error: 'Todo ID is required' },
            };
          }

          const deletedTodo = await sql`
            DELETE FROM todos
            WHERE id = ${id}
            RETURNING *
          `;

          if (deletedTodo.length === 0) {
            return {
              status: 404,
              jsonBody: { error: 'Todo not found' },
            };
          }
          return {
            status: 200,
            jsonBody: { message: 'Todo deleted successfully' },
          };

        default:
          return {
            status: 405,
            jsonBody: { error: 'Method not allowed' },
          };
      }
    } catch (error) {
      context.error(`Error processing ${method} request:`, error);
      return {
        status: 500,
        jsonBody: { error: `Failed to process ${method} request` },
      };
    }
  },
});
```

This Azure Function (`todos.js`) serves as our API endpoint and handles all database operations. Here's the detailed breakdown:

Core Components:

1. Database setup

   ```javascript
   const { neon } = require('@neondatabase/serverless');
   const sql = neon(process.env.DATABASE_URL);
   ```

   Establishes connection to our Postgres using the Neon serverless driver.

2. Auto-initialization of the `todos` table

   ```javascript
   const getTodos = async () => {
     // Creates the todos table if it doesn't exist
     // Returns all todos from the table
   };
   ```

   Handles first-time setup and retrieves todos.

3. Main API handler

   ```javascript
   app.http('todos', {
     methods: ['GET', 'POST', 'PUT', 'DELETE'],
     authLevel: 'anonymous',
     handler: async (request, context) => {
       // Request handling logic based on HTTP method
     },
   });
   ```

   Supported operations:
   1. `GET`: Retrieves all todos
      - Automatically creates table on first request.
      - Returns array of todo items.

   2. `POST`: Creates new todo
      - Requires: `text` field.
      - Returns: newly created todo.

   3. `PUT`: Updates todo completion status
      - Requires: `id` and `completed` status.
      - Returns: updated todo or `404` if not found.

   4. `DELETE`: Removes a todo
      - Requires: todo `id`.
      - Returns: success message or `404` if not found.

Error Handling:

- Input validation for required fields.
- Database error handling.
- Proper `HTTP` status codes.
- Detailed error messages for debugging.
- Logging via `context.log` for monitoring.

### Adding Neon Postgres connection string

Start by configuring the `local.settings.json` in your `api` directory with your Neon database connection string:

```json
{
  "Values": {
    ...
    "DATABASE_URL": "postgresql://neondb_owner:<your_password>@<your_host>.neon.tech/neondb?sslmode=require&channel_binding=require"
  }
}
```

Replace the `DATABASE_URL` value with your Neon Postgres connection string which you saved earlier.

### Testing the Azure Functions locally

To test the Azure Functions locally, navigate to the root directory of your project and run the following command:

```bash
cd ..
swa start --api-location api
```

This will start the Azure Functions backend and serve the static web app locally. You can access the app at [`http://localhost:4280`](http://localhost:4280) and test the functionality. It should be fully functional, allowing you to add, update, and delete todos. It should look like this:

![Todo App Completed](/docs/guides/swa-todo-app-completed.png)

## Deploying your Azure Static Web App

Once you've tested your Azure Static Web App locally and are satisfied with the functionality, you can deploy it to Azure to make it accessible to users worldwide.

### Creating a Static Web App in Azure

To deploy your Azure Static Web App, follow these steps:

1. Open the command palette in Visual Studio Code by pressing `F1`.
2. Search for and run the command `Azure Static Web Apps: Create Static Web App`.
3. Sign in to your Azure account if prompted.
4. When prompted, commit your changes to a Git repository.
5. Enter name for your Static Web App, for example `swa-todo`.
6. Enter repo name for your GitHub repository to push the code.
7. Choose the region to deploy your app, for example `East US 2`.
8. Select `HTML` for the build preset.
9. Enter `/` for the app location and build output path.

Once you connect your repository, Azure automatically sets up a GitHub Actions workflow file in your repository. This workflow handles the build and deployment process whenever you push changes. You can watch your deployment progress in real-time through either the GitHub Actions tab in your repository or the Azure portal.

### Add environment variables

Now that your Azure Static Web App is deployed, you will need to add the Neon Postgres connection string to the Azure Static Web App environment variables. This will allow your Azure Functions to connect to the Neon Postgres database.

1. Go to the Azure Portal and navigate to your Azure Static Web App resource.
2. Click on the `Environment Variables` under `Settings`.
3. Add a new environment variable with the key `DATABASE_URL` and the value as your Neon Postgres connection string.
   ![Environment Variables](/docs/guides/swa-neon-postgres-env-vars.png)
4. Click on `Apply` to save the changes.
5. Now visit the URL of your Azure Static Web App from the Overview tab to see your todo app live. The app should be fully functional allowing you to add, update, and delete todos.

## Summary

In this guide, you've built a simple todo application using Azure Static Web Apps and Neon Postgres. You've covered the following key steps:

1. Setting up your development environment with Azure Static Web Apps and Neon Postgres.
2. Creating the frontend of your todo app with HTML, CSS, and JavaScript.
3. Setting up Azure Functions to handle API requests and interact with the Neon Postgres database.
4. Testing your app locally and deploying it to Azure.

By combining Azure Static Web Apps with Neon Postgres, you can build powerful data-driven applications that are fast, reliable, and scalable. Azure Static Web Apps provides a robust hosting platform for static assets and serverless APIs, while Neon Postgres offers a serverless database solution that scales with your application. Together, they provide a seamless development experience for building modern web applications.

This guide should have helped you get started with Azure Static Web Apps and Neon Postgres. As a next step, you can look at [Neon Authorize](/docs/guides/neon-authorize) to add authentication and authorization to your app, allowing users to securely log in and manage their own todo lists.

## Additional Resources

- [Azure Static Web Apps Documentation](https://docs.microsoft.com/en-us/azure/static-web-apps/)
- [Neon Postgres Documentation](/docs/introduction)
- [Azure Functions Documentation](https://docs.microsoft.com/en-us/azure/azure-functions/)
- [Neon Authorize Guide](/docs/guides/neon-authorize)

<NeedHelp />


# Getting started with Neon and Better Stack

---
title: Getting started with Neon and Better Stack
subtitle: Send Neon metrics and Postgres logs to Better Stack using the OpenTelemetry integration
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-08-13T00:00:00.000Z'
updatedOn: '2025-08-13T00:00:00.000Z'
---

[Better Stack](https://betterstack.com/) is an observability platform that unifies logging, monitoring, and alerting into a single dashboard. [Neon's OpenTelemetry (OTEL) integration](/docs/guides/opentelemetry) allows you to send your project's metrics and Postgres logs directly to Better Stack, giving you a centralized view of your database's performance and activity.

This guide will walk you through setting up the integration between Neon and Better Stack. You'll learn how to:

- Create and configure an OpenTelemetry source in Better Stack.
- Set up the OpenTelemetry integration in your Neon project.
- Verify that your Neon metrics and logs are successfully flowing into Better Stack.

By the end, you'll have a complete observability pipeline from your Neon database to your Better Stack dashboard.

## Prerequisites

Before you begin, ensure you have the following:

- **Neon account and project:** If you don't have one, sign up at [Neon](https://console.neon.tech/signup). Your Neon project must be on the **Scale** or **Business** plan to use the OpenTelemetry integration.
- **Better Stack account:** A free Better Stack account. You can sign up at [Better Stack](https://betterstack.com/users/sign-up).

<Steps>

## Set up a Better Stack source

First, you need to create a data source in Better Stack that will receive logs and metrics from Neon.

1.  Log in to your [Better Stack account](https://telemetry.betterstack.com/).
2.  In the left-hand navigation, go to **Telemetry** > **Sources**. Click the **Connect source** button.

    ![Better Stack sources page](/docs/guides/betterstack-sources-page.png)

3.  On the **Connect source** page:
    - Under **Basic information**, provide a **Name** for your source (e.g., `neon`) and select your preferred **Data region**.
    - Under **Platform**, scroll down to the **Logs + Metrics** section and select **OpenTelemetry**.
    - Click **Create source** at the bottom of the page.

    ![Configuring the OpenTelemetry source in Better Stack](/docs/guides/betterstack-otel-config.png)

4.  After the source is created, you will be redirected to the source's overview page. This page contains the credentials Neon needs to send data.

    ![Better Stack source created page with Source token and Ingesting host](/docs/guides/betterstack-otel-source-created.png)

    > Copy the **Source token** and **Ingesting host** URL. You will need these in the next step to configure Neon.

## Configure the Neon OpenTelemetry integration

Now, you will use the credentials from Better Stack to configure the integration in your Neon project.

1.  Navigate to the [Neon Console](https://console.neon.tech) and select your project.
2.  From the sidebar, go to the **Integrations** page.
3.  Find the **OpenTelemetry** card and click **Add**.

    ![Neon Integrations page with OpenTelemetry card](/docs/guides/neon-add-otel.png)

4.  A sidebar form will open. Fill in the configuration details using the information you copied from Better Stack:
    - **Telemetry to export:** Check both **Metrics** and **Postgres logs** to send all available data.
    - **Connection:** Select **HTTP**.
    - **Endpoint:** Paste the **Ingesting host** URL from Better Stack. It should look like `https://xxxx.betterstackdata.com`.
      <Admonition type="important">
      Better Stack provides the Ingesting host without the `https://` prefix, so make sure to manually add `https://` at the beginning of the URL when configuring the endpoint.
      </Admonition>
    - **Authentication:** Select **Bearer**.
    - **Bearer Token:** Paste the **Source token** you copied from Better Stack.
    - **Resource attributes:** It's good practice to add a `service.name` attribute to identify your data source within Better Stack. For example, set the key to `service.name` and the value to `neon`.
    - Click **Save**.

    <Admonition type="note" title="Data Scope">
    The Neon OpenTelemetry integration sends data for all computes in your Neon project. For example, if you have multiple branches, each with an attached compute, metrics and logs will be collected and sent for each one.
    </Admonition>

    ![Configuring the Neon OpenTelemetry integration with Better Stack credentials](/docs/guides/neon-otel-config.png)

    > The integration is now active and will begin sending data from your Neon project's computes to Better Stack.

## Verify the data flow in Better Stack

To confirm that your integration is working:

1. Go back to your Better Stack dashboard.
2. Select the source you created for Neon (e.g., `neon`).
3. Click **Logs** to open the **Live tail** view and monitor incoming logs in real time.

You should see data from your Neon project appearing here, indicating a successful connection.

<Admonition type="note">
It may take a few minutes for the first logs to appear after you enable the integration.
</Admonition>

You should see Postgres logs from your Neon compute streaming into the Live tail view. This confirms that the integration is working correctly.

![Live tail view in Better Stack showing logs from Neon](/docs/guides/betterstack-live-tail.png)

<Admonition type="info" title="Compute Activity">
Neon computes only send logs and metrics when they are active. If you have the [Scale to Zero](/docs/manage/endpoints#scale-to-zero) feature enabled and a compute is suspended due to inactivity, no telemetry data will be sent. If you notice gaps in your data, check your compute's status on the **Branches** page in the Neon console.
</Admonition>

## Visualizing Neon Metrics with Dashboards

While the **Live tail** view is perfect for real-time log inspection, Better Stack's **Dashboards** are the best way to visualize your Neon metrics, track trends over time, and get a high-level overview of your database's health.

### Clean up default dashboards

When you first navigate to the **Dashboards** page in Better Stack, you may see two pre-created dashboards: `OpenTelemetry collector` and `OpenTelemetry Tracing`.

![Better Stack dashboards page showing default dashboards.](/docs/guides/betterstack-default-dashboards.png)

These default dashboards are designed for a generic OpenTelemetry collector setup and are **not compatible** with the metrics sent by Neon. They will appear empty or show errors because they are looking for metrics that don't exist in Neon's data stream.

### Delete default dashboards

To avoid confusion, it is strongly recommended to delete these two default dashboards. Click on each dashboard, find the three-dot menu (...) in the top-right corner, and select **Remove**.

### Create a custom Neon dashboard

Create a new dashboard from scratch to visualize your Neon metrics.

1.  From the **Dashboards** page, click the **Create dashboard** button.
2.  Select **Blank dashboard** and click **Add dashboard**.
3.  Give your new dashboard a descriptive name, such as `Neon Project Metrics`, and click **Save**.

### Create your first chart

Add a chart to visualize a specific metric.

1.  On your blank dashboard, click the **+ Create chart** button.
2.  The first step is to connect the chart to your data. At the top, click the **source** button and choose your `neon` source.
3.  The chart builder will appear. Use the **Drag & drop** query builder:
    - The **X-axis** is already set to `time`, which is what you'll want for time-series data.
    - Delete the default Y-axis metric.
    - In the **Y-axis**, select the metric you want to visualize. From the list of available metrics, drag a metric like `neon_connection_counts` into the Y-axis box.
      ![Neon connection counts chart creation](/docs/guides/betterstack-neon-connection-counts-creation.png)
    - Click **Run query** to see the chart populated with data.
4.  You can customize the chart's appearance using the panel on the right. Once you're happy, click **Save** in the top-right corner to add the chart to your dashboard.

To view the metric on your dashboard, simply locate the chart you created and observe the visualized data in real time.

![Neon connection counts chart](/docs/guides/betterstack-neon-connection-counts.png)

### Explore available Neon metrics

Neon exports a rich set of metrics that you can use to build your dashboards. These include both Neon-specific metrics and general compute host metrics.

For example, you can build charts to visualize:

- `neon_connection_counts`: To monitor the number of active and idle database connections.
- `neon_db_total_size`: To monitor the total size of all databases in your project, in bytes.
- `host_cpu_seconds_total`: To track the number of CPU seconds accumulated in different operating modes (user, system, idle, etc.)

For a comprehensive list of all metrics you can use in your dashboards, see the [Neon Metrics and Logs Reference](/docs/reference/metrics-logs).

</Steps>

## Summary

Congratulations! You have successfully configured Neon to send metrics and Postgres logs to Better Stack using the OpenTelemetry integration. You now have a powerful, centralized observability setup that provides real-time insights into your database's health and activity.

By leveraging this integration, you can build dashboards, set up alerts, and troubleshoot issues more effectively, all from within the Better Stack platform.

## Resources

- [Neon OpenTelemetry Integration](/docs/guides/opentelemetry)
- [Neon Metrics and logs reference](/docs/reference/metrics-logs)
- [Better Stack Logs Documentation](https://betterstack.com/docs/logs/start/)
- [OpenTelemetry Protocol (OTLP) Specification](https://opentelemetry.io/docs/specs/otlp/)

<NeedHelp/>


# Caching Layer in Postgres

---
title: Caching Layer in Postgres
subtitle: A step-by-step guide describing how to use materialized views for caching in Postgres
author: vkarpov15
enableTableOfContents: true
createdAt: '2025-03-21T13:24:36.612Z'
updatedOn: '2025-03-21T13:24:36.612Z'
---

PostgreSQL provides powerful tools to optimize query performance, including caching layers that help reduce expensive computations.
[Materialized views](https://www.postgresql.org/docs/current/rules-materializedviews.html) can be used for caching: materialized views store the results of a query and can be refreshed on demand.
This approach is particularly useful for complex aggregations, expensive joins, and frequently accessed datasets that do not require real-time updates.

## Steps

- Create the orders table
- Insert sample data into the orders table
- Create a materialized view
- Refresh the materialized view
- Index the materialized view for performance
- Automate materialized view refreshes

### Create the Orders Table

Before inserting sample data, create the `orders` table:

```sql
CREATE TABLE orders (
  id SERIAL PRIMARY KEY,
  customer_id INT NOT NULL,
  total_price NUMERIC(10,2) NOT NULL,
  created_at TIMESTAMP NOT NULL DEFAULT NOW()
);
```

### Insert Sample Data into the Orders Table

Now, let's insert some sample data into the `orders` table so we can see real results:

```sql
INSERT INTO orders (customer_id, total_price, created_at) VALUES
(1, 100.00, NOW() - INTERVAL '1 day'),
(2, 250.50, NOW() - INTERVAL '2 days'),
(3, 75.25, NOW() - INTERVAL '3 days'),
(1, 300.00, NOW() - INTERVAL '3 days'),
(2, 450.75, NOW() - INTERVAL '4 days');
```

### Create a Materialized View

Suppose you want to answer queries like "which days did we receive the most orders over the last month?"
This would require a complex query that may be slow.
Instead of recalculating revenue per day on every query, you can store daily revenue as a materialized view.

```sql
CREATE MATERIALIZED VIEW daily_revenue AS
SELECT DATE(created_at) AS order_date, SUM(total_price) AS total_revenue
FROM orders
GROUP BY order_date;
```

### Query the materialized View

This materialized view stores total revenue per day, allowing for fast lookups of daily sales trends without needing to aggregate the full `orders` table repeatedly.
For example, you can execute a query to find the daily revenue for 3 days ago:

```sql
SELECT * FROM daily_revenue WHERE order_date = DATE(NOW() - INTERVAL '3 days');
```

Or you can sort days by `total_revenue` as follows.

```sql
SELECT * FROM daily_revenue ORDER BY total_revenue DESC;
```

The above query returns the following result, which shows the days with the most order revenue.

| #   | order_date | total_revenue |
| --- | ---------- | ------------- |
| 1   | 2025-03-17 | 450.75        |
| 2   | 2025-03-18 | 375.25        |
| 3   | 2025-03-19 | 250.50        |
| 4   | 2025-03-20 | 100.00        |

### Refresh the Materialized View

Materialized views need to be refreshed to reflect updated data.
You can refresh a materialized view manually as follows.

```sql
REFRESH MATERIALIZED VIEW daily_revenue;
```

If the query should be available while refreshing, use the `CONCURRENTLY` option:

```sql
REFRESH MATERIALIZED VIEW CONCURRENTLY daily_revenue;
```

This allows the materialized view to remain accessible while it's being refreshed, but requires a unique index on the view. Without `CONCURRENTLY`, the materialized view is locked during the refresh, making it temporarily unavailable for queries.

### Index the Materialized View for Performance

Adding indexes to materialized views can significantly improve query performance. For example, to index `order_date` for faster lookups:

```sql
CREATE INDEX idx_daily_revenue_date ON daily_revenue(order_date);
```

### Automate Materialized View Refreshes

To keep the materialized view updated automatically, use a **cron job** or **PostgreSQL's built-in job scheduler** (like pg_cron). Here’s an example using `pg_cron` to refresh every hour:

```sql
SELECT cron.schedule('refresh_daily_revenue', '0 * * * *', $$REFRESH MATERIALIZED VIEW CONCURRENTLY daily_revenue$$);
```


# Build a RAG chatbot with Astro, Postgres, and LlamaIndex

---
title: Build a RAG chatbot with Astro, Postgres, and LlamaIndex
subtitle: A step-by-step guide for building a RAG chatbot in an Astro application with LlamaIndex and Postgres
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-06-11T00:00:00.000Z'
updatedOn: '2024-06-11T00:00:00.000Z'
---

## Prerequisites

To follow the steps in this guide, you will need the following:

- [Node.js 18](https://nodejs.org/en) or later
- A [Neon](https://console.neon.tech/signup) account
- An [OpenAI](https://platform.openai.com/api-keys) account
- An [AWS](https://aws.amazon.com/free) account

## Steps

- [Generate the OpenAI API token](#generate-the-openai-api-token)
- [Provisioning a Serverless Postgres](#provisioning-a-serverless-postgres)
- [Create a new Astro application](#create-a-new-astro-application)
  - [Add Tailwind CSS to the application](#add-tailwind-css-to-the-application)
  - [Integrate React in your Astro project](#integrate-react-in-your-astro-project)
  - [Enabling Server Side Rendering in Astro using Node.js Adapter](#enabling-server-side-rendering-in-astro-using-nodejs-adapter)
- [Setting up a Postgres database connection](#setting-up-a-postgres-database-connection)
- [Define the Astro application routes](#define-the-astro-application-routes)
  - [Build Conversation User Interface using Vercel AI SDK](#build-conversation-user-interface-using-vercel-ai-sdk)
  - [Build UI to update Chabot’s Knowledge](#build-ui-to-update-chabots-knowledge)
  - [Build an entrypoint React component](#build-an-entrypoint-react-component)
  - [Initialize Postgres Vector Store in LlamaIndex](#initialize-postgres-vector-store-in-llamaindex)
  - [Build the Chat API Endpoint](#build-the-chat-api-endpoint)
  - [Build the Learn API Endpoint](#build-the-learn-api-endpoint)
- [Dockerize your Astro application](#dockerize-your-astro-application)
- [Deploy your Astro application to Amazon ECS](#deploy-your-astro-application-to-amazon-ecs)
  - [Create Amazon ECR private repository](#create-amazon-ecr-private-repository)
  - [Configure your IAM Roles](#configure-your-iam-roles)
  - [Create an Amazon ECS Task Definition](#create-an-amazon-ecs-task-definition)
  - [Create an Amazon ECS Cluster](#create-an-amazon-ecs-cluster)
  - [Create an Amazon ECS Service](#create-an-amazon-ecs-service)
  - [Create Access Keys for IAM users](#create-access-keys-for-iam-users)
- [Configure GitHub Actions for Continuous Deployment (CD) Workflow](#configure-github-actions-for-continuous-deployment-cd-workflow)

## Generate the OpenAI API token

To create vector embeddings, you will use OpenAI API with LlamaIndex. To set up OpenAI, do the following:

- Log in to your [OpenAI](https://platform.openai.com/) account.
- Navigate to the [API Keys](https://platform.openai.com/api-keys) page.
- Enter a name for your token and click the **Create new secret key** button to generate a new key.
- Copy and securely store this token for later use as the **OPENAI_API_KEY** environment variable.

## Provisioning a Serverless Postgres

Using a serverless Postgres database lets you scale compute resources down to zero, which helps you save on compute costs.

To get started, go to the [Neon Console](https://console.neon.tech/app/projects) and create a project.

You will then be presented with a dialog that provides a connection string of your database. You can enable the **Connection pooling** toggle for a pooled connection string.

![](/guides/images/chatbot-astro-postgres-llamaindex/c200c4ed-f62d-469c-9690-c572c482c536.png)

All Neon connection strings have the following format:

```bash
postgres://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>?sslmode=require&channel_binding=require
```

- `user` is the database user.
- `password` is the database user’s password.
- `endpoint_hostname` is the host with `neon.tech` as the [top-level domain (TLD)](https://www.cloudflare.com/en-gb/learning/dns/top-level-domain/).
- `port` is the Neon port number. The default port number is 5432.
- `dbname` is the name of the database. `neondb` is the default database created with a Neon project if you do not define your own database.
- `?sslmode=require&channel_binding=require` optional query parameters that enforce [SSL](https://www.cloudflare.com/en-gb/learning/ssl/what-is-ssl/) mode and channel binding for better security when connecting to the Postgres instance.

Save the connection string somewhere safe. It will be used to set the **POSTGRES_URL** variable later.

## Create a new Astro application

Let’s get started by creating a new Astro project. Open your terminal and run the following command:

```bash
npm create astro@latest my-app
```

`npm create astro` is the recommended way to scaffold an Astro project quickly.

When prompted, choose:

- `Empty` when asked how to start the new project.
- `Yes` when asked if you plan to write Typescript.
- `Strict` when asked how strict Typescript should be.
- `Yes` when prompted to install dependencies.
- `Yes` when prompted to initialize a git repository.

Once that’s done, change the project directory and start the app:

```bash
cd my-app
npm run dev
```

The app should be running on [localhost:4321](http://localhost:4321/). Let's close the development server for now.

Next, execute the command in your terminal window below to install the necessary libraries and packages for building the application:

```bash
npm install dotenv ai llamaindex@0.3.4
```

The command installs the following packages:

- `dotenv`: A library for handling environment variables.
- `ai`: A library to build AI-powered streaming text and chat UIs.
- `llamaindex`: A data framework for creating LLM applications.

Next, make the following additions in your `astro.config.mjs` file to populate the environment variables and make them accessible via `process.env` object:

```tsx
// File: astro.config.mjs

import 'dotenv/config'; // [!code ++]
import { defineConfig } from 'astro/config';

// https://astro.build/config
export default defineConfig({});
```

Then, add the following code to your `tsconfig.json` file to make relative imports within the project easier:

```json
{
  "extends": "astro/tsconfigs/base",
  "compilerOptions": {
    // [!code ++]
    "baseUrl": ".", // [!code ++]
    "paths": {
      // [!code ++]
      "@/*": ["src/*"] // [!code ++]
    } // [!code ++]
  } // [!code ++]
}
```

Let's move on to integrating Tailwind CSS into the Astro application.

### Add Tailwind CSS to the application

For styling the app, you will be using Tailwind CSS. Install and set up Tailwind at the root of our project's directory by running:

```bash
npx astro add tailwind
```

When prompted, choose:

- `Yes` when prompted to install the Tailwind dependencies.
- `Yes` when prompted to generate a minimal `tailwind.config.mjs` file.
- `Yes` when prompted to make changes to Astro configuration file.

After making the selections outlined above, the command finishes integrating TailwindCSS into your Astro project and installs the following dependencies:

- `tailwindcss`: TailwindCSS as a package to scan your project files and generate corresponding styles.
- `@astrojs/tailwind`: An adapter that brings Tailwind's utility CSS classes to every `.astro` file and framework component in your project.

Let's move on to integrating React into the Astro application.

### Integrate React in your Astro project

To prototype the reactive user interface quickly, you will use React as the library with Astro. In your terminal window, execute the following command:

```bash
npx astro add react
```

`npx` allows us to execute npm package binaries without having to install `npm` globally.

When prompted, choose the following:

- `Yes` to install the React dependencies.
- `Yes` to make changes to Astro configuration file.
- `Yes` to make changes to `tsconfig.json` file.

Let's move on to enabling server-side rendering in the Astro application.

### Enabling Server Side Rendering in Astro using Node.js Adapter

To interact with the chatbot over a server-side API, you will enable server-side rendering in your Astro application. Execute the following command in your terminal:

```bash
npx astro add node
```

When prompted, choose:

- `Yes` to install the Node.js dependencies.
- `Yes` to make changes to Astro configuration file.

After making the selections outlined above, the command finishes integrating the Node.js adapter into your Astro project and installs the following dependency:

- `@astrojs/node`: The adapter that allows your Astro SSR site to deploy to Node targets.

Let's move on to loading the Postgres URL through an environment variable in the Astro application.

## Setting up a Postgres database connection

Create an `.env` file in the root directory of your project with the following environment variable to initiate the setup of a database connection:

```bash
# Neon Postgres Pooled Connection URL

POSTGRES_URL="postgres://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>?sslmode=require&channel_binding=require&channel_binding=require"
```

The file, `.env`, should be kept secret and not included in your Git history. Ensure that `.env` is added to the `.gitignore` file in your project.

## Define the Astro application routes

The structure below is what our `src/pages` directory will look like at the end of this section:

```bash
├── index.astro
├── api/
└───── chat.ts
└───── learn.ts
```

- `index.astro` will serve responses with dynamically created HTML to incoming requests at the index route.
- `api/chat.ts` will serve responses as an API Endpoint to incoming requests at `/api/chat`.
- `api/learn.ts` will serve responses as an API Endpoint to incoming requests at `/api/learn`.

### Build Conversation User Interface using Vercel AI SDK

Inside the `src` directory, create a `Chat.jsx` file with the following code:

```tsx
// File: src/Chat.jsx

import { useChat } from 'ai/react';

export default function () {
  const { messages, handleSubmit, input, handleInputChange } = useChat();
  return (
    <form onSubmit={handleSubmit} className="flex w-full max-w-[300px] flex-col">
      <span className="text-2xl font-semibold">Chat</span>
      <input
        id="input"
        name="prompt"
        value={input}
        onChange={handleInputChange}
        placeholder="What's your next question?"
        className="mt-3 rounded border px-2 py-1 outline-none focus:border-black"
      />
      <button
        type="submit"
        className="mt-3 max-w-max rounded border px-5 py-1 outline-none hover:bg-black hover:text-white"
      >
        Ask &rarr;
      </button>
      {messages.map((message, i) => (
        <div className="mt-3 border-t pt-3" key={i}>
          {message.content}
        </div>
      ))}
    </form>
  );
}
```

The code above does the following:

- Imports the `useChat` hook by `ai` SDK to manage the conversation between the user and the chatbot. It simplifies the management of the conversation between the user and the chatbot. By default, it posts to the `/api/chat` endpoint to obtain responses from the chatbot.
- Exports a React component that returns a form containing an `<input>` element to allow users to enter their query.
- Creates a conversation UI looping over the set of messages (managed by the AI SDK).

Now, let’s create a component that will allow the user to add text to the chatbot's knowledge.

### Build UI to update Chabot’s Knowledge

Inside the `src` directory, create a `Learn.jsx` file with the following code:

```jsx
// File: src/Learn.jsx

import { useState } from 'react';

export default function () {
  const [message, setMessage] = useState();
  return (
    <form
      className="flex w-full max-w-[300px] flex-col"
      onSubmit={(e) => {
        e.preventDefault();
        if (message) {
          fetch('/api/learn', {
            method: 'POST',
            body: JSON.stringify({ message }),
            headers: { 'Content-Type': 'application/json' },
          });
        }
      }}
    >
      <span className="text-2xl font-semibold">Learn</span>
      <textarea
        value={message}
        placeholder="A text to learn."
        onChange={(e) => setMessage(e.target.value)}
        className="mt-3 min-h-[100px] rounded border px-2 py-1 outline-none focus:border-black"
      />
      <button
        type="submit"
        className="mt-3 max-w-max rounded border px-5 py-1 outline-none hover:bg-black hover:text-white"
      >
        Learn &rarr;
      </button>
    </form>
  );
}
```

The code above does the following:

- Imports `useState` hook by React.
- Exports a React component that returns a form containing a `<textarea>` element to accept a string.
- Upon form submission, it posts the message string input by the user as JSON to the `/api/learn` endpoint.

### Build an entrypoint React component

Inside the `src` directory, create a `App.jsx` file with the following code:

```tsx
// File: src/App.jsx

import Chat from './Chat';
import Learn from './Learn';

export default function () {
  return (
    <>
      <Chat />
      <Learn />
    </>
  );
}
```

The code above imports and renders both the Chat and the Learn component created earlier. Finally, update the `index.astro` file to import the `App` component:

```astro
---
// File: src/pages/index.astro // [!code ++]

import App from "../App"; // [!code ++]
---

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" type="image/svg+xml" href="/favicon.svg" />
    <meta name="viewport" content="width=device-width" />
    <meta name="generator" content={Astro.generator} />
    <title>Astro</title>
  </head>
  <body
   class="flex w-screen flex-col md:flex-row items-center md:items-start md:justify-center md:gap-x-3 md:mt-12" // [!code ++]
  >
    <h1>Astro</h1> // [!code --]
    <App client:load /> // [!code ++]
  </body>
</html>
```

The changes above import the App component. Additionally, using Astro's [`client:load` directive](https://docs.astro.build/en/reference/directives-reference/#clientload) the code makes sure that the React application is hydrated immediately on the page.

Let's move on to using Postgres as the vector store for your chatbot.

### Initialize Postgres Vector Store in LlamaIndex

To query and add documents to the Postgres vector store, you are going to use `PGVectorStore` class to communicate. Inside the `src` directory, create `vectorStore.ts` with the following code:

```tsx
// File: src/vectorStore.ts

import 'dotenv/config';
import { PGVectorStore } from 'llamaindex';

export default new PGVectorStore({
  connectionString: process.env.POSTGRES_URL,
});
```

The code above begins with importing the `dotenv/config`, loading all the environment variables into the scope. Additionally, it exports an instance of `PGVectorStore` initialized using the Postgres pooled connection URL obtained earlier.

Let's move on to building the chat API endpoint.

### Build the Chat API Endpoint

The Vercel AI SDK uses `/api/chat` by default to obtain the chatbot responses. Create a file `src/pages/api/chat.ts` with the following code:

```tsx
// File: src/pages/api/chat.ts

import type { APIContext } from 'astro';

import vectorStore from '@/vectorStore';
import { VectorStoreIndex } from 'llamaindex';

export async function POST({ request }: APIContext) {
  const { messages = [] } = await request.json();
  const userMessages = messages.filter((i: { role: string }) => i.role === 'user');
  const encoder = new TextEncoder();
  const index = await VectorStoreIndex.fromVectorStore(vectorStore);
  const queryEngine = index.asQueryEngine();
  const query = userMessages[userMessages.length - 1].content;
  const stream = await queryEngine.query({ query, stream: true });
  const customReadable = new ReadableStream({
    async start(controller) {
      for await (const chunk of stream) {
        controller.enqueue(encoder.encode(chunk.response));
      }
      controller.close();
    },
  });
  return new Response(customReadable, {
    headers: {
      Connection: 'keep-alive',
      'Content-Encoding': 'none',
      'Cache-Control': 'no-cache, no-transform',
      'Content-Type': 'text/event-stream; charset=utf-8',
    },
  });
}
```

The code above does the following:

- Imports the vector store instance that is using Postgres.
- Imports the VectorStoreIndex helper by llamaindex.
- Exports a POST HTTP Handler which responds to incoming POST requests on `/api/chat`.
- Destructs messages array from the request body.
- Creates the LlamaIndex's query engine using Postgres as the vector store.
- Creates a stream handler that streams the response from LlamaIndex's query engine.
- Returns the stream handler as a standard Web Response.

Let's move on to building the endpoint to update chatbot's knowledge.

### Build the Learn API Endpoint

As you saw earlier, with LlamaIndex you do not need to manually create context and pass it to an external API. The vector store is searched for similar vector embeddings based on the user query, internally. To keep the knowledge of the chatbot up-to-date, create a file `src/pages/api/learn.ts` with the following code:

```tsx
// File: src/pages/api/learn.ts

import type { APIContext } from 'astro';

import {
  Document,
  Settings,
  OpenAIEmbedding,
  VectorStoreIndex,
  storageContextFromDefaults,
} from 'llamaindex';
import vectorStore from '@/vectorStore';

export async function POST({ request }: APIContext) {
  Settings.embedModel = new OpenAIEmbedding();
  const { text } = await request.json();
  if (!text) return new Response(null, { status: 400 });
  const storageContext = await storageContextFromDefaults({ vectorStore });
  const document = new Document({ text });
  await VectorStoreIndex.fromDocuments([document], { storageContext });
}
```

The code above does the following:

- Imports the vector store instance that is using Postgres.
- Imports the VectorStoreIndex and storageContextFromDefaults helpers by llamaindex.
- Exports a POST HTTP Handler which responds to incoming POST requests on `/api/learn`.
- Destructs the text message from the request body.
- Creates a LlamaIndex document with text as its sole data.
- Pushes the vector embeddings generated for the text along with the document metadata to the Postgres database.

Let's move on to dockerizing the Astro application.

## Dockerize your Astro application

To dockerize your Astro application, you are going to create two files at the root of your Astro project:

- `.dockerignore`: The set of files that would not be included in your Docker image.
- `Dockerfile`: The set of instructions that would be executed while your Docker image builds.

Create the `.dockerignore` file at the root of your Astro project with the following code:

```
# build output
dist/

# generated types
.astro/

# dependencies
node_modules/

# logs
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*

# macOS-specific files
.DS_Store
```

Create the `Dockerfile` file at the root of your Astro project with the following code:

```bash
ARG NODE_VERSION=20.11.0
FROM node:${NODE_VERSION}-slim as base

WORKDIR /app

# Set production environment
ENV NODE_ENV="production"

# Throw-away build stage to reduce size of final image
FROM base as build

# Install packages needed to build node modules
RUN apt-get update -qq && \
    apt-get install --no-install-recommends -y build-essential node-gyp pkg-config python-is-python3

# Install node modules
COPY --link package-lock.json package.json ./
RUN npm install

# Copy application code
COPY --link . .

# Build application
RUN npm run build

# Final stage for app image
FROM base

# Copy built application
COPY --from=build /app/node_modules /app/node_modules
COPY --from=build /app/dist /app/dist

ENV PORT=80
ENV HOST=0.0.0.0

# Start the server by default, this can be overwritten at runtime
EXPOSE 80
CMD [ "node", "./dist/server/entry.mjs" ]
```

The Dockerfile above defines the following set of actions:

- Sets up Node.js 20.11.0.
- Sets the environment to `production` with `NODE_ENV` environment variable.
- Instasll the dependencies of your Astro project.
- Builds the application with `astro build`.
- Sets the `PORT` environment variable to `80` (default port on Amazon ECS).
- Sets the `HOST` environment variable to `0.0.0.0` to listen to all incoming requests on the host.
- Runs the production server with `node ./dist/server/entry.mjs` command.

Now, let's create a file that creates Amazon ECS Task definition during the deployment via GitHub Actions. This is useful as it protects the secrets stored in the GitHub repo. Create an `env.mjs` file at the root of your Astro application with the following code:

```tsx
// File: env.mjs

import 'dotenv/config';
import { join } from 'node:path';
import { writeFileSync } from 'node:fs';

if (!process.env.AWS_ACCOUNT_ID || !process.env.POSTGRES_URL || !process.env.OPENAI_API_KEY) {
  console.error(`AWS_ACCOUNT_ID, POSTGRES_URL or OPENAI_API_KEY environment variable not found.`);
  process.exit();
}

writeFileSync(
  join(process.cwd(), 'task-definition.json'),
  JSON.stringify({
    containerDefinitions: [
      {
        cpu: 256,
        memory: 512,
        portMappings: [
          {
            containerPort: 80,
            hostPort: 80,
            protocol: 'tcp',
          },
        ],
        essential: true,
        name: 'astro-app',
        image: `${process.env.AWS_ACCOUNT_ID}.dkr.ecr.ap-south-1.amazonaws.com/${process.env.AWS_ECR_REPOSITORY_NAME}`,
        environment: [
          {
            name: 'POSTGRES_URL',
            value: process.env.POSTGRES_URL,
          },
          {
            name: 'OPENAI_API_KEY',
            value: process.env.OPENAI_API_KEY,
          },
        ],
      },
    ],
    cpu: '1024',
    memory: '3072',
    family: 'astro-task-definitions',
    networkMode: 'awsvpc',
    taskRoleArn: `arn:aws:iam::${process.env.AWS_ACCOUNT_ID}:role/ecsTaskRole`,
    executionRoleArn: `arn:aws:iam::${process.env.AWS_ACCOUNT_ID}:role/ecsTaskExecutionRole`,
  }),
  'utf8'
);
```

The code above does the following:

- Imports `dotenv/config` to load all the environment variables into the scope and make them accessible via the `process.env` object.
- Validates the presense of `AWS_ACCOUNT_ID`, `POSTGRES_URL`, and `OPENAI_API_KEY` environment variables.
- Writes a `task-definition.json` file at the root of your Astro application following the format used earlier, and adds the `OPENAI_API_KEY` and `POSTGRES_URL` environment variables.

## Deploy your Astro application to Amazon ECS

In this section, you will learn how to create an Amazon ECR repository for your Docker-based deployments, spin up Amazon ECS Cluster and an ECS service, create AWS ECS Task Definition(s) and grant ECS Full Access to your AWS IAM user.

### Create Amazon ECR private repository

- Open the [Amazon ECR console](https://console.aws.amazon.com/ecr/repositories), and click **Get started**.

![](/guides/images/chatbot-astro-postgres-llamaindex/bf86c622-e7b2-4d43-80d0-140757512e7d.png)

- Enter a repository name, say `astro-repo`, for example. Scroll down and choose **Create repository**.

![](/guides/images/chatbot-astro-postgres-llamaindex/9c566960-540a-448e-880e-97ec16ba4f0f.png)

You are now done setting up an Amazon ECR repository. Let’s move on to configuring IAM roles for your account.

### Configure your IAM Roles

- Open the [IAM console](https://console.aws.amazon.com/iam/), and click **Create role**.

![](/guides/images/chatbot-astro-postgres-llamaindex/7fdeea5f-f2f6-4115-9f55-0b904b510944.png)

- Select **AWS Service** and choose **Elastic Container Service** as the Service or use case.

![](/guides/images/chatbot-astro-postgres-llamaindex/fd0ce7ed-21f7-492c-ac01-9c4cff96d590.png)

- Filter the large set of permissions policies, select **AmazonECS_FullAccess** only, and click **Next**.

![](/guides/images/chatbot-astro-postgres-llamaindex/202bcbe5-5614-404a-ac45-562ef580ede6.png)

- Enter `ecsTaskRole` as the **Role name**.

![](/guides/images/chatbot-astro-postgres-llamaindex/6f33848c-8d6a-42eb-922a-2a4916a430c3.png)

- Go back to the IAM Console, and click **Create role**.

![](/guides/images/chatbot-astro-postgres-llamaindex/25510e84-f354-47fc-8bdd-0380e0480477.png)

- Select **AWS Service** and choose **Elastic Container Service Task** as the Service or use case.

![](/guides/images/chatbot-astro-postgres-llamaindex/325c52ce-dbc4-4906-97e0-5f29ba510560.png)

- Filter the large set of permissions policies, select **AmazonECSTaskExecutionRolePolicy** only, and click **Next**.

![](/guides/images/chatbot-astro-postgres-llamaindex/6fbb4748-9380-4264-9d81-d36f2d6f83aa.png)

- Enter `ecsTaskExecutionRole` as the **Role name**.

![](/guides/images/chatbot-astro-postgres-llamaindex/c6f45124-7441-4562-81d4-d45e93f9a567.png)

You are now done setting up IAM Roles for your account. Let’s move on to creating an Amazon ECS task definition.

### Create an Amazon ECS Task Definition

- Open the [Amazon ECS Console](https://console.aws.amazon.com/ecs/v2) and choose **Task Definitions**. Further, select **Create new task definition with JSON**.

![](/guides/images/chatbot-astro-postgres-llamaindex/afdc546d-2214-45b0-916d-bd0868fbe2dc.png)

- Copy the following JSON in the field, and click **Create**.

```json
{
  "containerDefinitions": [
    {
      "cpu": 256,
      "memory": 512,
      "portMappings": [
        {
          "containerPort": 80,
          "hostPort": 80,
          "protocol": "tcp"
        }
      ],
      "essential": true,
      "name": "astro-app",
      "image": "2*.dkr.ecr.ap-south-1.amazonaws.com/astro-repo"
    }
  ],
  "cpu": "1024",
  "memory": "3072",
  "family": "astro-task-definitions",
  "networkMode": "awsvpc",
  "taskRoleArn": "arn:aws:iam::2*:role/ecsTaskRole",
  "executionRoleArn": "arn:aws:iam::2*:role/ecsTaskExecutionRole"
}
```

![](/guides/images/chatbot-astro-postgres-llamaindex/b812ef9a-df98-4d46-aa0a-71711a967eeb.png)

You are now done setting up an Amazon ECS task definition for your service. Let's move on to creating an Amazon ECS Cluster.

### Create an Amazon ECS Cluster

- Open [Amazon ECS Console](https://console.aws.amazon.com/ecs/v2) and click **Create cluster**.

![](/guides/images/chatbot-astro-postgres-llamaindex/fe2342b6-fd6e-4f6f-8e0d-39aff8a87060.png)

- Enter a name for your cluster, say `astro-cluster`, and choose **Create**.

![](/guides/images/chatbot-astro-postgres-llamaindex/cb8e6d74-5745-4c2a-88c7-2598ac85e286.png)

You are now done setting up an Amazon ECS Cluster for your service.

![](/guides/images/chatbot-astro-postgres-llamaindex/9a88a385-3ff9-496c-8ec0-7389fc4c69b0.png)

Let's move on to creating an Amazon ECS Service.

### Create an Amazon ECS Service

- Click on the Cluster created in the section earlier, and click on **Create** in the Services section.

![](/guides/images/chatbot-astro-postgres-llamaindex/9a88a385-3ff9-496c-8ec0-7389fc4c69b0.png)

- Enter a name for your service, say `astro-service`, and expand the **Networking** section.

![](/guides/images/chatbot-astro-postgres-llamaindex/eccf2a66-c1e8-43e4-95dc-4e8550488a8d.png)

- Select the **VPC** created earlier (or the default one). Select **Create a new security group** option and enter the following details for it:
  - Security group name: `astro-sg`
  - Security group description: `astro sg`
  - Inbound rules for security groups:
    - Type: `HTTP`
    - Source: `Anywhere`

![](/guides/images/chatbot-astro-postgres-llamaindex/7dc7a5ea-e593-40a0-adb3-f9519536bdea.png)

You are now done creating an ECS Service in your ECS Cluster. Let's move on to creating access keys for IAM users for your account.

### Create Access Keys for IAM users

- In the navigation bar in your AWS account, choose your user name, and then choose **Security credentials**.

![](/guides/images/chatbot-astro-postgres-llamaindex/75769ba6-07ee-48d3-9421-4f7005c9127d.png)

- Scroll down to **Access keys** and click on **Create access key**.

![](/guides/images/chatbot-astro-postgres-llamaindex/780854a9-c705-4a86-9c16-f4a54fec02fa.png)

- Again, click on **Create access key**.

![](/guides/images/chatbot-astro-postgres-llamaindex/68d86de4-ab60-4a94-bd10-a9492a458f0e.png)

- Copy the **Access key** and **Secret access key** generated to be used as `AWS_ACCESS_KEY_ID` and `AWS_ACCESS_KEY_SECRET` respectively.

![](/guides/images/chatbot-astro-postgres-llamaindex/8586ea3d-d5b2-43dc-b81c-38c1a991b593.png)

Let's move on to configuring GitHub Workflows for continuous deployments.

## Configure GitHub Actions for Continuous Deployment (CD) Workflow

First, let's add all the required environment variables obtained in the AWS steps above to your GitHub repo as repository Secrets. Go to your GitHub repository's Settings, and click on **Secrets and Variables**. Then, click on **New repository secret**.

![](/guides/images/chatbot-astro-postgres-llamaindex/c95dacdf-987c-4a78-97e3-68668f0e1ad5.png)

- Enter **AWS_ACCOUNT_ID** as the value obtained earlier.

![](/guides/images/chatbot-astro-postgres-llamaindex/357f312c-af9d-4f11-991c-71b7df20a285.png)

- Enter **AWS_ACCESS_KEY_ID** as the value obtained earlier.

![](/guides/images/chatbot-astro-postgres-llamaindex/816e7d04-5723-4e11-bde1-5debcd788e1a.png)

- Enter **AWS_ACCESS_KEY_SECRET** as the value obtained earlier.

![](/guides/images/chatbot-astro-postgres-llamaindex/bd5e7d9a-ea81-464a-8949-1f65d292c10b.png)

- Enter **POSTGRES_URL** as the value obtained earlier.

![](/guides/images/chatbot-astro-postgres-llamaindex/80e9eb62-3ffa-4ab9-a6cd-ed0190692ae6.png)

- Enter **OPENAI_API_KEY** as the value obtained earlier.

![](/guides/images/chatbot-astro-postgres-llamaindex/6848ea49-3eb5-4201-927c-183fdf9cb0d7.png)

Next, to automate deployments of your Astro application, you are going to use [GitHub Actions](https://github.com/features/actions). Create a `.github/workflows/deploy.yml` with the following code:

```yml
name: Deploy Astro to Amazon ECS on AWS Fargate

on:
  push:
    branches:
      - master
  workflow_dispatch:

env:
  # AWS ECS
  AWS_CONTAINER_NAME_NAME: astro-app
  AWS_ECS_CLUSTER_NAME: astro-cluster
  AWS_ECS_SERVICE_NAME: astro-service
  AWS_ECS_TASK_DEFINITION: ./task-definition.json

  # AWS ECR
  AWS_ECR_REPOSITORY_NAME: astro-repo

  # AWS Account
  AWS_REGION: us-west-1
  AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

jobs:
  deploy:
    name: Deploy
    runs-on: ubuntu-latest
    environment: production

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ env.AWS_REGION }}
          aws-access-key-id: ${{ env.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1

      - name: Load Environment Variables
        id: environment-variables
        env:
          AWS_ACCOUNT_ID: ${{ env.AWS_ACCOUNT_ID }}
          POSTGRES_URL: ${{ secrets.POSTGRES_URL }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          npm install
          node ./env.mjs

      - name: Build, tag, and push image to Amazon ECR
        id: build-image
        env:
          IMAGE_TAG: ${{ github.sha }}
          AWS_ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        run: |
          # Build a docker container and
          # push it to ECR so that it can
          # be deployed to ECS.
          docker build -t $AWS_ECR_REGISTRY/$AWS_ECR_REPOSITORY_NAME:$IMAGE_TAG .
          docker push $AWS_ECR_REGISTRY/$AWS_ECR_REPOSITORY_NAME:$IMAGE_TAG
          echo "image=$AWS_ECR_REGISTRY/$AWS_ECR_REPOSITORY_NAME:$IMAGE_TAG" >> $GITHUB_OUTPUT

      - name: Fill in the new image ID in the Amazon ECS task definition
        id: task-def
        uses: aws-actions/amazon-ecs-render-task-definition@v1
        with:
          container-name: ${{ env.AWS_CONTAINER_NAME_NAME }}
          image: ${{ steps.build-image.outputs.image }}
          task-definition: ${{ env.AWS_ECS_TASK_DEFINITION }}

      - name: Deploy Amazon ECS task definition
        uses: aws-actions/amazon-ecs-deploy-task-definition@v1
        with:
          service: ${{ env.AWS_ECS_SERVICE_NAME }}
          cluster: ${{ env.AWS_ECS_CLUSTER_NAME }}
          wait-for-service-stability: true
          task-definition: ${{ steps.task-def.outputs.task-definition }}
```

The workflow above does the following:

- Allows itself to be triggered manually or when a git push is done to the master branch.
- Sets global environment variables as per your AWS setup variables (we obtained earlier during the setup).
- Loads the environment variables added to the GitHub repo as secrets into the scope.
- Writes the task definition including the environment variables.
- Builds and pushes the Docker image to Amazon ECR.
- Loads the updated (if) task definition to Amazon ECS.

Now, push the added GitHub workflow file to your GitHub repo. Follow the steps below to trigger the deployment:

- Go to your GitHub repository's **Actions** tab.

- Select **Deploy to Amazon ECS** workflow.

- Click **Run workflow**.

- Once the action is completed, open [ECS Console](https://console.aws.amazon.com/ecs/v2) and select your service.

![image](/guides/images/chatbot-astro-postgres-llamaindex/932be3bf-f3b1-43d9-b465-828b72a8cd4c.png)

- Click on the **Tasks** tab.

![image (1)](/guides/images/chatbot-astro-postgres-llamaindex/b1904935-d5e3-411f-9e57-922d450ed26a.png)

- Click on the completed **Task**.

![image (2)](/guides/images/chatbot-astro-postgres-llamaindex/3b6f3c70-ee7f-4d80-b1b8-c73cb7d649a0.png)

- Click **open address** to open your deployment.

![image (3)](/guides/images/chatbot-astro-postgres-llamaindex/d4c77bfb-83ab-46e1-a70c-02f25e6c6314.png)

## Summary

In this guide, you learned how to build a RAG Chatbot using LlamaIndex, Astro, and Neon Postgres. Additionally, you learned how to automate deployments of your Astro application using GitHub Actions to Amazon ECS on Amazon Fargate.

<NeedHelp />


# Get started with Claude Code and Neon Postgres MCP Server

---
title: 'Get started with Claude Code and Neon Postgres MCP Server'
subtitle: 'Interact with Neon APIs using Claude Code through natural language'
author: pedro-figueiredo
enableTableOfContents: true
createdAt: '2025-08-27T00:00:00.000Z'
updatedOn: '2025-08-27T00:00:00.000Z'
---

Imagine adjusting your database schema simply by describing the change in plain English. This is possible by combining [Claude Code](https://docs.anthropic.com/en/docs/claude-code) with the [Neon MCP Server](https://github.com/neondatabase/mcp-server-neon).

This guide demonstrates how to use Claude Code's command-line interface and Neon's MCP server to perform database migrations in your Neon project.

<Admonition type="important" title="Neon MCP Server Security Considerations">
The Neon MCP Server grants powerful database management capabilities through natural language requests. **Always review and authorize actions requested by the LLM before execution.** Ensure that only authorized users and applications have access to the Neon MCP Server.

The Neon MCP Server is intended for local development and IDE integrations only. **We do not recommend using the Neon MCP Server in production environments.** It can execute powerful operations that may lead to accidental or unauthorized changes.

For more information, see [MCP security guidance](/docs/ai/neon-mcp-server#mcp-security-guidance).
</Admonition>

## Key components

Let's break down the key components in this setup:

- **Claude Code**: Claude Code is Anthropic's official CLI tool that supports Model Context Protocol (MCP) for interfacing with external tools (APIs, databases, etc.)

- **Neon MCP Server**: Neon's MCP server acts as a bridge between MCP clients like Claude Code and [Neon's API](https://api-docs.neon.tech/reference/getting-started-with-neon-api), letting you work with Neon databases using natural language commands.

- **Model Context Protocol (MCP)**: MCP is a lightweight communication standard that allows Claude Code and Neon MCP Server to work together.

## Setting up Neon MCP Server in Claude Code

You have two options for connecting Claude Code to the Neon MCP Server:

1. **Remote MCP Server (Preview):** Connect to Neon's managed MCP server using OAuth for authentication. This method is more convenient as it eliminates the need to manage API keys in Claude Code. Additionally, you will automatically receive the latest features and improvements as soon as they are released.

2. **Local MCP Server:** Run the Neon MCP server locally on your machine, authenticating with a Neon API key.

### Prerequisites

Before you begin, ensure you have the following:

1. **Claude Code:** Ensure you have Claude Code installed. Visit [docs.anthropic.com/en/docs/claude-code](https://docs.anthropic.com/en/docs/claude-code) for installation instructions.
2. **Neon API Key (for Local MCP server):** After signing up, get your Neon API Key from the [Neon console](https://console.neon.tech/app/settings/api-keys). This API key is needed to authenticate your application with Neon. For instructions, see [Manage API keys](/docs/manage/api-keys).

   <Admonition type="important" title="Neon API Key Security">
   Keep your Neon API key secure, and never share it publicly. It provides access to your Neon projects.
   </Admonition>

3. **Node.js (>= v18) and npm:** Ensure Node.js (version 18 or later) and npm are installed. Download them from [nodejs.org](https://nodejs.org).

### Option 1: Setting up the Remote Hosted Neon MCP Server

<Admonition type="note">
By default, the Remote MCP Server connects to your personal Neon account. To connect to an organization's account, you must authenticate with an API key. For more information, see [API key-based authentication](/docs/ai/neon-mcp-server#api-key-based-authentication).
</Admonition>

This method uses Neon's managed server and OAuth authentication.

1. Open your terminal.
2. Add the Neon MCP server to Claude Code with the following command:
   ```sh
   claude mcp add --transport http neon https://mcp.neon.tech/mcp
   ```
3. Start a new session of `claude` to trigger the OAuth authentication flow:
   ```sh
   claude
   ```
4. You can also trigger authentication with `/mcp` within Claude Code.

<Admonition type="tip">

If you prefer to authenticate using a Neon API key, provide the `Authorization` header to the `mcp add` command:

```sh
claude mcp add --transport http neon https://mcp.neon.tech/mcp \
    --header "Authorization: Bearer <YOUR_NEON_API_KEY>"
```

Replace `<YOUR_NEON_API_KEY>` with your actual Neon API key which you obtained from the [prerequisites](#prerequisites) section.

</Admonition>

<Admonition type="note">
The remote hosted MCP server is in preview due to the [new OAuth MCP specification](https://spec.modelcontextprotocol.io/specification/2025-03-26/basic/authorization/), expect potential changes as we continue to refine the OAuth integration.
</Admonition>

### Option 2: Setting up the Local Neon MCP Server

This method runs the Neon MCP server locally on your machine, using a Neon API key for authentication.

1. Open your terminal.
2. Add the Neon MCP server to Claude Code with the following command, replacing `<YOUR_NEON_API_KEY>` with your actual Neon API key:

   ```sh
   claude mcp add neon -- npx -y @neondatabase/mcp-server-neon start "<YOUR_NEON_API_KEY>"
   ```

   > Replace `<YOUR_NEON_API_KEY>` with your actual Neon API key which you obtained from the [prerequisites](#prerequisites) section.

3. Start a new Claude Code session with the `claude` command and start using the Neon MCP server:
   ```sh
   claude
   ```

You've now configured the Neon MCP Server in Claude Code and can manage your Neon Postgres databases using AI.

### Verification

Now that you have the Neon MCP server set up either remotely or locally, you can verify the connection and test the available tools.

1. Start Claude Code:

   ```sh
   claude
   ```

2. Type `/mcp` to see the available MCP servers and tools.

3. Try out a Neon MCP Server tool by typing a query like `List my Neon projects` to see your projects and verify the connection.

<MCPTools />

These actions enable any MCP client like Claude Code to interact with various functionalities of the **Neon platform via the Neon API.** Certain tools, especially database migration ones, are tailored for AI agent and LLM usage, leveraging Neon's branching for safe preview and commit.

## Development Workflow: Adding a Column with Claude Code and Neon MCP

Let's walk through a typical development scenario: Quickly adding a column for prototyping using natural language within Claude Code. The following chat log demonstrates a real interaction with Claude Code and the Neon MCP server.

**Scenario:** During development, you decide to track timestamps for entries in your `playing_with_neon` table. You want to quickly add a `created_at` column.

<Admonition type="tip" title="Security Reminder">
For your security, review the tool's purpose before permitting the operation to proceed. Remember that LLMs can sometimes produce unexpected results, so careful monitoring is always recommended.
</Admonition>

Here's the conversation log between the user and Claude Code:

```text shouldWrap
User: In my neon project id: round-salad-44063611 list all the tables
Claude Code: I'll help you list all the tables in your Neon project. I'll use the default database name 'neondb' since none was specified.
> Called MCP Tool (get_database_tables)
> I found one table in your database:
> playing_with_neon (in the public schema)
> Would you like to see more details about this table's structure or perform any other operations with it?

User: Can you show me first 5 rows?
Claude Code: I'll query the first 5 rows from the playing_with_neon table.
> Called MCP Tool (run_sql)
> Here are the first 5 rows from the table. The table has three columns:
> id (sequential numbers)
> name (string values)
> value (decimal numbers)
> Would you like to perform any other operations with this data?

User: can you add a created_at column?
Claude Code: I'll help you add a created_at column to the playing_with_neon table. I'll set it up with a timestamp data type and set it to automatically record the creation time of new rows.
> Called MCP Tool (prepare_database_migration)
> Let me verify the change in the temporary branch to make sure it was applied correctly.
> Called MCP Tool (run_sql)
> I've added the created_at column to the playing_with_neon table in a temporary branch. Would you like to commit this migration to the production branch?
> Migration Details:
> Migration ID: a65babcd-d1cc-401f-89b5-762570570102
> Temporary Branch Name: br-silent-cherry-a1eqrl6i
> Temporary Branch ID: br-silent-cherry-a1eqrl6i
> Migration Result: Success
> Please let me know if you'd like to proceed with applying this change to the production branch.

User: yeah do it
Claude Code: I'll complete the migration and apply the changes to the production branch.
> Called MCP Tool (complete_database_migration)
> The migration has been successfully completed! The created_at column has been added to your table in the production branch, and the temporary branch has been cleaned up.
```

**Key takeaways:**

- **Natural language interaction:** You can use simple, conversational English to interact with your database.
- **Step-by-step guidance:** Claude Code confirms each step and provides details like branch names and migration IDs.
- **MCP Tools in Action:** The underlying MCP tools (`get_database_tables`, `run_sql`, `prepare_database_migration`, `complete_database_migration`) illustrate the automated process.
- **Branching for safety:** The agent automatically creates a temporary branch for schema changes. The user will be prompted to confirm the migration before applying it to the production branch.

You can verify the changes in your Neon Console or by querying the database directly.

<Admonition type="note">
While the Neon MCP server allows you to utilize all of Neon's functionalities, you can also use a [generic Postgres MCP server](https://github.com/modelcontextprotocol/servers/tree/main/src/postgres) to connect to Neon.
</Admonition>

## Bonus: Creating a project and branch

Beyond schema changes, you can also manage your Neon projects and branches directly within Claude Code using natural language commands. This can be useful for quickly setting up a new development or test environment.

### Creating a new project

Let's say you want to create a new Neon project named "my-new-project". You can simply tell Claude Code:

```text shouldWrap
User: Create a Neon project named "my-new-project"
Claude Code: I'll help you create a new Neon project with the specified name.
> Called MCP Tool (create_project)
> Great! I've created a new Neon project for you with the name "my-new-project". Here are the important details:
> Project ID: orange-dawn-33739962
> Default branch: main
> Default database: neondb
> The project is ready to use. You can start creating tables and working with your database right away.
```

Claude Code will then use the `create_project` MCP tool to initiate the project creation. It will provide you with a project ID and name.

### Creating a New Branch

Once you have a project, you can easily create new branches for development or experimentation. For example, to create a branch named "feature-x" in your "my-new-project" project:

```text shouldWrap
User: Create a branch named "feature-x"
Claude Code: Perfect! I've created a new branch named "feature-x". Here are the details:
> Branch ID: br-cold-mountain-a523ka2w
> Branch name: feature-x
> Parent branch ID: br-odd-pine-a5y53krm
> The branch has been created successfully and is ready for use.
```

Claude Code will use the `create_branch` MCP tool to create the branch and provide you with the branch name and ID. Notice how we don't need to specify the project ID, as Claude Code remembers the active project context.

## Conclusion

Claude Code combined with the Neon MCP Server, whether using the **Remote Hosted (Preview)** option or the **Local Server** setup, lets you use natural language to interact with your database and take advantage of Neon's branching capabilities for fast iteration. This approach is ideal for quickly testing database ideas and making schema changes during development.

## Resources

- [MCP Protocol](https://modelcontextprotocol.org)
- [Claude Code Documentation](https://docs.anthropic.com/en/docs/claude-code)
- [Neon Docs](/docs)
- [Neon API Keys](/docs/manage/api-keys#creating-api-keys)
- [Neon MCP server GitHub](https://github.com/neondatabase/mcp-server-neon)

<NeedHelp/>


# Get started with Cline and Neon Postgres MCP Server

---
title: 'Get started with Cline and Neon Postgres MCP Server'
subtitle: 'Make schema changes with natural language using Cline and Neon MCP Server'
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-02-22T00:00:00.000Z'
updatedOn: '2025-02-22T00:00:00.000Z'
---

Imagine adjusting your database schema simply by describing the change in plain English. This is possible by combining [Cline](https://cline.bot) with the [Neon MCP Server](https://github.com/neondatabase/mcp-server-neon).

This guide demonstrates how to use [Cline](https://docs.cline.bot/mcp-servers/mcp) and Neon's MCP server to perform database migrations in your Neon project.

<Admonition type="important" title="Neon MCP Server Security Considerations">
The Neon MCP Server grants powerful database management capabilities through natural language requests. **Always review and authorize actions requested by the LLM before execution.** Ensure that only authorized users and applications have access to the Neon MCP Server.

The Neon MCP Server is intended for local development and IDE integrations only. **We do not recommend using the Neon MCP Server in production environments.** It can execute powerful operations that may lead to accidental or unauthorized changes.

For more information, see [MCP security guidance →](/docs/ai/neon-mcp-server#mcp-security-guidance).
</Admonition>

## Key components

Let's break down the key components in this setup:

- **Cline**: Cline is an autonomous coding agent streamlining your development process within your IDE. It offers powerful features for creating, editing, executing, and even browsing, all under your guidance. Cline has support for the Model Context Protocol (MCP), facilitating seamless interaction with external tools.

- **Neon MCP Server**: Neon's MCP server acts as a bridge between MCP clients like Cline and [Neon's API](https://api-docs.neon.tech/reference/getting-started-with-neon-api), letting you work with Neon databases using natural language commands.

- **Model Context Protocol (MCP)**: MCP is a lightweight communication standard that allows Cline and Neon MCP Server to work together.

## Setting up Neon MCP Server in Cline

You have two options for connecting Cline to the Neon MCP Server:

1. **Remote MCP Server (Preview):** Connect to Neon's managed MCP server using OAuth for authentication. This method is more convenient as it eliminates the need to manage API keys in Cline. Additionally, you will automatically receive the latest features and improvements as soon as they are released.

2. **Local MCP Server:** Run the Neon MCP server locally on your machine, authenticating with a Neon API key.

### Prerequisites

Before you begin, ensure you have the following:

1.  **Cline extension and Setup:**
    - Download and install the Cline VS Code extension from the [VS Code Marketplace](https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev).
    - Set up Cline by following the [Getting Started guide](https://docs.cline.bot/getting-started/getting-started-new-coders#setting-up-openrouter-api-key) which involves obtaining an [OpenRouter API key](https://openrouter.ai) to work with Cline.
2.  **A Neon Account and Project:** You'll need a Neon account and a project. You can quickly create a new Neon project here [pg.new](https://pg.new)
3.  **Neon API Key (for Local MCP server):** After signing up, get your Neon API Key from the [Neon console](https://console.neon.tech/app/settings/profile). This API key is needed to authenticate your application with Neon. For instructions, see [Manage API keys](/docs/manage/api-keys).
    <Admonition type="warning" title="Neon API Key Security">
    Keep your Neon API key secure, and never share it publicly. It provides access to your Neon projects.
    </Admonition>
4.  **Node.js (>= v18) and npm:** Ensure Node.js (version 18 or later) and npm are installed. Download them from [nodejs.org](https://nodejs.org).

### Option 1: Setting up the remote hosted Neon MCP Server

This method uses Neon's managed server and OAuth authentication.

### Installation and configuration

<Admonition type="note">
By default, the Remote MCP Server connects to your personal Neon account. To connect to an organization's account, you must authenticate with an API key. For more information, see [API key-based authentication](/docs/ai/neon-mcp-server#api-key-based-authentication).
</Admonition>

1. Open Cline by clicking on the Cline icon in the VS Code sidebar.
2. To configure MCP Servers in Cline, you need to modify the `cline_mcp_settings.json` file.
   ![Cline Add MCP Tool](/docs/guides/cline-add-mcp.gif)
3. This will open the `cline_mcp_settings.json` file.
4. In the `cline_mcp_settings.json` file, you need to specify a list of MCP servers.
5. Paste the following JSON configuration into it:

   ```json
   {
     "mcpServers": {
       "Neon": {
         "command": "npx",
         "args": ["-y", "mcp-remote@latest", "https://mcp.neon.tech/sse"]
       }
     }
   }
   ```

   If you have other MCP servers configured, you can copy just the `Neon` part.

6. **Save** the `cline_mcp_settings.json` file.
7. You should see a notification in VS Code that says: "MCP servers updated".
   ![Cline MCP Server Updated](/docs/guides/cline-mcp-config-update.png)
8. An OAuth window will open in your browser. Follow the prompts to authorize Cline to access your Neon account. Once authorized, Cline will be connected to Neon's remote MCP server.
   ![Neon OAuth window](/docs/guides/neon-oauth-window.png)

<Admonition type="note">
  The remote hosted MCP server is in preview due to the [new OAuth MCP specification](https://spec.modelcontextprotocol.io/specification/2025-03-26/basic/authorization/), expect potential changes as we continue to refine the OAuth integration.
</Admonition>

### Option 2: Setting up the Local Neon MCP Server

This method runs the Neon MCP server locally on your machine, using a Neon API key for authentication.

1. Open Cline by clicking on the Cline icon in the VS Code sidebar.
2. To configure MCP Servers in Cline, you need to modify the `cline_mcp_settings.json` file.
   ![Cline Add MCP Tool](/docs/guides/cline-add-mcp.gif)
3. This will open the `cline_mcp_settings.json` file.
4. In the `cline_mcp_settings.json` file, you need to specify a list of MCP servers.
5. Paste the following JSON configuration into it. Replace `<YOUR_NEON_API_KEY>` with your actual Neon API key which you obtained from the [prerequisites](#prerequisites) section:

<CodeTabs labels={["MacOS/Linux", "Windows", "Windows (WSL)"]}>

```json
{
  "mcpServers": {
    "neon": {
      "command": "npx",
      "args": ["-y", "@neondatabase/mcp-server-neon", "start", "<YOUR_NEON_API_KEY>"]
    }
  }
}
```

```json
{
  "mcpServers": {
    "neon": {
      "command": "cmd",
      "args": ["/c", "npx", "-y", "@neondatabase/mcp-server-neon", "start", "<YOUR_NEON_API_KEY>"]
    }
  }
}
```

```json
{
  "mcpServers": {
    "neon": {
      "command": "wsl",
      "args": ["npx", "-y", "@neondatabase/mcp-server-neon", "start", "<YOUR_NEON_API_KEY>"]
    }
  }
}
```

</CodeTabs>

5. **Save** the `cline_mcp_settings.json` file.
6. You should see a notification in VS Code that says: "MCP servers updated".
   ![Cline MCP Server Updated](/docs/guides/cline-mcp-config-update.png)
7. Cline is now connected to Neon's remote MCP server.

### Verifying the Integration

Now that you have the Neon MCP server set up either remotely or locally, you can verify the connection and test the available tools. If the integration is successful, you should see the Neon MCP server listed in the Cline MCP Servers Installed section.

![Cline Available MCP Tools](/docs/guides/cline-available-mcp-tools.png)

You've now configured Neon MCP Server in Cline and can manage your Neon Postgres databases using AI.

<MCPTools />

These actions enable any MCP client like Cline to interact with various functionalities of the **Neon platform via the Neon API.** Certain tools, especially database migration ones, are tailored for AI agent and LLM usage, leveraging Neon's branching for safe preview and commit.

## Development Workflow: Adding a Column with Cline and Neon MCP

Let's walk through a typical development scenario: Quickly adding a column for prototyping using natural language within Cline. The following chat log demonstrates a real interaction with Cline and the Neon MCP server.

**Scenario:** During development, you decide to track timestamps for entries in your `playing_with_neon` table. You want to quickly add a `created_at` column.

<Admonition type="tip" title="Security Reminder">
For your security, review the tool's purpose before permitting the operation to proceed. Remember that LLMs can sometimes produce unexpected results, so careful monitoring is always recommended.
</Admonition>

Following is a sample interaction with Cline where you can see how it uses the Neon MCP server to add a column to your table:

```text shouldWrap
User: in my neon project id: fancy-bush-59303206, list all my tables
Cline: I need to list all tables in the specified Neon project. Since I have access to the Neon MCP server, I can use the get_database_tables tool to accomplish this task. The tool requires:
> projectId - which the user provided: "fancy-bush-59303206"
> databaseName - which I can default to "neondb" as it's the standard default database name in Neon
> Cline wants to use a tool on the neon MCP server: get_database_tables
> Your Neon project (fancy-bush-59303206) has 1 table in the public schema:
> - playing_with_neon (Base Table)


User: can you show me first 5 rows in it?
Cline: I'll use the run_sql tool to execute a SELECT statement that retrieves the first 5 rows from the playing_with_neon table. I'll use LIMIT 5 to ensure we only get the first 5 rows.
> Cline wants to use a tool on the neon MCP server: run_sql
> I have successfully retrieved the first 5 rows from the playing_with_neon table. I'll present this data in a clear, formatted way.
> | id | name       | value     |
> |----|------------|-----------|
> | 1  | c4ca4238a0 | 0.857180 |
> | 2  | c81e728d9d | 0.416151 |
> | 3  | eccbc87e4b | 0.440935 |
> | 4  | a87ff679a2 | 0.438505 |
> | 5  | e4da3b7fbb | 0.686455 |

User: can you add a created_at column, use a temporary branch
Cline: I'll use the prepare_database_migration tool to add a created_at column to the playing_with_neon table. This tool will automatically create a temporary branch for safety. After the migration is prepared, I'll need to verify it before completing it.
> Cline wants to use a tool on the neon MCP server: prepare_database_migration
> Let me verify the migration by checking the table structure in the temporary branch. I'll use run_sql to describe the table and ensure the created_at column was added correctly.
> Cline wants to use a tool on the neon MCP server: run_sql
> The created_at column has been successfully added to the table in the temporary branch with the correct data type and default value. Let me verify the data by selecting a few rows to ensure everything looks correct.
> Cline wants to use a tool on the neon MCP server: run_sql
> The migration looks good - the created_at column has been added successfully and is populated with timestamps. Now I can complete the migration to apply these changes to the production branch.
> Cline wants to use a tool on the neon MCP server: complete_database_migration
> The migration has been successfully completed and the temporary branch has been deleted. Let me verify the changes in the production branch and show the updated table structure.
> Cline wants to use a tool on the neon MCP server: run_sql
> The migration has been successfully completed. The created_at column has been added to the table and is populated with timestamps.
```

**Key takeaways:**

- **Natural language interaction:** You can use simple, conversational English to interact with your database through Cline.
- **Step-by-step guidance:** Cline confirms each step and provides details like branch names and migration IDs.
- **MCP Tools in Action:** The underlying MCP tools (`get_database_tables`, `run_sql`, `prepare_database_migration`, `complete_database_migration`) illustrate the automated process.
- **Branching for safety:** The agent automatically creates a temporary branch for schema changes. The user will be prompted to confirm the migration before applying it to the production branch.

You can verify the changes in your Neon Console or by querying the database directly.

![Neon Console with added column](/docs/guides/neon-cursor-mcp-added-column.png)

<Admonition type="note">
While the Neon MCP server allows you to utilize all of Neon's functionalities, you can also use a [generic Postgres MCP server](https://github.com/modelcontextprotocol/servers/tree/main/src/postgres) to connect to Neon.
</Admonition>

## Bonus: Creating a project and branch

Beyond schema changes, you can also manage your Neon projects and branches directly within Cline using natural language commands. This can be useful for quickly setting up a new development or test environment.

### Creating a new project

Let's say you want to create a new Neon project named "my-new-project". You can simply tell Cline:

```text shouldWrap
User: Create a Neon project named "my-new-project"
Cline: I see that I have access to the Neon MCP server which provides tools for interacting with Neon's API. Looking at the available tools, I can use the create_project tool to create a new Neon project with the specified name "my-new-project".
> Cline wants to use a tool on the neon MCP server:
> create_project ARGUMENTS: { "name": "my-new-project" }
> I've created a new Neon project named "my-new-project". Here are the project details:
> Project ID: muddy-bar-98083629
> Default branch: main
> Default database: neondb
```

![Cline creating a new Neon project](/docs/guides/cline-mcp-create-new-project.png)

Cline will then use the `create_project` MCP tool to initiate the project creation. It will provide you with a project ID and name.

### Creating a New Branch

Once you have a project, you can easily create new branches for development or experimentation. For example, to create a branch named "feature-x" in your "my-new-project" project:

```text shouldWrap
User: Create a branch named "feature-x"
Cline: I'll use the create_branch tool from the Neon MCP server to create a new branch named "feature-x" in the project we just created (project ID: muddy-bar-98083629).
> Cline wants to use a tool on the neon MCP server:
> create_branch ARGUMENTS: { "project_id": "muddy-bar-98083629", branchName": "feature-x" }
> I've created a new branch named "feature-x" in your Neon project. Here are the branch details:
> Branch ID: br-broad-base-a5kh7wyi
> Branch name: feature-x
> Parent branch: br-lively-cherry-a5kz48n6
```

![Cline creating a new Neon branch](/docs/guides/cline-mcp-create-new-branch.png)

Cline will use the `create_branch` MCP tool to create the branch and provide you with the branch name and ID. Notice how we don't need to specify the project ID, as Cline remembers the active project context.

## Conclusion

Cline with Neon MCP Server lets you use natural language to interact with your database and take advantage of Neon's branching capabilities for fast iteration. This approach is ideal for quickly testing database ideas and making schema changes during development.

## Resources

- [MCP Protocol](https://modelcontextprotocol.org)
- [Cline Documentation](https://docs.cline.bot)
- [Neon Docs](/docs)
- [Neon API Keys](/docs/manage/api-keys#creating-api-keys)
- [Neon MCP server GitHub](https://github.com/neondatabase/mcp-server-neon)

<NeedHelp/>


# The Complete Supabase to Neon Database & Auth Migration Guide

---
title: The Complete Supabase to Neon Database & Auth Migration Guide
subtitle: A comprehensive guide to migrating your Postgres database, user accounts, and RLS policies from Supabase to Neon
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-09-03T00:00:00.000Z'
updatedOn: '2025-09-03T00:00:00.000Z'
---

This guide walks you through migrating your Postgres database, user accounts, and Row-Level Security (RLS) policies from Supabase to Neon. It addresses key differences between the platforms, including the reassignment of `user_id` values during the auth migration, and provides steps to remap IDs, restore data integrity, and update your application code.

### Prerequisites

Before you begin, ensure you have the following:

- An active Supabase project.
- A Neon project. For setup instructions, see [Create a project](/docs/manage/projects#create-a-project).
- The PostgreSQL `psql` and `pg_dump` command-line utilities installed locally.
- A Node.js environment (for running the user import script).

<Steps>

## Part 1: Data and Authentication migration

This part covers the migration of your user accounts and public schema data, followed by remapping user IDs to restore data integrity.

### Step 1: Migrate user accounts from Supabase to Neon Auth

If your project does not use Supabase Auth, you can skip this section.

Supabase Auth is the authentication system used by Supabase. It manages user accounts, passwords, and session tokens.

#### 1.1: Export users and password hashes from Supabase

Connect to your Supabase project using the SQL Editor in the dashboard and run the following SQL function. This function retrieves all user emails and their encrypted **bcrypt** password hashes.

```sql
CREATE OR REPLACE FUNCTION ufn_get_user_emails_and_passwords()
RETURNS table (email text, encrypted_password character varying(255)) AS
$$
BEGIN
RETURN QUERY
   SELECT DISTINCT ON (i.email)
       i.email,
       u.encrypted_password
   FROM auth.users u
   JOIN auth.identities i ON u.id = i.user_id;
END;
$$ LANGUAGE plpgsql;

-- Execute the function
SELECT * FROM ufn_get_user_emails_and_passwords();
```

After running the query, export the results as a CSV file and save it locally as `user_data.csv`.

#### 1.2: Set up Neon Auth and Data API

In your Neon project dashboard:

1. Navigate to the **Data API** page from the sidebar.
2. Select **Neon Auth** as the authentication provider.
3. Follow the on-screen instructions to set up **Neon Auth** with the **Neon Data API**.
4. Navigate to the **Neon Auth** section from the sidebar.
5. In the **Configuration** tab, copy your **Project ID** and **Stack Secret Server Key** from the **Environment Variables** section.

#### 1.3: Import Users into Neon Auth

Now, we'll use a Node.js script to import the users from your `user_data.csv` file into Neon Auth.

First, create a new project directory and install the necessary packages:

```bash
npm init -y
npm install csv-parse
```

Copy the `user_data.csv` file to this directory.

Next, create a file named `migrate_users.ts` and add the following script.

> Replace the `YOUR_PROJECT_ID` and `YOUR_SERVER_KEY` placeholders in the `CONFIG` object with your actual Project ID and Server Key, which you copied from the Neon dashboard.

```typescript
import fs from 'fs';
import { parse } from 'csv-parse/sync';

const CONFIG = {
  csvFilePath: './user_data.csv',
  apiUrl: 'https://api.stack-auth.com/api/v1/users',
  headers: {
    'Content-Type': 'application/json',
    'X-Stack-Project-Id': 'YOUR_PROJECT_ID', // Update with your actual keys
    'X-Stack-Secret-Server-Key': 'YOUR_SERVER_KEY', // Update with your actual keys
    'X-Stack-Access-Type': 'server',
  },
  // Delay between requests in ms (to avoid rate limiting)
  requestDelay: 500,
};

const sleep = (ms: number | undefined) => new Promise((resolve) => setTimeout(resolve, ms));

async function migrateUsers() {
  try {
    console.log(`Reading CSV file from ${CONFIG.csvFilePath}...`);
    let fileContent = fs.readFileSync(CONFIG.csvFilePath, 'utf8');

    if (fileContent.charCodeAt(0) === 0xfeff) {
      console.log('Removing UTF-8 BOM from CSV file...');
      fileContent = fileContent.slice(1);
    }

    type UserRecord = { email: string; encrypted_password?: string };
    const records: UserRecord[] = parse(fileContent, {
      columns: true,
      skip_empty_lines: true,
      trim: true,
      bom: true,
    });

    console.log(`Found ${records.length} users to migrate.`);

    let successCount = 0;
    let failureCount = 0;

    for (const [index, user] of records.entries()) {
      try {
        // Extract email and password from CSV record
        const { email, encrypted_password } = user;

        if (!email) {
          console.error(`Row ${index + 1}: Missing email`);
          failureCount++;
          continue;
        }

        const payload: {
          primary_email: string;
          primary_email_verified: boolean;
          primary_email_auth_enabled: boolean;
          password_hash?: string;
        } = {
          primary_email: email,
          primary_email_verified: true,
          primary_email_auth_enabled: true,
        };

        // Include the password_hash in the payload if encrypted_password is provided and not the string "null"
        // (CSV exports represent null values as "null" for OAuth users)
        if (encrypted_password && encrypted_password !== 'null') {
          payload.password_hash = encrypted_password;
        } else {
          console.warn(`Row ${index + 1}: No password hash for ${email}`);
        }

        // Send the request to create the user
        console.log(`[${index + 1}/${records.length}] Creating user: ${email}...`);

        const response = await fetch(CONFIG.apiUrl, {
          method: 'POST',
          headers: CONFIG.headers,
          body: JSON.stringify(payload),
        });

        const responseData = await response.json();

        if (!response.ok) {
          console.error(`Failed to create user ${email}: ${JSON.stringify(responseData)}`);
          failureCount++;
        } else {
          console.log(`Successfully created user: ${email}`);
          successCount++;
        }

        // Add delay between requests to avoid rate limiting
        await sleep(CONFIG.requestDelay);
      } catch (error) {
        console.error(
          `Error processing row ${index + 1}:`,
          error instanceof Error ? error.message : String(error)
        );
        failureCount++;
      }
    }

    console.log('\n===== Migration Summary =====');
    console.log(`Total users: ${records.length}`);
    console.log(`Successfully migrated: ${successCount}`);
    console.log(`Failed: ${failureCount}`);
  } catch (error) {
    console.error('Migration failed:', error);
  }
}

migrateUsers()
  .then(() => {
    console.log('====== Migration process completed. ======');
  })
  .catch((err) => {
    console.error('Fatal error:', err);
  });
```

Before running the script, **update the `CONFIG` section** with your Neon Auth Project ID, Server Key, and the correct path to your `user_data.csv` file.

Execute the script from your terminal:

```bash
npx ts-node migrate_users.ts
```

Upon completion, all your users will be migrated into Neon Auth.

<Admonition type="important" title="User IDs Have Changed">
It's important to note that this migration process has assigned **new, unique `user_id`** values to all your users within Neon Auth. In the next steps, we will fix the broken references in your database that result from this change.
</Admonition>

### Step 2: Export the Supabase Public Schema

Use `pg_dump` to export the schema and data from your `public` Supabase schema.

> If your database includes schemas other than `public`, adjust the `--schema` flag accordingly (e.g., `--schema=public --schema=other_schema`).

```shell shouldWrap
pg_dump -v -d "SUPABASE_CONNECTION_STRING" --schema=public --no-acl -f supabase_dump.sql
```

- `-d "..."`: Your full Supabase database connection string.
- `--schema=public`: Dumps only the `public` schema.
- `--no-acl`: Excludes access control lists (`GRANT`/`REVOKE`). We will re-apply these manually.
- `-f ...`: Specifies the output file name.

### Step 3: Pre-process the SQL Dump File

This is a crucial manual step. Open `supabase_dump.sql`, make the following changes, and save it as `supabase_dump.sql`.

#### 3.1. Update RLS policies

Supabase and Neon Auth use different functions to identify the current user. You must replace all instances of `auth.uid()` with `auth.user_id()`.

- **Search for:** `auth.uid()`
- **Replace with:** `auth.user_id()`

**Example:**

```sql
-- BEFORE (Supabase Policy)
CREATE POLICY "Users can access their own todos" ON public.todos FOR SELECT USING ((auth.uid() = user_id));

-- AFTER (Neon-compatible Policy)
CREATE POLICY "Users can access their own todos" ON public.todos FOR SELECT USING ((auth.user_id() = user_id));
```

#### 3.2. Temporarily remove foreign key constraints

Your tables may include foreign key constraints that reference the `auth.users` table. These constraints will fail during the import process because Neon does not have an `auth.users` table.

To handle this:

1. Search your `supabase_dump.sql` file for all `ALTER TABLE ... ADD CONSTRAINT ... FOREIGN KEY` statements that reference `auth.users`. You can use your text editor's search function for `auth.users`.
2. **Cut** these statements from the file and paste them into a separate temporary text file named `foreign_keys.sql`. You will reapply them in Step 6.

### Step 4: Import the modified data into Neon

Use `psql` to import the edited schema and data into your Neon database.

```shell shouldWrap
psql -d "NEON_CONNECTION_STRING" -f supabase_dump.sql
```

Your tables, data, and RLS policies are now in Neon, but the `user_id` columns still contain old Supabase IDs.

### Step 5: Create a User ID mapping table

To fix the user references, we'll create a temporary table in Neon that maps old Supabase `user_id` values to emails. This command dumps the original `auth.users` data from Supabase, retargets the `INSERT` statements to a new `public.temp_users` table, and pipes it directly into Neon.

```shell shouldWrap
pg_dump -t auth.users --data-only --column-inserts "SUPABASE_CONNECTION_STRING" \
| sed 's/INSERT INTO auth.users/INSERT INTO public.temp_users/g' \
| psql "NEON_CONNECTION_STRING"
```

You now have a `public.temp_users` table in Neon containing the original Supabase `id` and `email` for each user.

### Step 6: Update foreign keys and re-establish relations

Now, we perform the remapping. For each table that contains a `user_id`, run a script to replace the old IDs with the new ones by joining through the user email address.

<Admonition type="tip" title="Which Constraints to Reapply">
Refer to your `foreign_keys.sql` file to identify which constraints need to be reapplied and to which tables.
</Admonition>

**Example script for a `todos` table:** (Repeat this process for every relevant table)

```sql
-- 1. Update the user_id column with the new ID from Neon Auth.
UPDATE
  public.todos AS t
SET
  user_id = ns.id::uuid -- Cast to UUID
FROM
  public.temp_users AS tu
JOIN
  neon_auth.users_sync AS ns
  ON tu.email = ns.email
WHERE
  t.user_id = tu.id;

-- 2. Adjust the column type to match Neon Auth's 'text' user ID type.
ALTER TABLE public.todos ALTER COLUMN user_id TYPE text;

-- 3. Re-add the foreign key constraint, pointing to the new Neon Auth user table.
ALTER TABLE public.todos
ADD CONSTRAINT todos_user_id_fkey -- Use your original constraint name
FOREIGN KEY (user_id) REFERENCES neon_auth.users_sync(id) ON DELETE CASCADE;
```

Once all tables have been updated, your data integrity will be fully restored. You can now safely remove the temporary table by executing the following SQL command:

```sql
DROP TABLE public.temp_users;
```

## Part 2: Finalize: Row level security

> If your Supabase project does not utilize Row-Level Security (RLS), you can safely skip this section.

The next step is to configure table permissions in Neon so your RLS policies behave correctly. The primary difference is the name of the anonymous role.

<Admonition type="important" title="Role Name Change: `anon` to `anonymous`">
Supabase uses the role `anon` for unauthenticated users. Neon uses the standard Postgres role `anonymous`. The `authenticated` role name is the same on both platforms.
</Admonition>

Apply the following general permissions to enable access for both roles. Your RLS policies will then enforce the fine-grained control.

```sql
-- Grant permissions for existing tables
GRANT SELECT, UPDATE, INSERT, DELETE ON ALL TABLES IN SCHEMA public TO authenticated;
GRANT SELECT, UPDATE, INSERT, DELETE ON ALL TABLES IN SCHEMA public TO anonymous;

-- Ensure permissions for future tables
ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT, UPDATE, INSERT, DELETE ON TABLES TO authenticated;
ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT, UPDATE, INSERT, DELETE ON TABLES TO anonymous;

-- Grant usage on the schema
GRANT USAGE ON SCHEMA public TO authenticated;
GRANT USAGE ON SCHEMA public TO anonymous;
```

#### Migrating specific permissions

To achieve an exact 1:1 migration of permissions, extract the current permissions from Supabase using the following command:

```shell
pg_dump --schema-only -d "SUPABASE_CONNECTION_STRING" --schema=public | grep -E "^(GRANT|REVOKE)" > permissions.sql
```

This command generates a `permissions.sql` file containing only the `GRANT` and `REVOKE` statements from your Supabase public schema.

If the `permissions.sql` file looks like this example:

```sql
GRANT USAGE ON SCHEMA public TO postgres;
GRANT USAGE ON SCHEMA public TO anon;
GRANT USAGE ON SCHEMA public TO authenticated;
GRANT USAGE ON SCHEMA public TO service_role;
GRANT ALL ON TABLE public.todos TO anon;
GRANT ALL ON TABLE public.todos TO authenticated;
GRANT ALL ON TABLE public.todos TO service_role;
```

Edit it as follows:

- Replace `anon` with `anonymous` (e.g., `GRANT USAGE ON SCHEMA public TO anonymous;`).
- Remove roles specific to Supabase, such as `service_role` and `postgres` (if not needed in Neon).
- After editing, apply the modified permissions to your Neon database using `psql`:

After edits it should look like this:

```sql
GRANT USAGE ON SCHEMA public TO anonymous;
GRANT USAGE ON SCHEMA public TO authenticated;
GRANT ALL ON TABLE public.todos TO anonymous;
GRANT ALL ON TABLE public.todos TO authenticated;
```

```shell
psql -d "NEON_CONNECTION_STRING" -f permissions.sql
```

## Part 3: Migrating your application code (Next.js example)

After migrating your database and user accounts, the final step is to update your application code to work with Neon Auth and the Neon Data API. This section guides you through refactoring a Next.js application from using Supabase's client libraries (`@supabase/ssr`, `@supabase/supabase-js`) to using Neon Auth's SDK (`@stackframe/stack`) and the standard `postgrest-js` library for data access.

The primary change in this migration is moving from Supabase's single, integrated client library to a composable stack:

1.  **Authentication:** You will replace Supabase Auth functions (`supabase.auth.getUser()`, custom middleware, and callback routes) with Neon Auth's SDK. Neon Auth handles session management and provides simple hooks (`useUser`) and server-side helpers (`stackServerApp.getUser()`) to access user data.
2.  **Data Access:** You will replace the data access portion of the Supabase client (`supabase.from(...)`) with a dedicated PostgREST client (`postgrest-js`). The Neon Data API is PostgREST-compliant, meaning **your query syntax (e.g., `.select()`, `.insert()`, `.eq()`) will remain almost identical.** The main difference is how you initialize the client and authenticate requests using a JWT from Neon Auth.

### Step 1: Update project dependencies

First, remove the Supabase packages and install the PostgREST client library.

```bash
# Remove Supabase libraries
npm uninstall @supabase/ssr @supabase/supabase-js

# Install PostgREST library
npm install @supabase/postgrest-js@1.19.4
```

### Step 2: Initialize Neon Auth in your project

Neon Auth (powered by [Stack Auth](https://stack-auth.com), an open-source auth solution) provides a setup command to configure your Next.js application automatically. This command will scaffold necessary files, such as auth handlers and provider components.

Run the following command in your project's root directory:

```bash
npx @stackframe/init-stack@latest --no-browser
```

This command will perform the following actions:

- **Create Auth Handlers:** Adds a catch-all route at `app/handler/[...stack]/page.tsx`. This single file handles all authentication UI flows (sign-up, sign-in, password reset, OAuth callbacks) provided by Neon Auth.
- **Update Layout:** Wraps your root layout (`app/layout.tsx`) in a `<StackProvider>` to make authentication state available throughout your app.
- **Create Server Configuration:** Adds a `stack.tsx` file for server-side initialization of the auth SDK.

### Step 3: Configure data access client for Neon Data API

Unlike the integrated Supabase client, you need to configure the PostgREST client to use the access token (JWT) generated by Neon Auth for authenticated requests.

1.  **Create an Access token provider:** This provider uses a React Context to make the current user's access token available to components that perform data fetching.

    _Create file `access-token-context.tsx`:_

    ```typescript
    import { createContext } from 'react';
    export const AccessTokenContext = createContext<string | null>(null);
    ```

    _Create file `access-token-provider.tsx`:_

    ```typescript
    "use client";

    import { AccessTokenContext } from "@/access-token-context";
    import { useUser } from "@stackframe/stack";
    import { useEffect, useState } from "react";

    export function AccessTokenProvider({ children }: { children: React.ReactNode }) {
        const user = useUser();
        const [accessToken, setAccessToken] = useState<string | null>(null);
        const [isLoading, setIsLoading] = useState(true);

        useEffect(() => {
            const fetchAccessToken = async () => {
                if (user) {
                    setAccessToken((await user.getAuthJson()).accessToken);
                }
                setIsLoading(false);
            };

            fetchAccessToken();
            // Refresh the token periodically before it expires
            const intervalId = setInterval(fetchAccessToken, 1000 * 60);

            return () => clearInterval(intervalId);
        }, [user]);

        if (isLoading) {
            return null;
        }

        return (
            <AccessTokenContext.Provider value={accessToken}>
                {children}
            </AccessTokenContext.Provider>
        );
    }
    ```

2.  **Update root layout:** Wrap your application with the `AccessTokenProvider` inside the existing `<StackProvider>`.

    _File: `app/layout.tsx`_

    ```tsx {4-5,14-15,17} shouldWrap
    import type { Metadata } from 'next';
    import { StackProvider, StackTheme } from '@stackframe/stack';
    import { stackServerApp } from '../stack'; // Created by init command
    import { AccessTokenProvider } from '@/access-token-provider';
    import { Suspense } from 'react';
    import './globals.css';

    export default function RootLayout({ children }: { children: React.ReactNode }) {
      return (
        <html lang="en">
          <body>
            <StackProvider app={stackServerApp}>
              <StackTheme>
                <Suspense fallback={<div>Loading...</div>}>
                  <AccessTokenProvider>{children}</AccessTokenProvider>
                </Suspense>
              </StackTheme>
            </StackProvider>
          </body>
        </html>
      );
    }
    ```

3.  **Create PostgREST client hook:** Create a custom hook `usePostgrest` that initializes the PostgREST client and automatically injects the access token into the request headers.

    _Create file `lib/postgrest.ts`:_

    ```typescript
    import { AccessTokenContext } from '@/access-token-context';
    import { PostgrestClient } from '@supabase/postgrest-js';
    import { useContext } from 'react';

    // Add your Neon Data API endpoint to your .env.local file
    // NEXT_PUBLIC_DATA_API_URL=https://<project-id>.dpl.myneon.app
    const dataApiUrl = process.env.NEXT_PUBLIC_DATA_API_URL!;

    const postgrestWithHeaders = (headers: Record<string, string>) => {
      return new PostgrestClient(dataApiUrl, {
        fetch: async (...args) => {
          const [url, options = {}] = args;
          return fetch(url, {
            ...options,
            headers: {
              ...options.headers,
              ...headers,
            },
          });
        },
      });
    };

    export function usePostgrest() {
      const accessToken = useContext(AccessTokenContext);
      return postgrestWithHeaders({
        Authorization: `Bearer ${accessToken}`,
      });
    }
    ```

### Step 4: Refactor application code

Now, replace Supabase-specific logic with Neon Auth and PostgREST calls.

#### 4.1. Protecting routes (Server-Side)

Replace `supabase.auth.getUser()` with `stackServerApp.getUser()` to protect pages and server actions.

<CodeTabs labels={["Before (Supabase)", "After (Neon Auth)"]}>

```typescript shouldWrap
// File: app/protected/page.tsx (Supabase)

import { redirect } from 'next/navigation'
import { createClient } from '@/lib/supabase/server'

export default async function PrivatePage() {
    const supabase = await createClient()

    const { data, error } = await supabase.auth.getUser()
    if (error || !data?.user) {
        redirect('/login')
    }

    return <p>Hello {data.user.email}</p>
}
```

```typescript shouldWrap
// File: app/protected/page.tsx (Neon Auth)

import { redirect } from 'next/navigation'
import { stackServerApp } from "@/stack";

export default async function PrivatePage() {
    const user = await stackServerApp.getUser();

    if (!user || !user.id) {
        redirect('/handler/login') // Redirect to Neon Auth's built-in login page
    }

    return <p>Hello {user.primaryEmail}</p>
}
```

</CodeTabs>

#### 4.2. Data fetching and mutations (client-side)

Replace the `supabase` client instance with the new `usePostgrest()` hook for data operations. Notice how the query syntax remains unchanged.

<CodeTabs labels={["Before (Supabase)", "After (Neon Auth + PostgREST)"]}>

```typescript shouldWrap
// File: components/TodoApp.tsx (Supabase)

import { createClient } from '@/lib/supabase/client';
import type { User } from '@supabase/supabase-js';

// ... inside component
const supabase = createClient();
const userId = user.id;

async function loadTodos() {
  const { data, error } = await supabase
    .from('todos')
    .select('*')
    .order('inserted_at', { ascending: false });
  // ... update state
}

async function addTodo(e: React.FormEvent) {
  // ... logic
  const { data, error } = await supabase
    .from('todos')
    .insert([{ title, user_id: userId }])
    .select()
    .single();
  // ... update state
}

async function signout() {
  await supabase.auth.signOut();
}
```

```typescript shouldWrap
// File: components/TodoApp.tsx (Neon Auth + PostgREST)

import { usePostgrest } from '@/lib/postgrest';
import type { CurrentUser } from '@stackframe/stack';

// ... inside component
const postgrest = usePostgrest(); // Use the new hook
const userId = user.id;

async function loadTodos() {
  const { data, error } = await postgrest // Client instance changed
    .from('todos') // Query syntax is identical
    .select('*')
    .order('inserted_at', { ascending: false });
  // ... update state
}

async function addTodo(e: React.FormEvent) {
  // ... logic
  const { data, error } = await postgrest // Client instance changed
    .from('todos') // Query syntax is identical
    .insert([{ title, user_id: userId }])
    .select()
    .single();
  // ... update state
}

async function signout() {
  await user.signOut(); // Use Neon Auth user object method
}
```

</CodeTabs>

#### 4.3. Client-side authentication state

Replace Supabase session handling (`getSession`, `onAuthStateChange`) with the `useUser` hook from Neon Auth for a simpler, more modern React approach.

<CodeTabs labels={["Before (Supabase)", "After (Neon Auth)"]}>

```typescript shouldWrap
// File: app/page.tsx (Supabase)

"use client";
import { useEffect, useState } from "react";
import type { Session } from "@supabase/supabase-js";
import { createClient } from "@/lib/supabase/client";

export default function Page() {
  const [session, setSession] = useState<Session | null>(null);
  const supabase = createClient();

  useEffect(() => {
    supabase.auth.getSession().then(({ data: { session } }) => {
      setSession(session);
    });
    const { data: { subscription } } = supabase.auth.onAuthStateChange(
      (_event, session) => setSession(session)
    );
    return () => subscription?.unsubscribe();
  }, []);

  if (!session) {
      return <a href="/login">Sign up or sign in</a>;
  }
  return <TodoApp user={session.user} />;
}
```

```typescript shouldWrap
// File: app/page.tsx (Neon Auth)

"use-client";
import { useUser } from '@stackframe/stack';
import Link from 'next/link';

export default function Page() {
  const user = useUser(); // Replaces all session management logic

  if (!user) {
    return (
      <Link href="/handler/login">Sign up or sign in</Link>
    );
  }
  return <TodoApp user={user} />;
}
```

</CodeTabs>

    <Admonition type="info" title="Neon Auth Hooks">
      The Neon Auth SDK for Next.js offers a comprehensive set of hooks to manage authentication and user data throughout your application. It provides distinct tools tailored for different rendering environments, such as the `useUser` hook for Client Components and the `stackServerApp` object for server-side logic.

    To explore the full API, including hooks for more advanced features like handling teams and permissions, refer to the [Neon Auth: Next.js SDK Overview](/docs/neon-auth/sdk/nextjs/overview).
    </Admonition>

### Step 5: Clean up deprecated Supabase files

After refactoring, you can safely remove the Supabase-specific helper files and custom authentication routes, as Neon Auth's SDK handles these functionalities automatically.

Delete the following files and directories:

- `lib/supabase/client.ts`
- `lib/supabase/server.ts`
- `lib/supabase/middleware.ts` (and remove middleware configuration from `middleware.ts`)
- `app/login/` (directory)
- `app/auth/callback/` (directory)
- `app/auth/confirm/` (directory)

Your application code is now fully migrated to Neon Auth and the Neon Data API.

For a detailed example of the code migration process, refer to this example pull request: [Supabase to Neon Todo App Migration](https://github.com/neondatabase-labs/supabase-to-neon-todo-app/pull/3/files).

The repository includes two branches: [supabase](https://github.com/neondatabase-labs/supabase-to-neon-todo-app/tree/supabase) and [neon](https://github.com/neondatabase-labs/supabase-to-neon-todo-app/tree/neon) showcasing the before and after states of a sample todo application. This demonstrates the transition from Supabase Auth, Row-Level Security (RLS), and the Supabase Postgres Data API to Neon Auth, RLS, and the Neon PostgREST Data API.

## Part 4: Upgrading your development workflow with Database Branching

If you used Supabase's branching feature for preview environments, you'll feel right at home with Neon. In fact, you'll be working with the original, more powerful version of the concept: **Neon was the first postgres database provider to introduce instant, serverless copy-on-write database branching.**

While the goal is similar, creating isolated environments for development and testing the implementation and capabilities are fundamentally different. Migrating to Neon offers a significant upgrade to your CI/CD and development workflows.

### The Neon Advantage: True Copy-on-Write Branching

The most significant difference is how branches are created. Supabase branches are **data-less by default**, meaning they create a new, empty database environment that you must then populate using seed scripts.

Neon branches are **instant, copy-on-write clones of your entire database, including the data.**

<Admonition type="info" title="What This Means For Your Workflow">
With Neon, creating a new branch for a pull request takes milliseconds and gives you a fully-functional, isolated copy of your production database. This completely eliminates the need to write and maintain complex seed scripts for every preview environment. You can test new features and schema migrations against real-world data, safely and instantly.
</Admonition>

This approach provides several key benefits:

- **Test with production-like data:** Safely test schema changes and queries against a full replica of your production data.
- **Zero setup time:** Eliminate the time and effort spent hydrating databases for preview deployments.
- **Cost-efficient:** Because branches are copy-on-write, you only store the changes (the delta) from the parent branch, making it incredibly storage-efficient.

### Branching workflows and tooling

Neon provides a complete toolkit for managing branches, allowing you to integrate this powerful feature into any part of your workflow.

- **Neon Console:** Create, manage, and inspect branches visually through the dashboard. Perfect for quick manual operations or getting started. Learn more: [Manage branches](/docs/manage/branches)
- **Neon CLI:** Programmatically manage branches from your terminal. Ideal for local development, scripting, and automation. Learn more: [Branching with the Neon CLI](/docs/guides/branching-neon-cli)
- **Neon API:** The most powerful option for full programmatic control. Integrate branching directly into your custom tools, scripts, and platforms. Learn more: [Branching with the Neon API](/docs/guides/branching-neon-api)

### Automating with CI/CD (Vercel & GitHub Actions)

For most developers the primary use case for branching is creating preview environments for pull requests. Neon excels here with zero-config integrations and powerful, composable actions.

- **Vercel Integration:** The simplest way to get started. The [Neon Vercel Integration](/docs/guides/neon-managed-vercel-integration) automatically creates a new database branch for every preview deployment. It injects the correct connection string as an environment variable, giving you a fully isolated database environment for each PR with no configuration required.

- **GitHub Actions:** For more granular control over your CI/CD pipeline, Neon offers a suite of official GitHub Actions. These allow you to automate your entire branching lifecycle directly from your workflows. You can:
  - [**Create a branch**](https://github.com/marketplace/actions/neon-create-branch-github-action) when a pull request is opened.
  - [**Reset a branch**](https://github.com/marketplace/actions/neon-database-reset-branch-action) to the latest state of `main` to refresh it with new data.
  - [**Perform a schema diff**](https://github.com/marketplace/actions/neon-schema-diff-github-action) and post the results as a comment on the pull request.
  - [**Delete the branch**](https://github.com/marketplace/actions/neon-database-delete-branch) automatically when the pull request is merged or closed.
    > Checkout [The Neon GitHub integration](/docs/guides/neon-github-integration) for a detailed walkthrough.

## Conclusion

Congratulations! You've successfully migrated your Supabase database, users, and Row-Level Security (RLS) policies to Neon. Data integrity is intact, security policies are fully operational, and users can sign in using their original passwords with no resets required.

If your users were authenticated via OAuth providers like GitHub or Google in Supabase, you can seamlessly continue using these in Neon Auth without any issues. Note that Neon Auth currently supports OAuth for Microsoft, Google, and GitHub. For more details on setting up OAuth in production, refer to the [Neon Auth best practices documentation](https://neon.com/docs/neon-auth/best-practices#production-oauth-setup).

</Steps>

## Resources

- [pg_dump](https://www.postgresql.org/docs/current/app-pgdump.html)
- [Migrating data to Neon](/docs/import/migrate-from-postgres)
- [Migrate from Supabase](/docs/import/migrate-from-supabase)
- [Getting started with Neon Data API](/docs/data-api/get-started)
- [Neon Auth](/docs/neon-auth/overview)
- [Neon RLS](/docs/guides/neon-rls)
- [Getting started with Neon Auth and Next.js](/guides/neon-auth-nextjs)
- [A Simple 3-Step Process to Migrate from Supabase Auth to Neon Auth](/blog/supabase-auth-neon-auth)
- [Ship software faster using Neon branches as ephemeral environments](/branching)

<NeedHelp/>


# Building AI Agents with CrewAI, Composio, and Neon

---
title: Building AI Agents with CrewAI, Composio, and Neon
subtitle: A step-by-step guide to building AI agents using CrewAI, Composio, and Neon API
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-01-31T00:00:00.000Z'
updatedOn: '2025-01-31T00:00:00.000Z'
---

In today's AI-driven world, the ability to connect intelligent agents with powerful tools is key to building sophisticated applications. Imagine AI agents that can not only think and plan but also seamlessly interact with your database to retrieve information, manage resources, and perform complex tasks. This guide explores exactly that, demonstrating how to harness the power of **CrewAI** for agent orchestration, **Composio** for tool integration, and **Neon API** for database management.

Composio acts as a crucial bridge, empowering your CrewAI agents with the ability to interact directly with the Neon API. This integration unlocks a range of possibilities, from querying your Neon project details to managing your database infrastructure programmatically through AI. By leveraging Composio's wide range of tools, you can equip your AI agents with a comprehensive suite of actions, all without needing to wrestle with complex API integrations manually.

This guide provides a practical, hands-on approach to building an AI agent capable of retrieving information from your Neon account. You'll not only see how easily these technologies integrate but also gain a foundational understanding of how to extend this setup for more advanced AI-driven database interactions.

## Prerequisites

Before you start, make sure you have the following prerequisites in place:

- **Python 3.7 or higher:** This guide uses Python. If you don't have it already, download and install it from [python.org](https://www.python.org/downloads/).

- **Neon account and API Key:**
  - Sign up for a free Neon account at [neon.tech](https://console.neon.tech/signup).
  - Once signed up, you can find your Neon API Key [here](https://console.neon.tech/app/settings/profile). You'll need this key to authenticate your application with Neon.

- **Composio account and API Key:**
  - Create a Composio account by visiting [composio.dev](https://composio.dev/).
  - After signing up and logging in, your Composio API key will be available in your [Composio dashboard](https://app.composio.dev/dashboard). You will need this to authenticate your application.

- **OpenAI account and API Key:**
  - This guide uses `gpt-4o-mini` from OpenAI to power the AI agent. If you don't have one, sign up for an account at [platform.openai.com](https://platform.openai.com/).
  - Create a new API key in the [OpenAI Platform API keys section](https://platform.openai.com/api-keys). This key will allow CrewAI to interact with OpenAI's GPT-4o model.

Once these prerequisites in place, you'll be ready to follow the guide and build your AI agent.

## Building your AI Agent to interact with Neon API

Now, let's dive into building your AI agent that can interact with Neon API using CrewAI and Composio. We'll go through each step, from setting up your project to running your first agent.

### Project structure

For this guide, we'll keep the project structure simple. Create a directory for your project, for example `neon-composio-crewai`, and inside it, you'll have the following files:

    ```bash
    mkdir neon-composio-crewai
    ```

    ```bash
    neon-composio-crewai/
    ├── main.py         # Main Python script to run the AI agent
    ├── requirements.txt # Lists Python dependencies
    ├── .env            # Your environment variables
    ```

### Setting up a virtual environment

It's a good practice to create a virtual environment for your project to manage dependencies. You can create a virtual environment using `venv`:

    ```bash
    cd neon-composio-crewai
    python3 -m venv venv
    source venv/bin/activate # on Windows, use `venv\Scripts\activate`
    ```

### Installing required libraries

Next, you need to install the necessary Python libraries for this project. Create a `requirements.txt` file in your project directory and add the following lines:

    ```
    composio-crewai
    crewai
    python-dotenv
    ```

Then, install the libraries using pip:

    ```bash
    pip install -r requirements.txt
    ```

### Configuring API Keys in `.env`

Create a new file named `.env` in your project directory and add the following lines:

    ```env
    OPENAI_API_KEY = YOUR_OPENAI_API_KEY
    COMPOSIO_API_KEY = YOUR_COMPOSIO_API_KEY
    NEON_API_KEY = YOUR_NEON_API_KEY
    ```

**Replace the placeholders** `YOUR_OPENAI_API_KEY`, `YOUR_COMPOSIO_API_KEY`, and `YOUR_NEON_API_KEY` with your actual API keys that you obtained in the [Prerequisites](#prerequisites) section.

<Admonition type="note">
    Make sure you have added `.env` to your `.gitignore` file if you are using Git. This prevents your API keys from being accidentally committed to your code repository.
</Admonition>

### Creating the `main.py` file

Create a new file named `main.py` in your project root directory and paste the following code into it:

    ```python
    import os

    from crewai import Agent, Task, Crew
    from composio_crewai import ComposioToolSet, App
    from dotenv import load_dotenv

    load_dotenv()

    toolset = ComposioToolSet()

    # To connect to Neon, either create a new connection or use an existing one configured in your Composio dashboard (Apps -> Integrations).
    # You can comment out the connection creation if you have already created a connection in the dashboard.
    connection = toolset.initiate_connection(
        app=App.NEON, connected_account_params={"api_key": os.getenv("NEON_API_KEY")}
    )

    tools = toolset.get_tools(actions=["NEON_GET_CURRENT_USER_INFORMATION"])

    # Define agent
    crewai_agent = Agent(
        role="Assistant",
        goal="""You are an AI agent that is responsible for taking actions based on the tools you have""",
        backstory=(
            "You are AI agent that is responsible for taking actions based on the tools you have"
        ),
        verbose=True,
        tools=tools,
        llm="gpt-4o-mini",
    )

    task = Task(
        description="List me my neon current user details",
        agent=crewai_agent,
        expected_output="All important details of the current user in a single sentence.",
    )

    my_crew = Crew(agents=[crewai_agent], tasks=[task])

    result = my_crew.kickoff()
    print(result)
    ```

Let's break down what this python script does step by step:

### Import necessary libraries and load environment variables

    ```python
    import os

    from crewai import Agent, Task, Crew
    from composio_crewai import ComposioToolSet, App
    from dotenv import load_dotenv

    load_dotenv()
    ```

Just like any Python script, we start by importing the necessary libraries. In this case, we import the CrewAI library, the Composio CrewAI library, and the `load_dotenv` function from the `python-dotenv` library. We also call `load_dotenv()` to load environment variables from the `.env` file.

### Initialize `ComposioToolSet`

    ```python
    toolset = ComposioToolSet()
    ```

    This creates an instance of `ComposioToolSet`, which is the main entry point to interact with Composio tools from CrewAI.

### Initiate connection to Neon

    ```python
    connection = toolset.initiate_connection(
        app=App.NEON, connected_account_params={"api_key": os.getenv("NEON_API_KEY")}
    )
    ```

    - `toolset.initiate_connection(...)` initiates a connection to a specific app in Composio.
    - `app=App.NEON` specifies that the connection is for the Neon app.
    - `connected_account_params={"api_key": os.getenv("NEON_API_KEY")}` provides the Neon API key for authentication. This API key is retrieved from your environment variables.

<Admonition type="note">
    If you have already set up a Neon connection in your [Composio dashboard](https://app.composio.dev/integrations), you can comment out these lines. The existing connection will be used automatically when you specify the app in the toolset methods. However, for the guide, we are showing how to establish a connection programmatically.
</Admonition>

### Retrieve tools

    ```python
    tools = toolset.get_tools(actions=["NEON_GET_CURRENT_USER_INFORMATION"])
    ```

    - `toolset.get_tools(actions=[...])` fetches the specified tools (actions) from the Composio toolset.
    - `actions=["NEON_GET_CURRENT_USER_INFORMATION"]` indicates that we want to use the `NEON_GET_CURRENT_USER_INFORMATION` action, which retrieves your Neon user details. This action is part of the Neon toolset in Composio.

### Define the AI Agent

    ```python
    crewai_agent = Agent(
        role="Assistant",
        goal="""You are an AI agent that is responsible for taking actions based on the tools you have""",
        backstory=(
            "You are AI agent that is responsible for taking actions based on the tools you have"
        ),
        verbose=True,
        tools=tools,
        llm="gpt-4o-mini",
    )
    ```

This code defines a CrewAI agent named `crewai_agent`.

- `role`, `goal`, `backstory`: These attributes define the agent's identity and purpose.
- `verbose=True`: Enables detailed output from the agent, useful for debugging and understanding the agent's thought process.
- `tools=tools`: Assigns the Composio Neon tools we retrieved in the previous step to this agent. The agent can now use these tools to perform actions.
- `llm="gpt-4o-mini"`: Specifies that the agent will use the `gpt-4o-mini` language model from OpenAI.

### Define the Task

    ```python
    task = Task(
        description="List me my neon current user details",
        agent=crewai_agent,
        expected_output="All important details of the current user in a single sentence.",
    )
    ```

This creates a task for the agent to perform.

- `description`: Describes the task for the agent: "List me my neon current user details".
- `agent=crewai_agent`: Assigns the task to the `crewai_agent` we defined.
- `expected_output`: (Optional) Specifies the desired output format for the task.

### Create and run the Crew

    ```python
    my_crew = Crew(agents=[crewai_agent], tasks=[task])

    result = my_crew.kickoff()
    print(result)
    ```

    - `my_crew = Crew(...)`: Creates a CrewAI crew with the defined agents and tasks.
    - `result = my_crew.kickoff()`: Starts the crew execution. The agent will now execute the assigned task.
    - `print(result)`: Prints the result returned by the agent after completing the task. This will be the Neon user information.

### Running the example

Now that you have set up your project, installed dependencies, and configured your API keys, you are ready to run the example

In your terminal, run:

```bash
python main.py
```

This command will:

- Run the `main.py` Python script.
- The script will connect to Composio and Neon using your provided API keys.
- It will create a CrewAI agent.
- The agent will use the `NEON_GET_CURRENT_USER_INFORMATION` Composio tool action to retrieve your Neon user information.
- Finally, it will print the retrieved user information in your terminal.

### Expected output

After running `python main.py`, you should see the information about your Neon user printed in the terminal! The output will look something like this:

![Example output](/docs/guides/composio-crewai-neon-example-output.png)

**Congratulations!** You have successfully built and run an AI agent that can interact with your Neon account using CrewAI and Composio!

## Explore Further Neon Actions

The Composio Neon tool provides a wide range of actions you can use to manage your Neon projects. The example we just ran used the `NEON_GET_CURRENT_USER_INFORMATION` action to retrieve your user details. You can modify the `main.py` script to experiment with other actions. For example, to get a list of your Neon projects, you would change the `actions` list in `toolset.get_tools(...)` to:

```python
tools = toolset.get_tools(actions=["NEON_RETRIEVE_PROJECTS_LIST"])
```

and update the task description accordingly.

Here's a list of all the available actions that you can use with the Neon Composio tool:

<Admonition type="important">
    These actions are subject to change. For the latest information and a complete list of available actions, please check the availaible actions under [Neon app in your Composio dashboard](https://app.composio.dev/app/neon).
    ![Neon Composio Tool Actions](/docs/guides/neon-composio-tool-actions.png)
</Admonition>

| Action name                                     | Description                                                                                             |
| ----------------------------------------------- | ------------------------------------------------------------------------------------------------------- |
| `NEON_RETRIEVE_PROJECTS_LIST`                   | Retrieves a list of all Neon projects associated with the authenticated user's account.                 |
| `NEON_CREATE_VPC_ENDPOINT_WITH_LABEL`           | Updates the label of a specific VPC endpoint within an organization's VPC in a particular AWS region.   |
| `NEON_RETRIEVE_ORGANIZATION_BY_ID`              | Retrieves detailed information about a specific organization within the Neon platform.                  |
| `NEON_FETCH_VPCENDPOINT_DETAILS_BY_ID`          | Retrieves detailed information about a specific VPC endpoint within an organization's infrastructure.   |
| `NEON_TRANSFER_USER_PROJECTS_TO_ORGANIZATION`   | Transfers multiple projects from the authenticated user's personal account to a specified organization. |
| `NEON_CREATE_VPC_ENDPOINT_LABEL`                | Updates the label of a specific VPC endpoint within a project.                                          |
| `NEON_GET_BRANCHES_FOR_PROJECT`                 | Retrieves a list of branches associated with a specific project.                                        |
| `NEON_GET_CURRENT_USER_INFORMATION`             | Retrieves the profile information for the currently authenticated user.                                 |
| `NEON_DELETE_VPC_ENDPOINT_BY_IDS`               | Deletes a specific VPC endpoint within a given organization and region.                                 |
| `NEON_GET_USER_ORGANIZATIONS`                   | Retrieves a list of organizations associated with the currently authenticated user.                     |
| `NEON_FETCH_ORGANIZATION_MEMBERS_BY_ID`         | Retrieves a list of all members associated with a specific organization.                                |
| `NEON_RETRIEVE_PROJECT_OPERATIONS`              | Retrieves a list of operations associated with a specific project.                                      |
| `NEON_GET_PROJECT_CONNECTION_URI`               | Retrieves the connection URI for a specified project.                                                   |
| `NEON_GET_PROJECT_ENDPOINT_INFORMATION`         | Retrieves a list of all endpoints associated with a specific project.                                   |
| `NEON_RETRIEVE_ORGANIZATION_MEMBER_INFO`        | Retrieves detailed information about a specific member within an organization.                          |
| `NEON_RETRIEVE_ALL_REGIONS`                     | Retrieves a list of available geographic regions supported by the Neon platform.                        |
| `NEON_UPDATE_ORGANIZATION_MEMBER_ROLE`          | Updates the role of a specific member within an organization.                                           |
| `NEON_SEND_ORGANIZATION_INVITATIONS`            | Creates and sends invitations to join an organization.                                                  |
| `NEON_GET_BRANCH_ROLES_FOR_PROJECT`             | Retrieves the roles associated with a specific branch within a project.                                 |
| `NEON_LIST_SHARED_PROJECTS`                     | Retrieves a list of shared projects accessible to the authenticated user.                               |
| `NEON_ACCESS_PROJECT_DETAILS_BY_ID`             | Retrieves detailed information about a specific project.                                                |
| `NEON_FETCH_DATABASE_FOR_BRANCH`                | Retrieves a list of databases associated with a specific project and branch.                            |
| `NEON_DELETE_API_KEY_BY_ID`                     | Deletes a specific API key from the Neon platform.                                                      |
| `NEON_RETRIEVE_PROJECT_ENDPOINT_DETAILS`        | Retrieves detailed information about a specific endpoint within a project.                              |
| `NEON_RETRIEVE_ACCOUNT_CONSUMPTION_HISTORY`     | Retrieves the consumption history for a specified account.                                              |
| `NEON_DELETE_PROJECT_PERMISSION`                | Deletes a specific permission associated with a project.                                                |
| `NEON_GET_SCHEMA_FOR_PROJECT_BRANCH`            | Retrieves the schema definition for a specific branch within a project.                                 |
| `NEON_RETRIEVE_ORGANIZATION_INVITATIONS`        | Retrieves a list of all pending invitations for a specified organization.                               |
| `NEON_DELETE_VPC_ENDPOINT_BY_PROJECT_ID`        | Deletes a specific VPC endpoint within a designated project.                                            |
| `NEON_GET_VPC_REGION_ENDPOINTS`                 | Retrieves a list of VPC endpoints for a specified organization within a particular AWS region.          |
| `NEON_RETRIEVE_BRANCH_DATABASE_DETAILS`         | Retrieves detailed information about a specific database within a Neon project and branch.              |
| `NEON_RESET_ROLE_PASSWORD_FOR_BRANCH`           | Resets the password for a specific role within a project branch.                                        |
| `NEON_DELETE_PROJECT_BRANCH_BY_ID`              | Deletes a specific branch within a project.                                                             |
| `NEON_DELETE_PROJECT_ENDPOINT`                  | Deletes a specific endpoint within a Neon project.                                                      |
| `NEON_LIST_API_KEYS`                            | Retrieves a list of API keys associated with the authenticated user's account.                          |
| `NEON_ADD_NEW_JWKS_TO_PROJECT_ENDPOINT`         | Adds a new JSON Web Key Set (JWKS) to a specific endpoint of a project.                                 |
| `NEON_CREATE_NEW_API_KEY`                       | Creates a new API key for accessing the Neon platform.                                                  |
| `NEON_RETRIEVE_JWKS_FOR_PROJECT`                | Retrieves the JSON Web Key Set (JWKS) for a specified project.                                          |
| `NEON_GET_CONSUMPTION_HISTORY_PROJECTS`         | Retrieves the consumption history for specified projects.                                               |
| `NEON_SUSPEND_PROJECT_ENDPOINT_BY_ID`           | Suspends a specific endpoint within a project.                                                          |
| `NEON_DELETE_PROJECT_JWKS_BY_ID`                | Deletes a specific JSON Web Key Set (JWKS) associated with a given project.                             |
| `NEON_GET_PROJECT_OPERATION_BY_ID`              | Retrieves detailed information about a specific operation within a project.                             |
| `NEON_UPDATE_PROJECT_SETTINGS_BY_ID`            | Updates the configuration and settings of a specific Neon project.                                      |
| `NEON_GET_PROJECT_BRANCHES`                     | Retrieves detailed information about a specific branch within a Neon project.                           |
| `NEON_DELETE_PROJECT_BY_ID`                     | Deletes a specific project from the Neon platform.                                                      |
| `NEON_DELETE_DATABASE_FROM_BRANCH`              | Deletes a specific database from a designated branch within a project.                                  |
| `NEON_RETRIEVE_BRANCH_ENDPOINTS`                | Retrieves a list of endpoints associated with a specific branch of a project.                           |
| `NEON_ADD_PROJECT_EMAIL_PERMISSION`             | Adds permissions for a specified email address to a particular project.                                 |
| `NEON_UPDATE_PROJECT_COMPUTE_ENDPOINT_SETTINGS` | Updates the configuration of a specific compute endpoint within a Neon project.                         |
| `NEON_RETRIEVE_VPC_ENDPOINTS_FOR_PROJECT`       | Retrieves a list of VPC endpoints associated with a specific project.                                   |
| `NEON_CREATE_BRANCH_DATABASE`                   | Creates a new database within a specified project and branch.                                           |
| `NEON_DELETE_ORGANIZATION_MEMBER`               | Removes a specific member from an organization.                                                         |
| `NEON_ADD_ROLE_TO_BRANCH`                       | Creates a new role within a specific branch of a project.                                               |
| `NEON_GET_PROJECT_BRANCH_ROLE`                  | Retrieves detailed information about a specific role within a particular branch of a Neon project.      |
| `NEON_CREATE_COMPUTE_ENDPOINT`                  | Creates a new compute endpoint for a specified branch within a Neon project.                            |
| `NEON_RETRIEVE_PROJECT_PERMISSIONS`             | Retrieves the current permission settings for a specific project.                                       |
| `NEON_GET_ORGANIZATION_API_KEYS`                | Retrieves a list of all API keys associated with a specific organization.                               |
| `NEON_MODIFY_BRANCH_DETAILS_IN_PROJECT`         | Updates the details of a specific branch within a project.                                              |
| `NEON_SET_BRANCH_AS_DEFAULT`                    | Sets a specified branch as the default branch for a given project.                                      |
| `NEON_CREATE_API_KEY_FOR_ORGANIZATION`          | Creates a new API key for the specified organization, with optional project-specific access.            |
| `NEON_START_ENDPOINT_FOR_PROJECT`               | Initiates a specific process or workflow associated with a particular endpoint within a project.        |
| `NEON_DELETE_PROJECT_BRANCH_ROLE`               | Deletes a specific role from a branch within a project.                                                 |
| `NEON_RESTORE_PROJECT_BRANCH`                   | Restores a branch to a specific state or point in time.                                                 |
| `NEON_PATCH_BRANCH_DATABASE_INFORMATION`        | Updates the properties of a specific database within a project branch.                                  |
| `NEON_CREATE_NEW_PROJECT_BRANCH`                | Creates a new branch in a Neon project with optional compute endpoints.                                 |
| `NEON_RESTART_PROJECT_ENDPOINT`                 | Restarts a specific endpoint within a project.                                                          |
| `NEON_DELETE_ORGANIZATION_API_KEY`              | Deletes a specific API key associated with an organization.                                             |
| `NEON_CREATE_PROJECT_WITH_QUOTA_AND_SETTINGS`   | Creates a new Neon project with specified configuration settings.                                       |
| `NEON_REVEAL_ROLE_PASSWORD_IN_BRANCH`           | Reveals the password for a specific role within a branch of a Neon project.                             |

To effectively use the wide array of Neon actions available through Composio, it's important to understand that **each action may require specific input parameters**. These parameters are essential for Composio to correctly execute the desired operation against your Neon account.

You can find detailed information about each action, including its required parameters and their descriptions under [Neon app in your Composio dashboard](https://app.composio.dev/app/neon)

**To utilize actions that require parameters, you simply need to include these parameters within the `description` of the task you assign to your CrewAI agent.** The agent will intelligently extract these parameters from the task description when it uses the Composio tool.

For instance, let's consider the `NEON_GET_PROJECT_CONNECTION_URI` action.

![Composio Neon Get Connection URI Action](/docs/guides/composio-neon-get-connection-uri-action.png)

This action needs the `project_id`, `database_name`, and `role_name` to retrieve the correct connection string. Here's how you would define a task to use this action, embedding the necessary parameters directly in the task description:

```python
get_connection_string_task = Task(
    description="Get the connection string for the Neon project with ID 'crimson-sea-41647396', for the database named 'neondb', using the role 'neondb_owner'.",
    agent=crewai_agent,
    expected_output="The Neon connection string.",
)
```

In this example, the task description clearly provides all the necessary information for the `NEON_GET_PROJECT_CONNECTION_URI` action. When the `crewai_agent` executes this task, it will understand from the description which action to use and what parameters are needed, making it seamless to interact with more complex Neon functionalities through your AI agents. Remember to tailor your task descriptions to accurately reflect the parameters needed for the specific Neon action you intend to use.

## Summary

In this guide, we've successfully built an AI agent capable of interacting with your Neon API using CrewAI and Composio. We covered the following steps:

- Setting up your development environment with Python and installing the necessary libraries (`crewai`, `composio-crewai`, `python-dotenv`).
- Configuring your API keys for Neon, Composio, and OpenAI.
- Creating the script to define your AI agent, establish a connection to Neon via Composio, and execute a task using the `NEON_GET_CURRENT_USER_INFORMATION` action.
- Running the example script and observing your AI agent successfully retrieve and display your Neon user information.
- Exploring the wide range of available Neon actions within the Composio toolset, understanding how to extend your AI agent's capabilities.

As a next step, consider expanding your AI agent's capabilities by utilizing more of the available Neon actions. Imagine automating project creation using `NEON_CREATE_PROJECT_WITH_QUOTA_AND_SETTINGS`, programmatically retrieving database connection URIs with `NEON_GET_PROJECT_CONNECTION_URI`, and then using a Postgres library of your choice to execute database queries. This opens the door to building sophisticated AI-driven workflows for database management, data analysis, and countless other applications tailored to your specific needs.

You can find the source code for the application described in this guide on GitHub.

<DetailIconCards>
    <a href="https://github.com/neondatabase-labs/composio-tool-example" description="CrewAI + Composio + Neon Example" icon="github">Building AI Agents with CrewAI, Composio, and Neon</a>
</DetailIconCards>

## Resources

- [CrewAI Documentation](https://docs.crewai.com/introduction)
- [Composio Documentation](https://docs.composio.dev)
- [Neon API Reference](https://api-docs.neon.tech/reference/getting-started-with-neon-api)
- [Neon API keys](/docs/manage/api-keys#creating-api-keys)

<NeedHelp/>


# Creating a Content Moderation System with Laravel, OpenAI API, and Neon Postgres

---
title: Creating a Content Moderation System with Laravel, OpenAI API, and Neon Postgres
subtitle: Build an automated content moderation system for your application using Laravel Livewire, OpenAI's moderation API, and Neon Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2025-03-22T00:00:00.000Z'
updatedOn: '2025-03-22T00:00:00.000Z'
---

Content moderation is essential for maintaining healthy online communities and platforms. In this guide, we'll create a content moderation system that uses OpenAI's moderation API to automatically analyze and flag potentially problematic content before it reaches your users.

We will use Laravel, OpenAI's moderation API, and Neon's serverless Postgres database and build a system that can handle content moderation for comments, forum posts, product reviews, or any user-generated content.

## What You'll Build

In this guide, you'll build a content moderation system with the following features:

1. A form for users to submit content to our Neon database
2. Automatic content analysis using [OpenAI's moderation API](https://platform.openai.com/docs/guides/moderation)
3. A moderation queue for reviewing flagged content
4. A dashboard for viewing moderation statistics
5. Settings management for different content types

## Prerequisites

To follow the steps in this guide, you will need:

- PHP 8.2 or higher
- [Composer](https://getcomposer.org/) installed
- A [Neon](https://console.neon.tech/signup) account
- An [OpenAI](https://platform.openai.com/signup) account with API access
- Basic familiarity with Laravel and PHP

## Create a Neon Project

Neon provides a serverless Postgres database that automatically scales as your application grows. Let's set up a Neon database for our content moderation system:

1. Navigate to the [Projects](https://console.neon.tech/app/projects) page in the Neon Console.
2. Click "New Project" and select your preferred settings.
3. Once your project is created, you'll see the connection details. Save the connection string for later use.

## Set up a Laravel Project

Now, let's create a new Laravel project and set it up to work with our Neon database:

```bash
composer create-project laravel/laravel moderation-system
cd moderation-system
```

This creates a new Laravel 11 project in a directory called `moderation-system` and moves you into that directory.

## Configure Environment Variables

To configure your Laravel application to connect to Neon Postgres and OpenAI, you need to set up your environment variables.

1. Open the `.env` file in your Laravel project directory.
2. Update your database configuration with the Neon connection details:

```
DB_CONNECTION=pgsql
DB_HOST=your-neon-hostname.neon.tech
DB_PORT=5432
DB_DATABASE=neondb
DB_USERNAME=your-username
DB_PASSWORD=your-password
DB_SSLMODE=require
```

3. Add your OpenAI API key:

```
OPENAI_API_KEY=your-openai-api-key
```

The `OPENAI_API_KEY` will be used by our moderation service to communicate with [OpenAI's moderation API](https://platform.openai.com/docs/guides/moderation).

## Install Livewire and Other Required Packages

Let's install the necessary packages for our project:

```bash
composer require livewire/livewire openai-php/laravel
```

This installs:

- [Livewire](https://livewire.laravel.com/): A Laravel package that makes building dynamic web apps simple, without writing JavaScript
- [OpenAI Laravel Client](https://github.com/openai-php/laravel): A library for interacting with OpenAI's API within Laravel

Next, let's install Laravel Breeze with Livewire for authentication and UI scaffolding:

```bash
composer require laravel/breeze --dev
php artisan breeze:install livewire
```

After installing Breeze, follow the instructions to complete the setup:

```bash
npm install
npm run build
```

This will install the necessary NPM packages and build your static assets.

Let's also run the migrations to create the default Laravel tables:

```bash
php artisan migrate
```

<Admonition type="important">
Neon supports both direct and pooled database connection strings, which can be copied from the **Connection Details** widget on your Neon Project Dashboard. A pooled connection string connects your application to the database via a PgBouncer connection pool, allowing for a higher number of concurrent connections. However, using a pooled connection string for migrations can be prone to errors. For this reason, we recommend using a direct (non-pooled) connection when performing migrations. For more information about direct and pooled connections, see [Connection pooling](/docs/connect/connection-pooling).
</Admonition>

## Create Database Schema

Now we'll create the database schema for our content moderation system. We need to track three main types of data:

1. Content items that need moderation
2. Moderation results from the OpenAI API
3. Moderation settings for different content types

Let's create the migrations:

```bash
php artisan make:migration create_content_items_table
php artisan make:migration create_moderation_results_table
php artisan make:migration create_moderation_settings_table
```

This will create three migration files in the `database/migrations` directory. Now, let's define the schema for each table:

### 1. Content Items Table

This table stores the actual content that needs moderation:

```php
// database/migrations/xxxx_xx_xx_create_content_items_table.php
public function up(): void
{
    Schema::create('content_items', function (Blueprint $table) {
        // Primary key
        $table->id();

        // Foreign key to the user who created the content (optional)
        $table->foreignId('user_id')->nullable()->constrained()->onDelete('set null');

        // Type of content (e.g., 'comment', 'post', 'review')
        $table->string('content_type');

        // The actual content text
        $table->text('content');

        // Current moderation status ('pending', 'approved', 'rejected')
        $table->string('status')->default('pending');

        // Created/updated timestamps
        $table->timestamps();
    });
}
```

### 2. Moderation Results Table

This table stores the results returned by the OpenAI moderation API:

```php
// database/migrations/xxxx_xx_xx_create_moderation_results_table.php
public function up(): void
{
    Schema::create('moderation_results', function (Blueprint $table) {
        // Primary key
        $table->id();

        // Foreign key to the content item being moderated
        $table->foreignId('content_item_id')->constrained()->onDelete('cascade');

        // Whether the content was flagged by the moderation API
        $table->boolean('flagged');

        // Categories that were flagged (stored as JSON)
        $table->json('categories')->nullable();

        // Scores for each category (stored as JSON)
        $table->json('category_scores')->nullable();

        // Highest confidence score among all categories
        $table->decimal('confidence', 8, 6)->nullable();

        // Created/updated timestamps
        $table->timestamps();
    });
}
```

### 3. Moderation Settings Table

This table stores moderation settings for different content types:

```php
// database/migrations/xxxx_xx_xx_create_moderation_settings_table.php
public function up(): void
{
    Schema::create('moderation_settings', function (Blueprint $table) {
        // Primary key
        $table->id();

        // Type of content these settings apply to
        $table->string('content_type');

        // Categories to flag (stored as JSON)
        $table->json('flagged_categories')->nullable();

        // Threshold for auto-rejection (0-1)
        $table->decimal('confidence_threshold', 8, 6)->default(0.5);

        // Whether to auto-approve content that passes moderation
        $table->boolean('auto_approve')->default(false);

        // Created/updated timestamps
        $table->timestamps();
    });
}
```

Now run the migrations to create the tables in your Neon database:

```bash
php artisan migrate
```

After completing your migrations, you can switch to a pooled connection for better performance in your application.

## Create Models

Now let's create the Eloquent models for our database tables. These models will help us interact with the database using Laravel's ORM:

```bash
php artisan make:model ContentItem
php artisan make:model ModerationResult
php artisan make:model ModerationSetting
```

This will create three model files in the `app/Models` directory. Let's define each model with their relationships and attributes:

### 1. ContentItem Model

```php
// app/Models/ContentItem.php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;
use Illuminate\Database\Eloquent\Relations\HasOne;
use Illuminate\Database\Eloquent\Relations\BelongsTo;

class ContentItem extends Model
{
    use HasFactory;

    /**
     * The attributes that are mass assignable.
     * These fields can be set when creating or updating a model.
     */
    protected $fillable = [
        'user_id',
        'content_type',
        'content',
        'status',
    ];

    /**
     * Get the moderation result associated with this content item.
     * This establishes a one-to-one relationship with ModerationResult.
     */
    public function moderationResult(): HasOne
    {
        return $this->hasOne(ModerationResult::class);
    }

    /**
     * Get the user who created this content item.
     * This establishes a many-to-one relationship with User.
     */
    public function user(): BelongsTo
    {
        return $this->belongsTo(User::class);
    }
}
```

In the `ContentItem` model, we define the `$fillable` array to specify which fields can be mass-assigned. We also define relationships with the `ModerationResult` and `User` models which will allow us to retrieve related data without writing complex SQL queries.

### 2. ModerationResult Model

```php
// app/Models/ModerationResult.php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;
use Illuminate\Database\Eloquent\Relations\BelongsTo;

class ModerationResult extends Model
{
    use HasFactory;

    /**
     * The attributes that are mass assignable.
     */
    protected $fillable = [
        'content_item_id',
        'flagged',
        'categories',
        'category_scores',
        'confidence',
    ];

    /**
     * The attributes that should be cast.
     * This tells Laravel how to handle special data types.
     */
    protected $casts = [
        'flagged' => 'boolean',       // Convert to PHP boolean
        'categories' => 'array',      // Convert JSON to PHP array
        'category_scores' => 'array', // Convert JSON to PHP array
        'confidence' => 'float',      // Convert to PHP float
    ];

    /**
     * Get the content item associated with this moderation result.
     */
    public function contentItem(): BelongsTo
    {
        return $this->belongsTo(ContentItem::class);
    }
}
```

Here again, we define the `$fillable` array to specify which fields can be mass-assigned. We also define a relationship with the `ContentItem` model to retrieve the content item associated with this moderation result.

### 3. ModerationSetting Model

```php
// app/Models/ModerationSetting.php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class ModerationSetting extends Model
{
    use HasFactory;

    /**
     * The attributes that are mass assignable.
     */
    protected $fillable = [
        'content_type',
        'flagged_categories',
        'confidence_threshold',
        'auto_approve',
    ];

    /**
     * The attributes that should be cast.
     */
    protected $casts = [
        'flagged_categories' => 'array',    // Convert JSON to PHP array
        'confidence_threshold' => 'float',  // Convert to PHP float
        'auto_approve' => 'boolean',        // Convert to PHP boolean
    ];
}
```

Similar to the other models, we define the structure of our data and the relationships between them. The `ModerationSetting` model will store the moderation settings for different content types.

## Build Moderation Service

Now, let's create a service class that will handle the content moderation logic. This service will use the OpenAI API to analyze content and store the results.

First, create a new directory for services:

```bash
mkdir -p app/Services
```

Now, create the moderation service file:

```php
// app/Services/ModerationService.php
<?php

namespace App\Services;

use App\Models\ContentItem;
use App\Models\ModerationResult;
use App\Models\ModerationSetting;
use OpenAI;
use Exception;
use Illuminate\Support\Facades\Log;

class ModerationService
{
    /**
     * The OpenAI client instance.
     */
    private $client;

    /**
     * Create a new ModerationService instance.
     */
    public function __construct()
    {
        // Initialize the OpenAI client with the API key from .env
        $this->client = OpenAI::client(env('OPENAI_API_KEY'));
    }

    /**
     * Moderate a content item using OpenAI's moderation API.
     *
     * @param ContentItem $contentItem The content item to moderate
     * @return ModerationResult The result of the moderation
     * @throws Exception If the moderation API request fails
     */
    public function moderateContent(ContentItem $contentItem)
    {
        try {
            // Get the content and settings
            $content = $contentItem->content;

            // Find or create settings for this content type
            $settings = ModerationSetting::where('content_type', $contentItem->content_type)->first();

            if (!$settings) {
                // Create default settings if none exist
                $settings = ModerationSetting::create([
                    'content_type' => $contentItem->content_type,
                    'flagged_categories' => null, // Consider all categories
                    'confidence_threshold' => 0.5, // Medium threshold
                    'auto_approve' => false, // Don't auto-approve
                ]);
            }

            // Call OpenAI moderation API
            $response = $this->client->moderations()->create([
                'input' => $content,
            ]);

            // Process response
            $result = $response->results[0];
            $flagged = $result->flagged;

            // Extract categories and scores
            $categories = [];
            $categoryScores = [];

            // Loop through each category in the response
            foreach ($result->categories as $key => $category) {
                $categoryScores[$key] = $category->score;

                if ($category->violated) {
                    $categories[] = $key;
                }
            }

            // Determine highest score as overall confidence
            $confidence = !empty($categoryScores) ? max($categoryScores) : 0;

            // Save moderation result to database
            $moderationResult = ModerationResult::create([
                'content_item_id' => $contentItem->id,
                'flagged' => $flagged,
                'categories' => $categories,
                'category_scores' => $categoryScores,
                'confidence' => $confidence,
            ]);

            // Auto-approve or auto-reject based on settings
            if (!$flagged && $settings->auto_approve) {
                // Content is clean and auto-approve is enabled
                $contentItem->update(['status' => 'approved']);
            } elseif ($flagged && $confidence >= $settings->confidence_threshold) {
                // Content is flagged with confidence above threshold
                $contentItem->update(['status' => 'rejected']);
            }

            return $moderationResult;
        } catch (Exception $e) {
            // Log the error and rethrow
            Log::error('Moderation API error: ' . $e->getMessage());
            throw $e;
        }
    }

    /**
     * Approve a content item.
     *
     * @param ContentItem $contentItem The content item to approve
     * @return bool Whether the update was successful
     */
    public function approveContent(ContentItem $contentItem)
    {
        return $contentItem->update(['status' => 'approved']);
    }

    /**
     * Reject a content item.
     *
     * @param ContentItem $contentItem The content item to reject
     * @return bool Whether the update was successful
     */
    public function rejectContent(ContentItem $contentItem)
    {
        return $contentItem->update(['status' => 'rejected']);
    }
}
```

There are a few key points that the `ModerationService` class does, let's break it down:

1. It initializes an OpenAI client using your API key.
2. The `moderateContent` method:
   - Finds or creates settings for the content type
   - Calls the OpenAI moderation API
   - Processes the response to extract flagged categories and scores
   - Saves the moderation result to the database
   - Auto-approves or auto-rejects content based on settings
3. It provides methods to manually approve or reject content.

A service provider in Laravel is a class that binds services to the Laravel service container. This allows us to use dependency injection to access the service in our controllers, models, or other classes.

Let's register this service in the Laravel service container by creating a new service provider:

```bash
php artisan make:provider ModerationServiceProvider
```

The new service provider will be created in the `app/Providers` directory.

Now, configure the service provider:

```php
// app/Providers/ModerationServiceProvider.php
<?php

namespace App\Providers;

use App\Services\ModerationService;
use Illuminate\Support\ServiceProvider;

class ModerationServiceProvider extends ServiceProvider
{
    /**
     * Register services.
     */
    public function register(): void
    {
        // Register the ModerationService as a singleton
        // This ensures we use the same instance throughout the application
        $this->app->singleton(ModerationService::class, function ($app) {
            return new ModerationService();
        });
    }

    /**
     * Bootstrap services.
     */
    public function boot(): void
    {
        //
    }
}
```

Add this new service provider to the providers array in `bootstrap/providers.php`:

```php
// bootstrap/providers.php
'providers' => [
    // ... other providers
    App\Providers\ModerationServiceProvider::class,
],
```

With the service provider in place along the models and migration files, we can now move to the next step of creating the Livewire components.

## Create Livewire Components

Now, let's create Livewire components for our content moderation system. Livewire allows us to create interactive UI components without writing JavaScript. We'll create a component for content submission, a moderation queue, and a dashboard for moderation statistics.

1. `ContentSubmission` component - for users to submit content
2. `ModerationQueue` component - for moderators to review content
3. `DashboardStats` component - to display moderation statistics

Let's create these components:

```bash
php artisan livewire:make ContentSubmission
php artisan livewire:make ModerationQueue
php artisan livewire:make DashboardStats
```

This will create three new Livewire components in the `app/Livewire` directory along with their corresponding views in the `resources/views/livewire` directory.

### 1. `ContentSubmission` Component

First, let's implement the component class:

```php
// app/Livewire/ContentSubmission.php
<?php

namespace App\Livewire;

use App\Models\ContentItem;
use App\Services\ModerationService;
use Illuminate\Support\Facades\Auth;
use Livewire\Component;

class ContentSubmission extends Component
{
    /**
     * The content entered by the user.
     */
    public $content;

    /**
     * The type of content being submitted.
     */
    public $contentType = 'comment';

    /**
     * Message to display after submission.
     */
    public $message = '';

    /**
     * Status of the submitted content.
     */
    public $status = '';

    /**
     * Validation rules for the form.
     */
    protected $rules = [
        'content' => 'required|string|min:5',
        'contentType' => 'required|string',
    ];

    /**
     * Handle form submission.
     */
    public function submitContent()
    {
        // Validate form input
        $this->validate();

        // Create content item in the database
        $contentItem = ContentItem::create([
            'user_id' => Auth::id(), // Current logged-in user
            'content_type' => $this->contentType,
            'content' => $this->content,
            'status' => 'pending', // Initial status is pending
        ]);

        // Moderate the content immediately
        try {
            // Get the moderation service from the container
            $moderationService = app(ModerationService::class);

            // Send the content to OpenAI for moderation
            $moderationService->moderateContent($contentItem);

            // Update the message based on moderation status
            $this->message = 'Content submitted for review';
            $this->status = $contentItem->status;

            if ($contentItem->status === 'approved') {
                $this->message = 'Content approved and published';
            } elseif ($contentItem->status === 'rejected') {
                $this->message = 'Content rejected due to policy violations';
            }
        } catch (\Exception $e) {
            // Handle moderation API errors
            $this->message = 'Content submitted for review, but moderation service is currently unavailable.';
        }

        // Clear form after submission
        $this->reset('content');
    }

    /**
     * Render the component.
     */
    public function render()
    {
        return view('livewire.content-submission');
    }
}
```

The `ContentSubmission` component class handles form submission, content validation, and moderation using the `ModerationService`. It also updates the message based on the moderation status.

Now, let's create the view for this component:

```php
<!-- resources/views/livewire/content-submission.blade.php -->
<div>
    <div class="p-6 bg-white rounded-lg shadow-md card">
        <h2 class="mb-4 text-xl font-semibold">Submit Content</h2>

        <form wire:submit="submitContent">
            <!-- Content Type Dropdown -->
            <div class="mb-4">
                <label for="contentType" class="block text-sm font-medium text-gray-700">Content Type</label>
                <select wire:model="contentType" id="contentType" class="block w-full mt-1 border-gray-300 rounded-md shadow-sm focus:border-indigo-300 focus:ring focus:ring-indigo-200 focus:ring-opacity-50">
                    <option value="comment">Comment</option>
                    <option value="forum_post">Forum Post</option>
                    <option value="review">Product Review</option>
                    <option value="code_snippet">Code Snippet</option>
                </select>
                @error('contentType') <span class="text-xs text-red-500">{{ $message }}</span> @enderror
            </div>

            <!-- Content Textarea -->
            <div class="mb-4">
                <label for="content" class="block text-sm font-medium text-gray-700">Content</label>
                <textarea wire:model="content" id="content" rows="4" class="block w-full mt-1 border-gray-300 rounded-md shadow-sm focus:border-indigo-300 focus:ring focus:ring-indigo-200 focus:ring-opacity-50"></textarea>
                @error('content') <span class="text-xs text-red-500">{{ $message }}</span> @enderror
            </div>

            <!-- Submit Button -->
            <div class="flex justify-end">
                <button type="submit" class="inline-flex items-center px-4 py-2 text-xs font-semibold tracking-widest text-white uppercase transition duration-150 ease-in-out bg-indigo-600 border border-transparent rounded-md hover:bg-indigo-700 active:bg-indigo-900 focus:outline-none focus:border-indigo-900 focus:ring ring-indigo-300 disabled:opacity-25">
                    Submit
                </button>
            </div>
        </form>

        <!-- Status Message -->
        @if ($message)
            <div class="mt-4 {{ $status === 'rejected' ? 'bg-red-100 text-red-700' : ($status === 'approved' ? 'bg-green-100 text-green-700' : 'bg-blue-100 text-blue-700') }} p-3 rounded">
                {{ $message }}
            </div>
        @endif
    </div>
</div>
```

The view contains a form for users to submit content, including a dropdown for selecting the content type and a textarea for entering the content. The form submission is handled by the `submitContent` method in the component class.

### 2. `ModerationQueue` Component

Now, let's implement the moderation queue component class:

```php
// app/Livewire/ModerationQueue.php
<?php

namespace App\Livewire;

use App\Models\ContentItem;
use App\Services\ModerationService;
use Livewire\Component;
use Livewire\WithPagination;

class ModerationQueue extends Component
{
    // Use Laravel's pagination with Livewire
    use WithPagination;

    // Use Tailwind CSS for pagination styling
    protected $paginationTheme = 'tailwind';

    /**
     * Current filter for content status.
     */
    public $statusFilter = 'pending';

    /**
     * Initialize the component.
     */
    public function mount()
    {
        // Check if user has permissions to view this page
        $this->authorize('viewModeration');
    }

    /**
     * Approve a content item.
     *
     * @param int $id The ID of the content item
     */
    public function approve($id)
    {
        $contentItem = ContentItem::findOrFail($id);
        app(ModerationService::class)->approveContent($contentItem);

        // Notify other components that content was moderated
        $this->dispatch('content-moderated');
    }

    /**
     * Reject a content item.
     *
     * @param int $id The ID of the content item
     */
    public function reject($id)
    {
        $contentItem = ContentItem::findOrFail($id);
        app(ModerationService::class)->rejectContent($contentItem);

        // Notify other components that content was moderated
        $this->dispatch('content-moderated');
    }

    /**
     * Moderate a content item using OpenAI.
     *
     * @param int $id The ID of the content item
     */
    public function moderate($id)
    {
        $contentItem = ContentItem::findOrFail($id);
        app(ModerationService::class)->moderateContent($contentItem);

        // Notify other components that content was moderated
        $this->dispatch('content-moderated');
    }

    /**
     * Filter content items by status.
     *
     * @param string $status The status to filter by
     */
    public function filterByStatus($status)
    {
        $this->statusFilter = $status;

        // Reset pagination when filter changes
        $this->resetPage();
    }

    /**
     * Render the component.
     */
    public function render()
    {
        // Build the query for content items
        $query = ContentItem::query()->with(['moderationResult', 'user']);

        // Apply status filter if not 'all'
        if ($this->statusFilter !== 'all') {
            $query->where('status', $this->statusFilter);
        }

        // Get paginated results
        $contentItems = $query->latest()->paginate(10);

        return view('livewire.moderation-queue', [
            'contentItems' => $contentItems
        ]);
    }
}
```

Here we define methods for approving, rejecting, and moderating content items. The `filterByStatus` method allows us to filter content items by status. The `render` method builds the query based on the status filter and paginates the results.

And the view for the moderation queue:

```php
<!-- resources/views/livewire/moderation-queue.blade.php -->
<div>
    <!-- Status Filter Buttons -->
    <div class="flex mb-4 space-x-2">
        <button wire:click="filterByStatus('pending')" class="px-4 py-2 rounded-md {{ $statusFilter === 'pending' ? 'bg-blue-600 text-white' : 'bg-gray-200 text-gray-700' }}">
            Pending
        </button>
        <button wire:click="filterByStatus('approved')" class="px-4 py-2 rounded-md {{ $statusFilter === 'approved' ? 'bg-blue-600 text-white' : 'bg-gray-200 text-gray-700' }}">
            Approved
        </button>
        <button wire:click="filterByStatus('rejected')" class="px-4 py-2 rounded-md {{ $statusFilter === 'rejected' ? 'bg-blue-600 text-white' : 'bg-gray-200 text-gray-700' }}">
            Rejected
        </button>
        <button wire:click="filterByStatus('all')" class="px-4 py-2 rounded-md {{ $statusFilter === 'all' ? 'bg-blue-600 text-white' : 'bg-gray-200 text-gray-700' }}">
            All
        </button>
    </div>

    <!-- Content Items Table -->
    <div class="overflow-x-auto bg-white rounded-lg shadow-md">
        <table class="min-w-full divide-y divide-gray-200">
            <thead class="bg-gray-50">
                <tr>
                    <th scope="col" class="px-6 py-3 text-xs font-medium tracking-wider text-left text-gray-500 uppercase">ID</th>
                    <th scope="col" class="px-6 py-3 text-xs font-medium tracking-wider text-left text-gray-500 uppercase">Type</th>
                    <th scope="col" class="px-6 py-3 text-xs font-medium tracking-wider text-left text-gray-500 uppercase">Content</th>
                    <th scope="col" class="px-6 py-3 text-xs font-medium tracking-wider text-left text-gray-500 uppercase">Status</th>
                    <th scope="col" class="px-6 py-3 text-xs font-medium tracking-wider text-left text-gray-500 uppercase">Flags</th>
                    <th scope="col" class="px-6 py-3 text-xs font-medium tracking-wider text-left text-gray-500 uppercase">Actions</th>
                </tr>
            </thead>
            <tbody class="bg-white divide-y divide-gray-200">
                @forelse ($contentItems as $item)
                    <tr>
                        <td class="px-6 py-4 text-sm font-medium text-gray-900 whitespace-nowrap">{{ $item->id }}</td>
                        <td class="px-6 py-4 text-sm text-gray-500 whitespace-nowrap">{{ $item->content_type }}</td>
                        <td class="max-w-md px-6 py-4 text-sm text-gray-500">
                            <div class="overflow-y-auto max-h-20">
                                {{ $item->content }}
                            </div>
                        </td>
                        <td class="px-6 py-4 text-sm text-gray-500 whitespace-nowrap">
                            @if ($item->status === 'pending')
                                <span class="inline-flex px-2 text-xs font-semibold leading-5 text-yellow-800 bg-yellow-100 rounded-full">
                                    Pending
                                </span>
                            @elseif ($item->status === 'approved')
                                <span class="inline-flex px-2 text-xs font-semibold leading-5 text-green-800 bg-green-100 rounded-full">
                                    Approved
                                </span>
                            @elseif ($item->status === 'rejected')
                                <span class="inline-flex px-2 text-xs font-semibold leading-5 text-red-800 bg-red-100 rounded-full">
                                    Rejected
                                </span>
                            @endif
                        </td>
                        <td class="px-6 py-4 text-sm text-gray-500 whitespace-nowrap">
                            @if ($item->moderationResult)
                                @if ($item->moderationResult->flagged)
                                    <span class="inline-flex px-2 text-xs font-semibold leading-5 text-red-800 bg-red-100 rounded-full">
                                        Flagged
                                    </span>
                                    <div class="mt-1 text-xs">
                                        @foreach ($item->moderationResult->categories as $category)
                                            <span class="px-1 mr-1 text-red-700 rounded bg-red-50">{{ $category }}</span>
                                        @endforeach
                                    </div>
                                @else
                                    <span class="inline-flex px-2 text-xs font-semibold leading-5 text-green-800 bg-green-100 rounded-full">
                                        Clean
                                    </span>
                                @endif
                            @else
                                <span class="inline-flex px-2 text-xs font-semibold leading-5 text-gray-800 bg-gray-100 rounded-full">
                                    Not Checked
                                </span>
                            @endif
                        </td>
                        <td class="px-6 py-4 text-sm font-medium text-right whitespace-nowrap">
                            <div class="flex space-x-2">
                                @if ($item->status !== 'approved')
                                    <button wire:click="approve({{ $item->id }})" class="text-green-600 hover:text-green-900">Approve</button>
                                @endif

                                @if ($item->status !== 'rejected')
                                    <button wire:click="reject({{ $item->id }})" class="text-red-600 hover:text-red-900">Reject</button>
                                @endif

                                @if (!$item->moderationResult)
                                    <button wire:click="moderate({{ $item->id }})" class="text-blue-600 hover:text-blue-900">Check</button>
                                @endif
                            </div>
                        </td>
                    </tr>
                @empty
                    <tr>
                        <td colspan="6" class="px-6 py-4 text-sm text-center text-gray-500">No content items found</td>
                    </tr>
                @endforelse
            </tbody>
        </table>
    </div>

    <!-- Pagination Links -->
    <div class="mt-4">
        {{ $contentItems->links() }}
    </div>
</div>
```

The view displays a table of content items with columns for ID, type, content, status, flags, and actions. It also includes buttons to approve, reject, or moderate content items.

### 3. `DashboardStats` Component

Finally, let's implement the dashboard stats component:

```php
// app/Livewire/DashboardStats.php
<?php

namespace App\Livewire;

use App\Models\ContentItem;
use Livewire\Component;

class DashboardStats extends Component
{
    /**
     * Number of pending content items.
     */
    public $pendingCount;

    /**
     * Number of approved content items.
     */
    public $approvedCount;

    /**
     * Number of rejected content items.
     */
    public $rejectedCount;

    /**
     * Number of flagged content items.
     */
    public $flaggedCount;

    /**
     * Initialize the component.
     */
    public function mount()
    {
        // Check if user has permissions to view this page
        $this->authorize('viewModeration');

        // Load initial statistics
        $this->loadStats();
    }

    /**
     * Load moderation statistics from the database.
     */
    public function loadStats()
    {
        // Count items by status
        $this->pendingCount = ContentItem::where('status', 'pending')->count();
        $this->approvedCount = ContentItem::where('status', 'approved')->count();
        $this->rejectedCount = ContentItem::where('status', 'rejected')->count();

        // Count items that were flagged by the moderation API
        $this->flaggedCount = ContentItem::whereHas('moderationResult', function($query) {
            $query->where('flagged', true);
        })->count();
    }

    /**
     * Render the component.
     */
    public function render()
    {
        return view('livewire.dashboard-stats');
    }
}
```

The `DashboardStats` component class loads moderation statistics from the database and displays them in the view. The `loadStats` method counts content items by status and flags. The `render` method renders the component view.

And the view for the dashboard stats:

```php
<!-- resources/views/livewire/dashboard-stats.blade.php -->
<div>
    <!-- Stats Cards Grid -->
    <div class="grid grid-cols-1 gap-4 mb-6 md:grid-cols-2 lg:grid-cols-4">
        <!-- Pending Items -->
        <div class="p-6 bg-white rounded-lg shadow-md">
            <div class="flex items-center">
                <div class="p-3 mr-4 text-yellow-500 bg-yellow-100 rounded-full">
                    <svg xmlns="http://www.w3.org/2000/svg" class="w-6 h-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z" />
                    </svg>
                </div>
                <div>
                    <p class="text-sm font-medium text-gray-500">Pending Review</p>
                    <p class="text-xl font-semibold text-gray-700">{{ $pendingCount }}</p>
                </div>
            </div>
        </div>

        <!-- Approved Items -->
        <div class="p-6 bg-white rounded-lg shadow-md">
            <div class="flex items-center">
                <div class="p-3 mr-4 text-green-500 bg-green-100 rounded-full">
                    <svg xmlns="http://www.w3.org/2000/svg" class="w-6 h-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7" />
                    </svg>
                </div>
                <div>
                    <p class="text-sm font-medium text-gray-500">Approved</p>
                    <p class="text-xl font-semibold text-gray-700">{{ $approvedCount }}</p>
                </div>
            </div>
        </div>

        <!-- Rejected Items -->
        <div class="p-6 bg-white rounded-lg shadow-md">
            <div class="flex items-center">
                <div class="p-3 mr-4 text-red-500 bg-red-100 rounded-full">
                    <svg xmlns="http://www.w3.org/2000/svg" class="w-6 h-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12" />
                    </svg>
                </div>
                <div>
                    <p class="text-sm font-medium text-gray-500">Rejected</p>
                    <p class="text-xl font-semibold text-gray-700">{{ $rejectedCount }}</p>
                </div>
            </div>
        </div>

        <!-- Flagged Items -->
        <div class="p-6 bg-white rounded-lg shadow-md">
            <div class="flex items-center">
                <div class="p-3 mr-4 text-orange-500 bg-orange-100 rounded-full">
                    <svg xmlns="http://www.w3.org/2000/svg" class="w-6 h-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 9v2m0 4h.01m-6.938 4h13.856c1.54 0 2.502-1.667 1.732-3L13.732 4c-.77-1.333-2.694-1.333-3.464 0L3.34 16c-.77 1.333.192 3 1.732 3z" />
                    </svg>
                </div>
                <div>
                    <p class="text-sm font-medium text-gray-500">Flagged</p>
                    <p class="text-xl font-semibold text-gray-700">{{ $flaggedCount }}</p>
                </div>
            </div>
        </div>
    </div>

    <!-- Link to Moderation Queue -->
    <div class="flex justify-center">
        <a href="{{ route('admin.moderation-queue') }}" class="inline-flex items-center px-4 py-2 text-xs font-semibold tracking-widest text-white uppercase bg-indigo-600 border border-transparent rounded-md hover:bg-indigo-700">
            View Moderation Queue
        </a>
    </div>
</div>
```

The view displays statistics for pending, approved, rejected, and flagged content items in a grid layout. It also includes a link to the moderation queue.

## Set Up Routes

Now let's define the routes for our admin dashboard and content submission page. We'll use Laravel's route middleware to protect the admin routes with the `viewModeration` gate. You can learn more about Laravel's authorization gates in the [Laravel authorization guide here](/guides/laravel-authorization).

```php
// routes/web.php
<?php

use App\Livewire\ContentSubmission;
use App\Livewire\ModerationQueue;
use App\Livewire\DashboardStats;
use Illuminate\Support\Facades\Route;

// Routes that require authentication
Route::middleware(['auth'])->group(function () {
    // User content submission
    Route::get('/submit', ContentSubmission::class)->name('content.submit');

    // Admin routes
    Route::middleware(['can:viewModeration'])->prefix('admin')->group(function () {
        Route::get('/', DashboardStats::class)->name('admin.dashboard');
        Route::get('/moderation-queue', ModerationQueue::class)->name('admin.moderation-queue');
    });
});

// Existing routes
```

These routes define:

1. A public homepage
2. A route for authenticated users to submit content
3. Admin routes for the dashboard and moderation queue, protected by the `viewModeration` gate

Now, let's define the authorization gates in the `AppServiceProvider`. This gate will determine who can access the moderation dashboard:

```php
// app/Providers/AppServiceProvider.php
<?php

namespace App\Providers;

use Illuminate\Support\Facades\Gate;
use Illuminate\Support\ServiceProvider;

class AppServiceProvider extends ServiceProvider
{
    /**
     * Register any application services.
     */
    public function register(): void
    {
        //
    }

    /**
     * Bootstrap any application services.
     */
    public function boot(): void
    {
        // Define who can view moderation pages
        // For simplicity, we're allowing user ID 1 or any user with is_admin=true
        Gate::define('viewModeration', function ($user) {
            return $user->id === 1 || $user->is_admin;
        });
    }
}
```

This `AppServiceProvider` defines who can access the moderation dashboard. In a real application, you would want to implement more sophisticated access control logic. For more information on Laravel authorization, check out the [official documentation](https://laravel.com/docs/8.x/authorization) and the [Laravel authorization guide](/guides/laravel-authorization).

## Create Admin Dashboard

Let's create a layout for our admin dashboard. First, create an admin layout file:

```php
<!-- resources/views/layouts/admin.blade.php -->
<!DOCTYPE html>
<html lang="{{ str_replace('_', '-', app()->getLocale()) }}">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="csrf-token" content="{{ csrf_token() }}">

    <title>{{ config('app.name', 'Laravel') }} - Admin</title>

    <!-- Scripts and Styles -->
    @vite(['resources/css/app.css', 'resources/js/app.js'])

    <!-- Livewire Styles -->
    @livewireStyles
</head>
<body class="font-sans antialiased bg-gray-100">
    <div class="min-h-screen">
        <!-- Navigation -->
        <nav class="p-4 text-white bg-gray-800">
            <div class="container flex items-center justify-between mx-auto">
                <div class="flex items-center space-x-8">
                    <a href="{{ route('admin.dashboard') }}" class="text-xl font-bold">Content Moderation</a>

                    <div class="hidden space-x-4 md:flex">
                        <a href="{{ route('admin.dashboard') }}" class="px-3 py-2 rounded hover:bg-gray-700 {{ request()->routeIs('admin.dashboard') ? 'bg-gray-700' : '' }}">Dashboard</a>
                        <a href="{{ route('admin.moderation-queue') }}" class="px-3 py-2 rounded hover:bg-gray-700 {{ request()->routeIs('admin.moderation-queue') ? 'bg-gray-700' : '' }}">Moderation Queue</a>
                    </div>
                </div>

                <div class="flex items-center space-x-4">
                    <div class="text-sm text-gray-400">{{ Auth::user()->name }}</div>

                    <form method="POST" action="{{ route('logout') }}">
                        @csrf
                        <button type="submit" class="text-sm text-gray-400 hover:text-white">
                            Log Out
                        </button>
                    </form>
                </div>
            </div>
        </nav>

        <!-- Page Content -->
        <main class="container px-4 py-6 mx-auto">
            <div class="mb-6">
                <h1 class="text-2xl font-bold text-gray-800">@yield('title', 'Admin Dashboard')</h1>
            </div>

            {{ $slot }}
        </main>
    </div>

    <!-- Livewire Scripts -->
    @livewireScripts
</body>
</html>
```

Now, let's create a content submission page that users can access:

```php
<!-- resources/views/content/submit.blade.php -->
<x-app-layout>
    <div class="py-12">
        <div class="mx-auto max-w-7xl sm:px-6 lg:px-8">
            <div class="overflow-hidden bg-white shadow-sm sm:rounded-lg">
                <div class="p-6 text-gray-900">
                    <h1 class="mb-6 text-2xl font-bold">Submit Content</h1>
                    <livewire:content-submission />
                </div>
            </div>
        </div>
    </div>
</x-app-layout>
```

Here, we're using the `x-app-layout` component to wrap the content submission form. This layout includes the navigation bar and other common elements. The `content-submission` Livewire component is embedded in this view using the `livewire:content-submission` directive.

After making all the views and components, you should run the following command to compile the assets:

```bash
npm run dev
```

This will compile the assets and make them available for your application. You can now test the content submission page by visiting the `/submit` route and submitting some content, and then check the moderation queue at `/admin/moderation-queue` and the stats dashboard at `/admin`.

## Test the System

With everything set up, you can now test the moderation system.

Let's create a basic command to test our moderation system. This will help you check if everything is working correctly:

```bash
php artisan make:command TestModeration
```

Implement the command:

```php
// app/Console/Commands/TestModeration.php
<?php

namespace App\Console\Commands;

use App\Models\ContentItem;
use App\Services\ModerationService;
use Illuminate\Console\Command;

class TestModeration extends Command
{
    /**
     * The name and signature of the console command.
     *
     * @var string
     */
    protected $signature = 'test:moderation {content} {--type=comment}';

    /**
     * The console command description.
     *
     * @var string
     */
    protected $description = 'Test the moderation system with a sample content';

    /**
     * Execute the console command.
     */
    public function handle(ModerationService $moderationService)
    {
        // Get the content and content type from the command arguments
        $content = $this->argument('content');
        $type = $this->option('type');

        $this->info("Testing moderation on content: '$content'");

        // Create a test content item
        $contentItem = ContentItem::create([
            'content_type' => $type,
            'content' => $content,
            'status' => 'pending',
        ]);

        try {
            // Moderate the content using OpenAI
            $result = $moderationService->moderateContent($contentItem);

            // Display the results
            $this->info("Moderation completed for content ID: {$contentItem->id}");
            $this->info("Status: {$contentItem->status}");
            $this->info("Flagged: " . ($result->flagged ? 'Yes' : 'No'));

            if ($result->categories) {
                $this->info("Flagged categories: " . implode(', ', $result->categories));
            }

            if ($result->category_scores) {
                $this->info("Category scores:");
                foreach ($result->category_scores as $category => $score) {
                    $this->info("  - $category: $score");
                }
            }

            $this->info("Confidence: {$result->confidence}");
        } catch (\Exception $e) {
            $this->error("Moderation failed: {$e->getMessage()}");
        }

        return Command::SUCCESS;
    }
}
```

With the above command we are doing the following:

- Accepting the content and content type as command arguments
- Creating a test content item in the database
- Moderating the content using the `ModerationService`
- Displaying the moderation results where we loop through the categories and scores

You can test the moderation system with this command:

- Test with harmless content:

  ```bash
  php artisan test:moderation "This is a friendly comment about your tech blog."
  ```

  Output:

  ```
  Testing moderation on content: 'This is a friendly comment about your tech blog.'
  Moderation completed for content ID: 4
  Status: pending
  Flagged: No
  Category scores:
      - hate: 2.4322733338522E-7
      - hate/threatening: 3.4327216069663E-10
      - harassment: 4.3629752326524E-6
      - harassment/threatening: 4.6775343776062E-7
      - self-harm: 6.9364503474389E-8
      - self-harm/intent: 1.178076942665E-7
      - self-harm/instructions: 4.2720111892436E-9
      - sexual: 0.00012831370986532
      - sexual/minors: 2.4699570531084E-6
      - violence: 2.0182131265756E-5
      - violence/graphic: 8.1019070421462E-6
  Confidence: 0.00012831370986532
  ```

* Test with potentially problematic content:

  ```bash
  php artisan test:moderation "I hate this product. It's the worst thing I've ever used."
  ```

* Test with obviously harmful content:

  ```
  php artisan test:moderation "I'm going to harm you."
  ```

  With this you will get a status of `rejected` and the content will be flagged as `violence` and `threatening`.

## How the System Works

Let's walk through how the content moderation system works in practice:

1. Content Submission:
   - A user submits content through the `ContentSubmission` component
   - The content is saved to the database with status "pending"
   - The `ModerationService` immediately sends the content to OpenAI's moderation API

2. AI Moderation:
   - OpenAI analyzes the content and returns categories, scores, and a flagged status
   - The `ModerationService` saves these results to the `ModerationResult` table in our Neon Postgres database
   - Based on settings, content may be auto-approved or auto-rejected

3. Manual Review:
   - Content that isn't auto-approved or auto-rejected stays in the "pending" state
   - Moderators use the `ModerationQueue` component to review pending content
   - They can see which categories were flagged and why
   - Moderators can manually approve or reject content

4. Dashboard Statistics:
   - The `DashboardStats` component shows counts of pending, approved, rejected, and flagged content
   - This helps moderators monitor the system's overall health

## Conclusion

In this guide, we've built a content moderation system using Laravel, Livewire, OpenAI, and Neon Postgres. This system can:

- Accept user-generated content and automatically analyze it for harmful content
- Store moderation results in Neon with detailed information about flagged categories
- Provide different moderation settings for different content types
- Offer an interactive admin dashboard for manual review of flagged content

This moderation system can be integrated into various applications, from forums and social networks to review platforms and comment systems.

As a next step, you can use Laravel queues to process moderation tasks asynchronously, improving performance and scalability. You can check out the [Laravel queues guide](/guides/laravel-queue-workers-job-processing) for more information.

<NeedHelp />


# Getting started with Convex and Neon

---
title: Getting started with Convex and Neon
subtitle: A step-by-step guide to integrating Convex with Neon Postgres
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-02-14T00:00:00.000Z'
updatedOn: '2025-02-14T00:00:00.000Z'
---

This guide explores Convex's self-hosting capability and demonstrates how to use it with Neon Postgres. [Convex](https://www.convex.dev) is a reactive backend platform ideal for building real-time applications. A [recent release](https://news.convex.dev/self-hosting) significantly enhances the self-hosted experience, overcoming limitations of the initial open-source version which lacked a dashboard and relied solely on SQLite. The new self-hosted Convex includes the [dashboard](https://docs.convex.dev/dashboard) and supports Postgres as a robust and scalable database option.

Convex empowers developers to create dynamic, live-updating applications. Self-hosting retains these core features while granting you greater control over your deployment environment. While SQLite remains the default for simplicity, Postgres integration unlocks enhanced scalability and resilience, especially beneficial for production applications.

This guide provides a step-by-step walkthrough of integrating Convex with Neon Postgres. You will learn how to:

- Set up Convex for self-hosting using Docker Compose.
- Configure Convex to utilize Neon Postgres for persistent data storage.
- Run the Convex [chat application tutorial](https://docs.convex.dev/tutorial) as a practical example.
- Test the integration to ensure everything functions correctly.

## Prerequisites

Before you begin, ensure you have the following prerequisites installed and configured:

- **Neon Account:** Sign up for a free [Neon account](https://console.neon.tech/signup) if you don't have one already. Neon will provide a managed Postgres database for your Convex chat application.
- **Docker:** Docker is essential for running the Convex backend and dashboard locally. If Docker is not installed, download and install Docker Desktop from [docker.com](https://www.docker.com/get-started). Make sure Docker is running before proceeding.
- **Node.js v18+:** Node.js (version 18 or higher) is required to run the Convex chat application example. Download and install it from [nodejs.org](https://nodejs.org).

<Admonition type="note" title="Database Location and Latency Considerations">

    Remember that the physical distance between your Neon database and your self-hosted Convex backend can impact your application's performance due to latency.  Increased distance generally means higher latency and potentially slower response times.

    For optimal performance, especially in production, it's highly recommended to locate your Neon database and Convex backend in the same geographical region. Convex's cloud-hosted platform achieves extremely low query times because the database and backend are co-located within their infrastructure.

    While this guide focuses on setup and integration specifically for local development, for production applications, consider the physical proximity of your Neon Postgres and Convex Backend server to minimize latency.

</Admonition>

## Setting up Neon Database

To get started with your Postgres database, create a new Neon project using [pg.new](https://pg.new). This project will provide the Postgres instance that Convex will use to store your application data. Within this Neon project, you'll need to create a database named `convex_self_hosted` – this is the specific database Convex is configured to use for storing chat messages. Follow these steps to set up your Neon Postgres database:

- Navigate to the [SQL Editor](/docs/get-started/query-with-neon-sql-editor) in your Neon project console to create the `convex_self_hosted` database.
- Execute the following SQL command to create the database:

  ```sql
  CREATE DATABASE convex_self_hosted;
  ```

- Once the database is created, you can retrieve the connection string by clicking on "Connect" in the Neon project's dashboard. Select the `convex_self_hosted` database and copy the connection string. You will need this connection string later to configure the Convex backend to use Neon Postgres.

  ![Neon Connection string for convex_self_hosted database](/docs/guides/neon-connection-string-for-convex-database.png)

## Setting up Self-Hosted Convex with Docker Compose

Now, you'll set up the self-hosted Convex backend using Docker Compose, configuring it to use your Neon Postgres database.

1.  **Create a Project Directory:** Open your terminal and create a new directory for your Convex project. Navigate into it:

    ```bash
    mkdir convex-neon-integration
    cd convex-neon-integration
    ```

2.  **Download Docker Compose Configuration:** Download the default `docker-compose.yml` file provided by Convex directly into your project directory:

    ```bash
    npx degit get-convex/convex-backend/self-hosted/docker/docker-compose.yml docker-compose.yml
    ```

    This command uses [`npx degit`](https://www.npmjs.com/package/degit) to fetch the `docker-compose.yml` file from the [Convex GitHub repository](https://github.com/get-convex/convex-backend/blob/main/self-hosted/docker/docker-compose.yml).

3.  **Set up Neon connection string:** Add your Neon connection string you copied earlier to a `.env` file to configure Convex.
    1.  Create a `.env` file in the same directory as `docker-compose.yml`.
    1.  Add this line:
        ```env
        POSTGRES_URL=[YOUR_NEON_CONNECTION_STRING]
        ```
    1.  Modify `[YOUR_NEON_CONNECTION_STRING]` for Convex:

        Convex requires a specific connection string format for Neon:

        `postgres://username:password@hostname`

        Remove the database name and extra parameters from your Neon connection string.

        **Neon default:**

        ```bash
        postgresql://neondb_owner:password@ep-xxxxx.aws.neon.tech/convex_self_hosted?sslmode=require&channel_binding=require
        ```

        **For Convex:**

        ```bash
        postgres://neondb_owner:password@ep-xxxxx.aws.neon.tech
        ```

4.  **Start Convex services with Docker Compose:** With the configuration in place, start the Convex backend and dashboard services using Docker Compose. Execute the following command in your terminal within the `convex-neon-integration` directory:

    ```bash
    docker compose up -d
    ```

    The `-d` flag runs the containers in detached mode (in the background). Docker Compose will download the necessary images, create containers, and start the Convex services.

5.  **Access the Convex Dashboard:** Once `docker compose up -d` completes, the Convex dashboard should be accessible in your browser at [http://localhost:6791](http://localhost:6791). It might take a few moments for the services to fully start. If it's not immediately available, wait a short time and refresh the page.

    You should see the Convex dashboard login screen:

    ![Convex Dashboard](/docs/guides/convex-dashboard.png)

    **Login to the Convex Dashboard:**
    - When you access the dashboard for the first time, you will be prompted to log in.
    - For the password, you will use the `CONVEX_SELF_HOSTED_ADMIN_KEY` generated in the next step.

6.  **Verify Neon Postgres Connection (Optional but Recommended):** You can confirm that Convex is using your Neon Postgres database by checking the Docker container logs. This verifies that the `POSTGRES_URL` environment variable was correctly processed.

    Run this command in your terminal within the `convex-neon-integration` directory:

    ```bash
    docker compose logs -f
    ```

    Examine the logs for messages indicating a successful connection to a Postgres database, similar to the example below:

    ![Convex Postgres Logs](/docs/guides/convex-postgres-logs.png)

7.  **Retrieve the Admin Key:** You need the `CONVEX_SELF_HOSTED_ADMIN_KEY` to log into the Convex dashboard and configure your chat application. Execute this command to retrieve it:

    ```bash
    docker compose exec backend ./generate_admin_key.sh
    ```

    The output will display the generated admin key. **Copy this key carefully.** You'll need it in the next steps to log in to the dashboard and configure the chat application.

    ```
    convex-self-hosted|01xxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
    ```

    **Use this admin key as the password when logging into the Convex dashboard at http://localhost:6791**

## Setting up the Convex chat application example

With the self-hosted Convex backend powered by Neon running, the next step is to set up the Convex chat application example to connect to this backend and complete the chat functionality as described in the Convex tutorial.

1.  **Clone the Convex tutorial repository:** Open a new terminal window, ensuring the Docker Compose process from the previous step remains running. Clone the Convex tutorial repository to your local machine. This repository contains the source code for the chat application example.

    ```bash
    git clone https://github.com/get-convex/convex-tutorial.git
    cd convex-tutorial
    ```

2.  **Install application dependencies:** Navigate into the `convex-tutorial` directory and install the required npm packages. This step downloads all necessary JavaScript libraries and dependencies for the chat application.

    ```bash
    npm install
    ```

3.  **Update Convex library version:** It is required to update the `convex` npm package to the latest version within the `convex-tutorial` project. This is needed as the existing version present in the tutorial repository is not the latest and will cause issues with the self-hosted Convex backend.

    ```bash
    npm install convex@latest
    ```

4.  **Configure environment variables for chat app:** To connect the chat application to your self-hosted Convex backend, you need to configure specific environment variables. Create a `.env.local` file in the root of the `convex-tutorial` directory. Add the following variables to this file:

    ```env
    VITE_CONVEX_URL=http://localhost:3210
    CONVEX_SELF_HOSTED_URL='http://localhost:3210'
    CONVEX_SELF_HOSTED_ADMIN_KEY='<your_generated_admin_key>'
    ```

    - `VITE_CONVEX_URL`: Specifies the URL of your self-hosted Convex backend. In this case, it's set to `http://localhost:3210`, the default for local Convex backends.
    - `CONVEX_SELF_HOSTED_URL`: Also set to the same URL, `http://localhost:3210`.
    - `CONVEX_SELF_HOSTED_ADMIN_KEY`: This key is essential for authenticating development operations against your self-hosted Convex instance. Replace `<your_generated_admin_key>` with the admin key you generated in the previous step [Setting up self-hosted Convex with Docker Compose](#setting-up-self-hosted-convex-with-docker-compose).

5.  **Initialize Convex project:** Run the following command to initialize the Convex project and generate the necessary TypeScript files for the chat application:

    ```bash
    npm run predev
    ```

    This starts the Convex server and generates the necessary TypeScript files for the chat application.

6.  **Implement the `sendMessage` Mutation:** Following the [Convex tutorial - Your first mutation](https://docs.convex.dev/tutorial/#your-first-mutation) section, create a new file `convex/chat.ts` in your `convex-tutorial` project. Add the following code to this file. This code defines a Convex mutation function to insert new messages into the database:

    ```typescript
    // convex/chat.ts
    import { mutation } from './_generated/server';
    import { v } from 'convex/values';

    export const sendMessage = mutation({
      args: {
        user: v.string(),
        body: v.string(),
      },
      handler: async (ctx, args) => {
        console.log('This TypeScript function is running on the server.');
        await ctx.db.insert('messages', {
          user: args.user,
          body: args.body,
        });
      },
    });
    ```

7.  **Update `src/App.tsx` to use `sendMessage` mutation:** Now, update the `src/App.tsx` file. Modify the `src/App.tsx` file to include the `useMutation` hook and call the `sendMessage` mutation when a user submits a message. Replace the relevant section in `src/App.tsx` with the following code:

    ```tsx
    // src/App.tsx
    import { useMutation } from 'convex/react'; // [!code ++]
    import { api } from '../convex/_generated/api'; // [!code ++]
    // ... other imports and component setup

    export default function App() {
      const sendMessage = useMutation(api.chat.sendMessage); // [!code ++]
      // ... other hooks and state variables

      return (
        <main className="chat">
          {/* ... other JSX elements */}
          <form
            onSubmit={async (e) => {
              e.preventDefault();
              alert('Mutation not implemented yet'); // [!code --]
              await sendMessage({ user: NAME, body: newMessageText }); // [!code ++]
              setNewMessageText('');
            }}
          >
            {/* ... form input and button */}
          </form>
        </main>
      );
    }
    ```

8.  **Implement the `getMessages` query:** Following the [Convex tutorial - Your first query](https://docs.convex.dev/tutorial/#your-first-query) section, add a Convex query function to `convex/chat.ts` to fetch messages from the database. Add the following `getMessages` query function to your `convex/chat.ts` file:

    ```typescript
    // convex/chat.ts
    import { query, mutation } from './_generated/server'; // [!code ++]
    // ... existing sendMessage mutation

    export const getMessages = query({
      // [!code ++]
      args: {}, // [!code ++]
      handler: async (ctx) => {
        // [!code ++]
        const messages = await ctx.db.query('messages').order('desc').take(50); // [!code ++]
        return messages.reverse(); // [!code ++]
      }, // [!code ++]
    }); // [!code ++]
    ```

9.  **Update `src/App.tsx` to Use `getMessages` query:** Finally, update `src/App.tsx` to fetch and display messages using the `useQuery` hook and the `getMessages` query function. Replace the relevant section in `src/App.tsx` with the following code:

    ```tsx
    // src/App.tsx
    import { useQuery, useMutation } from 'convex/react'; // [!code ++]
    // ... other imports and component setup

    export default function App() {
      const messages = [
        // [!code --]
        { _id: '1', user: 'Alice', body: 'Good morning!' }, // [!code --]
        { _id: '2', user: NAME, body: 'Beautiful sunrise today' }, // [!code --]
      ]; // [!code --]
      const messages = useQuery(api.chat.getMessages); // [!code ++]

      // ... remaining component code
    }
    ```

10. Your `App.tsx` file should look like the following code after all updates:

    ```tsx
    import { useEffect, useState } from 'react';
    import { faker } from '@faker-js/faker';
    import { api } from '../convex/_generated/api';
    import { useQuery, useMutation } from 'convex/react';

    const NAME = getOrSetFakeName();

    export default function App() {
      const messages = useQuery(api.chat.getMessages);
      const sendMessage = useMutation(api.chat.sendMessage);
      const [newMessageText, setNewMessageText] = useState('');

      useEffect(() => {
        setTimeout(() => {
          window.scrollTo({ top: document.body.scrollHeight, behavior: 'smooth' });
        }, 0);
      }, [messages]);

      return (
        <main className="chat">
          <header>
            <h1>Convex Chat</h1>
            <p>
              Connected as <strong>{NAME}</strong>
            </p>
          </header>
          {messages?.map((message) => (
            <article key={message._id} className={message.user === NAME ? 'message-mine' : ''}>
              <div>{message.user}</div>
              <p>{message.body}</p>
            </article>
          ))}
          <form
            onSubmit={async (e) => {
              e.preventDefault();
              await sendMessage({ user: NAME, body: newMessageText });
              setNewMessageText('');
            }}
          >
            <input
              value={newMessageText}
              onChange={async (e) => {
                const text = e.target.value;
                setNewMessageText(text);
              }}
              placeholder="Write a message…"
              autoFocus
            />
            <button type="submit" disabled={!newMessageText}>
              Send
            </button>
          </form>
        </main>
      );
    }

    function getOrSetFakeName() {
      const NAME_KEY = 'tutorial_name';
      const name = sessionStorage.getItem(NAME_KEY);
      if (!name) {
        const newName = faker.person.firstName();
        sessionStorage.setItem(NAME_KEY, newName);
        return newName;
      }
      return name;
    }
    ```

11. Your `convex/chat.ts` file should look like the following code after all updates:

    ```typescript
    // convex/chat.ts
    import { query, mutation } from './_generated/server';
    import { v } from 'convex/values';

    export const sendMessage = mutation({
      args: {
        user: v.string(),
        body: v.string(),
      },
      handler: async (ctx, args) => {
        console.log('This TypeScript function is running on the server.');
        await ctx.db.insert('messages', {
          user: args.user,
          body: args.body,
        });
      },
    });

    export const getMessages = query({
      args: {},
      handler: async (ctx) => {
        const messages = await ctx.db.query('messages').order('desc').take(50);
        return messages.reverse();
      },
    });
    ```

12. **Run the Convex chat application** by executing the following command in your terminal within the `convex-tutorial` directory:

    ```bash
    npm run dev
    ```

## Using the chat application

With the Convex chat application running and connected to your self-hosted Convex backend powered by Neon Postgres, you can now test the chat functionality.

1.  **Access the Chat application in your browser:** Open your web browser and navigate to[http://localhost:5173](http://localhost:5173). You should see the Convex chat application interface.

2.  **Open a second browser window:** Open a second browser window and navigate to [http://localhost:5173](http://localhost:5173).

3.  **Send and receive real-time messages:** In one chat window, type and send a message. Verify that the message appears in real-time in both chat windows. Send messages from both windows and observe the bidirectional real-time updates.

Congratulations! You have successfully integrated Convex with Neon Postgres and implemented a real-time chat application using Convex queries and mutations.

## Resources

- [Convex documentation](https://docs.convex.dev)
- [Convex self-hosting guide](https://stack.convex.dev/self-hosted-develop-and-deploy)
- [Neon documentation](/docs)
- [Neon Console](https://console.neon.tech)
- [Convex tutorial: A chat app](https://docs.convex.dev/tutorial)

<NeedHelp/>


# Building Full Stack apps in minutes with Anything

---
title: Building Full Stack apps in minutes with Anything
subtitle: Go from Text prompt to Full-Stack Database backed applications in minutes with Anything
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-03-12T00:00:00.000Z'
updatedOn: '2025-03-12T00:00:00.000Z'
---

The landscape of application development is rapidly changing, with AI-powered tools empowering even non technical users to build faster and more intuitively than ever before. Imagine describing your app idea in a simple conversation and watching it materialize in seconds, complete with a fully functional database. This is now possible with [Anything](https://www.createanything.com) (formerly [Create](https://create.xyz)), a text-to-app builder that works with out-of-the-box support for 50+ integrations such as Stripe, ElevenLabs, Google Maps, Stable Diffusion, OpenAI, and more.

This guide will introduce you to Anything and demonstrate how you can use it to make building database-backed applications incredibly easy and fast. We'll walk through creating a simple AI Image Generator, showcasing how you can go from a text prompt to a functional, full-stack application.

## Anything & Neon

Anything leverages Neon as the database backend for its AI-powered app development platform. This integration delivers a fully managed database solution, which is fundamental to Anything's rapid app development experience. By abstracting away database complexities, Anything users can concentrate solely on their application's functionality and design.

This experience is immediately apparent during app creation. Neon's instant database provisioning lets users bypass database setup and and focus on developing their application. Neon operates invisibly in the background. To learn more about how Anything uses Neon, see [From Idea to Full Stack App in One Conversation with Anything](/blog/from-idea-to-full-stack-app-in-one-conversation-with-create).

## Prerequisites

Before you start, ensure you have an **Anything Account**. You can sign up for a free account at [createanything.com](https://createanything.com/). The free plan is sufficient to follow this guide.

## Building an AI Image Generator app

This app will allow users to generate images using Stable Diffusion, view them in a gallery, and track download counts for each image. We'll leverage Anything's AI capabilities to build this app in minutes without writing a single line of code.

### Start a new project

1. Navigate to the [Anything](https://createanything.com) website and log in to your account.
2. Click on "New Project" to begin. You'll be presented with the builder interface.

   ![Start a New Project](/docs/guides/create_xyz_new_project.png)

### Describe your app

In the chat window, describe your app idea. For example, you can say, "Create a Stable Diffusion powered image generator. Ensure that image generation history is saved in a database, allowing users to view their past generations."

![Describe Your App](/docs/guides/create_xyz_describe_app.png)

Anything will immediately begin building your app based on your description. You'll see the AI agent working in real-time within the chat window, assembling all the necessary components and code for your application.

![Creating Your App](/docs/guides/create_xyz_initial_app.png)

You can see that as requested, Anything has created a database for image generation history.

You can verify the app's functionality by generating an image. Simply type your desired image description in the text field and click 'Generate.' You'll see your newly created image appear and automatically be added to the gallery display

![Testing the App](/docs/guides/create_xyz_test_app_working.png)

<Admonition type="note">
If the app doesn't work as expected, provide specific details in the chat window to help Anything understand the issue. For example, you can say, "The image generation is working, but the gallery is not displaying the images."
</Admonition>

### Database Schema

You can easily review your app's database schema directly from the Anything Dashboard at any point. Simply navigate to the **"Databases"** tab, then select your project's database to explore its schema, tables, columns, and relationships. This visual overview helps you understand how your app's data is structured.

![Database Schema](/docs/guides/create_xyz_database_schema.png)

### Continuous Iteration

You may want to add new features or refine existing ones as you iterate on your app. Anything makes it easy to enhance your app by simply describing the new features you want to add. Let's add a download counter feature to track the number of downloads for each generated image.

In the chat window, you can say: "Allow users to download images and track the number of downloads for each image". Anything will start adding the necessary components to your app to support this feature.

![Adding a New Feature](/docs/guides/create_xyz_add_new_feature.png)

You can view the database schema to understand how the new download count feature is integrated. Anything might have added a new table or column to track downloads.

To test the new feature, download an image. Click the 'Download' button on any image in the gallery. You'll see the download count increase for that image.

![Final App with Download Feature](/docs/guides/create_xyz_final_app.png)

You've successfully built an AI Image Generator with download tracking! You can now customize it further by enhancing the UI, adding features like user authentication, or integrating services such as Stripe to charge $1 per generated image. To add user authentication, say something like: 'The app should allow users to sign in and sign up. Image generation should only be available to signed-in users.'

Anything will add the necessary components to your app to support user authentication.

![Adding User Authentication](/docs/guides/create_xyz_add_user_auth.png)

<Admonition type="note" title="Configuring Authentication Providers for User Accounts">
User accounts are built-in and fully supported in Anything. If you want to let users sign in with Google, Facebook, or other providers, simply visit your project's **Auth Providers** section in the Project Settings. There, you can enable additional sign-in options with just a few clicks. Learn more in the [Anything Docs: User Accounts](https://www.createanything.com/docs/builder/user-accounts).

![User Accounts](/docs/guides/create_xyz_user_accounts.png)
</Admonition>

Your app should now require users to sign in before generating images. You can test this by trying to generate an image without being signed in.

![Testing User Authentication](/docs/guides/create_xyz_test_user_auth.png)

Finally, you can deploy the app by clicking on the "Publish" button. You can also publish your app to the Apple iOS Store (mobile app support is currently in beta.) For more details, see the [Anything Docs: Mobile Apps](https://www.createanything.com/docs/builder/mobile).

![Publishing the App](/docs/guides/create_xyz_publish_app.png)

<Admonition type="note" title="Version history for restoring a past version of your app">

Anything offers a robust version history. This feature enables instant restoration to any past version of your app, in case you need to rewind.

To restore a past version:

1. **Browse Version History:** Click on the down arrow next to the "Anything" logo in the top left corner.

   ![Restore Past Version](/docs/guides/create_xyz_restore_project.png)

2. **Restore:** Click on the desired version from the list to restore it.

Anything instantly switches your app back to that earlier state.
</Admonition>

## Tips for building apps with Anything

To make the most of Anything and build apps efficiently, consider the following tips:

- **Prompting Best Practices**:
  - **Context is key**: Start prompts with clear context. For example describe the app's purpose and main features. For example say, "I want to add a new feature to allow users to download images."
  - **Iterate in small steps**: Break down complex changes. For a whole new page, start by describing the header, then the body, then the footer in separate prompts. This gives you more control.
  - **Show, Don't just tell**: Use images! Paste screenshots or drag and drop images into the chat to show Anything exactly what you want the style or layout to be wherever possible.
  - **Pinpoint errors**: Be specific when things go wrong. Instead of saying "it's broken", paste error messages or describe exactly what you expected to happen vs. what did happen.

- **Leverage Anything's integrations**:
  - **Explore the Integration library**: Anything has many integrations ready to use. Type `/` in the chat to see them. Integrations include AI models, UI libraries, and services like Stripe.
  - **Choose the right AI model**: Experiment with different AI models for different tasks. For example, use Stable Diffusion for image generation, OpenAI/Claude for text generation etc.

## Resources

- [Anything](https://createanything.com)
- [Anything Docs](https://www.createanything.com/docs/welcome)
- [From Idea to Full Stack App in One Conversation with Anything](/blog/from-idea-to-full-stack-app-in-one-conversation-with-create)
- [Anything Templates](https://www.createanything.com/templates)

<NeedHelp />


# Get started with Cursor and Neon Postgres MCP Server

---
title: 'Get started with Cursor and Neon Postgres MCP Server'
subtitle: 'Make schema changes with natural language using Cursor and Neon MCP Server'
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-02-20T00:00:00.000Z'
updatedOn: '2025-02-20T00:00:00.000Z'
---

Imagine adjusting your database schema simply by describing the change in plain English. This is possible by combining [Cursor](https://cursor.com) with the [Neon MCP Server](https://github.com/neondatabase/mcp-server-neon).

This guide demonstrates how to use [Cursor's Composer](https://docs.cursor.com/composer) and Neon's MCP server to perform database migrations in your Neon project.

<Admonition type="important" title="Neon MCP Server Security Considerations">
The Neon MCP Server grants powerful database management capabilities through natural language requests. **Always review and authorize actions requested by the LLM before execution.** Ensure that only authorized users and applications have access to the Neon MCP Server.

The Neon MCP Server is intended for local development and IDE integrations only. **We do not recommend using the Neon MCP Server in production environments.** It can execute powerful operations that may lead to accidental or unauthorized changes.

For more information, see [MCP security guidance →](/docs/ai/neon-mcp-server#mcp-security-guidance).
</Admonition>

## Key components

Let's break down the key components in this setup:

- **Cursor**: Cursor is an AI-first code editor that supports Model Context Protocol (MCP) for interfacing with external tools (APIs, databases, etc.)

- **Neon MCP Server**: Neon's MCP server acts as a bridge between MCP clients like Cursor and [Neon's API](https://api-docs.neon.tech/reference/getting-started-with-neon-api), letting you work with Neon databases using natural language commands.

- **Model Context Protocol (MCP)**: MCP is a lightweight communication standard that allows Cursor and Neon MCP Server to work together.

## Setting up Neon MCP Server in Cursor

You have two options for connecting Cursor to the Neon MCP Server:

1. **Remote MCP Server (Preview):** Connect to Neon's managed MCP server using OAuth for authentication. This method is more convenient as it eliminates the need to manage API keys in Cursor. Additionally, you will automatically receive the latest features and improvements as soon as they are released.

2. **Local MCP Server:** Run the Neon MCP server locally on your machine, authenticating with a Neon API key.

### Prerequisites

Before you begin, ensure you have the following:

1. **Cursor Editor:** Download and install Cursor from [cursor.com](https://cursor.com).
2. **A Neon Account and Project:** You'll need a Neon account and a project. You can quickly create a new Neon project here [pg.new](https://pg.new)
3. **Neon API Key (for Local MCP server):** After signing up, get your Neon API Key from the [Neon console](https://console.neon.tech/app/settings/api-keys). This API key is needed to authenticate your application with Neon. For instructions, see [Manage API keys](/docs/manage/api-keys).

   <Admonition type="important" title="Neon API Key Security">
   Keep your Neon API key secure, and never share it publicly. It provides access to your Neon projects.
   </Admonition>

4. **Node.js (>= v18) and npm:** Ensure Node.js (version 18 or later) and npm are installed. Download them from [nodejs.org](https://nodejs.org).

### Option 1: Setting up the Remote Hosted Neon MCP Server

<Admonition type="note">
By default, the Remote MCP Server connects to your personal Neon account. To connect to an organization's account, you must authenticate with an API key. For more information, see [API key-based authentication](/docs/ai/neon-mcp-server#api-key-based-authentication).
</Admonition>

#### Quick Install (Recommended)

Click the button below to install the Neon MCP server in Cursor. When prompted, click **Install** within Cursor.

<a href="cursor://anysphere.cursor-deeplink/mcp/install?name=Neon&config=eyJ1cmwiOiJodHRwczovL21jcC5uZW9uLnRlY2gvbWNwIn0%3D"><img src="https://cursor.com/deeplink/mcp-install-dark.svg" alt="Add Neon MCP server to Cursor" height="32" /></a>

#### Manual Setup

This method uses Neon's managed server and OAuth authentication.

1. Open Cursor.
2. Create a `.cursor` directory in your project's root directory. This is where Cursor will look for the MCP server configuration.
3. Paste the following JSON configuration into a file named `mcp.json` in the
   `.cursor` directory:

   ```json
   {
     "mcpServers": {
       "Neon": {
         "command": "npx",
         "args": ["-y", "mcp-remote@latest", "https://mcp.neon.tech/mcp"]
       }
     }
   }
   ```

   If you have other MCP servers configured, you can copy just the `Neon` part.

   Save the `mcp.json` file after pasting the configuration.

4. **Restart Cursor** or reload the window (`Developer: Reload Window` from the Command Palette). If Cursor prompts you to Enable the MCP server, click **Enable**.

5. An OAuth window will open in your browser. Follow the prompts to authorize Cursor to access your Neon account.
   ![Neon OAuth window](/docs/guides/neon-oauth-window.png)

6. You can verify that the connection is successful by checking the **MCP Servers** section in Cursor settings.
   ![Cursor with Neon MCP Tools](/docs/guides/cursor-with-neon-mcp-tools.png)

7. Cursor is now connected to Neon's remote MCP server.

<Admonition type="note">
The remote hosted MCP server is in preview due to the [new OAuth MCP specification](https://spec.modelcontextprotocol.io/specification/2025-03-26/basic/authorization/), expect potential changes as we continue to refine the OAuth integration.
</Admonition>

### Option 2: Setting up the Local Neon MCP Server

This method runs the Neon MCP server locally on your machine, using a Neon API key for authentication.

1. Open Cursor.
2. Create a `.cursor` directory in your project's root directory. This is where Cursor will look for the MCP server configuration.
3. Paste the following JSON configuration into a file named `mcp.json` in the `.cursor` directory. Replace `<YOUR_NEON_API_KEY>` with your actual Neon API key which you obtained from the [prerequisites](#prerequisites) section:

   <CodeTabs labels={["MacOS/Linux", "Windows", "Windows (WSL)"]}>

   ```json
   {
     "mcpServers": {
       "Neon": {
         "command": "npx",
         "args": ["-y", "@neondatabase/mcp-server-neon", "start", "<YOUR_NEON_API_KEY>"]
       }
     }
   }
   ```

   ```json
   {
     "mcpServers": {
       "Neon": {
         "command": "cmd",
         "args": [
           "/c",
           "npx",
           "-y",
           "@neondatabase/mcp-server-neon",
           "start",
           "<YOUR_NEON_API_KEY>"
         ]
       }
     }
   }
   ```

   ```json
   {
     "mcpServers": {
       "Neon": {
         "command": "wsl",
         "args": ["npx", "-y", "@neondatabase/mcp-server-neon", "start", "<YOUR_NEON_API_KEY>"]
       }
     }
   }
   ```

   </CodeTabs>

   If you have other MCP servers configured, you can copy just the `Neon` part.

   ![Cursor Local MCP JSON](/docs/guides/cursor-local-mcp-server-json.png)

4. Save the `mcp.json` file after pasting the configuration.

5. **Restart Cursor** or reload the window (`Developer: Reload Window` from the Command Palette).

6. Cursor is now connected to Neon's MCP server. You can verify that the connection is successful by checking the **MCP Servers** section in Cursor settings.

   ![Cursor with Neon MCP Tools](/docs/guides/cursor-with-neon-mcp-tools.png)

You've now configured Neon MCP Server in Cursor and can manage your Neon Postgres databases using AI.

### Verification

Now that you have the Neon MCP server set up either remotely or locally, you can verify the connection and test the available tools.

1. Open a Cursor **Chat** using the keyboard shortcut **Command + I** (on Mac) or **Control + I** (on Windows) and select the **Agent** option from the drop-down menu.

   ![Cursor Composer view](/docs/guides/cursor_chat_view.png)

2. Type `List your available MCP tools` in the Composer text field, select the **agent** option in the corner of the field, and click **Submit**.

   **_Note: The agent option here is tiny and easy to miss!_**

   ![Cursor list available tools](/docs/guides/cursor_list_tools.png)

3. Try out an Neon MCP Server tool by typing `Tell me about my Neon project <project_id>`. **You'll need to provide your Neon project ID.**

   ![Cursor list project details](/docs/guides/cursor_project_details.png)

   Cursor uses the Neon MCP Server `describe_project` tool to retrieve your project's details.

### Global MCP Server in Cursor

You can also set up a global MCP server in Cursor. This allows you to use the same MCP server configuration across all your projects. To do this, follow these steps:

1. Open Cursor.
2. Go to the **Settings**.
3. In the **MCP Servers** section, click on **+ Add new Global MCP Server**.
   ![Cursor add new global MCP server](/docs/guides/cursor-global-mcp-server.png)
4. Paste the same JSON configuration either for the **Remote Hosted** or **Local MCP Server** as mentioned above.
5. Save the configuration.
6. Restart Cursor or reload the window (`Developer: Reload Window` from the Command Palette).
7. You now have Neon MCP Server set up globally in Cursor. You can use it in any project without needing to configure it again for each project.

### Troubleshooting

If you are on a version of Cursor that does not support JSON configuration for MCP servers, you can use the following command when prompted:

```bash
npx -y @neondatabase/mcp-server-neon start <YOUR_NEON_API_KEY>
```

<MCPTools />

These actions enable any MCP client like Cursor to interact with various functionalities of the **Neon platform via the Neon API.** Certain tools, especially database migration ones, are tailored for AI agent and LLM usage, leveraging Neon’s branching for safe preview and commit.

## Development Workflow: Adding a Column with Cursor and Neon MCP

Let's walk through a typical development scenario: Quickly adding a column for prototyping using natural language within Cursor. The following chat log demonstrates a real interaction with Cursor and the Neon MCP server.

**Scenario:** During development, you decide to track timestamps for entries in your `playing_with_neon` table. You want to quickly add a `created_at` column.

<Admonition type="tip">
Use `⌘I` to open Cursor's Composer and `⌘N` to create a new Composer.
</Admonition>

<Admonition type="tip" title="Security Reminder">
For your security, review the tool's purpose before permitting the operation to proceed. Remember that LLMs can sometimes produce unexpected results, so careful monitoring is always recommended.
</Admonition>

Here's the conversation log between the user and Cursor:

```text shouldWrap
User: In my neon project id: round-salad-44063611 list all the tables
Cursor: I'll help you list all the tables in your Neon project. I'll use the default database name 'neondb' since none was specified.
> Called MCP Tool (get_database_tables)
> I found one table in your database:
> playing_with_neon (in the public schema)
> Would you like to see more details about this table's structure or perform any other operations with it?

User: Can you show me first 5 rows?
Cursor: I'll query the first 5 rows from the playing_with_neon table.
> Called MCP Tool (run_sql)
> Here are the first 5 rows from the table. The table has three columns:
> id (sequential numbers)
> name (string values)
> value (decimal numbers)
> Would you like to perform any other operations with this data?

User: can you add a created_at column?
Cursor: I'll help you add a created_at column to the playing_with_neon table. I'll set it up with a timestamp data type and set it to automatically record the creation time of new rows.
> Called MCP Tool (prepare_database_migration)
> Let me verify the change in the temporary branch to make sure it was applied correctly.
> Called MCP Tool (run_sql)
> I've added the created_at column to the playing_with_neon table in a temporary branch. Would you like to commit this migration to the production branch?
> Migration Details:
> Migration ID: a65babcd-d1cc-401f-89b5-762570570102
> Temporary Branch Name: br-silent-cherry-a1eqrl6i
> Temporary Branch ID: br-silent-cherry-a1eqrl6i
> Migration Result: Success
> Please let me know if you'd like to proceed with applying this change to the production branch.

User: yeah do it
Cursor: I'll complete the migration and apply the changes to the production branch.
> Called MCP Tool (complete_database_migration)
> The migration has been successfully completed! The created_at column has been added to your table in the production branch, and the temporary branch has been cleaned up.
```

**Key takeaways:**

- **Natural language interaction:** You can use simple, conversational English to interact with your database.
- **Step-by-step guidance:** Cursor (Claude) confirms each step and provides details like branch names and migration IDs.
- **MCP Tools in Action:** The underlying MCP tools (`get_database_tables`, `run_sql`, `prepare_database_migration`, `complete_database_migration`) illustrate the automated process.
- **Branching for safety:** The agent automatically creates a temporary branch for schema changes. The user will be prompted to confirm the migration before applying it to the production branch.

You can verify the changes in your Neon Console or by querying the database directly.

![Neon Console with added column](/docs/guides/neon-cursor-mcp-added-column.png)

<Admonition type="note">
While the Neon MCP server allows you to utilize all of Neon's functionalities, you can also use a [generic Postgres MCP server](https://github.com/modelcontextprotocol/servers/tree/main/src/postgres) to connect to Neon.
</Admonition>

## Bonus: Creating a project and branch

Beyond schema changes, you can also manage your Neon projects and branches directly within Cursor using natural language commands. This can be useful for quickly setting up a new development or test environment.

### Creating a new project

Let's say you want to create a new Neon project named "my-new-project". You can simply tell Cursor:

```text shouldWrap
User: Create a Neon project named "my-new-project"
Cursor: I'll help you create a new Neon project with the specified name.
> Called MCP Tool (create_project)
> Great! I've created a new Neon project for you with the name "my-new-project". Here are the important details:
> Project ID: orange-dawn-33739962
> Default branch: main
> Default database: neondb
> The project is ready to use. You can start creating tables and working with your database right away.
```

![Cursor creating a new Neon project](/docs/guides/cursor-mcp-create-new-project.png)

Cursor will then use the `create_project` MCP tool to initiate the project creation. It will provide you with a project ID and name.

### Creating a New Branch

Once you have a project, you can easily create new branches for development or experimentation. For example, to create a branch named "feature-x" in your "my-new-project" project:

```text shouldWrap
User: Create a branch named "feature-x"
Cursor: Perfect! I've created a new branch named "feature-x". Here are the details:
> Branch ID: br-cold-mountain-a523ka2w
> Branch name: feature-x
> Parent branch ID: br-odd-pine-a5y53krm
> The branch has been created successfully and is ready for use.
```

![Cursor creating a new Neon branch](/docs/guides/cursor-mcp-create-new-branch.png)

Cursor will use the `create_branch` MCP tool to create the branch and provide you with the branch name and ID. Notice how we don't need to specify the project ID, as Cursor remembers the active project context.

## Conclusion

Cursor combined with the Neon MCP Server, whether using the **Remote Hosted (Preview)** option or the **Local Server** setup, lets you use natural language to interact with your database and take advantage of Neon's branching capabilities for fast iteration. This approach is ideal for quickly testing
database ideas and making schema changes during development.

## Resources

- [MCP Protocol](https://modelcontextprotocol.org)
- [Adding an MCP Server to Cursor](https://docs.cursor.com/context/model-context-protocol)
- [Neon Docs](/docs)
- [Neon API Keys](/docs/manage/api-keys#creating-api-keys)
- [Neon MCP server GitHub](https://github.com/neondatabase/mcp-server-neon)

<NeedHelp/>


# Using DBeaver with a Hosted Postgres

---
title: Using DBeaver with a Hosted Postgres
subtitle: A comprehensive guide on how to manage your Postgres database using DBeaver.
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-12-21T00:00:00.000Z'
updatedOn: '2024-12-21T00:00:00.000Z'
---

DBeaver is a versatile database management tool that allows you to interact with a wide range of databases, including PostgreSQL. This guide will walk you through the steps to set up and use DBeaver with a hosted Postgres database, enabling you to perform various database operations efficiently.

## Table of Contents

- [Setting Up DBeaver](#setting-up-dbeaver)
- [Connecting to Your Hosted Postgres Database](#connecting-to-your-hosted-postgres-database)
- [Basic Operations in DBeaver](#basic-operations-in-dbeaver)

## Setting Up DBeaver

1. **Download and Install DBeaver**: If you haven't already, download DBeaver from the [official website](https://dbeaver.io/download/). Choose the version suitable for your operating system and follow the installation instructions.

2. **Launch DBeaver**: Open DBeaver from your applications menu and ensure it is running.

## Provisioning a Serverless Postgres

To get started, go to the [Neon console](https://console.neon.tech/app/projects) and create a new project by entering a project name of your choice.

![](/guides/images/pg-notify/index.png)

All Neon connection strings have the following format:

```bash
postgres://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>
```

- `user` is the database user.
- `password` is the database user’s password.
- `endpoint_hostname` is the host with neon.tech as the [TLD](https://www.cloudflare.com/en-gb/learning/dns/top-level-domain/).
- `port` is the Neon port number. The default port number is 5432.
- `dbname` is the name of the database. “neondb” is the default database created with each Neon project.
- `?sslmode=require&channel_binding=require` is an optional query parameter that enforces the [SSL](https://www.cloudflare.com/en-gb/learning/ssl/what-is-ssl/) mode while connecting to the Postgres instance for better security.

You will be using these connection string components in the following steps to connect DBeaver to your Postgres database.

## Connecting to Your Hosted Postgres Database

1. **Open DBeaver**: Ensure DBeaver is running. You will see the main dashboard.

2. **Create a New Database Connection**:
   - Click on the "New Database Connection" button (usually a plug icon or from the "Database" menu).
   - In the "Connect to Database" wizard, select "PostgreSQL" from the list of database types and click "Next".

3. **Enter Connection Details**:

   ![](/guides/images/dbeaver/conn-1.png)
   - Fill in the required fields based on your Neon connection string:
     - **Host**: The endpoint of your hosted Postgres database (e.g., `ep-...us-east-2.aws.neon.tech`).
     - **Port**: The port number (default is `5432`).
     - **Database**: The database name (e.g., `neondb`).
     - **Username**: Your database username.
     - **Password**: Your database password.
   - Enable "Show all databases" to ensure all databases in your Neon project are listed.

   ![](/guides/images/dbeaver/conn-2.png)
   - Click "Edit Driver Settings" if needed to ensure SSL is enabled. Under the "Driver Properties" tab, set `sslmode` to `require`.

4. **Test the Connection**:
   - Click the "Test Connection" button to verify the connection details.
   - If successful, click "Finish" to save the connection. Your new database connection will appear in the left sidebar.

## Basic Operations in DBeaver

### 1. Running SQL Queries

- Right-click on your database connection in the left sidebar and select "SQL Editor" > "New SQL Script".
- Enter your SQL queries in the editor and click the "Execute" button (play icon) to run them.
- View the results in the results pane below the editor.

### 2. Managing Tables

- Expand your database connection in the left sidebar, then navigate to "Databases" > "neondb" > "Schemas" > "public" > "Tables".
- Right-click on "Tables" to create a new table or manage existing ones (e.g., view, edit, or drop tables).

### 3. Importing and Exporting Data

- To import data:
  - Right-click on a table and select "Import Data".
  - Choose the source file (e.g., CSV) and follow the prompts to map the columns.
- To export data:
  - Right-click on a table and select "Export Data".
  - Choose the format (e.g., CSV, JSON) and follow the prompts to save the file.

## Conclusion

DBeaver is a powerful tool for managing your hosted Postgres database. With its intuitive interface and robust features, you can easily perform tasks such as creating tables, running queries, and visualizing data. By following this guide, you should be well-equipped to utilize DBeaver effectively for your database management needs.

<NeedHelp />


# Using Directus CMS with Neon Postgres and Astro to build a blog

---
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-12-22T00:00:00.000Z'
updatedOn: '2024-12-22T00:00:00.000Z'
title: Using Directus CMS with Neon Postgres and Astro to build a blog
subtitle: A step-by-step guide for building your own blog in an Astro application with Directus CMS and Postgres powered by Neon
---

In this guide, you will learn how to set up a serverless Postgres database with Neon, configure Directus CMS with Postgres, define a blog schema, and author content using Directus CMS. The guide also covers configuring API read permissions and building a dynamic frontend with Astro to display blog pages fetched from the Directus CMS instance.

## Prerequisites

To follow the steps in this guide, you will need the following:

- [Node.js 18](https://nodejs.org/en) or later
- A [Neon](https://console.neon.tech/signup) account
- [Docker Desktop](https://www.docker.com/products/docker-desktop/) set up locally

## Provisioning a serverless Postgres database

Using a serverless Postgres database powered by Neon lets you scale compute resources down to zero, which helps you save on compute costs.

To get started, go to the [Neon console](https://console.neon.tech/app/projects) and create a project. You will then be presented with a dialog that provides a connection string for your database. You will be using the connection string to connect the Directus CMS instance to your Postgres database.

## Setting up Directus locally with Postgres

Let's begin with creating a Directus CMS backend to serve the content for blog posts. Open your terminal and run the following command:

```bash
mkdir directus-cms
cd directus-cms
```

and create a `docker-compose.yml` with the following code:

```yml
services:
  directus:
    image: directus/directus:11.3.5
    ports:
      - 8055:8055
    volumes:
      - ./database:/directus/database
      - ./uploads:/directus/uploads
      - ./extensions:/directus/extensions
    environment:
      SECRET: 'replace-with-secure-random-value'
      ADMIN_EMAIL: 'admin@example.com'
      ADMIN_PASSWORD: 'd1r3ctu5'
      DB_CLIENT: 'pg'
      DB_CONNECTION_STRING: 'postgresql://neondb_owner:...@ep-...us-east-1.aws.neon.tech/neondb?sslmode=require&channel_binding=require'
      DB_SSL__REJECT_UNAUTHORIZED: 'true'
      WEBSOCKETS_ENABLED: 'true'
      CORS_ENABLED: 'true'
      CORS_ORIGIN: '*'
```

Now, set the `DB_CONNECTION_STRING` value to the connection string you obtained previously. Finally, run the following command to start the local Directus CMS instance:

```shell
docker-compose up -d
```

Once the migrations are run successfully, the Directus CMS instance will be accessible at [localhost:8055/admin](http://localhost:8055/admin/). Sign in with the credentials set in the `docker-compose.yml` file.

## Configure the authors schema in Directus CMS

Navigate to the [Data Model](http://localhost:8055/admin/settings/data-model) view, and click the `+` icon to create a new data model.

Set the **Name** field to **authors**, press the next arrow icon, and save by clicking the tick icon.

![](/guides/images/directus-cms/author-1.png)

Once that's done, click on `Create Field` to add a field to the `authors` schema.

![](/guides/images/directus-cms/author-2.png)

Select `Input` as the field type and set the `Key` as **name** to indicate that the field represents the author's name.

![](/guides/images/directus-cms/author-3.png)

Finally, click `Save` to finish adding the `name` field to the `authors` schema.

![](/guides/images/directus-cms/author-4.png)

Now, let's move on to creating the `post` schema in Directus CMS.

## Configure the posts schema in Directus CMS

Navigate to the [Data Model](http://localhost:8055/admin/settings/data-model) view, and click the `+` icon to start creating a new data model.

Enter **posts** in the **Name** field, press the next arrow icon, and save by clicking the `+` icon.

Then, follow the same process as earlier to add the following fields to the `posts` schema:

- An **Input** field called **title**.
- A **WYSIWYG** field called **content**.
- An **Image** field called **image**.
- A **Datetime** field called **published_date**.
- A **Many to One** field called **author** with the **Related Collection** set to **authors**.

## Configure API Read Permissions in Directus CMS

To be able to fetch the data authored in your local Directus CMS instance, you will need to configure what is readable and writeable using APIs. Navigate to [Settings > Access Policies](http://localhost:8055/admin/settings/policies), click on **Public**, and add the permissions for the `authors`, `posts` and `directus_files` to be read publicly.

![](/guides/images/directus-cms/public-api.png)

Now, let's move on to creating an Astro application to create dynamic blog pages based on blog data that's accessible via your locally hosted instance of Directus CMS.

## Create a new Astro application

Let’s get started by creating a new Astro project. Open your terminal and run the following command:

```bash
npm create astro@latest blog-ui
```

`npm create astro` is the recommended way to scaffold an Astro project quickly.

When prompted, choose:

- `Empty` when prompted on how to start the new project.
- `Yes` when prompted if plan to write Typescript.
- `Strict` when prompted how strict Typescript should be.
- `Yes` when prompted to install dependencies.
- `Yes` when prompted to initialize a git repository.

Once that’s done, change to the project directory and start the app:

```bash
cd blog-ui
npm run dev
```

The app should be running on [localhost:4321](http://localhost:4321/). Let's close the development server for now.

Next, execute the following command to install the necessary libraries and packages for building the application:

```bash
npm install @directus/sdk
npm install -D typescript
```

The commands above install the packages, with the `-D` flag specifying the libraries intended for development purposes only.

The libraries installed include:

- [@directus/sdk](https://npmjs.com/package/@directus/sdk): Typescript SDK to query from your Directus CMS instance.

The development-specific libraries include:

- [typescript](https://npmjs.com/package/typescript): TypeScript is a language for application-scale JavaScript.

Then, add the following lines to your `tsconfig.json` file to make relative imports within the project easier:

```diff
{
  "extends": "astro/tsconfigs/base",
  "include": [".astro/types.d.ts", "**/*"],
  "exclude": ["dist"],
  "compilerOptions": { // [!code ++]
    "baseUrl": ".", // [!code ++]
    "paths": { // [!code ++]
      "@/*": ["src/*"] // [!code ++]
    } // [!code ++]
  } // [!code ++]
}
```

Now, create a `.env` file. You are going to add the API token obtained earlier.

The `.env` file should contain the following keys:

```bash
# .env

DIRECTUS_URL="http://localhost:8055"
```

## Create dynamic blog routes in Astro

To programmatically create pages as you keep authoring more content in your locally hosted Directus CMS, you are going to use [dynamic routes](https://docs.astro.build/en/guides/routing/#dynamic-routes) in Astro. With dynamic routes, you create a single file with a name like `[slug].astro`, where slug represents a [unique and dynamic variable](https://docs.astro.build/en/reference/api-reference/#contextparams) for each blog. Using [getStaticPaths](https://docs.astro.build/en/reference/api-reference/#getstaticpaths), you can programmatically create multiple blog pages with custom data using Directus CMS as your data source. Let's see this in action. Create a file named `[slug].astro` in the `src/pages` directory with the following code:

```astro
---
// File: src/pages/[slug].astro

import Layout from "@/layouts/Layout.astro";
import directus from "@/lib/directus";
import { readItems } from "@directus/sdk";

export async function getStaticPaths() {
  const posts = await directus.request(
    readItems("posts", {
      fields: ['*'],
    })
  );
  return posts.map((post) => ({ params: { slug: post.slug }, props: post }));
}
const post = Astro.props;
---

<Layout title={post.title}>
  <main>
    <img src={`${import.meta.env.DIRECTUS_URL}/assets/${post.image}`} />
    <h1>{post.title}</h1>
    <div set:html={post.content} />
  </main>
</Layout>
```

Let's understand the code above in two parts:

- Inside `getStaticPaths` function, a fetch call is made to the locally hosted Directus CMS API to get all the blogs with their **title**, **image** and **content** values. Looping over each blog item, an array is created that passes all the data obtained as the [props](https://docs.astro.build/en/reference/api-reference/#contextprops), and its **slug** as the unique variable to be associated with each blog.

- The HTML section represents the content of a particular blog page. The blog data attributes such as Title, Image URL, and the blog content in HTML are obtained from `Astro.props` (passed in `getStaticPaths` as props).

## Build and Test your Astro application locally

To test the Astro application in action, prepare a build and run the preview server using the following command:

```bash
npm run build && npm run preview
```

## Summary

In this guide, you learned how to build a blog in an Astro application using Directus CMS and a serverless Postgres database (powered by Neon). Additionally, you learned how to create content collections in Directus CMS and dynamic blog routes in an Astro application.

<NeedHelp />


# Building an API with Django, Django REST Framework, and Neon Postgres

---
title: Building an API with Django, Django REST Framework, and Neon Postgres
subtitle: Learn how to create a robust RESTful API for an AI Model Marketplace using Django, Django REST Framework, and Neon's serverless Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-09-15T00:00:00.000Z'
updatedOn: '2024-09-15T00:00:00.000Z'
---

Django is one of the most popular Python web frameworks for building web applications and APIs. Django REST Framework extends Django to provide powerful tools for building RESTful APIs quickly and efficiently based on your Django models with minimal code.

In this guide, we will go over how to build a RESTful API for a fictional AI Model Marketplace using Django and Django REST Framework with Neon's serverless Postgres as the database backend.

## Prerequisites

To follow this guide, you'll need:

- Python 3.8 or higher installed on your machine
- A [Neon account](https://console.neon.tech/signup) with a project created
- Basic familiarity with Django and RESTful API concepts

## Setting up the project

### Create a virtual environment

First, let's set up a new Python virtual environment for our project:

```bash shouldWrap
python -m venv neon-django-ai-marketplace
source neon-django-ai-marketplace/bin/activate  # On Windows, use `neon-django-ai-marketplace\Scripts\activate`
```

This creates a new virtual environment named `neon-django-ai-marketplace` and activates it, ensuring our project dependencies are isolated.

After activating the virtual environment, you should see `(neon-django-ai-marketplace)` in your terminal prompt.

### Install required packages

Now, let's install the necessary packages:

```bash
pip install django djangorestframework psycopg2-binary python-dotenv
```

This command installs Django, Django REST Framework, the PostgreSQL adapter for Python, and a package to manage environment variables. We'll use Django for the web framework, DRF for building the API, and psycopg2-binary to connect to the Neon Postgres database.

### Create a new Django project

With the dependencies installed, create a new Django project named `ai_marketplace`:

```bash
django-admin startproject ai_marketplace
```

Once the project is created, navigate to the project directory:

```bash
cd ai_marketplace
```

### Configure the database connection

To connect to Neon's serverless Postgres database, we need to set up the database connection in the Django project.

Open the `settings.py` file to configure the database connection. By default, Django uses SQLite as the database backend. Replace the `DATABASES` section with the following:

```python
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'your_database_name',
        'USER': 'your_username',
        'PASSWORD': 'your_password',
        'HOST': 'your_neon_hostname',
        'PORT': '5432',
    }
}
```

Replace the placeholders with your Neon database details. You can find these details in the Neon Console under **Connection Details**.

To verify the connection, run the Django development server:

```bash
python manage.py runserver
```

If the server starts without errors, you've successfully connected to the Neon database.

## Creating the API

Now that we have the Django project set up and connected to the Neon database, let's create the simple API for our AI Model Marketplace.

### Define the models

In Django, models are Python classes that represent database tables. Create a new Django app for our AI Model Marketplace:

```bash
python manage.py startapp models_api
```

Add the new app to `INSTALLED_APPS` in `settings.py`, this essentially registers the app with the Django project:

```python
INSTALLED_APPS = [
    # ... (existing apps)
    'rest_framework',
    'models_api',
]
```

Now, let's define our models in `models_api/models.py`:

```python
from django.db import models
from django.core.validators import MinValueValidator, MaxValueValidator

class ModelAuthor(models.Model):
    name = models.CharField(max_length=200)
    bio = models.TextField()
    contact_info = models.EmailField()
    rating = models.FloatField(validators=[MinValueValidator(0), MaxValueValidator(5)])

    def __str__(self):
        return self.name

class AIModel(models.Model):
    MODEL_TYPES = [
        ('NLP', 'Natural Language Processing'),
        ('CV', 'Computer Vision'),
        ('RL', 'Reinforcement Learning'),
        ('OTHER', 'Other'),
    ]
    FRAMEWORKS = [
        ('PT', 'PyTorch'),
        ('TF', 'TensorFlow'),
        ('KRS', 'Keras'),
        ('OTHER', 'Other'),
    ]
    name = models.CharField(max_length=200)
    model_type = models.CharField(max_length=5, choices=MODEL_TYPES)
    description = models.TextField()
    framework = models.CharField(max_length=5, choices=FRAMEWORKS)
    version = models.CharField(max_length=50)
    download_url = models.URLField()
    price = models.DecimalField(max_digits=10, decimal_places=2)
    tags = models.JSONField()
    author = models.ForeignKey(ModelAuthor, on_delete=models.CASCADE, related_name='models_uploaded')

    def __str__(self):
        return f"{self.name} - {self.version}"

class ModelPurchase(models.Model):
    user = models.CharField(max_length=200)  # Simplified for this example
    ai_model = models.ForeignKey(AIModel, on_delete=models.CASCADE)
    purchase_date = models.DateTimeField(auto_now_add=True)
    price_paid = models.DecimalField(max_digits=10, decimal_places=2)
    license_key = models.CharField(max_length=100)
    download_link = models.URLField()

    def __str__(self):
        return f"{self.user} - {self.ai_model.name}"

class UsageScenario(models.Model):
    ai_model = models.ForeignKey(AIModel, on_delete=models.CASCADE, related_name='usage_scenarios')
    title = models.CharField(max_length=200)
    description = models.TextField()
    code_snippet = models.TextField()
    usage_frequency = models.IntegerField(default=0)

    def __str__(self):
        return f"{self.ai_model.name} - {self.title}"

class ModelBenchmark(models.Model):
    ai_model = models.ForeignKey(AIModel, on_delete=models.CASCADE, related_name='benchmarks')
    metric_name = models.CharField(max_length=100)
    value = models.FloatField()
    benchmark_date = models.DateTimeField(auto_now_add=True)
    hardware_used = models.CharField(max_length=200)

    def __str__(self):
        return f"{self.ai_model.name} - {self.metric_name}: {self.value}"
```

Here we define five models:

- `ModelAuthor`: Represents the creator of AI models. It includes fields for name, bio, contact info, and rating.
- `AIModel`: Represents individual AI models with their details. It includes fields for name, model type, description, framework, version, download URL, price, tags, and author.
- `ModelPurchase`: Tracks purchases and downloads of AI models. It includes fields for the user, AI model, purchase date, price paid, license key, and download link.
- `UsageScenario`: Represents suggested use cases for each AI model. It includes fields for the AI model, title, description, code snippet, and usage frequency.
- `ModelBenchmark`: Stores performance benchmarks for AI models. It includes fields for the AI model, metric name, value, benchmark date, and hardware used.

The models are related to each other using foreign keys and related names to establish relationships between entities.

Django's ORM will automatically create the corresponding database migrations for tables based on these models. You can customize the models further by adding fields, methods, or meta options as needed.

### Create and apply migrations

Unlike other web frameworks like Laravel where you need to manually create database migrations which are separate from your models, Django allows you to define your models and then generate migrations automatically based on those models.

With the models defined in the previous step, all that's left is to create and apply the migrations to create the corresponding tables in the database. To generate the migrations, run:

```bash
python manage.py makemigrations
```

You should see output similar to:

```bash
Migrations for 'models_api':
  models_api/migrations/0001_initial.py
    + Create model ModelAuthor
    + Create model AIModel
    + Create model ModelBenchmark
    + Create model ModelPurchase
    + Create model UsageScenario
```

You can review the generated migration files in the `models_api/migrations` directory to see the actual migration operations that will be applied to the database based on your models.

Apply the migrations to create the corresponding tables in the Neon database:

```bash
python manage.py migrate
```

This command will create the tables for the models defined in the `models_api` app in the Neon database. The output should indicate that the migrations were applied successfully:

```
Operations to perform:
  Apply all migrations: admin, auth, contenttypes, models_api, sessions
Running migrations:
  Applying models_api.0001_initial... OK
```

You can verify that the tables were created in the Neon Console or by connecting to the database using a PostgreSQL client like `psql`.

### Implement serializers

With our models defined, we need to create serializers to convert model instances to JSON and vice versa. Serializers are a key component of Django REST Framework and are used to handle the conversion between complex data types (like Django model instances) and Python datatypes that can be easily rendered into JSON, XML, or other content types.

Start by creating a new file `models_api/serializers.py`:

```python
from rest_framework import serializers
from .models import ModelAuthor, AIModel, ModelPurchase, UsageScenario, ModelBenchmark

class ModelAuthorSerializer(serializers.ModelSerializer):
    class Meta:
        model = ModelAuthor
        fields = ['id', 'name', 'bio', 'contact_info', 'rating']

class AIModelSerializer(serializers.ModelSerializer):
    author = ModelAuthorSerializer(read_only=True)
    author_id = serializers.PrimaryKeyRelatedField(
        queryset=ModelAuthor.objects.all(), source='author', write_only=True
    )

    class Meta:
        model = AIModel
        fields = ['id', 'name', 'model_type', 'description', 'framework', 'version',
                  'download_url', 'price', 'tags', 'author', 'author_id']

class ModelPurchaseSerializer(serializers.ModelSerializer):
    class Meta:
        model = ModelPurchase
        fields = ['id', 'user', 'ai_model', 'purchase_date', 'price_paid', 'license_key', 'download_link']

class UsageScenarioSerializer(serializers.ModelSerializer):
    class Meta:
        model = UsageScenario
        fields = ['id', 'ai_model', 'title', 'description', 'code_snippet', 'usage_frequency']

class ModelBenchmarkSerializer(serializers.ModelSerializer):
    class Meta:
        model = ModelBenchmark
        fields = ['id', 'ai_model', 'metric_name', 'value', 'benchmark_date', 'hardware_used']
```

Let's break down each serializer to better understand their purpose:

1. `ModelAuthorSerializer`:
   - This serializer is used for the `ModelAuthor` model, it basically represents the author details.
   - It includes all fields of the model (`id`, `name`, `bio`, `contact_info`, `rating`).
   - By using `ModelSerializer`, we automatically get create and update functionality that matches the model fields.

2. `AIModelSerializer`:
   - This serializer is more complex due to its relationship with `ModelAuthor`.
   - We include a nested `author` field using `ModelAuthorSerializer(read_only=True)`. This means when serializing an `AIModel`, it will include all the author's details, but this field can't be used for writing (creating or updating).
   - We also include an `author_id` field, which is write-only. This allows clients to specify an author when creating or updating an `AIModel` by just providing the author's ID.
   - The `source='author'` in the `author_id` field tells DRF to use this field to set the `author` attribute of the `AIModel`.

3. `ModelPurchaseSerializer`:
   - This serializer includes all fields from the `ModelPurchase` model.
   - It will handle the serialization of purchase records, including details like the user, the AI model purchased, purchase date, and license information.

4. `UsageScenarioSerializer`:
   - This serializer corresponds to the `UsageScenario` model.
   - It includes all fields, allowing for the representation of different use cases or scenarios for AI models.

5. `ModelBenchmarkSerializer`:
   - This serializer is for the `ModelBenchmark` model.
   - It includes all fields, enabling the representation of performance benchmarks for AI models.

These serializers provide a powerful abstraction layer between your Python objects and the JSON representations of your API. They handle both serialization (Python to JSON) and deserialization (JSON to Python), including validation of incoming data.

By using `ModelSerializer`, we get a lot of functionality out of the box, such as automatically generated fields based on the model fields, default implementations of `create()` and `update()` methods, and validation based on model field types.

This approach reduces the amount of code we need to write while still providing flexibility where needed (like in the `AIModelSerializer` where we customize the author-related fields).

### Create API views

Now, let's create views to handle API requests. We'll use ViewSets for a clean, RESTful API structure.

Start by opening the `models_api/views.py` file and defining the views:

```python
from rest_framework import viewsets
from rest_framework.decorators import action
from rest_framework.response import Response
from .models import ModelAuthor, AIModel, ModelPurchase, UsageScenario, ModelBenchmark
from .serializers import (ModelAuthorSerializer, AIModelSerializer, ModelPurchaseSerializer,
                          UsageScenarioSerializer, ModelBenchmarkSerializer)

class ModelAuthorViewSet(viewsets.ModelViewSet):
    queryset = ModelAuthor.objects.all()
    serializer_class = ModelAuthorSerializer

    @action(detail=True, methods=['get'])
    def models(self, request, pk=None):
        author = self.get_object()
        models = author.models_uploaded.all()
        serializer = AIModelSerializer(models, many=True)
        return Response(serializer.data)

class AIModelViewSet(viewsets.ModelViewSet):
    queryset = AIModel.objects.all()
    serializer_class = AIModelSerializer

    @action(detail=True, methods=['get'])
    def usage_scenarios(self, request, pk=None):
        model = self.get_object()
        scenarios = model.usage_scenarios.all()
        serializer = UsageScenarioSerializer(scenarios, many=True)
        return Response(serializer.data)

    @action(detail=True, methods=['get'])
    def benchmarks(self, request, pk=None):
        model = self.get_object()
        benchmarks = model.benchmarks.all()
        serializer = ModelBenchmarkSerializer(benchmarks, many=True)
        return Response(serializer.data)

class ModelPurchaseViewSet(viewsets.ModelViewSet):
    queryset = ModelPurchase.objects.all()
    serializer_class = ModelPurchaseSerializer

class UsageScenarioViewSet(viewsets.ModelViewSet):
    queryset = UsageScenario.objects.all()
    serializer_class = UsageScenarioSerializer

class ModelBenchmarkViewSet(viewsets.ModelViewSet):
    queryset = ModelBenchmark.objects.all()
    serializer_class = ModelBenchmarkSerializer
```

This code defines ViewSets for each model, providing CRUD operations for all entities in our AI Model Marketplace. The `ModelAuthorViewSet` and `AIModelViewSet` include custom actions to retrieve related data (uploaded models for authors, usage scenarios and benchmarks for AI models).

### Configure URL routing

Create a new file `models_api/urls.py` to define the URL patterns for our API:

```python
from django.urls import path, include
from rest_framework.routers import DefaultRouter
from .views import (ModelAuthorViewSet, AIModelViewSet, ModelPurchaseViewSet,
                    UsageScenarioViewSet, ModelBenchmarkViewSet)

router = DefaultRouter()
router.register(r'authors', ModelAuthorViewSet)
router.register(r'models', AIModelViewSet)
router.register(r'purchases', ModelPurchaseViewSet)
router.register(r'usage-scenarios', UsageScenarioViewSet)
router.register(r'benchmarks', ModelBenchmarkViewSet)

urlpatterns = [
    path('', include(router.urls)),
]
```

This sets up the URL routing for our API views using DRF's `DefaultRouter`.

Now, update the project's main `urls.py` file to include the app's URLs:

```python
from django.contrib import admin
from django.urls import path, include

urlpatterns = [
    path('admin/', admin.site.urls),

    # Include the API URLs:
    path('api/', include('models_api.urls')),
]
```

This configuration makes our API accessible under the `/api/` path.

## Testing the API

With our API views and URL routing configured, we can now test the API by running the Django development server:

```bash
python manage.py runserver
```

If you were to now visit `http://localhost:8000/api/` in your browser, you would see a list of available API endpoints. This is the default behavior of DRF's `DefaultRouter`. If you were to visit `http://localhost:8000/api/authors/`, you would see a list of authors (which is currently empty), and so on for other endpoints. The web interface provided by DRF allows you to interact with the API endpoints directly from the browser like a simple API client, you can view, create, update, and delete records by interacting with the API endpoints directly.

Alternatively, you can use tools like `curl` or Postman to interact with the API programmatically. Here are some example `curl` commands to test the API:

1. Create a new model author:

   ```bash shouldWrap
   curl -X POST http://localhost:8000/api/authors/ -H "Content-Type: application/json" -d '{"name":"AI Innovations Inc.", "bio":"Leading AI research company", "contact_info":"contact@aiinnovations.com", "rating":4.8}'
   ```

   This will create a new model author with the specified details.

2. Create a new AI model:

   ```bash shouldWrap
   curl -X POST http://localhost:8000/api/models/ -H "Content-Type: application/json" -d '{"name":"AdvancedNLP", "model_type":"NLP", "description":"State-of-the-art NLP model", "framework":"PT", "version":"1.0", "download_url":"https://example.com/model", "price":"99.99", "tags":["NLP", "transformer"], "author_id":1}'
   ```

   This will create a new AI model associated with the author created in the previous step.

3. Get all AI models:

   ```bash
   curl http://localhost:8000/api/models/
   ```

4. Get usage scenarios for a specific AI model:

   ```bash
   curl http://localhost:8000/api/models/1/usage_scenarios/
   ```

5. Add a benchmark for an AI model:
   ```bash shouldWrap
   curl -X POST http://localhost:8000/api/benchmarks/ -H "Content-Type: application/json" -d '{"ai_model":1, "metric_name":"Accuracy", "value":0.95, "hardware_used":"NVIDIA A100 GPU"}'
   ```

## Conclusion

In this guide, we've built a RESTful API for a simple AI Model Marketplace using Django, Django REST Framework, and Neon's serverless Postgres. We covered setting up the project, defining models for AI models, authors, purchases, usage scenarios, and benchmarks, creating serializers and views, and configuring URL routing.

This API provides a solid foundation for an AI Model Marketplace platform. You can extend it with features like user authentication, advanced search and filtering, model versioning, and integration with payment systems. The combination of Django's powerful ORM, DRF's flexibility, and Neon's scalable Postgres database makes it easy to build and deploy robust, performant APIs for complex applications like AI model distribution platforms.

## Additional Resources

- [Django REST Framework Documentation](https://www.django-rest-framework.org/)
- [Django Documentation](https://docs.djangoproject.com/en/stable/)
- [Neon Documentation](/docs/)

<NeedHelp />


# Document Store using JSONB in Postgres

---
title: Document Store using JSONB in Postgres
subtitle: A step-by-step guide describing how to use Postgres as a document store using JSONB
author: vkarpov15
enableTableOfContents: true
createdAt: '2024-12-17T13:24:36.612Z'
updatedOn: '2024-12-17T13:24:36.612Z'
---

The JSONB type enables you to store and query nested JSON-like data in Postgres.
With JSONB, you can to store arbitrarily complex objects and arrays in your Postgres tables, as well as query based on properties in those objects and arrays.
You can even use GIN indexes to index nested properties within JSONB objects.

## Steps

- Set up a table with a JSONB column
- Insert and retrieve JSONB data
- Query based on JSONB fields
- Document store using Sequelize ORM
- Query arrays and objects in JSONB
- Type casting in JSONB queries
- Update and modify JSONB data
- Index JSONB fields using GIN indexes

## Set up a table with a JSONB column

To use Postgres as a document store, you can create a table with two columns: an `id` and a `data` property that is of type `JSONB`.
You can run the following `CREATE TABLE` statement in the Neon SQL Editor or from a client such as `psql` that is connected to Neon.

```sql
CREATE TABLE documents (
  id SERIAL PRIMARY KEY,
  data JSONB
);
```

JSONB columns can store any JSON object, including objects, arrays, and even nested objects.

## Insert and retrieve JSONB data

Run the following SQL to insert two new rows into the `documents` table. These rows will have `data` columns with slightly different properties: the first row has a `steps` property, and the second row has a nested object property, `author`.

```sql
INSERT INTO documents (data)
VALUES (
  '{
    "title": "Neon and JSONB",
    "body": "Using JSONB to store flexible data structures in Postgres.",
    "tags": ["Postgres", "Neon", "JSONB"],
    "steps": ["Set up a table with a JSONB column", "Insert and retrieve JSONB data"]
  }'
),
(
  '{
    "title": "Scaling Neon with Postgres",
    "body": "Learn how to scale your Neon instances with PostgreSQL features.",
    "tags": ["Neon", "Postgres", "scaling"],
    "author": { "name": "John Smith", "age": 30 }
  }'
);
```

You can then load rows from the `documents` collection by `id`. For example, you can load the "Neon and JSONB" row using the following query.

```sql
SELECT * FROM documents WHERE id = 1
```

## Query based on JSONB fields

You can also query based on properties in the JSONB column using the `->>` operator, which extracts values from the JSONB column.
For example, you can load all documents with a given `title` property using the following query.
Note the quotes around `title` in the `WHERE` clause.

```sql
SELECT * FROM documents WHERE data->>'title' = 'Neon and JSONB'
```

You can also query based on nested properties.
For example, the following query returns all documents whose `author` property is an object with a `name` property equal to 'John Smith'.

```sql
SELECT * FROM documents WHERE data->'author'->>'name' = 'John Smith'
```

## Document store using Sequelize ORM

Many developers use Postgres through an ORM, like [Sequelize](https://sequelize.org/) in Node.js.
ORMs often provide neat syntactic shortcuts for working with JSONB.
For example, the following Node.js code shows how you can connect to the existing `documents` table from previous examples using Sequelize.

```javascript
import Sequelize from 'sequelize';

const sequelize = new Sequelize(process.env.POSTGRES_CONNECTION_STRING);

const Document = sequelize.define(
  'Document',
  {
    data: {
      type: Sequelize.DataTypes.JSONB,
      allowNull: false,
    },
  },
  { tableName: 'documents', timestamps: false }
);
```

You can then create rows in the `documents` collection using the following:

```javascript
await Document.bulkCreate([
  {
    data: {
      title: 'Neon and JSONB',
      body: 'Using JSONB to store flexible data structures in Postgres.',
      tags: ['Postgres', 'Neon', 'JSONB'],
      author: { name: 'John Smith', age: 30 },
    },
  },
]);
```

Finally, you can find documents by the author's name using the following query.
Note that Sequelize takes care of converting `data.author.name` to `data->'author'->>'name'` under the hood.

```javascript
const documents = await Document.findAll({
  where: {
    'data.author.name': 'John Smith',
  },
});
```

You can read more about working with JSONB in [Sequelize](https://sequelize.org/docs/v7/querying/json/) [Prisma](https://www.prisma.io/docs/orm/prisma-client/special-fields-and-types/working-with-json-fields), and [Objection.js](https://vincit.github.io/objection.js/recipes/json-queries.html) on their respective documentation sites.

## Query arrays and objects in JSONB

Postgres has several operators that are useful for working with JSONB, including the `@>` operator, which checks if a given property contains the given value.
With arrays, `@>` can check whether the array contains a given value.
For example, the following query returns all documents whose `tags` property contains the string "JSONB".

```sql
SELECT * FROM documents WHERE data->'tags' @> '["JSONB"]'
```

Note that the right-hand side of `@>` is a JSON string.

With objects, `@>` can check whether the document contains one or more properties.
For example, the following query returns all documents whose `author` property has `author` equal to 'John Smith' and `age` equal to 30.

```sql
SELECT * FROM documents WHERE data->'author' @> '{"name":"John Smith","age":30}'
```

The query above is equivalent to this query:

```sql shouldWrap
SELECT * FROM documents WHERE data->'author'->>'name' = 'John Smith' AND data->'author'->>'age' = '30'
```

## Type casting in JSONB queries

Operators like `=` and `@>` are fairly easy to work with: they don't throw any errors if the JSONB property has the wrong type.
However, things get a bit more tricky if you want to find all documents whose `author`'s `age` property is greater than 25.
For example, this query throws an "operator does not exist" error:

```sql
SELECT *
FROM documents
WHERE (data -> 'author' ->> 'age') > 29;
```

You need to explicitly cast `age` to an `int` type for the above query to run, as shown here:

```sql
SELECT *
FROM documents
WHERE (data -> 'author' ->> 'age')::int > 29;
```

Depending on your data, you may need to add extra checks to avoid throwing an error if a document has an `age` property that can't be casted to an int.
The following query explicitly checks if `age` is a numeric string before attempting to cast to an `int`.

```sql
SELECT *
FROM documents
WHERE (data -> 'author' ->> 'age') ~ '^\d+$'
  AND (data -> 'author' ->> 'age')::int > 29;
```

## Update and modify JSONB data

You can also update individual properties within your JSONB document without overwriting the entire document using the [`jsonb_set()` function](/postgresql/postgresql-json-functions/postgresql-jsonb_set).
For example, the following code updates the `author.age` property to 35 for all documents whose `author.name` property is "John Smith".

```sql
UPDATE documents
SET data = jsonb_set(data, '{author,age}', '35'::jsonb)
WHERE data->'author'->>'name' = 'John Smith';
```

Note that `jsonb_set()` expects the nested property name separated by commas (`,`), not dots (`.`).

## Index JSONB fields using GIN indexes

[GIN indexes](https://www.postgresql.org/docs/current/gin-intro.html) allow you to index JSONB properties, which can make your queries faster as your data grows. This query shows how you can create a GIN index on the `data` property:

```sql
CREATE INDEX content_idx ON documents USING GIN (data);
```

To test out the GIN index, let's first insert 100 documents, 1 of which has `author.name` set to "John Smith", and 99 that do not. Sometimes Postgres decides to skip using indexes and use a sequential scan instead when a query matches most of the table.

```sql
DO $$
BEGIN
  FOR i IN 1..100 LOOP
    IF i = 1 THEN
      -- Insert the special document with author name 'John Smith'
      INSERT INTO documents (data)
      VALUES (
        '{
          "title": "Scaling Neon with Postgres",
          "body": "Learn how to scale your Neon instances with PostgreSQL features.",
          "tags": ["Neon", "Postgres", "scaling"],
          "author": { "name": "John Smith", "age": 30 }
        }'::jsonb
      );
    ELSE
      -- Insert general documents for other iterations
      INSERT INTO documents (data)
      VALUES (
        '{
          "title": "Neon and JSONB",
          "body": "Using JSONB to store flexible data structures in Postgres.",
          "tags": ["Postgres", "Neon", "JSONB"],
          "steps": ["Set up a table with a JSONB column", "Insert and retrieve JSONB data"]
        }'::jsonb
      );
    END IF;
  END LOOP;
END $$;
```

Next, you can run an `EXPLAIN ANALYZE` query (or just click the "Explain" button in the Neon SQL Editor) to confirm that Postgres is using your GIN index.

```sql
EXPLAIN ANALYZE
SELECT *
FROM documents
WHERE data @> '{"author": {"name": "John Smith"}}'::jsonb;
```

Note that the query above uses the containment operator `@>`, **not** `WHERE data->'author'->>'name' = 'John Smith'`. [GIN indexes only support certain operators with JSONB data](https://www.postgresql.org/docs/current/datatype-json.html#JSON-INDEXING), including `@>`.

The `EXPLAIN ANALYZE` query should produce output that resembles the following. The Bitmap Index Scan means that Postgres is using a GIN index rather than a sequential scan to answer the query.

```
Bitmap Heap Scan on documents  (cost=8.52..12.54 rows=1 width=245) (actual time=0.014..0.016 rows=3 loops=1)
  Recheck Cond: (data @> '{"author": {"name": "John Smith"}}'::jsonb)
  Heap Blocks: exact=1
  ->  Bitmap Index Scan on idx_documents_data  (cost=0.00..8.52 rows=1 width=0) (actual time=0.007..0.007 rows=3 loops=1)
        Index Cond: (data @> '{"author": {"name": "John Smith"}}'::jsonb)
Planning Time: 0.066 ms
Execution Time: 0.096 ms
```


# Building ASP.NET Core Applications with Neon and Entity Framework Core

---
title: Building ASP.NET Core Applications with Neon and Entity Framework Core
subtitle: Learn how to build a .NET application with Neon's serverless Postgres and Entity Framework Core
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-11-02T00:00:00.000Z'
updatedOn: '2024-11-02T00:00:00.000Z'
---

When building .NET applications, choosing the right database solution is an important step to good performance and scalability. Neon's serverless Postgres is a great choice for .NET developers, thanks to features like automatic scaling, branching, and connection pooling that integrate well with .NET's ecosystem.

In this guide, we'll walk through setting up a Neon database with a .NET application and explore best practices for connecting and interacting with Neon Postgres and structuring your application using Entity Framework Core.

## Prerequisites

- .NET 8.0 or later installed
- A [Neon account](https://console.neon.tech/signup)
- Basic familiarity with Entity Framework Core

## Setting Up Your Neon Database

1. Create a new Neon project from the [Neon Console](https://console.neon.tech)
2. Note your connection string from the connection details page

Your connection string will look similar to this:

```shell
postgres://[user]:[password]@[neon_hostname]/[dbname]?sslmode=require&channel_binding=require
```

## Creating a .NET Project with Neon Integration

With your Neon database set up, let's create a sample inventory management system to demonstrate Neon integration.

1. Create a new .NET Web API project:

   ```bash
   dotnet new webapi -n NeonInventoryApi
   ```

   This command creates a new Web API project with a basic structure including:
   - `Program.cs`: The entry point of your application
   - `appsettings.json`: Configuration files
   - `Properties/launchSettings.json`: Debug and launch configuration

   Then navigate to the project directory:

   ```bash
   cd NeonInventoryApi
   ```

2. Install the required NuGet packages:

   ```bash
   dotnet add package Npgsql.EntityFrameworkCore.PostgreSQL
   dotnet add package Microsoft.EntityFrameworkCore.Tools
   dotnet add package Microsoft.EntityFrameworkCore.Design
   ```

   These packages provide us with the following:
   - `Npgsql.EntityFrameworkCore.PostgreSQL`: The Postgres database provider for Entity Framework Core
   - `Microsoft.EntityFrameworkCore.Tools`: Command-line tools for migrations
   - `Microsoft.EntityFrameworkCore.Design`: Design-time tools for EF Core

3. Create a Models directory and add your entity models:

   ```bash
   mkdir Models
   ```

   Create a new file `Models/Product.cs`:

   ```csharp
   using System;
   using System.ComponentModel.DataAnnotations;

   namespace NeonInventoryApi.Models
   {
       public class Product
       {
           [Key]
           public int? Id { get; set; }

           [Required]
           [MaxLength(100)]
           public string? Name { get; set; }

           [Required]
           [MaxLength(20)]
           public string? SKU { get; set; }

           [Range(0, 999999.99)]
           public decimal? Price { get; set; }

           [Range(0, int.MaxValue)]
           public int? StockLevel { get; set; }

           public DateTime LastRestocked { get; set; }

           // Additional validation attributes
           [Timestamp]
           public byte[]? Version { get; set; }

       }
   }
   ```

   Our `Product` model includes the following:
   - Data annotations for validation
   - A unique identifier (`Id`)
   - Basic product information fields
   - Optimistic concurrency control (`Version`)
   - Comments indicating where to add relationships

4. Create a Data directory for your database context, this is where you will define your `DbContext` which represents your database schema:

   ```bash
   mkdir Data
   ```

   Create a new file `Data/InventoryContext.cs` with the following content:

   ```csharp
   using Microsoft.EntityFrameworkCore;
   using NeonInventoryApi.Models;
   using System.Reflection;

   namespace NeonInventoryApi.Data
   {
       public class InventoryContext : DbContext
       {
           public InventoryContext(DbContextOptions<InventoryContext> options)
               : base(options)
           { }

           public DbSet<Product> Products { get; set; }

           protected override void OnModelCreating(ModelBuilder modelBuilder)
           {
               // Configure the Product entity
               modelBuilder.Entity<Product>(entity =>
               {
                   // Create a unique index on SKU
                   entity.HasIndex(p => p.SKU)
                         .IsUnique();

                   // Configure the Name property
                   entity.Property(p => p.Name)
                         .IsRequired()
                         .HasMaxLength(100);

                   // Configure the Price property
                   entity.Property(p => p.Price)
                         .HasPrecision(10, 2);

                   // Add a default value for LastRestocked
                   entity.Property(p => p.LastRestocked)
                         .HasDefaultValueSql("CURRENT_TIMESTAMP");
               });

               modelBuilder.ApplyConfigurationsFromAssembly(Assembly.GetExecutingAssembly());

               base.OnModelCreating(modelBuilder);
           }
       }
   }
   ```

   The `DbContext` includes:
   - Entity configuration using Fluent API
   - Precision settings for decimal values
   - Default value configurations
   - Index definitions
   - Support for separate configuration classes

We've set up the basic structure for our application. Next, we'll configure the database connection and implement the repository pattern for database operations.

## Configuring Database Connection

With the database context in place, we need to configure the connection to our Neon database. Let's set this up securely.

### Basic Configuration

Update `Program.cs` to include the database context:

```csharp
using NeonInventoryApi.Data;

var connectionString = builder.Configuration.GetConnectionString("NeonConnection");

builder.Services.AddDbContext<InventoryContext>(options =>
    options.UseNpgsql(builder.Configuration.GetConnectionString("NeonConnection")));
```

### Managing Connection Strings Securely

There are two main approaches to storing your connection string securely:

1. **Development**: Use `appsettings.Development.json` for local development:

```json
{
  "ConnectionStrings": {
    "NeonConnection": "Server=your-neon-hostname;Database=neondb;User Id=your-username;Password=your-password;SSL Mode=Require;Trust Server Certificate=true"
  }
}
```

2. **Production**: Use environment variables:

```csharp
// Program.cs
var connectionString = Environment.GetEnvironmentVariable("NEON_CONNECTION_STRING")
    ?? builder.Configuration.GetConnectionString("NeonConnection");
```

That way, you can set the `NEON_CONNECTION_STRING` environment variable in your production environment to securely store your connection string.

As an alternative, you can use the `Azure Key Vault` to store your connection string securely. To learn more about this approach, check out the [Azure Key Vault documentation](https://learn.microsoft.com/en-us/azure/key-vault/general/basic-concepts).

### Testing the Configuration

To verify everything is working correctly, start your application:

```bash
dotnet run
```

If everything is set up correctly, you should see a message indicating that the application is running and listening on a specific port.

## Implementing Repository Pattern

The repository pattern acts as an abstraction layer between your application logic and data access code. This pattern helps maintain clean separation of concerns and makes your code more testable and maintainable.

In our inventory system, we'll implement this pattern to handle all database operations related to products.

First, let's define the interface that specifies what operations our repository can perform. Create a new file `Repositories/IProductRepository.cs` with the following content:

```csharp
using NeonInventoryApi.Models;

namespace NeonInventoryApi.Repositories
{
    public interface IProductRepository
    {
        Task<Product> GetByIdAsync(int id);
        Task<IEnumerable<Product>> GetAllAsync();
        Task<Product> CreateAsync(Product product);
        Task UpdateAsync(Product product);
        Task DeleteAsync(int id);
    }
}
```

This interface defines the contract for our repository. That way all implementations will provide these basic CRUD (Create, Read, Update, Delete) operations. Using an interface allows us to easily swap implementations or create mock versions for testing.

Next, let's implement the repository, starting with the `ProductRepository` class in `Repositories/ProductRepository.cs`:

```csharp
using System.Collections.Generic;
using System.Threading.Tasks;
using Microsoft.EntityFrameworkCore;
using NeonInventoryApi.Data;
using NeonInventoryApi.Models;

namespace NeonInventoryApi.Repositories{
    public class ProductRepository : IProductRepository
    {
        private readonly InventoryContext _context;

        public ProductRepository(InventoryContext context)
        {
            _context = context;
        }

        public async Task<Product> GetByIdAsync(int id)
        {
            return await _context.Products
                .FirstOrDefaultAsync(p => p.Id == id);
        }

        public async Task<IEnumerable<Product>> GetAllAsync()
        {
            return await _context.Products.ToListAsync();
        }

        public async Task<Product> CreateAsync(Product product)
        {
            product.LastRestocked = product.LastRestocked.ToUniversalTime();
            _context.Products.Add(product);
            await _context.SaveChangesAsync();
            return product;
        }

        public async Task UpdateAsync(Product product)
        {
            product.LastRestocked = product.LastRestocked.ToUniversalTime();
            _context.Products.Update(product);
            _context.Entry(product).State = EntityState.Modified;
            await _context.SaveChangesAsync();
        }

        public async Task DeleteAsync(int id)
        {
            var product = await GetByIdAsync(id);
            if (product != null)
            {
                _context.Products.Remove(product);
                await _context.SaveChangesAsync();
            }
        }
    }
}
```

Each method in the repository serves a specific purpose:

- `GetByIdAsync`: Retrieves a single product by its ID using asynchronous operations
- `GetAllAsync`: Returns all products from the database as an enumerable collection
- `CreateAsync`: Adds a new product to the database and returns the created product
- `UpdateAsync`: Modifies an existing product's information in the database
- `DeleteAsync`: Removes a product from the database by its ID

To use this repository in your application, register it with the dependency injection container in `Program.cs`:

```csharp
builder.Services.AddScoped<IProductRepository, ProductRepository>();
```

This registration makes the repository available throughout your application.

Then you can inject it into your controllers or services, create a new controller `Controllers/ProductsController.cs`:

```csharp
using Microsoft.AspNetCore.Mvc;
using NeonInventoryApi.Models;
using NeonInventoryApi.Repositories;
using System.Collections.Generic;
using System.Threading.Tasks;

namespace NeonInventoryApi.Controllers
{
    [ApiController]
    [Route("api/[controller]")]
    public class ProductsController : ControllerBase
    {
        private readonly IProductRepository _repository;

        public ProductsController(IProductRepository repository)
        {
            _repository = repository;
        }

        [HttpGet]
        public async Task<ActionResult<IEnumerable<Product>>> GetAll()
        {
            return Ok(await _repository.GetAllAsync());
        }

        [HttpPost]
        public async Task<ActionResult<Product>> Create([FromBody] Product product)
        {
            var createdProduct = await _repository.CreateAsync(product);
            return CreatedAtAction(nameof(GetById), new { id = createdProduct.Id }, createdProduct);
        }

        [HttpGet("{id}")]
        public async Task<ActionResult<Product>> GetById(int id)
        {
            var product = await _repository.GetByIdAsync(id);
            if (product == null)
                return NotFound();

            return Ok(product);
        }

        [HttpPut("{id}")]
        public async Task<ActionResult> Update(int id, [FromBody] Product product)
        {
            if (id != product.Id)
                return BadRequest();

            await _repository.UpdateAsync(product);
            return NoContent();
        }

        [HttpDelete("{id}")]
        public async Task<ActionResult> Delete(int id)
        {
            await _repository.DeleteAsync(id);
            return NoContent();
        }
    }
}
```

The controller uses the repository to interact with the database. Each action corresponds to a CRUD operation and returns appropriate HTTP status codes based on the operation's success.

The repository pattern is particularly useful when working with Postgres as it provides a single place to implement database-specific optimizations and connection handling strategies.

After implementing the repository pattern, make sure to register the controllers in `Program.cs`:

```csharp
app.MapControllers();
```

With the repository pattern in place, we are nearly ready to start using our Neon database. Before that, let's add some sample data to the database using migrations.

## Migrations

Similar to other web frameworks, Entity Framework Core uses migrations to keep track of changes to your database schema. With migrations, you can incrementally apply schema updates as your data models evolve over time.

### Installing `dotnet-ef`

Before creating migrations, make sure you have the `dotnet-ef` tool is installed. This tool provides command-line capabilities for managing Entity Framework migrations and database updates.

Install `dotnet-ef` globally by running:

```bash
dotnet tool install --global dotnet-ef
```

> **Note**: If you encounter any issues after installation, make sure your environment's `PATH` includes the directory where .NET global tools are installed. It is usually `~/.dotnet/tools` on macOS and Linux. You can add this to your `PATH` temporarily with the following command, or add it to your shell configuration file (like `.bashrc` or `.zshrc`) to make it permanent:

```bash
export PATH="$PATH:$HOME/.dotnet/tools"
```

### Creating and Applying Migrations

Once `dotnet-ef` is installed, you can create a migration to define your database schema based on your `DbContext` and entity classes.

1. The following command generates the initial migration files, representing the schema of your database:

   ```bash
   dotnet ef migrations add InitialCreate
   ```

   This command creates a new folder called `Migrations` (if it doesn't already exist) and generates files that contain code to create your database tables. It examines your `DbContext` and entity classes to determine the schema, so that you don't have to write SQL manually.

2. Now, apply the migration to your actual database:

   ```bash
   dotnet ef database update
   ```

   This command executes the generated migration code against the database, creating the necessary tables and constraints based on your model definitions.

After these steps, your database will be fully synchronized with your data model, and you're ready to start using it in your application.

Now if you run your application and navigate to `http://localhost:5221/api/products`, you should see an empty array `[]` as we haven't added any products yet.

## Testing CRUD Operations

With our API and database set up, we’re ready to test the CRUD operations. We’ll use simple HTTP requests to add, retrieve, update, and delete products from our Neon database.

### 1. Adding a New Product

To add a new product, send a `POST` request to `http://localhost:5221/api/products`. Here’s an example using `curl`:

```bash
curl -X POST http://localhost:5221/api/products \
     -H "Content-Type: application/json" \
     -d '{
           "name": "Sample Product",
           "sku": "SP123",
           "price": 29.99,
           "stockLevel": 100,
           "lastRestocked": "2024-01-01T00:00:00"
         }'
```

> Alternatively, you can use Postman by setting up a `POST` request to the same URL, setting the header `Content-Type` to `application/json`, and adding the JSON body.

If the request is successful, the API will return the newly created product with an auto-generated `Id`.

### 2. Retrieving All Products

To retrieve all products, send a `GET` request to `http://localhost:5221/api/products`:

```bash
curl -X GET http://localhost:5221/api/products
```

This should return a list of all products in the database. If you just added one product, you’ll see an array with that single product.

### 3. Retrieving a Product by ID

To retrieve a specific product, send a `GET` request to `http://localhost:5221/api/products/{id}`, replacing `{id}` with the actual product ID. For example:

```bash
curl -X GET http://localhost:5221/api/products/1
```

This request will return the product with the specified `Id`. If no product is found, it may return a `404 Not Found`.

### 4. Updating a Product

To update an existing product, send a `PUT` request to `http://localhost:5221/api/products/{id}`. Include the updated information in the request body. Here’s an example:

```bash
curl -X PUT http://localhost:5221/api/products/1 \
     -H "Content-Type: application/json" \
     -d '{
           "id": 1,
           "name": "Updated Product",
           "sku": "SP123",
           "price": 24.99,
           "stockLevel": 120,
           "lastRestocked": "2024-01-02T00:00:00"
         }'
```

This request updates the product with `Id = 1`. The API will return the updated product information if the operation succeeds.

### 5. Deleting a Product

To delete a product, send a `DELETE` request to `http://localhost:5221/api/products/{id}`, replacing `{id}` with the product’s actual ID:

```bash
curl -X DELETE http://localhost:5221/api/products/1
```

If successful, the API will delete the product from the database and return a status code indicating success.

## Conclusion

Connecting .NET applications to Neon gives you a strong and scalable database setup. By following these steps and using features like connection pooling and automatic scaling, you can create applications that perform well even as they grow.

As next steps, consider adding more features to your application, such as authentication and authorization. You can also explore advanced Neon features like branching and data replication to enhance your application's performance and reliability.

For more details, check out:

- [Neon Documentation](/docs)
- [Entity Framework Core Documentation](https://docs.microsoft.com/en-us/ef/core/)
- [Npgsql Documentation](https://www.npgsql.org/doc/index.html)

<NeedHelp />


# Connecting .NET Applications to Neon Database

---
title: Connecting .NET Applications to Neon Database
subtitle: Learn how to connect your .NET applications to Neon's serverless Postgres database
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-11-02T00:00:00.000Z'
updatedOn: '2024-11-02T00:00:00.000Z'
---

In this guide, we'll walk through the process of connecting a .NET application to Neon Postgres, exploring best practices for connection management and basic performance optimization.

## Prerequisites

Before we begin, make sure you have:

- .NET 8.0 or later installed
- A [Neon account](https://console.neon.tech/signup)
- Basic familiarity with .NET development

## Setting Up Your Neon Database

First, let's create a Neon database that we'll connect to from our .NET application.

1. Log in to the [Neon Console](https://console.neon.tech)
2. Click "New Project" and follow the creation wizard
3. Once created, you'll see your connection details. Your connection string will look like this:

```
postgres://[user]:[password]@[neon_hostname]/[dbname]?sslmode=require&channel_binding=require
```

Save these details - you'll need them when setting up your .NET application.

## Creating a Basic .NET Project

Let's create a simple .NET project and add the necessary dependencies to connect to your Neon database.

Open your terminal and run:

```bash
dotnet new console -n NeonDemo
cd NeonDemo
```

This creates a new console application named "NeonDemo" and navigates to the project directory.

The directory will contain a `NeonDemo.csproj` file, which is the project file for your application, similar to a `package.json` file in Node.js if you are coming from a JavaScript background.

Next, we need to add the required package for Postgres connectivity. We can do this using the `dotnet add package` command:

```bash
dotnet add package Npgsql.EntityFrameworkCore.PostgreSQL
```

This package provides the Postgres database provider for Entity Framework Core, Microsoft's object-database mapper for .NET applications and is a popular choice for working with Postgres databases.

## Configuring Database Connection

There are several ways to manage your database connection string. Let's look at the most common approaches for .NET applications and how to handle sensitive information securely.

### Basic Connection Setup

For local development, start by creating an `appsettings.json` file in your project root. This file will store your connection string:

```json
{
  "ConnectionStrings": {
    "NeonConnection": "Host=your-neon-hostname;Database=neondb;Username=your-username;Password=your-password;SSL Mode=Require;Trust Server Certificate=true"
  }
}
```

This approach works well for development but isn't recommended for production use since it stores sensitive information in a file that might be committed to source control or exposed in other ways.

### Using Environment Variables

For production environments, it's better to use environment variables. Here's how to implement this:

```csharp
var connectionString = Environment.GetEnvironmentVariable("NEON_CONNECTION_STRING")
    ?? builder.Configuration.GetConnectionString("NeonConnection");
```

This code first checks for an environment variable, falling back to the configuration file if not found. This gives you flexibility in different environments while keeping sensitive data secure.

You can set the `NEON_CONNECTION_STRING` environment variable in your production environment, or use a tool like [Azure Key Vault](https://azure.microsoft.com/en-us/services/key-vault/) to manage secrets.

## Understanding Connection Pooling

Connection pooling helps improve performance by maintaining and reusing database connections. In many cases, too many connections can lead to performance issues, so it's important to manage them effectively.

There are two levels of connection pooling available when working with Neon: [Neon's built-in connection pooling service](/docs/connect/connection-pooling) (PgBouncer) and application-side pooling through Npgsql.

### Neon's Connection Pooling

Neon uses PgBouncer to provide connection pooling at the infrastructure level, supporting up to 10,000 concurrent connections. To use Neon's pooled connections, select the "Pooled connection" option in your project's connection settings. Your connection string will look like this:

```
postgres://[user]:[password]@[pooled-hostname].pool.[region].neon.tech/[dbname]?sslmode=require&channel_binding=require
```

However, using a pooled connection string for database migrations can be prone to errors. For this reason, it is recommended to use a direct (non-pooled) connection when performing database migrations. For more information about direct and pooled connections, see [Connection pooling](/docs/connect/connection-pooling).

### Application-Side Pooling

While Neon handles connection pooling at the infrastructure level, you can also configure Npgsql's built-in connection pooling for additional control:

```csharp
using Npgsql;

var connectionStringBuilder = new NpgsqlConnectionStringBuilder(
    builder.Configuration.GetConnectionString("NeonConnection"))
{
    MaxPoolSize = 50,               // Maximum number of connections in the pool
    MinPoolSize = 5,                // Minimum number of connections to maintain
    ConnectionIdleLifetime = 300,   // How long in seconds an idle connection is kept
    ConnectionPruningInterval = 60, // How often to check for idle connections
    Pooling = true,                 // Enable connection pooling
    Timeout = 30,                   // Connection timeout in seconds
    CommandTimeout = 30             // Command timeout in seconds
};

services.AddDbContext<InventoryContext>(options =>
    options.UseNpgsql(connectionString));
```

This code snippet configures the connection pooling settings for Npgsql. You can adjust the `MaxPoolSize`, `MinPoolSize`, `ConnectionIdleLifetime`, and other parameters to suit your application's needs. In most cases, you can rely on Neon's built-in connection pooling service for optimal performance.

### PgBouncer vs Application-Side Pooling

Both pooling methods have their advantages, this is also valid for other frameworks and languages:

- **Neon's PgBouncer**: Handles connection pooling at the infrastructure level, reducing overhead and managing connections efficiently across multiple application instances.
- **Npgsql Pooling**: Provides fine-grained control at the application level and can be useful for specific application requirements.

For most applications, using Neon's connection pooling service is sufficient. You can consider configuring application-side pooling if you have specific requirements or need additional control over connection management.

As mentioned earlier, when performing database migrations, it's recommended to use a direct connection to avoid potential issues with pooled connections.

## Basic Performance Optimization

Besides connection pooling, let's look at some simple ways to optimize your database operations. Most of these optimizations apply to all types of frameworks, but we'll focus on Entity Framework Core, a popular ORM for .NET applications.

### Using Async Operations

Always use async methods for database operations. This improves application responsiveness by not blocking threads while waiting for database operations to complete:

```csharp
// Example of a basic repository method using async/await
public async Task<List<User>> GetUsersAsync()
{
    using var context = new YourDbContext();
    // ToListAsync() is non-blocking
    return await context.Users.ToListAsync();
}
```

This approach is more efficient than synchronous operations, especially in high-traffic applications. The `ToListAsync` method is provided by Entity Framework Core and returns a `Task<List<T>>`.

### Batch Operations

When working with multiple records, use batch operations to reduce database round trips.

For example, instead of saving one record at a time like this:

```csharp
foreach (var item in items)
{
    context.Items.Add(item);
    await context.SaveChangesAsync();
}
```

Which results in multiple database calls, causing performance overhead. You can batch the operations like this:

```csharp
context.Items.AddRange(items);
await context.SaveChangesAsync();
```

This approach is much more efficient as it reduces the number of database calls. Entity Framework Core will automatically generate a single `INSERT` statement for all the records.

This is especially useful when dealing with large datasets and large numbers of records.

### Use NoTracking for Read-Only Operations

The tracking feature in Entity Framework Core keeps track of changes to entities, which can be useful for update and delete operations.

However, for read-only operations where you don't need to track changes, you can disable tracking to improve performance:

```csharp
public async Task<List<User>> GetUsersAsync()
{
    using var context = new YourDbContext();
    return await context.Users.AsNoTracking().ToListAsync();
}
```

The `AsNoTracking` method tells Entity Framework Core not to track changes to entities, which can improve performance for read-only operations.

### Avoid N+1 Queries

Avoid N+1 queries, a common performance issue in ORMs where multiple queries are executed for each record in a collection. This can lead to a large number of database calls and performance degradation.

This also applies to Entity Framework Core, especially when using lazy loading. To avoid N+1 queries, use eager loading or explicit loading:

```csharp
// Eager loading
var users = context.Users.Include(u => u.Orders).ToList();

// Explicit loading
var users = context.Users.ToList();
foreach (var user in users)
{
    context.Entry(user).Collection(u => u.Orders).Load();
}
```

Eager loading fetches related entities in a single query, while explicit loading allows you to load related entities on demand.

## Best Practices

When working with Neon in your .NET applications, and any database in general, it's important to carefully consider best practices for connection management, error handling, and security.

### Proper Connection Management

Use `using` statements when working with database connections to make sure they're properly disposed of. This prevents connection leaks and helps maintain optimal pool performance:

```csharp
public async Task<User> GetUserByIdAsync(int id)
{
    // The using statement ensures the context is disposed properly
    using var context = new YourDbContext();
    return await context.Users.FindAsync(id);
}
```

For dependency injection scenarios, you can implement a scoped context:

```csharp
builder.Services.AddScoped<YourDbContext>();
```

For more information, consider the [Working with DbContext](https://docs.microsoft.com/en-us/ef/core/dbcontext-configuration/) documentation.

### Error Handling and Retry Logic

For production applications, it is important to handle errors gracefully and provide appropriate logging. For example, you can implement retry logic to handle failures that can occur in a distributed system:

```csharp
builder.Services.AddDbContext<YourDbContext>(options =>
{
    options.UseNpgsql(connectionString, npgsqlOptions =>
    {
        npgsqlOptions.EnableRetryOnFailure(
            maxRetryCount: 3,
            maxRetryDelay: TimeSpan.FromSeconds(30),
            errorCodesToAdd: null);
    });
});
```

In addition to retry logic, you can also consider adding logging and exception handling to your database operations:

```csharp
public async Task<User> GetUserByIdAsync(int id)
{
    try
    {
        using var context = new YourDbContext();
        return await context.Users.FindAsync(id);
    }
    catch (PostgresException ex)
    {
        _logger.LogError(ex, "Error retrieving user {UserId}", id);
        throw;
    }
}
```

## Monitoring Database Performance

Neon provides built-in monitoring capabilities, which you can complement with application-side monitoring in your .NET application.

### Using Neon's Monitoring Dashboard

The Neon Console includes a [monitoring dashboard](/docs/introduction/monitoring) that provides real-time insights into your database's performance. You can access it from the sidebar in the Neon Console and view some key metrics like CPU usage, memory, IOPS, and more.

Your Neon plan determines the range of metrics and historical data available. The monitoring dashboard makes it easy to identify performance trends and potential issues before they impact your application. Regular monitoring of these metrics helps you make informed decisions about scaling and optimization.

### The pg_stat_statements extension

In addition to the monitoring dashboard, you can use the `pg_stat_statements` extension to track query performance and identify slow queries. This extension provides detailed statistics about query execution, including the number of times a query is executed, the total execution time, and more.

You can check out the [pg_stat_statements documentation](/docs/extensions/pg_stat_statements) for more information on how to enable and use this extension.

This is very useful for identifying performance bottlenecks and optimizing your database queries. For example, once you identify slow queries, you can use tools like `EXPLAIN` to analyze query plans and then consider adding indexes or rewriting queries to improve performance. For more information, read the [Performance tips for Neon Postgres](/blog/performance-tips-for-neon-postgres) blog post.

### Application-Side Monitoring

Beyond Neon's monitoring capabilities, you can also implement application-side monitoring in your .NET application to track database operations and performance.

You can use the built-in health checks feature in ASP.NET Core to monitor database connectivity and performance. Here's an example of adding health checks for a Neon database:

```csharp
// Add health checks for database monitoring
builder.Services.AddHealthChecks()
    .AddNpgsql(
        connectionString,
        name: "neon-db",
        tags: new[] { "db", "postgres", "neon" },
        timeout: TimeSpan.FromSeconds(3));

// Add a health check endpoint
app.MapHealthChecks("/health/database");
```

Then, configure appropriate logging levels to track database operations:

```json
{
  "Logging": {
    "LogLevel": {
      "Default": "Information",
      "Microsoft.EntityFrameworkCore": "Warning",
      "Npgsql": "Information"
    },
    "Console": {
      "TimestampFormat": "yyyy-MM-dd HH:mm:ss"
    }
  }
}
```

And after that, add basic operation logging in your data access layer:

```csharp
public class DatabaseService
{
    private readonly ILogger<DatabaseService> _logger;

    public async Task ExecuteOperation()
    {
        try
        {
            // Your database operation
            _logger.LogInformation("Database operation completed successfully");
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Database operation failed");
            throw;
        }
    }
}
```

For more information on logging and monitoring in .NET applications, check out the [Logging in .NET Core](https://docs.microsoft.com/en-us/aspnet/core/fundamentals/logging/) documentation.

## Conclusion

You now have the foundational knowledge needed to connect your .NET application to Neon Postgres. We've covered the basics of setting up connections, implementing pooling, and following best practices for performance and security.

As a next step, consider checking out the [Building ASP.NET Core Applications with Neon and Entity Framework Core](/guides/dotnet-neon-entity-framework) guide for a more detailed example of integrating Neon with Entity Framework Core.

For more information, check out:

- [Neon Documentation](/docs)
- [Npgsql Documentation](https://www.npgsql.org/doc/index.html)
- [Entity Framework Core Documentation](https://docs.microsoft.com/en-us/ef/core/)

<NeedHelp />


# Drizzle with Local and Serverless Postgres

---
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-12-16T00:00:00.000Z'
updatedOn: '2024-12-16T00:00:00.000Z'
title: Drizzle with Local and Serverless Postgres
subtitle: A step-by-step guide to configure Drizzle ORM for local and serverless Postgres.
---

Drizzle is an ORM that simplifies database interactions in JavaScript applications. This guide will walk you through the steps to set up Drizzle to work with both local and hosted Postgres databases, and run schema migrations against them.

## Prerequisites

- **Install Docker Desktop**: To set up a local Postgres database, ensure you have [Docker Desktop](https://www.docker.com/products/docker-desktop/) installed on your machine.
- A [Neon](https://console.neon.tech) account to set up a hosted Postgres.

## Create a new Next.js application

Let’s get started by creating a new Next.js project with the following command:

```shell shouldWrap
npx create-next-app@latest my-app
```

When prompted, choose:

- `Yes` when prompted to use TypeScript.
- `No` when prompted to use ESLint.
- `Yes` when prompted to use Tailwind CSS.
- `No` when prompted to use `src/` directory.
- `Yes` when prompted to use App Router.
- `No` when prompted to customize the default import alias (`@/*`).

Once that is done, move into the project directory, and start the application in development mode with the following command:

```shell shouldWrap
cd my-app
npm run dev
```

## Setting Up a Local Postgres

You will use Docker to run your instance of local Postgres. First, create a `docker-compose.yml` file in the root directory with the following code:

```yaml
services:
  postgres:
    image: 'postgres:latest'
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres
    ports:
      - '5432:5432'
  pg_proxy:
    image: ghcr.io/neondatabase/wsproxy:latest
    environment:
      APPEND_PORT: 'postgres:5432'
      ALLOW_ADDR_REGEX: '.*'
      LOG_TRAFFIC: 'true'
    ports:
      - '5433:80'
    depends_on:
      - postgres
```

In the YAML configuration file above, you have set up two services using Docker: a PostgreSQL database and a WebSocket proxy for Neon. The `postgres` service uses the latest PostgreSQL image and configures the necessary environment variables for the database user, password, and database name. It exposes port 5432 for database connections. The `pg_proxy` service uses a WebSocket proxy image, allowing connections to the PostgreSQL service through port `5433`.

Next, spin up the services in Docker via the following command:

```shell shouldWrap
docker-compose up -d
```

Use the connection string (`postgres://postgres:postgres@localhost:5432/postgres`) of the Postgres instance created as an environment variable, designated as `LOCAL_POSTGRES_URL` in the `.env` file.

## Setting Up a Serverless Postgres

To set up Neon serverless Postgres, go to the [Neon console](https://console.neon.tech/app/projects) and create a new project. Once your project is created, you will receive a connection string that you can use to connect to your Neon database. The connection string will look like this:

```bash
postgresql://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>?sslmode=require&channel_binding=require
```

Replace `<user>`, `<password>`, `<endpoint_hostname>`, `<port>`, and `<dbname>` with your specific details.

Use this connection string as an environment variable designated as `POSTGRES_URL` in the `.env` file.

## Integrate Drizzle with Next.js

To use Drizzle with Next.js and Neon, install the necessary packages via the following command:

```bash
npm install ws postgres drizzle-orm @neondatabase/serverless
npm install -D @types/ws drizzle-kit
```

Note that the installation of the `postgres` package is important, as in local environments, Drizzle will automatically use that to apply the schema migrations to the Postgres. In production, Drizzle will use Neon’s serverless driver to apply schema migrations to Neon’s hosted Postgres instance.

Then, create a file named `drizzle.server.ts` with the following code:

```typescript
// File: drizzle.server.ts

import { neonConfig, Pool } from '@neondatabase/serverless';
import { drizzle } from 'drizzle-orm/neon-serverless';
import { WebSocket } from 'ws';

const connectionString =
  process.env.NODE_ENV === 'production' ? process.env.POSTGRES_URL : process.env.LOCAL_POSTGRES_URL;

if (process.env.NODE_ENV === 'production') {
  neonConfig.webSocketConstructor = WebSocket;
  neonConfig.poolQueryViaFetch = true;
} else {
  neonConfig.wsProxy = (host) => `${host}:5433/v1`;
  neonConfig.useSecureWebSocket = false;
  neonConfig.pipelineTLS = false;
  neonConfig.pipelineConnect = false;
}

const pool = new Pool({ connectionString });

export default drizzle(pool);
```

The code above determines the connection string based on the environment variable (production or local). In production, it configures WebSocket settings for Neon, while in local development, it sets up a WebSocket proxy. Finally, it creates a connection pool and exports a Drizzle instance for database interactions.

Next, create a file named `drizzle.config.ts` with the following code:

```typescript
// File: drizzle.config.ts

import { defineConfig } from 'drizzle-kit';

const url =
  process.env.NODE_ENV === 'production' ? process.env.POSTGRES_URL : process.env.LOCAL_POSTGRES_URL;
if (!url)
  throw new Error(
    `Connection string to ${process.env.NODE_ENV ? 'Neon' : 'local'} Postgres not found.`
  );

export default defineConfig({
  dialect: 'postgresql',
  dbCredentials: { url },
  schema: './lib/schema.ts',
});
```

The code above determines the Postgres connection string to be used based on the environment (production or local) for database operations, such as running schema migrations.

## Running Schema Migrations

Now, you can manage both the local and production environments and select the respective (local or production) Postgres to run the Drizzle migrations via the following commands:

```bash
npx drizzle-kit generate
npx drizzle-kit migrate
```

## Source code

You can find the source code for the application described in this guide on GitHub.

<DetailIconCards>

<a href="https://github.com/neondatabase/examples/tree/main/with-nextjs-drizzle-local-vercel" description="Drizzle with Local and Serverless Postgres" icon="github">Drizzle with Local and Serverless Postgres</a>

</DetailIconCards>

<NeedHelp />


# Automated E2E Testing with Neon Branching and Playwright

---
title: Automated E2E Testing with Neon Branching and Playwright
subtitle: Learn how to use GitHub Actions to create isolated database branches for running Playwright tests against your schema changes
enableTableOfContents: true
author: dhanush-reddy
createdAt: '2025-09-03T00:00:00.000Z'
---

End-to-end (E2E) testing is crucial for ensuring application quality, but it becomes complex when database changes are involved. Running tests that depend on a specific schema against a shared staging environment can lead to flaky results and development bottlenecks.

Database branching solves this problem by creating isolated database environments for each feature branch, perfectly mirroring your code branching strategy. This guide demonstrates how to combine the power of Neon's instant database branching with Playwright and GitHub Actions to create a fully automated E2E testing pipeline.

You will build a Next.js Todo application and configure a workflow that, for every pull request:

- Creates a new, isolated database branch.
- Applies schema migrations to that branch.
- Builds and runs the application against the new branch.
- Executes a full suite of Playwright tests.
- Posts a schema diff summary directly in the pull request.
- Cleans up resources and applies migrations to the production database upon merging.

By the end of this guide, you'll have a CI/CD pipeline where database-dependent E2E tests are run safely and reliably for every change, giving you the confidence to ship features faster. This concept can be extended to any E2E testing framework, not just Playwright.

## Prerequisites

- A [Neon account](https://console.neon.tech)
- A [GitHub account](https://github.com/)
- Node.js installed on your machine

## Setting up your Neon database

1.  Create a new Neon project from the [Neon Console](https://console.neon.tech). For instructions, see [Create a project](/docs/manage/projects#create-a-project).
2.  Navigate to the **Connection Details** page and copy your database connection string. You will need this later.

    Your connection string will look something like this:

    ```text
    postgres://[user]:[password]@[neon_hostname]/[dbname]?sslmode=require&channel_binding=require
    ```

## Set up the project

This guide uses Playwright with Next.js, but the concepts can be easily adapted to other frameworks by following the Playwright-specific steps.

Clone the [Neon Playwright Example](https://github.com/neondatabase-labs/neon-playwright-example) repository. You will use this as a starting point for your tests. The repository contains a simple Todo app built with Next.js and TypeScript using Drizzle ORM. It has Playwright tests set up.

1. Run the following commands to clone the repository and install dependencies:

   ```bash
   git clone https://github.com/dhanushreddy291/neon-playwright-example
   cd neon-playwright-example
   npm install
   cp .env.example .env
   ```

2. Populate the `.env` file with your Neon database connection details.

3. Apply the necessary migrations to your database:

   ```bash
   npm run db:migrate
   ```

4. Check the todo app works by running:

   ```bash
   npm run dev
   ```

   Open [localhost:3000](http://localhost:3000) in your browser to see the app. Verify the basic functionality of the todo app.

5. Check that the Playwright tests work by running:

   ```bash
   npm run test:e2e -- --headed
   ```

   You should see Chromium, Firefox, and WebKit browsers launching and running your tests.

6. Push your code to a new GitHub repository.

   ```bash
   rm -rf .git
   git init
   git add .
   git commit -m "Initial commit"
   git branch -M main
   git remote add origin <YOUR_GITHUB_REPO_URL>
   git push -u origin main
   ```

   Now that you have your code pushed to GitHub, you can set up the Neon GitHub integration.

## Set up the Neon GitHub integration

The [Neon GitHub integration](/docs/guides/neon-github-integration) securely connects your Neon project to your repository. It automatically creates a `NEON_API_KEY` secret and a `NEON_PROJECT_ID` variable in your repository, which are required for your GitHub Actions workflow.

1.  In the Neon Console, navigate to the **Integrations** page for your project.
2.  Locate the **GitHub** card and click **Add**.
    ![GitHub App card](/docs/guides/github_card.png)
3.  On the **GitHub** drawer, click **Install GitHub App**.
4.  If you have more than one GitHub account, select the account where you want to install the GitHub app.
5.  Select the GitHub repository to connect to your Neon project, and click **Connect**.
6.  **Add Production Database Secret**:
    - Navigate to your GitHub repository's **Settings** > **Secrets and variables** > **Actions**.
    - Create a new repository secret called `DATABASE_URL`.
    - Paste the connection string for your primary `main` branch (copied from the Neon Console).
    - Note that the `NEON_API_KEY` secret and `NEON_PROJECT_ID` variable should already be available from the GitHub integration setup.

    <Admonition type="note">
    It's important to understand the roles of your GitHub secrets. The `NEON_API_KEY` (created by the integration) is used to manage your Neon project, like creating and deleting branches. The `DATABASE_URL` secret you just created points exclusively to your primary production database. The workflow uses this only after a PR is successfully merged to apply migrations, ensuring a safe separation from the ephemeral preview databases used during testing.
    </Admonition>

## Understanding the workflow

Open the `.github/workflows/playwright.yml` file in your repository.
This workflow automates the entire testing lifecycle for each pull request.

```yaml
name: Playwright Tests
on:
  pull_request:
    branches: [main]
    types:
      - opened
      - reopened
      - synchronize
      - closed

# Ensures only the latest commit runs, preventing race conditions in concurrent PR updates
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}

jobs:
  setup:
    name: Setup
    timeout-minutes: 1
    runs-on: ubuntu-latest
    outputs:
      branch: ${{ steps.branch_name.outputs.current_branch }}
    steps:
      - name: Get branch name
        id: branch_name
        uses: tj-actions/branch-names@v8

  create_neon_branch_and_run_tests:
    name: Create Neon Branch and Run Tests
    needs: setup
    permissions:
      contents: read
      pull-requests: write
    if: |
      github.event_name == 'pull_request' && (
      github.event.action == 'synchronize' || github.event.action == 'opened' || github.event.action == 'reopened')
    runs-on: ubuntu-latest
    steps:
      - name: Create Neon Branch
        id: create_neon_branch
        uses: neondatabase/create-branch-action@v6
        with:
          project_id: ${{ vars.NEON_PROJECT_ID }}
          branch_name: preview/pr-${{ github.event.number }}-${{ needs.setup.outputs.branch }}
          api_key: ${{ secrets.NEON_API_KEY }}
          role: neondb_owner

      - uses: actions/checkout@v4

      - uses: actions/setup-node@v4
        with:
          node-version: lts/*

      - name: Install dependencies
        run: npm ci

      - name: Install Playwright Browsers
        run: npx playwright install --with-deps

      - name: Generate drizzle migrations
        run: npm run db:generate

      - name: Apply drizzle migrations
        run: npm run db:migrate
        env:
          DATABASE_URL: '${{ steps.create_neon_branch.outputs.db_url_pooled }}'

      - name: Build Next.js app
        run: npm run build
        env:
          NODE_ENV: production
          DATABASE_URL: '${{ steps.create_neon_branch.outputs.db_url_pooled }}'

      - name: Start Next.js app
        run: npm start &
        env:
          NODE_ENV: production
          DATABASE_URL: '${{ steps.create_neon_branch.outputs.db_url_pooled }}'

      - name: Wait for app to be ready
        run: |
          timeout 60 bash -c 'until curl -f http://localhost:3000 > /dev/null 2>&1; do sleep 1; done'

      - name: Run Playwright tests
        run: npm run test:e2e

      - uses: actions/upload-artifact@v4
        if: ${{ !cancelled() }}
        with:
          name: playwright-report
          path: playwright-report/
          retention-days: 30

      - name: Post Schema Diff Comment to PR
        uses: neondatabase/schema-diff-action@v1
        with:
          project_id: ${{ vars.NEON_PROJECT_ID }}
          compare_branch: preview/pr-${{ github.event.number }}-${{ needs.setup.outputs.branch }}
          api_key: ${{ secrets.NEON_API_KEY }}

  delete_neon_branch:
    name: Delete Neon Branch and Apply Migrations on Production branch
    needs: setup
    if: github.event_name == 'pull_request' && github.event.action == 'closed'
    runs-on: ubuntu-latest
    steps:
      - name: Delete Neon Branch
        uses: neondatabase/delete-branch-action@v3
        with:
          project_id: ${{ vars.NEON_PROJECT_ID }}
          branch: preview/pr-${{ github.event.number }}-${{ needs.setup.outputs.branch }}
          api_key: ${{ secrets.NEON_API_KEY }}

      - name: Checkout
        if: github.event.pull_request.merged == true
        uses: actions/checkout@v4

      - name: Apply migrations to production
        if: github.event.pull_request.merged == true
        run: |
          npm install
          npm run db:generate
          npm run db:migrate
        env:
          DATABASE_URL: '${{ secrets.DATABASE_URL }}'
```

<Admonition type="note" title="Note">
To set up GitHub Actions correctly, go to your repository's GitHub Actions settings, navigate to **Actions** > **General**, and set **Workflow permissions** to **Read and write permissions**.
</Admonition>

<Admonition type="tip">
The step outputs from the `create_neon_branch` action will only be available within the same job (`create_neon_branch_and_run_tests`). Therefore, write all test code, migrations, and related steps in that job itself. The outputs are marked as secrets. If you need separate jobs, refer to [GitHub's documentation on workflow commands](https://docs.github.com/en/actions/reference/workflows-and-actions/workflow-commands#workflow) for patterns on how to handle this.
</Admonition>

The workflow consists of three jobs:

- **Setup job**: Retrieves the current branch name for naming the Neon database branch.
- **Create branch & test job**: Creates a Neon database branch and runs Playwright tests whenever a pull request is opened or updated.
- **Cleanup job**: Cleans up resources after the pull request is closed.

### Create branch & test job

This job runs when a pull request is opened, reopened, or synchronized:

1. **Branch creation**:
   - Uses Neon's [`create-branch-action`](https://github.com/marketplace/actions/neon-create-branch-github-action) to create a new database branch
   - Names the branch using the pattern `preview/pr-{number}-{branch_name}`
   - Inherits the schema and data from the parent branch

2. **Migration handling**:
   - Installs project dependencies
   - Generates migration files using Drizzle
   - Applies migrations to the newly created branch
   - Uses the branch-specific `DATABASE_URL` for migration operations

3. **Application build and start**:
   - Builds the Next.js application in production mode
   - Starts the application, connecting it to the new database branch

4. **Playwright test execution**:
   - Installs Playwright browsers
   - Runs the full suite of Playwright tests against the live application
   - Uploads the Playwright report as an artifact for later review

5. **Schema diff generation**:
   - Uses Neon's [`schema-diff-action`](https://github.com/marketplace/actions/neon-schema-diff-github-action)
   - Compares the schema of the new branch with the parent branch
   - Automatically posts the differences as a comment on the pull request
   - Helps reviewers understand database changes at a glance

### Cleanup job

1. **Production migration**:
   - If the PR is merged, applies migrations to the production database
   - Uses the main `DATABASE_URL` stored in repository secrets
   - Ensures production database stays in sync with merged changes

2. **Cleanup**:
   - Removes the preview branch using Neon's [`delete-branch-action`](https://github.com/marketplace/actions/neon-database-delete-branch)

## Test the workflow

You can test the entire pipeline by making a schema change, updating the UI, and adding a new Playwright test to validate it.

1.  Create a new feature branch in your local repository:

    ```bash
    git checkout -b feature/add-created-at
    ```

2.  Modify the database schema in `app/db/schema.ts` to include a `created_at` timestamp:

    ```typescript {1,7}
    import { pgTable, text, bigint, boolean, timestamp } from 'drizzle-orm/pg-core';

    export const todos = pgTable('todos', {
      id: bigint('id', { mode: 'bigint' }).primaryKey().generatedByDefaultAsIdentity(),
      task: text('task').notNull(),
      isComplete: boolean('is_complete').notNull().default(false),
      createdAt: timestamp('created_at').notNull().defaultNow(),
    });
    ```

3.  Update the UI component in `app/todos.tsx` to display the new timestamp:

    ```tsx {6,13}
    // app/todos.tsx
    type Todo = {
      id: bigint;
      task: string;
      isComplete: boolean;
      createdAt: Date;
    };

    // ... inside the TodoList component's map function
    <li key={todo.id.toString()} className="flex items-center justify-between border-b py-2">
      <div>
        <span className={todo.isComplete ? 'text-gray-400 line-through' : ''}>{todo.task}</span>
        <p className="text-gray-500 text-xs">Created: {todo.createdAt.toLocaleDateString()}</p>
      </div>

      <div className="flex gap-2">{/* ... forms for toggle and delete */}</div>
    </li>;
    ```

4.  Add a new Playwright test in `tests/todos.spec.ts` to verify that the timestamp is displayed:

    ```typescript
    // tests/todos.spec.ts
    // ... inside the "Todo App" describe block
    test('should display created at timestamp for a new todo', async ({ page }) => {
      const todoText = 'Check the timestamp';
      await page.locator('input[name="task"]').fill(todoText);
      await page.locator('button:has-text("Add")').click();

      // Check that the todo text is visible
      await expect(page.locator(`text=${todoText}`)).toBeVisible();

      // Check that the "Created:" text is visible
      const expectedDate = new Date().toLocaleDateString();
      await expect(page.locator(`text=Created: ${expectedDate}`)).toBeVisible();
    });
    ```

5.  Commit your changes and push the branch to GitHub:

    ```bash
    git add .
    git commit -m "feat: add and display created_at timestamp for todos"
    git push origin feature/add-created-at
    ```

6.  Open a pull request on GitHub.

Once the PR is opened, the GitHub Actions workflow will trigger. You can watch as it creates a new database branch, runs migrations, starts your app, and successfully runs the Playwright tests including the new one you just added. The workflow will post a schema diff comment on the PR, and once merged, it will apply the changes to your production database and clean up the preview branch.

## Source code

You can find the complete source code for this example on GitHub.

<DetailIconCards>
<a href="https://github.com/neondatabase-labs/neon-playwright-example" description="Get started with automated E2E testing using Neon, Playwright, and GitHub Actions" icon="github">Neon Branching with E2E Playwright tests example</a>
</DetailIconCards>

## Conclusion

You have seen how to create isolated database branches for running Playwright tests, ensuring reliable and consistent E2E testing. This approach can be easily adapted to any other E2E testing framework, such as Cypress or Selenium, by modifying the test execution steps in the GitHub Actions workflow while keeping the Neon branching logic intact.

## Resources

- [Neon Database Branching](/branching)
- [Neon GitHub Integration](/guides/neon-github-integration)
- [Playwright Documentation](https://playwright.dev/docs/intro)
- [GitHub Actions Documentation](https://docs.github.com/en/actions)

<NeedHelp/>


# Getting started with ElectricSQL and Neon

---
title: Getting started with ElectricSQL and Neon
subtitle: A step-by-step guide to integrating ElectricSQL with Neon Postgres
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-05-28T00:00:00.000Z'
updatedOn: '2025-05-28T00:00:00.000Z'
---

This guide demonstrates how to integrate [ElectricSQL](https://electric-sql.com/) with Neon Postgres. ElectricSQL is a Postgres sync engine designed to handle partial replication, fan-out, and data delivery, making apps faster and more collaborative. It can scale to millions of users while maintaining low, stable, and predictable compute and memory usage.

ElectricSQL acts as a read-path sync engine, efficiently replicating partial subsets of your Postgres data to client applications. These data subsets are defined using [Shapes](https://electric-sql.com/docs/guides/shapes), which are similar to live queries. Writes are handled by your application's existing API and backend logic, ensuring ElectricSQL integrates smoothly with your current stack.

This guide provides a step-by-step walkthrough of setting up ElectricSQL with Neon Postgres. You will learn how to:

- Prepare your Neon Postgres database for ElectricSQL integration.
- Configure and run Electric using Docker.
- Set up a simple React application that subscribes to data changes on Neon Postgres via ElectricSQL.
- Test the real-time data synchronization.

## Prerequisites

Before you begin, ensure you have the following prerequisites installed and configured:

- **Neon Account:** Sign up for a free [Neon account](https://console.neon.tech/signup) if you don't have one already.
- **Node.js:** Node.js is required to run the React example application. Download and install it from [nodejs.org](https://nodejs.org).
- **Docker:** Docker is required to run Electric. Install Docker from [docker.com](https://www.docker.com/products/docker-desktop/).

## Setting up Neon Database

ElectricSQL requires a Postgres database with logical replication enabled. You'll configure your Neon project accordingly.

1.  **Create a Neon Project:** If you haven't already, create a new Neon project. You can use the Neon Console or [pg.new](https://pg.new).
2.  **Enable Logical Replication:** ElectricSQL uses Postgres logical replication (`wal_level = logical`) to receive changes from your database.
    - Navigate to your Neon Project in the [Neon Console](https://console.neon.tech/).
    - Open the **Settings** menu.
    - Click on **Logical Replication**.
    - Click the **Enable** button to enable logical replication.

      ![Neon dashboard settings with option to enable logical replication](/docs/guides/neon-console-settings-logical-replication.png)

3.  **Retrieve connection string:**
    - Navigate to the **Dashboard** of your Neon project.
    - Click on the **Connect** button which opens a modal.
    - Select your database and branch, and copy the connection string with connection pooling disabled.

      <Admonition type="important">
      Make sure to turn off connection pooling in the connection string modal. This is essential for Electric to maintain a persistent connection to the Neon database.
      </Admonition>

      ![Neon direct connection string modal](/docs/guides/neon-console-direct-connection-string.png)

## Setting up Electric

With your Neon database ready, you can now set up Electric to connect to it. We'll use Docker to run Electric. Run the following commands in your terminal to create a new directory for your ElectricSQL project and navigate into it:

```bash
mkdir neon-electric-quickstart
cd neon-electric-quickstart
```

Create a `docker-compose.yml` file in your project root with the following content:

```yaml
services:
  electric:
    container_name: electric
    image: electricsql/electric:1.0.17
    ports:
      - '3000:3000'
    environment:
      - DATABASE_URL=${NEON_DATABASE_URL}
      - ELECTRIC_INSECURE=true
    restart: always
```

<Admonition type="note">
The `ELECTRIC_INSECURE=true` setting is for local development only. Electric doesn't perform any authentication or authorization checks. You will need to proxy requests through an authorization layer in production to secure Electric. Please refer to [Using ElectricSQL in Production](#using-electricsql-in-production) for a typical production setup.
</Admonition>

Create a `.env` file in the same directory to store your Neon database connection string:

```env
NEON_DATABASE_URL="YOUR_NEON_UNPOOLED_CONNECTION_STRING"
```

Replace `YOUR_NEON_UNPOOLED_CONNECTION_STRING` with the actual unpooled connection string you copied from your Neon project dashboard.

Start Electric using Docker Compose. Run the following command in your terminal:

```bash
docker compose up -d
```

This command starts Electric in detached mode. You can view its logs using:

```bash
docker compose logs -f electric

# ... (other logs)
# Connected to Postgres xxxx and timeline
```

You should see logs indicating that Electric has connected to your Neon Postgres database.

## Sample application

Now that Electric is running and connected to your Neon database, you can test it with a simple React application that uses ElectricSQL to sync data from Neon. We will be following the [ElectricSQL Quickstart](https://electric-sql.com/docs/quickstart) to set up a basic React app that subscribes to changes in a Postgres table.

### Create sample data in Neon

Connect to your Neon database using `psql` or the [Neon SQL Editor](/docs/get-started/query-with-neon-sql-editor) and run the following SQL commands to create a sample table and insert some data. This is the schema our React application will use.

```sql
CREATE TABLE scores (
  id SERIAL PRIMARY KEY,
  name VARCHAR(255),
  value FLOAT
);

INSERT INTO scores (name, value) VALUES
  ('Alice', 3.14),
  ('Bob', 2.71);
```

### Set up the React application

1.  Create a new React application using Vite. Open your terminal and run the following commands:

    ```bash
    npm create vite@latest react-app -- --template react-ts
    cd react-app
    npm install
    ```

2.  Install ElectricSQL React client:

    ```bash
    npm install @electric-sql/react
    ```

3.  Replace the content of `src/App.tsx` with the following code.

    ```tsx
    import { useShape } from '@electric-sql/react';

    function Component() {
      const { data } = useShape({
        url: `http://localhost:3000/v1/shape`,
        params: {
          table: `scores`,
        },
      });

      return <pre>{JSON.stringify(data, null, 2)}</pre>;
    }

    export default Component;
    ```

    ElectricSQL uses Shapes to define subsets of your Postgres data for real-time synchronization. Here, `useShape` subscribes to a shape representing the `scores` table, ensuring your React app always has the latest score data.

4.  Start the React development server by running the following command in your terminal:

    ```bash
    npm run dev
    ```

    This will start the development server at [`localhost:5173`](http://localhost:5173)

## Using the demo application

Your React application should now be running in your browser. It's actively connected to the Electric, which maintains a real-time link to your Neon Postgres database via Logical Replication.

1.  **Access the application:** Open [`localhost:5173`](http://localhost:5173) in your browser. You should see the data from the `scores` table (`Alice` and `Bob`) displayed on the page.

    ```json
    [
      {
        "id": 1,
        "name": "Alice",
        "value": 3.14
      },
      {
        "id": 2,
        "name": "Bob",
        "value": 2.71
      }
    ]
    ```

2.  **Test real-time updates:**
    - Open the Neon SQL Editor or use `psql` to connect to your Neon database.
    - Insert a new row into the `scores` table:

      ```sql
      INSERT INTO scores (name, value) VALUES ('Charlie', 1.618);
      ```

    - Observe your React application. The new data for 'Charlie' should appear automatically without needing a page refresh. This demonstrates ElectricSQL's real-time sync capabilities.
    - Try updating or deleting rows in Neon and see the changes reflect in the app.

      ```sql
      UPDATE scores SET value = 6.28 WHERE name = 'Alice';
      ```

    - The value for Alice should update in the React app to `6.28`.

      ![React app displaying real-time data from Neon Postgres](/docs/guides/electric-sql-react-app.gif)

3.  **Understanding writes:**
    ElectricSQL handles the read-path synchronization (data from Postgres to client). To write data back to your Neon database (e.g., from user input in the React app), you would typically:
    - Implement an API endpoint in your backend application.
    - This API endpoint would receive write requests from your React app.
    - The API endpoint then performs these operations directly on your Neon Postgres database.
    - Once the data is written to Neon, Electric will detect these changes via Logical replication and automatically sync them to all connected clients.

    For detailed patterns on handling writes, refer to the [ElectricSQL Writes documentation](https://electric-sql.com/docs/guides/writes).

## Using ElectricSQL in Production

While ElectricSQL simplifies real-time data synchronization, Electric itself does not handle authentication or authorization. In production, you must implement a secure architecture to ensure that only authorized users can access and sync data.

The core principle for a secure and scalable ElectricSQL deployment is to place an **Authorization Proxy** in front of Electric. This proxy becomes the gatekeeper for data access, ensuring that clients only sync the data they are permitted to see. Additionally, you may whitelist Electric to only accept requests from your proxy, preventing direct access from end users.

### Production Architecture overview

A typical production architecture with ElectricSQL and Neon Postgres involves the following components:

1.  **Client application:** Your web or mobile application using an ElectricSQL client (e.g., `@electric-sql/react`).
2.  **Caching proxy (recommended for performance):** While optional, deploying Electric behind a caching proxy like Nginx, Caddy, Varnish, or a CDN (e.g., Cloudflare, Fastly) is recommended. This setup can significantly improve performance and reduce load by caching responses from Electric.
3.  **Authorization proxy:** A service (which could be part of your existing backend or a dedicated middleware) that intercepts requests destined for Electric. Its primary roles are authentication and authorization.
4.  **Electric:** Electric handles the real-time data synchronization between your client application requests and the Neon Postgres database.
5.  **Neon Postgres Database:** Your source of truth.

### Securing read access

The read path (data syncing from Neon to your client via ElectricSQL) needs to be robustly secured.

**Typical flow for read requests (`GET /v1/shape`):**

`User Client -> Caching Proxy (optional) -> Authorization Proxy -> ElectricSQL -> Neon Postgres`

1.  **Client request:** The ElectricSQL client in the user's application initiates a shape subscription request. This request should include authentication credentials (e.g., a JWT in an `Authorization` header) and the desired shape definition (e.g., `table=items`).

    ```typescript
    // Example: Client-side useShape hook with an auth header
    import { useShape } from '@electric-sql/react';

    const electricUrl = 'https://your-auth-proxy.com/electric/v1/shape'; // Points to your CDN/authorization proxy

    const MyComponent = () => {
      const { data } = useShape({
        url: electricUrl,
        params: {
          table: 'projects',
          // Base shape definition, will be augmented by the proxy
        },
        headers: {
          Authorization: `Bearer ${getAuthToken()}`, // Function to retrieve user's auth token
        },
      });
      // ... render component
    };
    ```

2.  **Authorization proxy:**
    - **Authentication:** The proxy validates the `Authorization` header (or other credentials) sent by the client. If authentication fails, it returns a `401 Unauthorized` or `403 Forbidden` error.
    - **Authorization & Dynamic Shape modification:** Upon successful authentication, the proxy determines the user's identity and permissions. It then _modifies_ the incoming shape request before forwarding it to Electric. This can be done by adding or augmenting `WHERE` clauses to the shape's `params`.
      For example, a user should only see projects belonging to their organization, the proxy would:
      - Extract `user_id` or `org_id` from the validated token.
      - If the original client request was for `table=projects`, the proxy might transform the request to Electric to include a `where` clause like:
        `GET /v1/shape?table=projects&where="organization_id"='user_actual_org_id'`
        This ensures that Electric only processes and syncs data relevant to that specific user.
    - **(Optional) Adding `ELECTRIC_SECRET`:** You can configure Electric by setting the `ELECTRIC_SECRET` environment variable when initializing the service. Your Authorization Proxy should then include this secret with requests it sends to Electric. This allows Electric to verify that requests originate from your trusted proxy, enhancing security by ensuring only authenticated requests are processed.

    For more details on securing ElectricSQL in production, refer to the [ElectricSQL Security Guide](https://electric-sql.com/docs/guides/security).

Congratulations! You have successfully set up ElectricSQL with Neon Postgres and built a basic real-time React application.

## Resources

- [ElectricSQL Documentation](https://electric-sql.com/docs/intro)
- [ElectricSQL Quickstart](https://electric-sql.com/docs/quickstart)
- [ElectricSQL Shapes](https://electric-sql.com/docs/guides/shapes)
- [Neon Documentation](/docs)

<NeedHelp/>


# Building an Async Product Management API with FastAPI, Pydantic, and PostgreSQL

---
title: Building an Async Product Management API with FastAPI, Pydantic, and PostgreSQL
subtitle: Learn how to create an asynchronous API for managing products using FastAPI, Pydantic for data validation, and PostgreSQL with connection pooling
author: sam-harri
enableTableOfContents: true
createdAt: '2024-10-08T00:00:00.000Z'
updatedOn: '2024-10-08T00:00:00.000Z'
---

Following this guide, you’ll build an asynchronous product management API and leverage FastAPI's async capabilities and connection pools to efficiently manage database connections, ensuring your API can scale and handle high traffic with ease. Whether you’re aiming to improve performance or simply learn the best practices for building async APIs, this guide has you covered.

## Prerequisites

Before starting, ensure you have the following tools and services ready:

- pip : Required for installing and managing Python packages, including [uv](https://docs.astral.sh/uv/) for creating virtual environments. You can check if `pip` is installed by running the following command:
  ```bash
  pip --version
  ```
- Neon serverless Postgres : you will need a Neon account for provisioning and scaling your `PostgreSQL` database. If you don't have an account yet, [sign up here](https://console.neon.tech/signup)

## Setting up the Project

Follow these steps to set up your project and virtual environment:

1.  Create a `uv` project

    If you don't already have uv installed, you can install it with:

    ```bash
    pip install uv
    ```

    Once `uv` is installed, create a new project:

    ```bash
    uv init async_postgres
    ```

    This will create a new project directory called `async_postgres`. Open this directory in your code editor of your choice.

2.  Set Up the Virtual Environment

        You will now create and activate a virtual environment in which your project's dependencies will be installed.

        <CodeTabs labels={["Linux/macOS", "Windows"]}>

            ```bash
            uv venv
            source .venv/bin/activate
            ```

            ```bash
            uv venv
            .venv\Scripts\activate
            ```

        </CodeTabs>

        You should see `(async_postgres)` in your terminal now, this means that your virtual environment is activated.

3.  Install Dependencies

    Next, add all the necessary dependencies for your project:

    ```bash
    uv add python-dotenv asyncpg loguru fastapi uvicorn
    ```

    Where each package does the following :
    - `FastAPI` : A Web / API framework
    - `AsyncPG` : An asynchronous PostgreSQL client
    - `Uvicorn` : An ASGI server for our app
    - `Loguru` : A logging library
    - `Python-dotenv` : To load environment variables from a .env file

4.  Create the project structure

    Now, create the following directory structure to organize your project files:

    ```md
    async_postgres
    ├── src/
    │ ├── database/
    │ │ └── postgres.py
    │ ├── models/
    │ │ └── product_models.py
    │ ├── routes/
    │ │ └── product_routes.py
    │ └── main.py
    ├── .env  
    ├── .python-version
    ├── README.md  
    ├── pyproject.toml  
    └── uv.lock
    ```

## Setting up your Database

In this section, you will set up the connection pool, ensure your database schema is in place, and manage database connections effectively. To connect to your `PostgreSQL` database, you will use the `asyncpg` library for asynchronous database connections.

First, create a `.env` file in the root of your project to store the database connection URL. This file will hold environment-specific variables, such as the connection string to your Neon PostgreSQL database.

```bash
DATABASE_URL=postgres://user:password@your-neon-hostname.neon.tech/neondb?sslmode=require&channel_binding=require
```

Make sure to replace the placeholders (user, password, your-neon-hostname, etc.) with your actual Neon database credentials which are available in the console.

In your project, the `database.py` file manages the connection to `PostgreSQL` using `asyncpg` and its connection pool, which is a mechanism for managing and reusing database connections efficiently. With this, you can use asynchronous queries, allowing the application to handle multiple requests concurrently.

```python
import os
import asyncpg
import dotenv
from loguru import logger
from typing import Optional

dotenv.load_dotenv()

conn_pool: Optional[asyncpg.Pool] = None

async def init_postgres() -> None:
    """
    Initialize the PostgreSQL connection pool and create the products table if it doesn't exist.

    This function is meant to be called at the startup of the FastAPI app to
    initialize a connection pool to PostgreSQL and ensure that the required
    database schema is in place.
    """
    global conn_pool
    try:
        logger.info("Initializing PostgreSQL connection pool...")

        conn_pool = await asyncpg.create_pool(
            dsn=os.getenv("DATABASE_URL"), min_size=1, max_size=10
        )
        logger.info("PostgreSQL connection pool created successfully.")

    except Exception as e:
        logger.error(f"Error initializing PostgreSQL connection pool: {e}")
        raise
    try:
        async with conn_pool.acquire() as conn:
            create_table_query = """
            CREATE TABLE IF NOT EXISTS products (
                id SERIAL PRIMARY KEY,
                name VARCHAR(100) NOT NULL,
                price NUMERIC(10, 2) NOT NULL CHECK (price >= 0),
                quantity INT NOT NULL CHECK (quantity >= 0),
                description VARCHAR(255)
            );
            """
            async with conn.transaction():
                await conn.execute(create_table_query)
            logger.info("Products table ensured to exist.")

    except Exception as e:
        logger.error(f"Error creating the products table: {e}")
        raise


async def get_postgres() -> asyncpg.Pool:
    """
    Return the PostgreSQL connection pool.

    This function returns the connection pool object, from which individual
    connections can be acquired as needed for database operations. The caller
    is responsible for acquiring and releasing connections from the pool.

    Returns
    -------
    asyncpg.Pool
        The connection pool object to the PostgreSQL database.

    Raises
    ------
    ConnectionError
        Raised if the connection pool is not initialized.
    """
    global conn_pool
    if conn_pool is None:
        logger.error("Connection pool is not initialized.")
        raise ConnectionError("PostgreSQL connection pool is not initialized.")
    try:
        return conn_pool
    except Exception as e:
        logger.error(f"Failed to return PostgreSQL connection pool: {e}")
        raise



async def close_postgres() -> None:
    """
    Close the PostgreSQL connection pool.

    This function should be called during the shutdown of the FastAPI app
    to properly close all connections in the pool and release resources.
    """
    global conn_pool
    if conn_pool is not None:
        try:
            logger.info("Closing PostgreSQL connection pool...")
            await conn_pool.close()
            logger.info("PostgreSQL connection pool closed successfully.")
        except Exception as e:
            logger.error(f"Error closing PostgreSQL connection pool: {e}")
            raise
    else:
        logger.warning("PostgreSQL connection pool was not initialized.")
```

`init_postgres` is responsible for opening the connection pool to the `PostgreSQL` database and setting up the required database schema. Specifically, it ensures that the necessary database tables (such as the `products` table) are created if they don’t already exist, preparing the application to start accepting requests.

To properly manage the lifecycle of the database, you need a function to close the connection pool when the API spins down `close_postgres` is responsible for gracefully closing all connections in the pool when the `FastAPI` app shuts down.

Throughout your API you will also need access to the pool to get connection instances and run queries. `get_postgres` returns the active connection pool. If the pool is not initialized, an error is raised. The term for passing this in is Dependency Injection.

## Defining the Pydantic Models

`Pydantic` is a data validation library in Python that ensures data entering or leaving your API is valid by enforcing constraints and data types.

In your application, you will define several models using Pydantic to represent the data for products. These models will be used to create, update, and manage products in the database, as well as handle validation when clients interact with our API.

```python
from pydantic import BaseModel, Field, ConfigDict
from typing import Optional


class Product(BaseModel):
    """
    Represents the product table in the database.
    """
    model_config = ConfigDict(from_attributes=True)
    id: int
    name: str
    price: float
    quantity: int
    description: Optional[str]


class ProductCreate(BaseModel):
    """
    Represents the required fields to create a new product.
    """
    name: str
    price: float = Field(..., ge=0)
    quantity: int = Field(..., ge=0)
    description: Optional[str] = Field(None, max_length=255)


class ProductUpdate(BaseModel):
    """
    Represents optional fields to update an existing product.
    Allows partial updates.
    """
    name: Optional[str] = None
    price: Optional[float] = Field(None, ge=0)
    quantity: Optional[int] = Field(None, ge=0)
    description: Optional[str] = Field(None, max_length=255)


class ProductStockUpdate(BaseModel):
    """
    Represents the stock update for a product's quantity.
    """
    quantity: int = Field(..., ge=0)
```

## Creating the API Endpoints

In this section, you will create the API endpoints that allow you to manage products in your `PostgreSQL` database. These endpoints will allow you to create, retrieve, update, delete, and manage product stock. You will leverage asynchronous database connections using `asyncpg`.

Each endpoint follows a similar flow for interacting with the database. You will first get a connection from the connection pool, execute the desired query, and release the connection back to the pool. Since the connection pool is used as a context manager, the connection will automatically be returned to the pool after each operation.

The common database flow goes as follows :

1. Getting the Connection Pool:
   - You inject the connection pool using FastAPI's `Depends()` function, which allows you to easily retrieve a connection from the pool.
2. Acquiring a Connection:
   - Using the connection pool, you acquire a connection by calling `async with db_pool.acquire() as conn:`. This ensures you obtain a database connection to run the query.
3. Running the Query:
   - Once the connection is acquired, you run the query using methods such as `fetchrow()` (for single rows) or `fetch()` (for multiple rows) depending on the operation.
4. Returning the Connection to the Pool:
   - Once the query is complete, the connection is automatically returned to the pool because the async with context manager handles the lifecycle of the connection.

```python
from fastapi import HTTPException, Query, Path, Body, APIRouter, Depends
from models.product_models import Product, ProductCreate, ProductUpdate, ProductStockUpdate
from database.postgres import get_postgres
from typing import List
import asyncpg
from loguru import logger

product_router = APIRouter()


@product_router.post("/products", response_model=Product)
async def create_product(
    product: ProductCreate = Body(...),
    db_pool: asyncpg.Pool = Depends(get_postgres),
) -> Product:
    """
    Create a new product.

    Parameters
    ----------
    product : ProductCreate
        The product details to create.
    db_pool : asyncpg.Pool
        Database connection pool injected by dependency.

    Returns
    -------
    Product
        The newly created product.
    """
    query = """
    INSERT INTO products (name, price, quantity, description)
    VALUES ($1, $2, $3, $4)
    RETURNING id, name, price, quantity, description
    """
    try:
        async with db_pool.acquire() as conn:
            result = await conn.fetchrow(
                query,
                product.name,
                product.price,
                product.quantity,
                product.description,
            )

        if result:
            return Product(**dict(result))
        else:
            logger.error("Failed to create product")
            raise HTTPException(status_code=500, detail="Failed to create product")
    except Exception as e:
        logger.error(f"Error during product creation: {e}")
        raise HTTPException(
            status_code=500, detail="Internal server error during product creation"
        )


@product_router.get("/products", response_model=List[Product])
async def get_all_products(
    db_pool: asyncpg.Pool = Depends(get_postgres),
) -> List[Product]:
    """
    Get a list of all products.

    Parameters
    ----------
    db_pool : asyncpg.Pool, optional
        Database connection pool injected by dependency.

    Returns
    -------
    List[Product]
        A list of all products in the inventory.
    """
    query = "SELECT id, name, price, quantity, description FROM products"

    try:
        async with db_pool.acquire() as conn:
            results = await conn.fetch(query)
            return [Product(**dict(result)) for result in results]
    except Exception as e:
        logger.error(f"Error fetching products: {e}")
        raise HTTPException(status_code=500, detail="Failed to retrieve products")


@product_router.get("/products/{id}", response_model=Product)
async def get_product_by_id(
    id: int = Path(..., ge=1),
    db_pool: asyncpg.Pool = Depends(get_postgres),
) -> Product:
    """
    Get a product by its ID.

    Parameters
    ----------
    id : int
        The ID of the product.
    db_pool : asyncpg.Pool, optional
        Database connection pool injected by dependency.

    Returns
    -------
    Product
        The product details for the given ID.
    """
    query = "SELECT id, name, price, quantity, description FROM products WHERE id = $1"

    try:
        async with db_pool.acquire() as conn:
            result = await conn.fetchrow(query, id)
            if result:
                return Product(**dict(result))
            else:
                logger.warning(f"Product with ID {id} not found")
                raise HTTPException(status_code=404, detail="Product not found")
    except Exception as e:
        logger.error(f"Error fetching product by ID: {e}")
        raise HTTPException(
            status_code=500, detail="Internal server error during product retrieval"
        )


@product_router.put("/products/{id}", response_model=Product)
async def update_product(
    id: int = Path(..., ge=1),
    product: ProductUpdate = Body(...),
    db_pool: asyncpg.Pool = Depends(get_postgres),
) -> Product:
    """
    Update a product by its ID.

    Parameters
    ----------
    id : int
        The ID of the product to update.
    product : ProductUpdate
        The fields to update (partial updates allowed).
    db_pool : asyncpg.Pool, optional
        Database connection pool injected by dependency.

    Returns
    -------
    Product
        The updated product details.
    """
    query = """
    UPDATE products
    SET name = COALESCE($1, name),
        price = COALESCE($2, price),
        quantity = COALESCE($3, quantity),
        description = COALESCE($4, description)
    WHERE id = $5
    RETURNING id, name, price, quantity, description
    """

    try:
        async with db_pool.acquire() as conn:
            result = await conn.fetchrow(
                query,
                product.name,
                product.price,
                product.quantity,
                product.description,
                id,
            )
            if result:
                return Product(**dict(result))
            else:
                logger.warning(f"Product with ID {id} not found for update")
                raise HTTPException(status_code=404, detail="Product not found")
    except Exception as e:
        logger.error(f"Error updating product: {e}")
        raise HTTPException(
            status_code=500, detail="Internal server error during product update"
        )


@product_router.delete("/products/{id}")
async def delete_product(
    id: int = Path(..., ge=1),
    db_pool: asyncpg.Pool = Depends(get_postgres)
) -> dict:
    """
    Delete a product by its ID.

    Parameters
    ----------
    id : int
        The ID of the product to delete.
    db_pool : asyncpg.Pool, optional
        Database connection pool injected by dependency.

    Returns
    -------
    dict
        A message indicating the product was deleted.
    """
    query = "DELETE FROM products WHERE id = $1 RETURNING id"

    try:
        async with db_pool.acquire() as conn:
            result = await conn.fetchrow(query, id)
            if result:
                return {"message": "Product deleted successfully"}
            else:
                logger.warning(f"Product with ID {id} not found for deletion")
                raise HTTPException(status_code=404, detail="Product not found")
    except Exception as e:
        logger.error(f"Error deleting product: {e}")
        raise HTTPException(
            status_code=500, detail="Internal server error during product deletion"
        )


@product_router.patch("/products/{id}/stock", response_model=Product)
async def update_product_stock(
    id: int = Path(..., ge=1),
    stock: ProductStockUpdate = Body(...),
    db_pool: asyncpg.Pool = Depends(get_postgres),
) -> Product:
    """
    Update the stock (quantity) of a product by its ID.

    Parameters
    ----------
    id : int
        The ID of the product to update.
    stock : ProductStockUpdate
        The new quantity for the product.
    db_pool : asyncpg.Pool, optional
        Database connection pool injected by dependency.

    Returns
    -------
    Product
        The updated product with new stock quantity.
    """
    query = """
    UPDATE products
    SET quantity = $1
    WHERE id = $2
    RETURNING id, name, price, quantity, description
    """
    try:
        async with db_pool.acquire() as conn:
            result = await conn.fetchrow(query, stock.quantity, id)
            if result:
                return Product(**dict(result))
            else:
                raise HTTPException(status_code=404, detail="Product not found")
    except Exception as e:
        logger.error(f"Error updating product stock: {e}")
        raise HTTPException(
            status_code=500, detail="Internal server error during product stock update"
        )


@product_router.get("/products/filter/price", response_model=List[Product])
async def filter_products_by_price(
    min_price: float = Query(...),
    max_price: float = Query(...),
    db_pool: asyncpg.Pool = Depends(get_postgres),
) -> List[Product]:
    """
    Get products within a specific price range.

    Parameters
    ----------
    min_price : float
        The minimum price for filtering.
    max_price : float
        The maximum price for filtering.
    db_pool : asyncpg.Pool, optional
        Database connection pool injected by dependency.

    Returns
    -------
    List[Product]
        A list of products within the specified price range.
    """
    query = """
    SELECT id, name, price, quantity, description
    FROM products
    WHERE price BETWEEN $1 AND $2
    """
    try:
        async with db_pool.acquire() as conn:
            results = await conn.fetch(query, min_price, max_price)
            return [Product(**dict(result)) for result in results]
    except Exception as e:
        logger.error(f"Error filtering products by price: {e}")
        raise HTTPException(
            status_code=500, detail="Internal server error during price filtering"
        )
```

The code defines endpoints for :

- `POST /products`: Creates a new product. It receives the product data (name, price, quantity, and description) and inserts it into the database. The newly created product is returned.

- `GET /products`: Retrieves all products from the database. The response is a list of products, each containing its ID, name, price, quantity, and description.

- `GET /products/{id}`: Retrieves a product by its unique ID. If the product exists, its details are returned; otherwise, a 404 error is raised.

- `PUT /products/{id}`: Updates an existing product by its ID. The update can be partial, as it uses `COALESCE` to only update the fields provided. The updated product is returned.

- `DELETE /products/{id}`: Deletes a product by its ID. If the product is successfully deleted, a success message is returned.

- `PATCH /products/{id}/stock`: Updates the stock (quantity) of a specific product by its ID. The updated product, with the new quantity, is returned.

- `GET /products/filter/price`: Retrieves products within a specific price range. You pass min_price and max_price as query parameters, and the endpoint returns a list of products that fall within that range.

## Running the Application

After setting up the database, models, and API routes, the next step is to run the `FastAPI` application. The `main.py` file is the entry point for the application, and `Uvicorn` starts and serves it.

The `main.py` file defines the `FastAPI` application, manages lifecycle events like starting and closing the `PostgreSQL` connection pool, and includes the product-related routes. Here, you will use the `@asynccontextmanager` decorator to manage the database connection pool lifecycle.

```python
from fastapi import FastAPI
from contextlib import asynccontextmanager
from database.postgres import init_postgres, close_postgres
from routes.product_routes import product_router
import uvicorn


@asynccontextmanager
async def lifespan(app: FastAPI):
    await init_postgres()
    yield
    await close_postgres()


app: FastAPI = FastAPI(lifespan=lifespan, title="Async FastAPI PostgreSQL Inventory Manager")
app.include_router(product_router)

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8080, reload=True)

```

To run the application, use the following command:

```bash
uv run src/main.py
```

Once the server is running, you can access the API documentation and test the endpoints directly in your browser:

- Interactive API Docs (Swagger UI):  
  Visit `http://127.0.0.1:8080/docs` to access the automatically generated API documentation where you can test the endpoints.
- Alternative Docs (ReDoc):  
  Visit `http://127.0.0.1:8080/redoc` for another style of API documentation.

## Testing the API

You can also use tools like `httpie`, `curl`, and `Postman` to test the API.

Below are examples of how to interact with the API using `httpie`, a command-line HTTP client.

1. Create a Product

   Start by creating a new product:

   ```
   http POST http://127.0.0.1:8080/products name="Test Product" price:=9.99 quantity:=100 description="A test product"
   ```

   You should see a response with the created product data:

   ```
   {
       "id": 1,
       "name": "Test Product",
       "price": 9.99,
       "quantity": 100,
       "description": "A test product"
   }
   ```

2. Retrieve All Products

   Next, retrieve all products from the database:

   ```
   http GET http://127.0.0.1:8080/products
   ```

   This will return a list of all products in the database:

   ```
   [
       {
           "id": 1,
           "name": "Test Product",
           "price": 9.99,
           "quantity": 100,
           "description": "A test product"
       }
   ]
   ```

3. Retrieve a Specific Product by ID

   You can also retrieve a specific product by its ID:

   ```
   http GET http://127.0.0.1:8080/products/1
   ```

   This will return the product details for the product with ID `1`:

   ```
   {
       "id": 1,
       "name": "Test Product",
       "price": 9.99,
       "quantity": 100,
       "description": "A test product"
   }
   ```

4. Update a Product

   To update an existing product, use the following command:

   ```
   http PUT http://127.0.0.1:8080/products/1 name="Updated Product" price:=12.99 quantity:=150 description="An updated product description"
   ```

   This will return the updated product data:

   ```
   {
       "id": 1,
       "name": "Updated Product",
       "price": 12.99,
       "quantity": 150,
       "description": "An updated product description"
   }
   ```

5. Update Product Stock

   You can also update just the stock (quantity) of a product:

   ```
   http PATCH http://127.0.0.1:8080/products/1/stock quantity:=200
   ```

   This will return the updated product with the new quantity:

   ```
   {
       "id": 1,
       "name": "Updated Product",
       "price": 12.99,
       "quantity": 200,
       "description": "An updated product description"
   }
   ```

6. Filter Products by Price Range

   To filter products by a specific price range, use this command:

   ```
   http GET http://127.0.0.1:8080/products/filter/price min_price==5.00 max_price==15.00
   ```

   This will return products that fall within the specified price range:

   ```
   [
       {
           "id": 1,
           "name": "Updated Product",
           "price": 12.99,
           "quantity": 200,
           "description": "An updated product description"
       }
   ]
   ```

7. Delete a Product

   To delete a product by its ID, use the following command:

   ```
   http DELETE http://127.0.0.1:8080/products/1
   ```

   If successful, you will receive a confirmation message:

   ```
   {
       "message": "Product deleted successfully"
   }
   ```

## Conclusion

Using this guide, you have built a fully functional API for managing products using `FastAPI`, `Pydantic`, and `PostgreSQL` with `asyncpg`.

This stack provides a solid foundation for building high-performance and scalable web services. `FastAPI`'s asynchronous support, combined with `Pydantic`'s robust data validation and `asyncpg`'s efficient database interactions, allows for fast and reliable API development.

As a next step, you can look at deploying this application in the cloud using scalable technologies like `Docker` and `Kubernetes`, or implementing automated test, build, and deployment workflows using `GitHub CI`


# Implementing Secure User Authentication in FastAPI using JWT Tokens and Neon Postgres

---
title: Implementing Secure User Authentication in FastAPI using JWT Tokens and Neon Postgres
subtitle: Learn how to build a secure user authentication system in FastAPI using JSON Web Tokens (JWT) and Neon Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-08-17T00:00:00.000Z'
updatedOn: '2024-08-17T00:00:00.000Z'
---

In this guide, we'll walk through the process of implementing secure user authentication in a FastAPI application using JSON Web Tokens (JWT) and Neon Postgres.

We'll cover user registration, login, and protecting routes with authentication, using PyJWT for handling JWT operations.

By the end of this guide, you'll have a FastAPI application with an authentication system that uses JWT tokens for secure user management.

## Prerequisites

Before we begin, make sure you have the following:

- Python 3.9 or later installed on your system
- [pip](https://pip.pypa.io/en/stable/installation/) for managing Python packages
- A [Neon](https://console.neon.tech/signup) account for serverless Postgres
- Basic knowledge of [FastAPI, SQLAlchemy, and Pydantic](/guides/fastapi-overview)

## How JWT Works

Before we dive into the building our API, let's understand how JWT works. If you're already familiar with JWT, feel free to skip ahead to the next section.

JSON Web Tokens or JWT for short provide a secure way to authenticate and authorize users in web applications.

A JWT consists of three parts, each separated by a dot (`.`):

```
Header.Payload.Signature
```

Each part is Base64Url encoded, resulting in a structure like this:

```
eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c
```

Let's break down each part of the JWT:

### JWT Header

The header typically consists of two parts:

- The type of token (JWT)
- The hashing algorithm being used (e.g., HMAC SHA256 or RSA)

Example:

```json
{
  "alg": "HS256",
  "typ": "JWT"
}
```

This JSON is then Base64Url encoded to form the first part of the JWT.

### Payload

The payload contains claims. Claims are statements about the user and additional metadata. There are three types of claims:

- Registered claims: Predefined claims such as `iss` (issuer), `exp` (expiration time), `sub` (subject), `aud` (audience)
- Public claims: Can be defined at will by those using JWTs
- Private claims: Custom claims to share additional information between the client and server

Example:

```json
{
  "sub": "1234567890",
  "name": "John Doe",
  "admin": true,
  "iat": 1516239022
}
```

This JSON is then Base64Url encoded to form the second part of the JWT.

### Signature

The signature is used to verify that the sender of the JWT is who it says it is and to ensure that the message wasn't changed along the way. To create the signature part, you have to take the encoded header, the encoded payload, a secret, and the algorithm specified in the header, and sign that.

For example, if you want to use the HMAC SHA256 algorithm, the signature will be created in the following way:

```
HMACSHA256(
  base64UrlEncode(header) + "." +
  base64UrlEncode(payload),
  secret)
```

This signature is then Base64Url encoded to form the third part of the JWT.

### The Process of Using JWTs

The overall process of using JWTs for authentication and authorization typically involves the following steps:

1. **User Authentication**:
   - The process begins when a user logs in with their credentials (e.g., username and password).
   - The server verifies these credentials against the stored user information.

2. **JWT Creation**:
   - Upon successful authentication, the server creates a JWT.
   - It generates the header and payload, encoding the necessary information.
   - Using a secret key (kept secure on the server), it creates the signature.
   - The three parts (header, payload, signature) are combined to form the complete JWT.

3. **Sending the Token**:
   - The server sends this token back to the client in the response.
   - The client stores this token, often in local storage or a secure cookie.

4. **Subsequent Requests**:
   - For any subsequent requests to protected routes or resources, the client includes this token in the Authorization header.
   - The format is: `Authorization: Bearer <token>`

5. **Server-side Token Validation**:
   - When the server receives a request with a JWT, it first splits the token into its three parts.
   - It base64 decodes the header and payload.
   - The server then recreates the signature using the header, payload, and its secret key.
   - If this newly created signature matches the signature in the token, the server knows the token is valid and hasn't been tampered with.

6. **Accessing Protected Resources**:
   - If the token is valid, the server can use the information in the payload without needing to query the database.
   - This allows the server to authenticate the user and know their permissions for each request without needing to store session data.

7. **Token Expiration**:
   - JWTs typically have an expiration time specified in the payload.
   - The server checks this expiration time with each request.
   - If the token has expired, the server will reject the request, requiring the client to authenticate again.

## Setting up the Project

With the theory out of the way, let's start by creating a new project directory and setting up a virtual environment:

1. Create a new directory and navigate to it:

   ```bash
   mkdir fastapi-auth-demo
   cd fastapi-auth-demo
   ```

2. Create a virtual environment:

   ```bash
   python -m venv venv
   ```

3. Activate the virtual environment:
   - On Windows:
     ```bash
     venv\Scripts\activate
     ```
   - On macOS and Linux:
     ```bash
     source venv/bin/activate
     ```

Now, let's install the necessary packages for our project:

```bash
pip install "fastapi[all]" sqlalchemy psycopg2-binary pyjwt "passlib[bcrypt]" python-dotenv
```

This command installs:

- FastAPI: Our web framework
- SQLAlchemy: An ORM for database interactions
- psycopg2-binary: PostgreSQL adapter for Python
- PyJWT: For working with JWT tokens instead of handling them manually
- passlib: For password hashing
- python-dotenv: To load environment variables from a .env file

You can also create a `requirements.txt` file to manage your dependencies using the following:

```bash
pip freeze > requirements.txt
```

This file can be used to install the dependencies in another environment using `pip install -r requirements.txt`.

## Connecting to Neon Postgres

Next, let's set up a connection to Neon Postgres for storing user data.

Create a `.env` file in your project root and add the following configuration:

```env
DATABASE_URL=postgres://user:password@your-neon-hostname.neon.tech/dbname?sslmode=require&channel_binding=require
```

Replace the placeholders with your actual Neon database credentials.

While editing the `.env` file, add the following configuration for JWT token signing:

```env
SECRET_KEY=your-secret-key
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30
```

Choose a secure secret key for signing the JWT tokens. The `ALGORITHM` specifies the hashing algorithm to use, and `ACCESS_TOKEN_EXPIRE_MINUTES` sets the token expiration time.

Now, create a `database.py` file to manage the database connection:

```python
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from dotenv import load_dotenv
import os

load_dotenv()

SQLALCHEMY_DATABASE_URL = os.getenv("DATABASE_URL")

engine = create_engine(SQLALCHEMY_DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

Base = declarative_base()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```

This script sets up the database connection using SQLAlchemy and provides a `get_db` function to manage database sessions.

The `DATABASE_URL` is read from the `.env` file for security, and the Neon Postgres connection string is used to connect to the database.

The `SessionLocal` object is a factory for creating new database sessions, and the `get_db` function ensures that sessions are properly closed after use.

## User Model and Schema

We will be using SQLAlchemy for database interactions and Pydantic for data validation. SQLAlchemy provides an ORM for working with databases, while Pydantic is used for defining data models. These models will be used to interact with the database and validate user input.

Start by creating a `models.py` file for the SQLAlchemy User model:

```python
from sqlalchemy import Column, Integer, String
from database import Base

class User(Base):
    __tablename__ = "users"

    id = Column(Integer, primary_key=True, index=True)
    username = Column(String, unique=True, index=True)
    email = Column(String, unique=True, index=True)
    hashed_password = Column(String)
```

This defines a `User` model with fields for `id`, `username`, `email`, and `hashed_password`. The `unique=True` constraint ensures that usernames and emails are unique across all users. The `index=True` constraint creates an index on these fields for faster lookups.

The `Base` object is imported from the `database` module and is used to create the database schema.

Next, create a `schemas.py` file for Pydantic models:

```python
from pydantic import BaseModel, EmailStr

class UserCreate(BaseModel):
    username: str
    email: EmailStr
    password: str

class User(BaseModel):
    id: int
    username: str
    email: EmailStr

    class Config:
        orm_mode = True

class Token(BaseModel):
    access_token: str
    token_type: str
```

These Pydantic models define the structure for user creation, user representation, and JWT tokens. The `EmailStr` type ensures that the email is in a valid format.

One of the benefits of using Pydantic models is that they can be used for data validation and serialization. The `orm_mode = True` configuration allows Pydantic to work with SQLAlchemy models directly.

## Authentication Utilities

Now that we have the database models and schemas in place, let's add some utility functions for authentication.

Create a file called `auth.py` where we will define functions for password hashing, verification, and JWT token creation:

```python
from passlib.context import CryptContext
import jwt
from datetime import datetime, timedelta
from dotenv import load_dotenv
import os

# Load environment variables
load_dotenv()

SECRET_KEY = os.getenv("SECRET_KEY")
ALGORITHM = os.getenv("ALGORITHM")
ACCESS_TOKEN_EXPIRE_MINUTES = int(os.getenv("ACCESS_TOKEN_EXPIRE_MINUTES"))

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

def verify_password(plain_password, hashed_password):
    return pwd_context.verify(plain_password, hashed_password)

def get_password_hash(password):
    return pwd_context.hash(password)

def create_access_token(data: dict):
    to_encode = data.copy()
    expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt

def verify_token(token: str):
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        username: str = payload.get("sub")
        if username is None:
            return None
        return username
    except jwt.PyJWTError:
        return None
```

This file includes functions for:

- Verifying and hashing passwords using bcrypt
- Creating JWT access tokens
- Verifying JWT tokens

The `CryptContext` from passlib is used for secure password hashing, while `PyJWT` is used for JWT token creation and verification. PyJWT provides a simpler and more focused API for JWT operations compared to `python-jose`.

## API Endpoints

With all the necessary components in place, we can now create the API endpoints for user registration, login, and protected routes.

To do this, create a `main.py` file with the following content:

```python
from fastapi import FastAPI, Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm
from sqlalchemy.orm import Session
from database import engine, get_db
import models, schemas, auth

# Run the database migrations
models.Base.metadata.create_all(bind=engine)

# Initialize the FastAPI app
app = FastAPI()

# Define the OAuth2 scheme for token-based authentication
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")

@app.post("/register", response_model=schemas.User)
def register_user(user: schemas.UserCreate, db: Session = Depends(get_db)):
    db_user = db.query(models.User).filter(models.User.username == user.username).first()
    if db_user:
        raise HTTPException(status_code=400, detail="Username already registered")
    hashed_password = auth.get_password_hash(user.password)
    new_user = models.User(username=user.username, email=user.email, hashed_password=hashed_password)
    db.add(new_user)
    db.commit()
    db.refresh(new_user)
    return new_user

@app.post("/token", response_model=schemas.Token)
def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends(), db: Session = Depends(get_db)):
    user = db.query(models.User).filter(models.User.username == form_data.username).first()
    if not user or not auth.verify_password(form_data.password, user.hashed_password):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password",
            headers={"WWW-Authenticate": "Bearer"},
        )
    access_token = auth.create_access_token(data={"sub": user.username})
    return {"access_token": access_token, "token_type": "bearer"}

async def get_current_user(token: str = Depends(oauth2_scheme), db: Session = Depends(get_db)):
    username = auth.verify_token(token)
    if username is None:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid authentication credentials",
            headers={"WWW-Authenticate": "Bearer"},
        )
    user = db.query(models.User).filter(models.User.username == username).first()
    if user is None:
        raise HTTPException(status_code=404, detail="User not found")
    return user

@app.get("/users/me", response_model=schemas.User)
async def read_users_me(current_user: models.User = Depends(get_current_user)):
    return current_user
```

Let's break down the key components of this file:

1. The `/register` endpoint allows new users to create an account. It checks if the username is already taken, hashes the password, and stores the new user in the database.

2. The `/token` endpoint handles user login. It verifies the username and password, and if correct, issues a JWT access token.

3. The `get_current_user` function is a dependency that verifies the JWT token and retrieves the current user. This is used to protect routes that require authentication.

4. The `/users/me` endpoint is an example of a protected route. It returns the current user's information, but only if a valid JWT token is provided.

The tables will be created in your Neon database when the application starts, thanks to the `Base.metadata.create_all(bind=engine)` line in the `main.py` file.

## Running the API

To run the API, use the following command:

```bash
python -m uvicorn main:app --reload
```

This starts the Uvicorn server with hot-reloading enabled for development. This means that the server will automatically restart when you make changes to the code thanks to the `--reload` flag.

## Testing the Authentication System

You can test the authentication system using tools like `curl`, `httpie`, or the FastAPI Swagger UI at `http://127.0.0.1:8000/docs`.

Here are some example requests using the `httpie` command-line HTTP client to go through the registration, login, and protected route access flow as we discussed earlier.

1. Start by registering a new user:

   ```bash
   http POST http://127.0.0.1:8000/register username=testuser email=test@example.com password=securepassword
   ```

   You should receive a response with the new user's details in JSON format if the registration is successful.

2. Login and get an access token using the registered user's credentials:

   ```bash
   http --form POST http://127.0.0.1:8000/token username=testuser password=securepassword
   ```

   This request should return a JSON response with an access token.

   If you were to copy the token and decode it at [jwt.io](https://jwt.io/), you would see the payload containing the username and expiration time.
   As we discussed earlier, in some cases the token might contain additional claims like `iss` (issuer), `aud` (audience), etc. These can be used for additional security checks.

   The token will be valid for the duration specified in the `.env` file.

3. Access the protected `/users/me` route using the access token:

   ```bash
   http GET http://127.0.0.1:8000/users/me "Authorization: Bearer <your_access_token>"
   ```

   This request should return the user's details if the token is valid.

Replace `<your_access_token>` with the token received from the login request.

You would see a `401 Unauthorized` response if the token is invalid or has expired. This is because the `get_current_user` dependency checks the token validity before allowing access to the protected route.

## Dockerizing the Application

In many cases, you may want to containerize your FastAPI application for deployment. You can use Docker to create a container image for your FastAPI application.

Let's create a Dockerfile to package the application into a Docker container:

```Dockerfile
FROM python:3.12-slim

WORKDIR /app

COPY requirements.txt .

RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

This Dockerfile uses the official Python image as the base image, installs the project dependencies, and copies the project files into the container. The `CMD` instruction specifies the command to run when the container starts.

To build the Docker image, run the following command:

```bash
docker build -t fastapi-auth-demo .
```

This command builds the Docker image with the tag `fastapi-auth-demo` based on the `Dockerfile` in the current directory.

Make sure that you don't include the `.env` file in the Docker image to keep your secrets secure. You can pass environment variables to the container using the `--env-file` flag when running the container.

To run the Docker container, use the following command:

```bash
docker run -d -p 8000:8000 --env-file .env fastapi-auth-demo
```

This command starts the container in detached mode, maps port 8000 on the host to port 8000 in the container, and loads environment variables from the `.env` file.

## Conclusion

In this guide, we've implemented a secure user authentication system in FastAPI using JWT tokens (with PyJWT) and Neon Postgres. This provides a good start for building secure web applications with user accounts and protected routes which can be integrated with other microservices or front-end applications.

## Additional Resources

- [FastAPI Security](https://fastapi.tiangolo.com/tutorial/security/)
- [JSON Web Tokens](https://jwt.io/introduction)
- [PyJWT Documentation](https://pyjwt.readthedocs.io/)
- [SQLAlchemy Documentation](https://docs.sqlalchemy.org/)
- [Neon Documentation](/docs)


# Building a High-Performance API with FastAPI, Pydantic, and Neon Postgres

---
title: Building a High-Performance API with FastAPI, Pydantic, and Neon Postgres
subtitle: Learn how to create an API for managing a tech conference system using FastAPI, Pydantic for data validation, and Neon's serverless Postgres for data storage
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-08-17T00:00:00.000Z'
updatedOn: '2024-08-17T00:00:00.000Z'
---

FastAPI is a high-performance Python web framework for building APIs quickly and efficiently.

When combined with Pydantic for data validation and Neon's serverless Postgres for data storage, you can create a powerful and efficient API with minimal effort.

In this guide, we'll walk through the process of building an API for managing a tech conference system, focusing on best practices and performance optimizations.

## Prerequisites

Before we begin, make sure you have the following:

- Python 3.9 or later installed on your system
- [pip](https://pip.pypa.io/en/stable/installation/) for managing Python packages
- A [Neon](https://console.neon.tech/signup) account for serverless Postgres

## Setting up the Project

Let's start by creating a new project directory and setting up a virtual environment:

1. Create a new directory and navigate to it:

   ```bash
   mkdir fastapi-neon-conference-api
   cd fastapi-neon-conference-api
   ```

2. Create a virtual environment:

   ```bash
   python -m venv venv
   ```

   Creating a virtual environment isolates your project dependencies from other Python installations on your system.

3. Activate the virtual environment:
   - On Windows:
     ```bash
     venv\Scripts\activate
     ```
   - On macOS and Linux:
     ```bash
     source venv/bin/activate
     ```

   You should see `(venv)` in your terminal prompt, indicating that the virtual environment is active.

Now, let's install the necessary packages for our project using `pip`:

```bash
pip install fastapi uvicorn sqlalchemy psycopg2-binary "pydantic[email]" python-dotenv
```

This command installs:

- FastAPI: Our web framework
- Uvicorn: An ASGI server to run our FastAPI application
- SQLAlchemy: An ORM for database interactions
- psycopg2-binary: PostgreSQL adapter for Python
- Pydantic: For data validation and settings management
- python-dotenv: To load environment variables from a .env file

With the dependencies installed, we can start building our API.

You can also create a `requirements.txt` file to manage your dependencies by running:

```bash
pip freeze > requirements.txt
```

This will create a `requirements.txt` file with all the installed packages in your virtual environment and their versions. This is useful for sharing your project with others or deploying it to a server.

## Connecting to Neon Postgres

First, let's set up our database connection. Create a `.env` file in your project root:

```env
DATABASE_URL=postgres://user:password@your-neon-hostname.neon.tech/neondb?sslmode=require&channel_binding=require
```

Replace the placeholders with your actual Neon database credentials.

Now, create a `database.py` file to manage the database connection:

```python
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from dotenv import load_dotenv
import os

load_dotenv()

SQLALCHEMY_DATABASE_URL = os.getenv("DATABASE_URL")

engine = create_engine(SQLALCHEMY_DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

Base = declarative_base()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```

This script does the following:

1. Loads the environment variables from `.env` using `load_dotenv`
2. Creates a SQLAlchemy engine using the database URL from the environment variables
3. Creates a SessionLocal class for database sessions
4. Defines a Base class for declarative models which will be used to create database tables
5. Provides a `get_db` function to manage database connections

We're now ready to define our database models and API endpoints.

## Defining Models and Schemas

Let's start by creating an API for managing a tech conference system. We'll need database models and Pydantic schemas for talks and speakers.

Create a `models.py` file for SQLAlchemy models and relationships:

```python
from sqlalchemy import Column, Integer, String, ForeignKey, DateTime
from sqlalchemy.orm import relationship
from database import Base

class Speaker(Base):
    __tablename__ = "speakers"

    id = Column(Integer, primary_key=True, index=True)
    name = Column(String, index=True)
    bio = Column(String)
    company = Column(String)

    talks = relationship("Talk", back_populates="speaker")

class Talk(Base):
    __tablename__ = "talks"

    id = Column(Integer, primary_key=True, index=True)
    title = Column(String, index=True)
    description = Column(String)
    speaker_id = Column(Integer, ForeignKey("speakers.id"))
    start_time = Column(DateTime)
    end_time = Column(DateTime)

    speaker = relationship("Speaker", back_populates="talks")
```

This defines `Speaker` and `Talk` models with their respective fields and relationships. The `speaker_id` field in the `Talk` model establishes a foreign key relationship with the `Speaker` model using SQLAlchemy's `ForeignKey` construct.

Now, create a `schemas.py` file for Pydantic models with data validation:

```python
from pydantic import BaseModel
from datetime import datetime
from typing import List, Optional

class TalkBase(BaseModel):
    title: str
    description: str
    start_time: datetime
    end_time: datetime

class TalkCreate(TalkBase):
    speaker_id: int

class Talk(TalkBase):
    id: int
    speaker_id: int

    class Config:
        orm_mode = True

class SpeakerBase(BaseModel):
    name: str
    bio: str
    company: str

class SpeakerCreate(SpeakerBase):
    pass

class Speaker(SpeakerBase):
    id: int
    talks: List[Talk] = []

    class Config:
        orm_mode = True

class SpeakerWithTalks(Speaker):
    talks: List[Talk]
```

Here we define Pydantic models for creating and returning speaker and talk data.

The `TalkCreate` model includes a `speaker_id` field for associating talks with speakers. The `SpeakerWithTalks` model extends the `Speaker` model to include a list of talks. The `orm_mode = True` configuration enables automatic data conversion between SQLAlchemy models and Pydantic models.

These models will be used to validate incoming data and serialize outgoing data in our API endpoints instead of manually handling data conversion.

## Creating API Endpoints

Now, let's create our FastAPI application with CRUD operations for speakers and talks. Create a `main.py` file:

```python
from fastapi import FastAPI, Depends, HTTPException
from sqlalchemy.orm import Session
from typing import List

import models, schemas
from database import engine, get_db

models.Base.metadata.create_all(bind=engine)

app = FastAPI()

@app.post("/speakers/", response_model=schemas.Speaker)
def create_speaker(speaker: schemas.SpeakerCreate, db: Session = Depends(get_db)):
    db_speaker = models.Speaker(**speaker.dict())
    db.add(db_speaker)
    db.commit()
    db.refresh(db_speaker)
    return db_speaker

@app.get("/speakers/", response_model=List[schemas.Speaker])
def read_speakers(skip: int = 0, limit: int = 100, db: Session = Depends(get_db)):
    speakers = db.query(models.Speaker).offset(skip).limit(limit).all()
    return speakers

@app.get("/speakers/{speaker_id}", response_model=schemas.SpeakerWithTalks)
def read_speaker(speaker_id: int, db: Session = Depends(get_db)):
    db_speaker = db.query(models.Speaker).filter(models.Speaker.id == speaker_id).first()
    if db_speaker is None:
        raise HTTPException(status_code=404, detail="Speaker not found")
    return db_speaker

@app.post("/talks/", response_model=schemas.Talk)
def create_talk(talk: schemas.TalkCreate, db: Session = Depends(get_db)):
    db_talk = models.Talk(**talk.dict())
    db.add(db_talk)
    db.commit()
    db.refresh(db_talk)
    return db_talk

@app.get("/talks/", response_model=List[schemas.Talk])
def read_talks(skip: int = 0, limit: int = 100, db: Session = Depends(get_db)):
    talks = db.query(models.Talk).offset(skip).limit(limit).all()
    return talks

@app.get("/talks/{talk_id}", response_model=schemas.Talk)
def read_talk(talk_id: int, db: Session = Depends(get_db)):
    db_talk = db.query(models.Talk).filter(models.Talk.id == talk_id).first()
    if db_talk is None:
        raise HTTPException(status_code=404, detail="Talk not found")
    return db_talk
```

This code defines endpoints for:

- Creating and retrieving speakers at `/speakers/`
- Retrieving a specific speaker with their talks at `/speakers/{speaker_id}`
- Creating and retrieving talks at `/talks/`
- Retrieving a specific talk at `/talks/{talk_id}`

Each endpoint uses dependency injection to get a database session, ensuring efficient connection management.

Pagination is supported for the `read_speakers` and `read_talks` endpoints using the `skip` and `limit` parameters to avoid loading large datasets at once.

## Running the API

To run the API, use the following command:

```bash
python -m uvicorn main:app --reload
```

This starts the Uvicorn server with hot-reloading enabled for development.

By default, the API will be available on port 8000. You can access the API documentation at `http://127.0.0.1:8000/docs` or `http://127.0.0.1:8000/redoc`.

Your database tables will be created automatically when you run the API for the first time thanks to the `models.Base.metadata.create_all(bind=engine)` line in `main.py`. You can check your database to see the tables using the Neon console.

## Testing the API

You can test the API using tools like `curl`, `Postman`, or even via the `/docs` endpoint provided by FastAPI directly via your browser.

Let's test this out using `httpie`, a command-line HTTP client:

1. Start by creating a speaker:

   ```bash
   http POST http://127.0.0.1:8000/speakers/ name="John Doe" bio="Software Engineer" company="Tech Inc."
   ```

   You should see a response with the created speaker data.

2. Next, create a talk associated with the speaker:

   ```bash
   http POST http://127.0.0.1:8000/talks/ title="Introduction to FastAPI" description="Learn how to build APIs with FastAPI" start_time="2024-08-18T09:00:00" end_time="2024-08-18T10:00:00" speaker_id=1
   ```

   This will create a talk associated with the speaker created earlier and return the talk data.

3. Retrieve all speakers:

   ```bash
   http http://127.0.0.1:8000/speakers/
   ```

   This endpoint will return a list of all speakers in the database.

4. Retrieve a specific speaker with talks:

   ```bash
   http http://127.0.0.1:8000/speakers/1
   ```

   This will return the speaker data along with any talks associated with them.

5. Retrieve all talks:

   ```bash
   http http://127.0.0.1:8000/talks/
   ```

6. Retrieve a specific talk:
   ```bash
   http http://127.0.0.1:8000/talks/1
   ```

You can modify these requests to test other endpoints and functionalities.

## Dockerizing the Application

In many cases, you may want to containerize your FastAPI application for deployment. Here's how you can create a Dockerfile for your project:

```Dockerfile
FROM python:3.12-slim

WORKDIR /app

COPY requirements.txt .

RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

This Dockerfile uses a slim Python image, installs dependencies from a `requirements.txt` file, copies the project files, and runs the Uvicorn server.

To build a new Docker image, run the following command in your project directory:

```bash
docker build -t fastapi-neon-conference-api .
```

You can then run the Docker container with:

```bash
docker run -d -p 8000:8000 fastapi-neon-conference-api
```

This will start the FastAPI application in a Docker container, accessible on port 8000 of your host machine.

You need to make sure that your `.env` file is not included in the Docker image. Instead, you can pass the environment variables as arguments when running the container.

## Performance Considerations

1. **Database Indexing**: We've added indexes to frequently queried fields (`id`, `name`, `title`) in our models. This improves query performance. To learn more about indexing, refer to the [Neon documentation](/docs/postgres/indexes).

2. **Pagination**: The `read_speakers` and `read_talks` endpoints include `skip` and `limit` parameters for pagination, preventing the retrieval of unnecessarily large datasets.

3. **Dependency Injection**: By using `Depends(get_db)`, we make sure that database connections are properly managed and closed after each request. This prevents connection leaks and improves performance.

4. **Pydantic Models**: Using Pydantic for request and response models provides automatic data validation and serialization, reducing the need for manual checks.

5. **Relationships**: We've used SQLAlchemy relationships to efficiently load related data (speakers and their talks) in a single query.

## Conclusion

In this guide, we've built a simple API for managing a tech conference system using FastAPI, Pydantic, and Neon Postgres.

This combination provides a very good foundation for building scalable and efficient web services. FastAPI's speed and ease of use, combined with Pydantic's powerful data validation and Neon's serverless Postgres, make for a formidable tech stack.

As a next step, you can extend the API with more features like authentication, authorization, and advanced query capabilities. You can check out the [Implementing Secure User Authentication in FastAPI using JWT Tokens and Neon Postgres](/guides/fastapi-jwt) guide for adding JWT-based authentication to your API.

## Additional Resources

- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Pydantic Documentation](https://docs.pydantic.dev/latest/)
- [SQLAlchemy Documentation](https://docs.sqlalchemy.org/)
- [Neon Documentation](/docs)

<NeedHelp />


# Implementing Webhooks with FastAPI and Neon Postgres

---
title: Implementing Webhooks with FastAPI and Neon Postgres
subtitle: Learn how to build a webhook system to receive and store event data using FastAPI and Neon Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2025-03-23T00:00:00.000Z'
updatedOn: '2025-03-23T00:00:00.000Z'
---

Webhooks are a way for services to communicate with each other by sending HTTP requests when specific events occur. They allow your application to receive real-time data from other services without having to constantly poll for updates.

In this guide, you'll learn how to implement a webhook system using FastAPI to receive event notifications and Neon Postgres to store and process the webhook data. We'll build a simple but practical webhook receiver that can handle events from GitHub, making this applicable to real-world development workflows.

## Prerequisites

To follow this guide, you need:

- [Python 3.9+](https://www.python.org/downloads/) installed
- Basic knowledge of Python and FastAPI
- A [Neon](https://console.neon.tech/signup) account
- [ngrok](https://ngrok.com/) or similar tool for exposing your local server (for testing)
- A [GitHub](https://github.com/) account (for testing the webhook)

## Create a Neon Project

Let's start by creating a new Neon project and setting up a Postgres database:

1. Log in to your [Neon Console](https://console.neon.tech)
2. Click "New Project"
3. Enter a name for your project, like "webhook-receiver"
4. Select your preferred region
5. Click "Create Project"

Once your project is created, you'll see the connection details. Save these details as we'll need them for our FastAPI application.

## Set Up a FastAPI Project

FastAPI is a modern web framework for building APIs with Python. It's based on standard Python type hints and provides automatic OpenAPI documentation.

Now, let's set up a basic FastAPI project structure:

1. Create a new directory for your project and navigate to it:

```bash
mkdir webhook-receiver
cd webhook-receiver
```

2. Create a virtual environment and activate it:

```bash
python -m venv venv
source venv/bin/activate  # On Windows, use: venv\Scripts\activate
```

3. Install the required packages:

```bash
pip install fastapi uvicorn sqlalchemy asyncpg python-dotenv pydantic pydantic-settings httpx psycopg2-binary greenlet
```

4. Create a basic directory structure:

```bash
mkdir app
touch app/__init__.py
touch app/main.py
touch app/config.py
touch app/models.py
touch app/database.py
touch .env
```

5. Set up the environment variables by adding the following to the `.env` file:

```
DATABASE_URL=postgres://[user]:[password]@[hostname]/[database]?sslmode=require&channel_binding=require
WEBHOOK_SECRET=your_webhook_secret  # We'll use this later for verification
```

Replace the placeholders in the `DATABASE_URL` with your Neon connection details.

## Design the Database Schema

Before implementing our webhook receiver, we need to design the database schema. For our GitHub webhook example, we'll create a table to store webhook events with the following fields:

- `id`: A unique identifier for each webhook event
- `event_type`: The type of event (e.g., "push", "pull_request")
- `delivery_id`: The unique ID provided by GitHub for the webhook delivery
- `signature`: The signature sent with the webhook for verification
- `payload`: The JSON payload of the webhook
- `processed`: A boolean indicating if the webhook has been processed
- `created_at`: When the webhook was received

Now, let's set up the database connection and models.

## Create the Database Models

First, let's set up the configuration file (`app/config.py`):

```python
# app/config.py
from pydantic_settings import BaseSettings
from dotenv import load_dotenv

load_dotenv()

class Settings(BaseSettings):
    database_url: str
    webhook_secret: str

    class Config:
        env_file = ".env"

settings = Settings()
```

Here, we're using `pydantic-settings` to load environment variables from the `.env` file.

Next, let's set up the database connection (`app/database.py`):

```python
# app/database.py
import ssl
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker, declarative_base
from app.config import settings
from urllib.parse import urlparse

# Setup SSL context
ssl_context = ssl.create_default_context()
ssl_context.check_hostname = False
ssl_context.verify_mode = ssl.CERT_NONE

# Parse the database URL to remove query parameters
# that can cause issues with asyncpg
parsed_url = urlparse(settings.database_url)
db_user = parsed_url.username
db_password = parsed_url.password
db_host = parsed_url.hostname
db_port = parsed_url.port or 5432
db_name = parsed_url.path.lstrip('/')

# Create an async database URL without the query parameters
ASYNC_DATABASE_URL = f"postgresql+asyncpg://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"

# Create an async SQLAlchemy engine with SSL configuration
engine = create_async_engine(
    ASYNC_DATABASE_URL,
    connect_args={"ssl": ssl_context},
    pool_size=5,
    max_overflow=10,
    pool_timeout=30,
    pool_recycle=1800,
    pool_pre_ping=True,
    echo=False
)

# Create a session factory for creating database sessions
async_session = sessionmaker(
    engine,
    class_=AsyncSession,
    expire_on_commit=False
)

# Create a base class for declarative models
Base = declarative_base()

# Dependency to get an async database session
async def get_db():
    async with async_session() as session:
        try:
            yield session
            await session.commit()
        except Exception:
            await session.rollback()
            raise
        finally:
            await session.close()
```

With the above code, we've set up the database connection and a dependency to get an async database session in our FastAPI application.

Now let's create our database models (`app/models.py`) which will represent the webhook events:

```python
# app/models.py
from sqlalchemy import Column, Integer, String, Boolean, DateTime, Text, JSON
from sqlalchemy.sql import func
from app.database import Base

class WebhookEvent(Base):
    __tablename__ = "webhook_events"

    id = Column(Integer, primary_key=True, index=True)
    event_type = Column(String, index=True)
    delivery_id = Column(String, unique=True, index=True)
    signature = Column(String)
    payload = Column(JSON)
    processed = Column(Boolean, default=False)
    created_at = Column(DateTime(timezone=True), server_default=func.now())

    def __repr__(self):
        return f"<WebhookEvent(id={self.id}, event_type='{self.event_type}')>"
```

This model represents a webhook event with the fields we defined earlier. We'll use this model to store webhook events in our Postgres database.

## Implement Webhook Endpoints

With the database models in place, we can now implement the webhook endpoint to receive and store GitHub webhook events.

Now, let's implement the FastAPI application with our webhook endpoint. Update `app/main.py`:

```python
# app/main.py
import json
import hmac
import hashlib
from fastapi import FastAPI, Request, Depends, Header, HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select
from app.database import get_db, engine, Base
from app.models import WebhookEvent
from app.config import settings

app = FastAPI(title="Webhook Receiver")

# Create database tables if they don't exist
@app.on_event("startup")
async def startup():
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

@app.get("/")
async def root():
    return {"message": "Webhook Receiver is running"}

@app.get("/webhooks/events")
async def view_webhook_events(limit: int = 10, db: AsyncSession = Depends(get_db)):
    """View recent webhook events - useful for debugging."""
    result = await db.execute(select(WebhookEvent).order_by(WebhookEvent.created_at.desc()).limit(limit))
    events = result.scalars().all()
    return events

@app.post("/webhooks/github")
async def github_webhook(
    request: Request,
    x_github_event: str = Header(None),
    x_github_delivery: str = Header(None),
    x_hub_signature_256: str = Header(None),
    db: AsyncSession = Depends(get_db)
):
    # Read the request body
    body = await request.body()

    # Verify the webhook signature (we'll implement this next)
    is_valid = verify_signature(body, x_hub_signature_256)
    if not is_valid:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid signature"
        )

    # Parse the JSON payload
    try:
        payload = json.loads(body)
    except json.JSONDecodeError:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid JSON payload"
        )

    # Store the webhook event in the database
    webhook_event = WebhookEvent(
        event_type=x_github_event,
        delivery_id=x_github_delivery,
        signature=x_hub_signature_256,
        payload=payload,
        processed=False
    )

    db.add(webhook_event)
    await db.commit()
    await db.refresh(webhook_event)

    # Process the webhook event (we'll implement this later)
    await process_webhook_event(webhook_event.id, db)

    return {"status": "success", "event_id": webhook_event.id}

# Placeholder functions to be implemented
def verify_signature(body, signature):
    # We'll implement this next
    return True

async def process_webhook_event(event_id, db):
    # We'll implement this later
    pass
```

Here we are setting up a FastAPI application with a `/webhooks/github` endpoint to receive GitHub webhook events. The endpoint reads the request body, verifies the webhook signature, parses the JSON payload, stores the event in the database, and processes the event asynchronously.

## Add Webhook Verification

GitHub sends a signature with each webhook to verify that the webhook is coming from GitHub. Let's implement the signature verification function:

```python
def verify_signature(body, signature):
    if not signature:
        return False

    # The signature from GitHub starts with 'sha256='
    if not signature.startswith("sha256="):
        return False

    # Remove the 'sha256=' prefix
    signature = signature[7:]

    # Calculate the HMAC SHA256 signature using our webhook secret
    secret = settings.webhook_secret.encode()
    expected_signature = hmac.new(secret, body, hashlib.sha256).hexdigest()

    # Compare the calculated signature with the one from GitHub
    return hmac.compare_digest(expected_signature, signature)
```

Replace the placeholder `verify_signature` function with this implementation. This function verifies the webhook signature by calculating the HMAC SHA256 signature using the webhook secret and comparing it with the signature sent by GitHub.

## Process Webhook Events

Now, let's implement the `process_webhook_event` function to handle different types of GitHub webhook events:

```python
async def process_webhook_event(event_id, db):
    # Fetch the webhook event from the database
    result = await db.execute(select(WebhookEvent).where(WebhookEvent.id == event_id))
    event = result.scalars().first()

    if not event:
        return

    try:
        # Process different event types
        if event.event_type == "push":
            await process_push_event(event)
        elif event.event_type == "pull_request":
            await process_pull_request_event(event)
        elif event.event_type == "issues":
            await process_issue_event(event)
        # Add more event types as needed

        # Mark the event as processed
        event.processed = True
        await db.commit()

    except Exception as e:
        print(f"Error processing webhook event {event_id}: {e}")

async def process_push_event(event):
    """Process a GitHub push event."""
    payload = event.payload
    repo_name = payload.get("repository", {}).get("full_name")
    ref = payload.get("ref")
    commits = payload.get("commits", [])

    print(f"Push to {repo_name} on {ref} with {len(commits)} commits")
    # Handle the push event based on the commits

async def process_pull_request_event(event):
    """Process a GitHub pull request event."""
    payload = event.payload
    action = payload.get("action")
    pr_number = payload.get("number")
    repo_name = payload.get("repository", {}).get("full_name")

    print(f"Pull request #{pr_number} {action} in {repo_name}")
    # Handle the pull request based on the action (opened, closed, etc.)

async def process_issue_event(event):
    """Process a GitHub issue event."""
    payload = event.payload
    action = payload.get("action")
    issue_number = payload.get("issue", {}).get("number")
    repo_name = payload.get("repository", {}).get("full_name")

    print(f"Issue #{issue_number} {action} in {repo_name}")
    # Handle the issue based on the action (opened, closed, etc.)
```

In the `process_webhook_event` function, we fetch the webhook event from the database and process it based on the event type. We've provided placeholder functions for processing different types of GitHub webhook events, such as push events, pull request events, and issue events. You can extend these functions to handle other event types as needed.

## Test Your Webhook Receiver

Now that we have implemented our webhook receiver, let's run it and test it with GitHub webhooks.

We will use ngrok to expose our local server to the internet so that GitHub can send webhook events to our FastAPI application. If you haven't installed ngrok yet, you can download it from [ngrok.com](https://ngrok.com/).

Once you have an account and ngrok installed, follow these steps to test your webhook receiver:

1. Start your FastAPI application:

```bash
uvicorn app.main:app --reload
```

2. Use ngrok to expose your local server to the internet:

```bash
ngrok http 8000
```

ngrok will provide you with a public URL (e.g., `https://abc123.ngrok.io`) that forwards to your local server. For testing, you can use this URL as the webhook endpoint. For production, you would want to deploy your FastAPI application to a server with a public IP address and domain along with an SSL certificate.

3. If you don't have a GitHub repository to test with, create a new repository or use an existing one, and set up a webhook:
   - Go to your GitHub repository
   - Click on "Settings" > "Webhooks" > "Add webhook"
   - Set "Payload URL" to your ngrok URL + `/webhooks/github` (e.g., `https://abc123.ngrok.io/webhooks/github`)
   - Set "Content type" to `application/json`
   - Set "Secret" to the same value as your `WEBHOOK_SECRET` in the `.env` file
   - Choose which events you want to receive (e.g., "Just the push event")
   - Click "Add webhook"

4. Trigger an event in your repository:
   - Make a commit and push to the repository
   - Create or update an issue
   - Open a pull request

5. Monitor your FastAPI application logs to see the webhook events being received and processed.

After following these steps, you should see the webhook events being received by your FastAPI application and processed based on the event type and then stored in your Neon Postgres database.

To view the stored webhook events, you can access the `/webhooks/events` endpoint. You should see the recent webhook events stored in the database returned as JSON.

If you were to visit the `/docs` endpoint of your FastAPI application, you would see the automatically generated API documentation with details about your webhook endpoint.

## Security Considerations

When implementing webhooks in a production environment, consider these security practices:

1. We implemented signature validation for GitHub webhooks, but make sure to do this for any webhook provider. Make sure to use a secure secret for signing the webhook payloads and store it securely.

2. Always use HTTPS to encrypt webhook payloads in transit.

3. Protect your webhook endpoint from abuse by implementing rate limiting.

4. Set reasonable timeouts for webhook processing to prevent long-running tasks from blocking your application.

5. Make sure your webhook handling is idempotent, meaning the same webhook can be processed multiple times without causing problems (useful for retries).

6. Store the raw webhook payload initially, then process it asynchronously. This helps with debugging and retrying failed webhooks.

## Conclusion

In this guide, you built a FastAPI application backed by Neon Postgres to securely receive and process webhook events. Along the way, you learned how to define a data model, implement a webhook endpoint, verify signatures, and handle different event types.

Webhooks are an important part of many API integrations, allowing your applications to respond to events in real-time. By combining FastAPI with Neon Postgres, you can build webhook receivers that can handle various types of event notifications from external services.

You can extend this basic webhook system to handle events from other services like Stripe (for payment notifications), Slack (for user interactions), or any other service that supports webhooks.

<NeedHelp />


# Implementing Feature Flags with Go, Neon Postgres, and Server-Side Rendering

---
title: Implementing Feature Flags with Go, Neon Postgres, and Server-Side Rendering
subtitle: Learn how to create a feature flag system using Go, Neon Postgres, and server-side rendering for controlled feature rollouts
author: bobbyiliev
enableTableOfContents: true
createdAt: '2025-03-29T00:00:00.000Z'
updatedOn: '2025-03-29T00:00:00.000Z'
---

Feature flags are a technique that allows developers to modify system behavior without changing code. They enable you to control when features are visible to specific users, perform A/B testing, and implement kill switches for problematic features.

In this guide, you'll learn how to implement a feature flag system using Go, Neon Postgres, and server-side rendering. This approach allows for feature visibility decisions to happen on the server, providing better security and performance compared to client-side feature flags.

## Prerequisites

To follow the steps in this guide, you will need the following:

- [Go](https://golang.org/dl/) 1.20 or later installed
- A [Neon](https://console.neon.tech/signup) account
- Basic familiarity with SQL and Go programming
- [Docker](https://www.docker.com/get-started) (optional, for containerization)

## Create a Neon project

First, let's create a Neon project to store our feature flag configurations.

1. Navigate to the [Neon Console](https://console.neon.tech/app/projects) and click "New Project".
2. Give your project a name, such as "feature-flags".
3. Choose your preferred region.
4. Click "Create Project".

After your project is created, you'll receive a connection string that looks like this:

```
postgres://[user]:[password]@[hostname]/[dbname]?sslmode=require&channel_binding=require
```

Save this connection string, you'll need it to connect your Go application to the Neon database.

## Set up the database schema

Now that we have our Neon project, let's create the database schema for our feature flag system. We'll need tables to store feature flags, their rules, and user segments.

Connect to your database using your preferred SQL client or the Neon SQL Editor in the console, and execute the following SQL:

```sql
-- Create feature flags table
CREATE TABLE feature_flags (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) UNIQUE NOT NULL,
    description TEXT,
    enabled BOOLEAN NOT NULL DEFAULT false,
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
);

-- Create user segments table (for targeting specific user groups)
CREATE TABLE segments (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) UNIQUE NOT NULL,
    description TEXT,
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
);

-- Create rules table (associates flags with segments and specifies conditions)
CREATE TABLE rules (
    id SERIAL PRIMARY KEY,
    flag_id INTEGER REFERENCES feature_flags(id) ON DELETE CASCADE,
    segment_id INTEGER REFERENCES segments(id) ON DELETE CASCADE,
    percentage INTEGER NOT NULL DEFAULT 100, -- For percentage rollouts
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT percentage_range CHECK (percentage >= 0 AND percentage <= 100),
    UNIQUE(flag_id, segment_id)
);

-- Create user attributes table (for identifying users that belong to segments)
CREATE TABLE segment_conditions (
    id SERIAL PRIMARY KEY,
    segment_id INTEGER REFERENCES segments(id) ON DELETE CASCADE,
    attribute VARCHAR(100) NOT NULL, -- e.g., "country", "email", "role"
    operator VARCHAR(20) NOT NULL, -- e.g., "equals", "contains", "startsWith"
    value TEXT NOT NULL,
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
);
```

This schema gives us a flexible feature flag setup that can:

- Define named feature flags
- Create user segments based on attributes
- Set rules for flag visibility, including percentage rollouts
- Control which segments see which features

Let's insert some sample data to work with:

```sql
-- Insert some feature flags
INSERT INTO feature_flags (name, description, enabled) VALUES
('new_dashboard', 'New user dashboard with improved visualizations', true),
('dark_mode', 'Dark mode theme across the application', false),
('beta_api', 'New API endpoints for beta testers', true);

-- Insert some user segments
INSERT INTO segments (name, description) VALUES
('beta_testers', 'Users who opted into beta features'),
('premium_users', 'Users with paid subscription accounts'),
('internal_staff', 'Employees and contractors');

-- Associate flags with segments
INSERT INTO rules (flag_id, segment_id, percentage) VALUES
(1, 2, 100), -- new_dashboard available to 100% of premium_users
(3, 1, 100), -- beta_api available to 100% of beta_testers
(2, 3, 50);  -- dark_mode available to 50% of internal_staff

-- Define conditions for segments
INSERT INTO segment_conditions (segment_id, attribute, operator, value) VALUES
(1, 'email', 'endsWith', '@example.com'),
(2, 'subscription', 'equals', 'premium'),
(3, 'email', 'endsWith', '@ourcompany.com');
```

With our database schema and sample data in place, we're ready to create our Go application.

## Create the Go application

Let's set up a new Go application for our feature flag system. We'll use standard Go modules and a clean project structure.

Create a new directory for your project and initialize a Go module:

```bash
mkdir feature-flag-system
cd feature-flag-system
go mod init github.com/yourusername/feature-flag-system
```

Now let's install the required dependencies:

```bash
go get github.com/jackc/pgx/v5          # PostgreSQL driver
go get github.com/gorilla/mux           # HTTP router
go get github.com/joho/godotenv         # Environment variable management
go get github.com/google/uuid           # For generating unique IDs
go get github.com/jmoiron/sqlx          # Enhanced database operations
```

Create a basic project structure:

```bash
mkdir -p cmd/server
mkdir -p internal/db
mkdir -p internal/featureflags
mkdir -p internal/handlers
mkdir -p web/templates
```

This structure follows an essential Go project layout:

- `cmd/server`: Entry point for the server application
- `internal`: Internal packages that aren't meant to be imported by other projects
- `web/templates`: HTML templates for server-side rendering

Now, let's create a configuration file to store our database connection details. Create a new file named `.env` in the project root:

```
DATABASE_URL=postgres://[user]:[password]@[hostname]/[dbname]?sslmode=require&channel_binding=require&channel_binding=require
SERVER_PORT=8080
```

Replace the placeholder values in `DATABASE_URL` with your actual Neon connection string.

## Implement the feature flag service

Now we'll create the core of our feature flag system, the service that checks if features should be enabled for specific users.

First, let's create the database connection layer. Create a file at `internal/db/db.go`:

```go
package db

import (
	"log"

	"github.com/jmoiron/sqlx"
	_ "github.com/jackc/pgx/v5/stdlib"
)

// DB is our database wrapper
type DB struct {
	*sqlx.DB
}

// NewDB creates a new database connection
func NewDB(connectionString string) (*DB, error) {
	db, err := sqlx.Connect("postgres", connectionString)
	if err != nil {
		return nil, err
	}

	// Test the connection
	if err := db.Ping(); err != nil {
		return nil, err
	}

	log.Println("Connected to the database successfully")
	return &DB{db}, nil
}
```

Now, let's create our feature flag models and service. Create a file at `internal/featureflags/models.go`:

```go
package featureflags

// FeatureFlag represents a feature flag in the system
type FeatureFlag struct {
	ID          int    `db:"id" json:"id"`
	Name        string `db:"name" json:"name"`
	Description string `db:"description" json:"description"`
	Enabled     bool   `db:"enabled" json:"enabled"`
}

// Segment represents a user segment
type Segment struct {
	ID          int    `db:"id" json:"id"`
	Name        string `db:"name" json:"name"`
	Description string `db:"description" json:"description"`
}

// Rule associates feature flags with segments
type Rule struct {
	ID         int `db:"id" json:"id"`
	FlagID     int `db:"flag_id" json:"flag_id"`
	SegmentID  int `db:"segment_id" json:"segment_id"`
	Percentage int `db:"percentage" json:"percentage"`
}

// Condition represents a condition for a segment
type Condition struct {
	ID        int    `db:"id" json:"id"`
	SegmentID int    `db:"segment_id" json:"segment_id"`
	Attribute string `db:"attribute" json:"attribute"`
	Operator  string `db:"operator" json:"operator"`
	Value     string `db:"value" json:"value"`
}

// User represents a user in the system for feature flag evaluation
type User struct {
	ID         string
	Attributes map[string]string
}
```

The models define the structure of our feature flags, segments, rules, and user attributes. The `User` struct will be used to represent users when checking feature flag visibility.

Now create the feature flag service at `internal/featureflags/service.go`:

```go
package featureflags

import (
	"fmt"
	"hash/fnv"
	"log"
	"strings"

	"github.com/yourusername/feature-flag-system/internal/db"
)

// Service provides methods for interacting with feature flags
type Service struct {
	db *db.DB
}

// NewService creates a new feature flag service
func NewService(db *db.DB) *Service {
	return &Service{db: db}
}

// IsEnabled checks if a feature flag is enabled for a specific user
func (s *Service) IsEnabled(flagName string, user *User) (bool, error) {
	// First, check if the flag exists and is globally enabled
	var flag FeatureFlag
	err := s.db.Get(&flag, "SELECT * FROM feature_flags WHERE name = $1", flagName)
	if err != nil {
		return false, fmt.Errorf("flag not found: %w", err)
	}

	// If the flag is disabled globally, return false immediately
	if !flag.Enabled {
		return false, nil
	}

	// Get all rules for this flag
	var rules []struct {
		Rule
		SegmentName string `db:"segment_name"`
	}
	err = s.db.Select(&rules, `
		SELECT r.*, s.name as segment_name
		FROM rules r
		JOIN segments s ON r.segment_id = s.id
		WHERE r.flag_id = $1
	`, flag.ID)
	if err != nil {
		return false, fmt.Errorf("error getting rules: %w", err)
	}

	// If no rules exist, the flag is enabled for everyone
	if len(rules) == 0 {
		return true, nil
	}

	// Check each rule to see if the user matches
	for _, rule := range rules {
		isInSegment, err := s.isUserInSegment(user, rule.SegmentID)
		if err != nil {
			log.Printf("Error checking segment: %v", err)
			continue
		}

		if isInSegment {
			// Check percentage rollout
			if rule.Percentage < 100 {
				hash := hashUserID(user.ID, flagName)
				percentage := hash % 100
				if percentage >= rule.Percentage {
					continue // Not included in the percentage rollout
				}
			}
			return true, nil
		}
	}

	// If no rules matched, the feature is disabled for this user
	return false, nil
}

// isUserInSegment checks if a user is in a specific segment
func (s *Service) isUserInSegment(user *User, segmentID int) (bool, error) {
	// Get the conditions for this segment
	var conditions []Condition
	err := s.db.Select(&conditions, "SELECT * FROM segment_conditions WHERE segment_id = $1", segmentID)
	if err != nil {
		return false, fmt.Errorf("error getting segment conditions: %w", err)
	}

	// If no conditions, segment is empty
	if len(conditions) == 0 {
		return false, nil
	}

	// Check all conditions
	for _, condition := range conditions {
		attributeValue, exists := user.Attributes[condition.Attribute]
		if !exists {
			return false, nil // User doesn't have this attribute
		}

		// Evaluate the condition
		match := false
		switch condition.Operator {
		case "equals":
			match = attributeValue == condition.Value
		case "contains":
			match = strings.Contains(attributeValue, condition.Value)
		case "startsWith":
			match = strings.HasPrefix(attributeValue, condition.Value)
		case "endsWith":
			match = strings.HasSuffix(attributeValue, condition.Value)
		default:
			return false, fmt.Errorf("unknown operator: %s", condition.Operator)
		}

		if !match {
			return false, nil
		}
	}

	return true, nil
}

// GetAllFlags returns all feature flags in the system
func (s *Service) GetAllFlags() ([]FeatureFlag, error) {
	var flags []FeatureFlag
	err := s.db.Select(&flags, "SELECT * FROM feature_flags ORDER BY name")
	if err != nil {
		return nil, fmt.Errorf("error getting flags: %w", err)
	}
	return flags, nil
}

// UpdateFlag updates a feature flag's enabled status
func (s *Service) UpdateFlag(id int, enabled bool) error {
	_, err := s.db.Exec(
		"UPDATE feature_flags SET enabled = $1, updated_at = CURRENT_TIMESTAMP WHERE id = $2",
		enabled, id,
	)
	if err != nil {
		return fmt.Errorf("error updating flag: %w", err)
	}
	return nil
}

// hashUserID creates a consistent hash of a user ID and flag name
// This ensures the same user gets the same behavior for a specific flag
func hashUserID(userID, flagName string) int {
	h := fnv.New32a()
	h.Write([]byte(userID + flagName))
	return int(h.Sum32() % 100)
}
```

The `Service` provides a set of methods to interact with the feature flags in the database:

- Check if a feature flag is enabled for a specific user
- Determine if a user belongs to a segment based on their attributes
- Get all feature flags in the system
- Update a feature flag's enabled status

The `IsEnabled` method is the core of our feature flag system. It:

1. Checks if the flag exists and is globally enabled
2. Gets all rules for the flag
3. For each rule, checks if the user is in the segment
4. For percentage rollouts, uses a hash of the user ID and flag name to ensure consistent behavior

## Create the web server

Now let's create the web server that will serve our application with server-side rendering. First, let's create the main server file at `cmd/server/main.go`:

```go
package main

import (
	"fmt"
	"log"
	"net/http"
	"os"

	"github.com/gorilla/mux"
	"github.com/joho/godotenv"
	"github.com/yourusername/feature-flag-system/internal/db"
	"github.com/yourusername/feature-flag-system/internal/featureflags"
	"github.com/yourusername/feature-flag-system/internal/handlers"
)

func main() {
	// Load environment variables
	if err := godotenv.Load(); err != nil {
		log.Printf("Warning: .env file not found: %v", err)
	}

	// Get database connection string
	dbURL := os.Getenv("DATABASE_URL")
	if dbURL == "" {
		log.Fatal("DATABASE_URL environment variable is required")
	}

	// Set up database
	database, err := db.NewDB(dbURL)
	if err != nil {
		log.Fatalf("Failed to connect to database: %v", err)
	}
	defer database.Close()

	// Create feature flag service
	flagService := featureflags.NewService(database)

	// Create router
	r := mux.NewRouter()

	// Create handlers
	h := handlers.NewHandlers(flagService)

	// Register routes
	r.HandleFunc("/", h.HomePage).Methods("GET")
	r.HandleFunc("/admin", h.AdminPage).Methods("GET")
	r.HandleFunc("/api/flags", h.GetAllFlags).Methods("GET")
	r.HandleFunc("/api/flags/{id}", h.UpdateFlag).Methods("PUT")

	// Serve static files
	r.PathPrefix("/static/").Handler(
		http.StripPrefix("/static/", http.FileServer(http.Dir("./web/static"))),
	)

	// Start server
	port := os.Getenv("SERVER_PORT")
	if port == "" {
		port = "8080"
	}

	log.Printf("Starting server on port %s", port)
	log.Fatal(http.ListenAndServe(fmt.Sprintf(":%s", port), r))
}
```

Here we are setting up the server with the following features:

- Loading environment variables from a `.env` file using `godotenv`
- Connecting to the Neon database using the `db` package
- Creating a `featureflags.Service` instance
- Setting up routes with the `gorilla/mux` router
- Serving static files from the `web/static` directory

Next, let's create the HTML templates for our application.

## Implement server-side rendering with feature flags

Now let's create the handlers that will render our templates based on the feature flags. Create a file at `internal/handlers/handlers.go`:

```go
package handlers

import (
	"encoding/json"
	"html/template"
	"log"
	"net/http"
	"strconv"

	"github.com/gorilla/mux"
	"github.com/yourusername/feature-flag-system/internal/featureflags"
)

// Handlers contains the HTTP handlers for the application
type Handlers struct {
	flagService *featureflags.Service
	templates   map[string]*template.Template
}

// NewHandlers creates a new Handlers instance
func NewHandlers(flagService *featureflags.Service) *Handlers {
	// Parse templates
	templates := make(map[string]*template.Template)
	templates["home"] = template.Must(template.ParseFiles(
		"web/templates/base.html",
		"web/templates/home.html",
	))
	templates["admin"] = template.Must(template.ParseFiles(
		"web/templates/base.html",
		"web/templates/admin.html",
	))

	return &Handlers{
		flagService: flagService,
		templates:   templates,
	}
}

// HomePage renders the home page with feature flags
func (h *Handlers) HomePage(w http.ResponseWriter, r *http.Request) {
	// Create a user from request information
	user := createUserFromRequest(r)

	// Check feature flags
	newDashboard, err := h.flagService.IsEnabled("new_dashboard", user)
	if err != nil {
		log.Printf("Error checking new_dashboard flag: %v", err)
		newDashboard = false
	}

	darkMode, err := h.flagService.IsEnabled("dark_mode", user)
	if err != nil {
		log.Printf("Error checking dark_mode flag: %v", err)
		darkMode = false
	}

	betaApi, err := h.flagService.IsEnabled("beta_api", user)
	if err != nil {
		log.Printf("Error checking beta_api flag: %v", err)
		betaApi = false
	}

	// Prepare template data
	data := map[string]interface{}{
		"Title":        "Feature Flag Demo",
		"User":         user,
		"NewDashboard": newDashboard,
		"DarkMode":     darkMode,
		"BetaApi":      betaApi,
	}

	// Render template
	h.templates["home"].ExecuteTemplate(w, "base", data)
}

// AdminPage renders the admin page for managing feature flags
func (h *Handlers) AdminPage(w http.ResponseWriter, r *http.Request) {
	flags, err := h.flagService.GetAllFlags()
	if err != nil {
		http.Error(w, "Error loading flags", http.StatusInternalServerError)
		log.Printf("Error loading flags: %v", err)
		return
	}

	data := map[string]interface{}{
		"Title": "Feature Flag Admin",
		"Flags": flags,
	}

	h.templates["admin"].ExecuteTemplate(w, "base", data)
}

// GetAllFlags returns all feature flags as JSON
func (h *Handlers) GetAllFlags(w http.ResponseWriter, r *http.Request) {
	flags, err := h.flagService.GetAllFlags()
	if err != nil {
		http.Error(w, "Error loading flags", http.StatusInternalServerError)
		log.Printf("Error loading flags: %v", err)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(flags)
}

// UpdateFlag updates a feature flag's enabled status
func (h *Handlers) UpdateFlag(w http.ResponseWriter, r *http.Request) {
	vars := mux.Vars(r)
	idStr := vars["id"]

	id, err := strconv.Atoi(idStr)
	if err != nil {
		http.Error(w, "Invalid flag ID", http.StatusBadRequest)
		return
	}

	var updateData struct {
		Enabled bool `json:"enabled"`
	}

	if err := json.NewDecoder(r.Body).Decode(&updateData); err != nil {
		http.Error(w, "Invalid request body", http.StatusBadRequest)
		return
	}

	if err := h.flagService.UpdateFlag(id, updateData.Enabled); err != nil {
		http.Error(w, "Error updating flag", http.StatusInternalServerError)
		log.Printf("Error updating flag %d: %v", id, err)
		return
	}

	w.WriteHeader(http.StatusOK)
	json.NewEncoder(w).Encode(map[string]string{"status": "success"})
}

// createUserFromRequest extracts user information from the request
func createUserFromRequest(r *http.Request) *featureflags.User {
	// In a real application, you'd get this from your authentication system
	// For demo purposes, we'll use query parameters or default values
	userID := r.URL.Query().Get("user_id")
	if userID == "" {
		userID = "anonymous"
	}

	// Create a user with attributes
	user := &featureflags.User{
		ID: userID,
		Attributes: map[string]string{
			"email":        r.URL.Query().Get("email"),
			"country":      r.URL.Query().Get("country"),
			"subscription": r.URL.Query().Get("subscription"),
		},
	}

	// Set defaults if not provided
	if user.Attributes["email"] == "" {
		// For testing segment conditions
		if userID == "premium" {
			user.Attributes["email"] = "premium@example.com"
			user.Attributes["subscription"] = "premium"
		} else if userID == "beta" {
			user.Attributes["email"] = "beta@example.com"
		} else if userID == "internal" {
			user.Attributes["email"] = "employee@ourcompany.com"
		} else {
			user.Attributes["email"] = "user@regular.com"
			user.Attributes["subscription"] = "free"
		}
	}

	if user.Attributes["country"] == "" {
		user.Attributes["country"] = "US"
	}

	return user
}
```

The handlers:

1. Render pages using Go's template package
2. Check if features should be enabled for the current user
3. Pass feature flag information to the templates
4. Provide API endpoints for the admin interface

Now, let's create the HTML templates for our application. First, create a base template at `web/templates/base.html`:

```html
<!doctype html>
<html {{if .DarkMode}}class="dark" {{end}}>
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>{{.Title}}</title>
    <!-- Add Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
      tailwind.config = {
        darkMode: 'class',
        theme: {
          extend: {},
        },
      };
    </script>
  </head>
  <body class="text-gray-800 dark:text-gray-100 dark:bg-gray-900 mx-auto max-w-7xl p-5 font-sans">
    <div
      class="border-gray-200 dark:border-gray-700 mb-8 flex items-center justify-between border-b pb-3"
    >
      <h1 class="text-2xl font-bold">{{.Title}}</h1>
      <div class="nav">
        <a href="/" class="text-blue-600 dark:text-blue-400 ml-4 no-underline hover:underline"
          >Home</a
        >
        <a href="/admin" class="text-blue-600 dark:text-blue-400 ml-4 no-underline hover:underline"
          >Admin</a
        >
      </div>
    </div>

    {{template "content" .}}

    <script>
      // Common JavaScript functionality
    </script>
  </body>
</html>
```

Next, create the home page template at `web/templates/home.html`:

```html
{{define "content"}}
<div
  class="border-gray-200 dark:bg-gray-800 dark:border-gray-700 mb-8 rounded-lg border bg-white p-6 shadow-sm"
>
  <h2 class="text-gray-800 dark:text-gray-100 mb-4 text-xl font-semibold">Current User</h2>

  <div class="mb-6 space-y-2">
    <p class="flex"><span class="w-32 font-medium">User ID:</span> <span>{{.User.ID}}</span></p>
    <p class="flex">
      <span class="w-32 font-medium">Email:</span> <span>{{.User.Attributes.email}}</span>
    </p>
    <p class="flex">
      <span class="w-32 font-medium">Subscription:</span>
      <span>{{.User.Attributes.subscription}}</span>
    </p>
    <p class="flex">
      <span class="w-32 font-medium">Country:</span> <span>{{.User.Attributes.country}}</span>
    </p>
  </div>

  <h3 class="text-gray-800 dark:text-gray-100 mb-2 text-lg font-medium">Active Features:</h3>
  <ul class="mb-4 list-disc space-y-1 pl-5">
    {{if .NewDashboard}}
    <li class="text-green-600 dark:text-green-400">New Dashboard</li>
    {{end}} {{if .DarkMode}}
    <li class="text-green-600 dark:text-green-400">Dark Mode</li>
    {{end}} {{if .BetaApi}}
    <li class="text-green-600 dark:text-green-400">Beta API</li>
    {{end}}
  </ul>

  <p class="text-gray-500 dark:text-gray-400 text-xs italic">
    You can simulate different users by adding query parameters:
    <code class="bg-gray-100 dark:bg-gray-700 rounded px-1 py-0.5">?user_id=premium</code> or
    <code class="bg-gray-100 dark:bg-gray-700 rounded px-1 py-0.5">?user_id=beta</code> or
    <code class="bg-gray-100 dark:bg-gray-700 rounded px-1 py-0.5">?user_id=internal</code>
  </p>
</div>

<div class="space-y-6">
  <h2 class="text-gray-800 dark:text-gray-100 mb-6 text-2xl font-bold">
    Welcome to the Feature Flag Demo
  </h2>

  {{if .NewDashboard}}
  <div
    class="border-gray-200 dark:bg-gray-800 dark:border-gray-700 rounded-lg border bg-white p-6 shadow transition-all duration-300 hover:shadow-md"
  >
    <div class="mb-4 flex items-center">
      <h3 class="text-gray-800 dark:text-gray-100 text-xl font-semibold">Analytics Dashboard</h3>
      <span class="bg-orange-500 ml-2 inline-block rounded-full px-2 py-0.5 text-xs text-white"
        >New</span
      >
    </div>
    <p class="text-gray-600 dark:text-gray-300 mb-4">
      This is the new analytics dashboard with improved visualizations. You're seeing this because
      the 'new_dashboard' feature flag is enabled for you.
    </p>
    <div
      class="text-gray-500 bg-gray-100 dark:bg-gray-700 dark:text-gray-400 flex h-48 items-center justify-center rounded-lg font-medium"
    >
      [Fancy New Dashboard Chart]
    </div>
  </div>
  {{else}}
  <div
    class="border-gray-200 dark:bg-gray-800 dark:border-gray-700 rounded-lg border bg-white p-6 shadow transition-all duration-300 hover:shadow-md"
  >
    <h3 class="text-gray-800 dark:text-gray-100 mb-4 text-xl font-semibold">Analytics Dashboard</h3>
    <p class="text-gray-600 dark:text-gray-300 mb-4">
      This is the classic analytics dashboard. You're seeing this because the 'new_dashboard'
      feature flag is disabled for you.
    </p>
    <div
      class="text-gray-500 bg-gray-100 dark:bg-gray-700 dark:text-gray-400 flex h-36 items-center justify-center rounded-lg font-medium"
    >
      [Classic Dashboard]
    </div>
  </div>
  {{end}} {{if .BetaApi}}
  <div
    class="border-gray-200 dark:bg-gray-800 dark:border-gray-700 rounded-lg border bg-white p-6 shadow transition-all duration-300 hover:shadow-md"
  >
    <div class="mb-4 flex items-center">
      <h3 class="text-gray-800 dark:text-gray-100 text-xl font-semibold">API Explorer</h3>
      <span class="bg-orange-500 ml-2 inline-block rounded-full px-2 py-0.5 text-xs text-white"
        >Beta</span
      >
    </div>
    <p class="text-gray-600 dark:text-gray-300 mb-4">
      Welcome to the API Explorer. You're seeing this because the 'beta_api' feature flag is enabled
      for you.
    </p>
    <pre
      class="text-gray-800 bg-gray-100 dark:bg-gray-700 dark:text-gray-200 overflow-x-auto rounded-lg p-4 font-mono text-sm"
    >
GET /api/v2/data
Authorization: Bearer {your_token}</pre
    >
  </div>
  {{end}}
</div>
{{end}}
```

Finally, create the admin page template at `web/templates/admin.html`:

```html
{{define "content"}}
<div class="dark:bg-gray-800 mb-8 rounded-lg bg-white p-6 shadow-md">
  <h2 class="text-gray-800 dark:text-gray-100 mb-3 text-2xl font-bold">
    Feature Flag Administration
  </h2>
  <p class="text-gray-600 dark:text-gray-300 mb-8">
    Toggle feature flags on and off. Changes take effect immediately for all users.
  </p>

  <div class="grid grid-cols-1 gap-6 lg:grid-cols-3 md:grid-cols-2">
    {{range .Flags}}
    <div
      class="border-gray-200 dark:bg-gray-800 dark:border-gray-700 rounded-lg border bg-white p-5 shadow-sm transition-all duration-300 hover:shadow"
    >
      <div class="mb-4 flex items-start justify-between">
        <h3 class="text-gray-800 dark:text-gray-100 text-lg font-semibold">{{.Name}}</h3>
        <label class="relative inline-flex cursor-pointer items-center">
          <input
            type="checkbox"
            class="flag-toggle peer sr-only"
            data-id="{{.ID}}"
            {{if
            .Enabled}}checked{{end}}
          />
          <div
            class="bg-gray-200 peer-focus:ring-blue-300 dark:peer-focus:ring-blue-800 dark:bg-gray-700 after:border-gray-300 dark:border-gray-600 peer-checked:bg-blue-600 peer h-6 w-11 rounded-full after:absolute after:left-[2px] after:top-[2px] after:h-5 after:w-5 after:rounded-full after:border after:bg-white after:transition-all after:content-[''] peer-checked:after:translate-x-full peer-checked:after:border-white peer-focus:outline-none peer-focus:ring-4"
          ></div>
        </label>
      </div>
      <p class="text-gray-600 dark:text-gray-300 mb-3">{{.Description}}</p>
      <p class="text-gray-500 dark:text-gray-400 font-mono text-xs">ID: {{.ID}}</p>
    </div>
    {{end}}
  </div>
</div>

<script>
  // Add JavaScript to handle toggle switches
  document.addEventListener('DOMContentLoaded', function () {
    const toggles = document.querySelectorAll('.flag-toggle');

    toggles.forEach((toggle) => {
      toggle.addEventListener('change', function () {
        const flagId = this.dataset.id;
        const enabled = this.checked;

        fetch(`/api/flags/${flagId}`, {
          method: 'PUT',
          headers: {
            'Content-Type': 'application/json',
          },
          body: JSON.stringify({
            enabled: enabled,
          }),
        })
          .then((response) => {
            if (!response.ok) {
              throw new Error('Failed to update flag');
            }
            return response.json();
          })
          .then((data) => {
            console.log('Flag updated:', data);
          })
          .catch((error) => {
            console.error('Error:', error);
            // Revert the toggle state on error
            this.checked = !enabled;
            alert('Error updating feature flag. Please try again.');
          });
      });
    });
  });
</script>
{{end}}
```

## Create an admin interface

The admin interface we've built allows administrators to toggle feature flags on and off through the UI. It includes:

1. A list of all feature flags in the system
2. Toggle switches for enabling/disabling flags
3. JavaScript to update flags via API calls

The admin interface is already integrated into our application with the `AdminPage` handler and `admin.html` template.

## Test the feature flag system

Now let's test our feature flag system by running the application and trying different user scenarios.

First, create a directory for static files:

```bash
mkdir -p web/static
```

Run the application:

```bash
go run cmd/server/main.go
```

Visit `http://localhost:8080` in your browser to see the home page with feature flags in action.

Try these different user scenarios by adding query parameters:

1. Regular user: `http://localhost:8080`
2. Premium user: `http://localhost:8080?user_id=premium`
3. Beta tester: `http://localhost:8080?user_id=beta`
4. Internal staff: `http://localhost:8080?user_id=internal`

Each user should see different features based on the rules we set up:

- Premium users should see the new dashboard
- Beta testers should see the beta API
- 50% of internal staff should see dark mode (based on the user ID hash)

You can also visit the admin interface at `http://localhost:8080/admin` to toggle features on and off.

## Deploy the application

To deploy the application, we'll package it in a Docker container and prepare it for deployment to your preferred platform.

Create a `Dockerfile` in the project root:

```dockerfile
FROM golang:1.20-alpine AS builder

WORKDIR /app

# Copy go.mod and go.sum first to leverage Docker cache
COPY go.mod go.sum ./
RUN go mod download

# Copy the rest of the application
COPY . .

# Build the application
RUN CGO_ENABLED=0 GOOS=linux go build -o server ./cmd/server

# Use a smaller image for the final container
FROM alpine:latest

WORKDIR /app

# Copy the binary from the builder stage
COPY --from=builder /app/server .
COPY --from=builder /app/web ./web

# Expose the port the server runs on
EXPOSE 8080

# Set environment variables
ENV SERVER_PORT=8080

# Run the server
CMD ["./server"]
```

Build the Docker image:

```bash
docker build -t feature-flag-system .
```

You can run the container locally to test it:

```bash
docker run -p 8080:8080 --env-file .env feature-flag-system
```

To deploy to production, push the container to your container registry and deploy it to your preferred cloud platform (AWS, GCP, Azure, etc.).

Remember to set the `DATABASE_URL` environment variable in your deployment environment to point to your Neon database.

## Summary

In this guide, you built a server-rendered feature flag system using Go and Neon Postgres. You implemented a way to define flags and user segments in the database, control feature visibility based on user attributes, and gradually roll out features using percentage-based targeting.

By handling feature flag logic on the server, you ensure that users only see what they're meant to, making the system both secure and performant. This approach gives you full control over feature exposure without relying on client-side logic.

## Additional Resources

- [Neon Documentation](/docs)
- [Go Documentation](https://golang.org/doc/)
- [Feature Toggles (Martin Fowler)](https://martinfowler.com/articles/feature-toggles.html)

<NeedHelp />


# Add feature flags in SvelteKit apps with Neon Postgres

---
title: Add feature flags in SvelteKit apps with Neon Postgres
subtitle: A step-by-step guide to integrating feature flags in SvelteKit apps with Postgres
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-05-24T13:24:36.612Z'
updatedOn: '2024-05-24T13:24:36.612Z'
---

This guide covers the step-by-step process of integrating feature flags in SvelteKit apps with Postgres (powered by Neon). Feature flags provide a way to control the behavior of your application without deploying new code, allowing you to test and roll out new features dynamically. Upon completing the guide, you will understand how to manage and roll out new features using dynamic feature flag integration.

## Prerequisites

To follow the steps in this guide, you will need the following:

- [Node.js 18](https://nodejs.org/en/blog/announcements/v18-release-announce) or later
- A [Neon](https://console.neon.tech/signup) account – The feature flags will be defined (or mutated) in a Postgres database

## Steps

- [Provisioning a Postgres database powered by Neon](#provisioning-a-postgres-database-powered-by-neon)
- [Creating a new SvelteKit application](#creating-a-new-sveltekit-application)
- [(Optional) Adding Tailwind CSS to the application](#optional-adding-tailwind-css-to-the-application)
- [Managing Feature Flags in Serverless Postgres](#managing-feature-flags-in-serverless-postgres)
- [Dynamic Feature Flag Integration for Testing Fast Payment Methods](#dynamic-feature-flag-integration-for-testing-fast-payment-methods)

## Provisioning a Postgres database powered by Neon

Using Serverless Postgres database powered by Neon helps you scale down to zero. With Neon, you only have to pay for what you use.

To get started, go to the [Neon console](https://console.neon.tech/app/projects) and enter the name of your choice as the project name.

You will then be presented with a dialog that provides a connecting string of your database. Enable the **Connection pooling** toggle for a pooled connection string.

![](/guides/images/feature-flags-sveltekit/index.png)

All Neon connection strings have the following format:

```bash
postgres://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>
```

- `user` is the database user.
- `password` is the database user’s password.
- `endpoint_hostname` is the host with neon.tech as the [TLD](https://www.cloudflare.com/en-gb/learning/dns/top-level-domain/).
- `port` is the Neon port number. The default port number is 5432.
- `dbname` is the name of the database. “neondb” is the default database created with each Neon project.
- `?sslmode=require&channel_binding=require` optional query parameters that enforce the [SSL](https://www.cloudflare.com/en-gb/learning/ssl/what-is-ssl/) mode and channel binding while connecting to the Postgres instance for better security.

Save this connecting string somewhere safe to be used as the `DATABASE_URL` further in the guide. Proceed further in this guide to create a SvelteKit application.

## Creating a new SvelteKit application

To start building the application, create a new SvelteKit project. Open your terminal and run the following command:

```bash
npm create svelte@latest my-app
```

When prompted, choose:

- `Skeleton project` for **Which Svelte app template?**
- `Yes, using Typescript syntax` for **Add type checking with Typescript?**

Press **Enter** to proceed. Now, follow the instructions to install the dependencies and start the development server:

```bash
npm run dev
```

The app now should be running on [localhost:5173](http://localhost:5173).

> Note: According to an [advanced SvelteKit guide](https://kit.svelte.dev/docs/server-only-modules), using `.server` in the filename allows you to mark the code to be executed on server only.

Next, run the commands below to install the necessary libraries and packages for building the application:

<CodeTabs labels={["Neon serverless driver", "node-postgres"]}>

```bash
npm install @neondatabase/serverless
npm install -D dotenv tsx
```

```bash
npm install pg
npm install -D dotenv tsx @types/pg
```

</CodeTabs>

The commands install the required libraries and packages, with the `-D` flag specifying the libraries intended for development purposes only.

The libraries installed include:

<CodeTabs labels={["Neon serverless driver", "node-postgres"]}>

```markdown
- `@neondatabase/serverless`: Neon's serverless Postgres driver for JavaScript and TypeScript.
```

```markdown
- `pg`: A Postgres client for Node.js.
```

</CodeTabs>

The development-specific libraries include:

<CodeTabs labels={["Neon serverless driver", "node-postgres"]}>

```markdown
- `tsx`: A library for executing and rebuilding TypeScript efficiently.
- `dotenv`: A library for handling environment variables.
```

```markdown
- `@types/pg`: Type definitions for pg.
- `tsx`: A library for executing and rebuilding TypeScript efficiently.
- `dotenv`: A library for handling environment variables.
```

</CodeTabs>

## (Optional) Adding Tailwind CSS to the application

For styling the app, we will use Tailwind CSS. Install and set up Tailwind at the root of our project's directory by running:

```bash
npm install -D tailwindcss postcss autoprefixer
npx tailwindcss init -p
```

Create an `app.css` file in the `src` directory, and add the snippet below:

```css
@tailwind base;
@tailwind components;
@tailwind utilities;
```

Next, add the paths to all of your template files in your `tailwind.config.js` file:

```js
/** @type {import('tailwindcss').Config} */
export default {
  content: [], // [!code --]
  content: ['./src/**/*.{html,js,svelte,ts}'], // [!code ++]
  theme: {
    extend: {},
  },
  plugins: [],
};
```

Finally, add an import to `app.css` in your `+page.svelte` file:

```svelte
<script lang="ts"> // [!code ++]
  import '../app.css' // [!code ++]
</script> // [!code ++]

<!-- +page.svelte's HTML -->
```

## Managing Feature Flags in Serverless Postgres

Feature flags offer a powerful way to control the behavior of your application without deploying new code. In a Serverless Postgres environment, you can easily create, read, and update feature flags using the following steps:

### Create a serverless Postgres client

To create a client that interacts with your serverless postgres, create a `postgres.server.ts` file inside the `src/lib` directory with the following content:

<CodeTabs labels={["Neon serverless driver", "node-postgres"]}>

```tsx
// File: src/lib/postgres.server.ts

// Load Environment Variables
import 'dotenv/config';

// Load the postgres module
import { neon } from '@neondatabase/serverless';

// Create a connection string to the Neon Postgres instance
const connectionString: string = process.env.DATABASE_URL as string;

// Create a in-memory query function
export default neon(connectionString);
```

```tsx
// File: src/lib/postgres.server.ts

// Load Environment Variables
import 'dotenv/config';

// Load the postgres module
import pg from 'pg';

// Create a connection string to the Neon Postgres instance
const connectionString: string = process.env.DATABASE_URL as string;

// Create a in-memory pool so that it's cached for multiple calls
export default new pg.Pool({ connectionString });
```

</CodeTabs>

The code above starts with importing the Postgres client. It further imports the config module by `dotenv` that makes sure that all environment variables are populated in application environment. It then creates a new instance of a Postgres connection pool.

To create, read or update the feature flags from your SvelteKit application, you can use re-usable helper functions. Let's create a new directory by executing the following command in your terminal window to start creating those functions:

```bash
mkdir src/lib/feature_flags
```

### Create & Populate Feature Flags Database

In the `feature_flags` directory, create a file named `setup.server.ts` with the following code which will allow you to create and populate a database table for feature flags.

<CodeTabs labels={["Neon serverless driver", "node-postgres"]}>

```tsx
// File: src/lib/feature_flags/setup.server.ts

import sql from '../postgres.server';

async function populateFeatureFlags() {
  await sql`CREATE TABLE IF NOT EXISTS feature_flags (flagName text PRIMARY KEY, enabled boolean)`;
  console.log('✅ Setup database for feature flag');
  await sql`INSERT INTO feature_flags (flagName, enabled) VALUES ('fast_payments', true)`;
  console.log('✅ Setup an enabled feature flag to accept fast payment methods.');
}

populateFeatureFlags();
```

```tsx
// File: src/lib/feature_flags/setup.server.ts

import pool from '../postgres.server';

async function populateFeatureFlags() {
  await pool.query(
    'CREATE TABLE IF NOT EXISTS feature_flags (flagName text PRIMARY KEY, enabled boolean)'
  );
  console.log('✅ Setup database for feature flag');
  await pool.query({
    text: 'INSERT INTO feature_flags (flagName, enabled) VALUES ($1, $2)',
    values: ['fast_payments', true],
  });
  console.log('✅ Setup an enabled feature flag to accept fast payment methods.');
}

populateFeatureFlags();
```

</CodeTabs>

The code snippet above first ensures the existence of a table named `feature_flags` in the Postgres database. Then, it inserts a feature flag named `fast_payments` with the value `true`, indicating that fast payment methods are enabled.

### Read and update the feature flags

In the `feature_flags` directory, create a file named `get.server.ts` with the following code which will allow you to read the feature flag value in the database.

<CodeTabs labels={["Neon serverless driver", "node-postgres"]}>

```tsx
// File: src/lib/feature_flags/get.server.ts

import sql from '../postgres.server';

export const isEnabled = async (flagName: string): Promise<boolean> => {
  const rows = await sql`SELECT enabled FROM feature_flags WHERE flagName = ${flagName}`;
  return rows[0].enabled;
};
```

```tsx
// File: src/lib/feature_flags/get.server.ts

import pool from '../postgres.server';

export const isEnabled = async (flagName: string): Promise<boolean> => {
  const { rows } = await pool.query({
    text: 'SELECT enabled FROM feature_flags WHERE flagName = $1',
    values: [flagName],
  });
  return rows[0].enabled;
};
```

</CodeTabs>

The `isEnabled` function queries the database to check whether a specific feature flag is enabled or not. In this example, you will use it to check if `fast_payments` feature flag is enabled or not.

In the `feature_flags` directory, create a file named `set.server.ts` with the following code which will allow you to update the feature flag value in the database.

<CodeTabs labels={["Neon serverless driver", "node-postgres"]}>

```tsx
// File: src/lib/feature_flags/set.server.ts

import sql from '../postgres.server';

export const setEnabled = async (flagName: string, flagValue: boolean) => {
  await sql`UPDATE feature_flags SET enabled = ${flagValue} WHERE flagName = ${flagName}`;
};
```

```tsx
// File: src/lib/feature_flags/set.server.ts

import pool from '../postgres.server';

export const setEnabled = async (flagName: string, flagValue: boolean) => {
  await pool.query({
    text: 'UPDATE feature_flags SET enabled = $2 WHERE flagName = $1',
    values: [flagName, flagValue],
  });
};
```

</CodeTabs>

The `setEnabled` function updates the value of a feature flag in the database. In this example, you will update the `fast_payments` feature dynamically per request to get a taste of how feature flags are used in production.

Great! You can use these helper functions in your application to manage and control feature flags dynamically.

## Dynamic feature flag integration for testing fast payment methods

In this section, you will get an example of how a feature flag helps test and roll out new features, dynamically. For example, you are a payment processing company. You have just added a payment method named `PayGM` that helps users pay faster. But you want to test it out on a random basis for each cart that you process. Let's walk through the hypothetical code to understand the usage of a feature flag in this case.

### Computing the user destination

In a SvelteKit route, the data from the server to the user interface is passed via `+page.server.ts` file to `+page.svelte`. For the sake of this example, you will load the feature flag dynamically and check if it's enabled to determine the user's destination experience. To do that, add the following snippet to `+page.server.ts` file:

```tsx
// File: src/routes/+page.server.ts

import { isEnabled } from '$lib/feature_flags/get.server';

/** @type {import('./$types').LayoutServerLoad} */
export async function load({ cookies }) {
  const bucket = cookies.get('destination_bucket');
  if (!bucket) {
    const tmp = await isEnabled('fast_payments');
    // If the feature is enabled, try bucketing users randomly
    if (tmp) cookies.set('destination_bucket', Math.random() > 0.5 ? '1' : '0', { path: '/' });
    // If the feature is disabled, do not bucket and preserve the current experience
    else cookies.set('destination_bucket', '0', { path: '/' });
  }
  const fast_payments = Boolean(Number(cookies.get('destination_bucket')));
  return {
    fast_payments,
  };
}
```

The code above first looks for the bucket assigned to the user. If no such bucket is found, it looks for the value of the feature flag in the database, randomly assigns a boolean whenever the `/` route is visited, and sets the value in the cookie. Finally, it reads the cookie as the source to determine the user experience and check if the fast payment methods are enabled or not.

### Creating a conditional user experience

Now, let's look at how the feature flag value can be used in the user interface to conditionally render UI elements. This will allow you to accept payments via PayGM if the `fast_payments` feature flag is enabled. To do that, use the following code in `+page.svelte` file:

```svelte
<script lang="ts">
  // File: src/routes/+page.svelte

  import '../app.css'

  import type { PageData } from './$types'

  export let data: PageData
</script>

<div class="w-screen h-screen flex flex-col items-center justify-center">
  {#if data.fast_payments}
    <div class="mb-6 w-full flex flex-col max-w-[300px]">
      <span class="font-semibold">Fast Payment Methods</span>
      <button class="mt-3 flex flex-col items-center border rounded w-full px-3 py-1">Pay via PayGM</button>
    </div>
  {/if}
  <form action="/" method="post" class="w-full flex flex-col max-w-[300px]">
    <span class="font-semibold">Pay with card</span>
    <input class="mt-3 w-full border px-2 py-1 rounded" type="text" placeholder="Full name on card" />
    <input class="mt-3 w-full border px-2 py-1 rounded" type="text" placeholder="1234 1234 1234 1234" />
    <div class="flex flex-row gap-x-2">
      <input class="w-1/2 border px-2 py-1 rounded" type="text" placeholder="MM/YY" />
      <input class="w-1/2 border px-2 py-1 rounded" type="text" placeholder="CVV" />
    </div>
  </form>
</div>
```

In the code above, UI elements related to fast payment methods are conditionally rendered based on the value of the `fast_payments` feature flag. If `fast_payments` feature flag is enabled, the UI will display options for paying via PayGM; otherwise, it will display options for paying with a card.

## Summary

In this guide, you learned how to add feature flags in your SvelteKit apps using Serverless Postgres powered by Neon. By dynamically updating and utilizing feature flags, you can effectively test and roll out new features like fast payment methods, providing a controlled and iterative approach to your deployments.

## Source code

You can find the source code for the application described in this guide on GitHub.

<DetailIconCards>
<a href="https://github.com/neondatabase/examples/tree/main/with-sveltekit-feature-flags" description="Feature flags with SvelteKit and Neon" icon="github">Feature flags with SvelteKit and Neon</a>
</DetailIconCards>

<NeedHelp />


# Managing database migrations and schema changes with Flask and Neon Postgres

---
title: Managing database migrations and schema changes with Flask and Neon Postgres
subtitle: Learn how to handle database migrations and schema changes in a Flask application using Flask-Migrate and Neon Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-09-14T00:00:00.000Z'
updatedOn: '2024-09-14T00:00:00.000Z'
---

Flask is a lightweight and flexible web framework for Python that makes it easy to build web applications. When working with databases in [Flask](https://flask.palletsprojects.com/), [SQLAlchemy](https://www.sqlalchemy.org/) is a popular choice for an ORM.

As your Flask application grows, so does your database schema and its complexity. Managing these changes effectively is important for maintaining data integrity and smooth deployments.

This guide will walk you through the process of handling database migrations and schema changes in a Flask application using Flask-Migrate and Neon Postgres.

## Prerequisites

Before we begin, make sure you have:

- Python 3.7 or later installed
- A [Neon](https://console.neon.tech/signup) account for Postgres hosting
- Basic familiarity with Flask and SQLAlchemy

## Setting up the Project

1. Create a new directory for your project and navigate into it:

   ```bash
   mkdir flask-migrations-demo
   cd flask-migrations-demo
   ```

2. Create a virtual environment and activate it:

   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
   ```

   This will isolate your project dependencies from other Python projects, so you can manage them independently.

3. Install the required packages:

   ```bash
   pip install flask flask-sqlalchemy flask-migrate psycopg2-binary python-dotenv
   ```

   We are installing Flask, Flask-SQLAlchemy, Flask-Migrate, psycopg2-binary (Postgres driver), and python-dotenv (for managing environment variables).

   An additional thing that you might want to do is to create a `requirements.txt` file with the installed packages:

   ```bash
   pip freeze > requirements.txt
   ```

   This will allow you to easily install the required packages on another machine by running:

   ```bash
   pip install -r requirements.txt
   ```

4. Create a `.env` file in your project root and add your Neon Postgres connection string:

   ```
   DATABASE_URL=postgresql://user:password@your-neon-host:5432/your-database
   ```

   Replace the placeholders with your actual Neon database credentials.

   Note that you should never commit your `.env` file to version control. Add it to your `.gitignore` file to prevent accidental commits.

   Instead, you can have a `.env.example` file with the required variables and commit that to your repository. Then, each developer can create their own `.env` file based on the `.env.example` template including their own credentials.

## Creating the Flask Application

With the project set up, let's create the Flask application and set up database migrations.

Create a new file called `app.py` with the following content:

```python
# Import necessary modules
from flask import Flask
from flask_sqlalchemy import SQLAlchemy
from flask_migrate import Migrate
import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Initialize Flask application
app = Flask(__name__)

# Configure the database URI using the environment variable
app.config['SQLALCHEMY_DATABASE_URI'] = os.getenv('DATABASE_URL')

# Disable SQLAlchemy modification tracking for better performance
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False

# Initialize SQLAlchemy with the Flask app
db = SQLAlchemy(app)

# Initialize Flask-Migrate with the Flask app and SQLAlchemy instance
migrate = Migrate(app, db)

# Define the User model
class User(db.Model):
    # Primary key for the User table
    id = db.Column(db.Integer, primary_key=True)

    # User's name (required field, maximum 100 characters)
    name = db.Column(db.String(100), nullable=False)

    # User's email (required field, unique, maximum 120 characters)
    email = db.Column(db.String(120), unique=True, nullable=False)

    # String representation of the User object
    def __repr__(self):
        return f'<User {self.name}>'

# Run the Flask application in debug mode if this file is executed directly
if __name__ == '__main__':
    app.run(debug=True)
```

This sets up a basic Flask application with SQLAlchemy and Flask-Migrate, and defines a simple User model. Here's a breakdown of what each part does:

1. **Imports**: We import necessary modules including Flask, SQLAlchemy, Flask-Migrate, os (for environment variables), and dotenv (for loading .env files).

2. **Environment Variables**: We use `load_dotenv()` to load environment variables from a .env file, which will include our database URL.

3. **Flask App Initialization**: We create a Flask application instance.

4. **Database Configuration**: We configure the SQLAlchemy database URI using the `DATABASE_URL` environment variable.

5. **SQLAlchemy and Flask-Migrate Setup**: We initialize SQLAlchemy and Flask-Migrate with our Flask app. This sets up our ORM and migration capabilities.

6. **User Model**: We define a `User` model that represents the structure of our `user` table in the database. It includes:
   - An `id` field as the primary key
   - A `name` field that's required and has a maximum length of 100 characters
   - An `email` field that's required, unique, and has a maximum length of 120 characters
   - A `__repr__` method that provides a string representation of the User object

7. **Application Run**: Finally, we include a conditional to run the application in debug mode if the script is executed directly.

This setup provides a foundation for building a Flask application with database integration and migration capabilities. The `User` model can be expanded or additional models can be added as the application grows.

## Initializing Migrations

To start using Flask-Migrate, you need to initialize it in your project. Run the following command in your terminal:

```bash
flask db init
```

This creates a `migrations` directory in your project that will store migration scripts. It also generates a `migrations/alembic.ini` file that contains the configuration for Alembic, the migration engine used by Flask-Migrate.

Make sure to add the `migrations` directory to your Git repository so that you can track changes to your database schema over time.

## Creating the Initial Migration

Now, let's create our first migration to set up the initial database schema. Run the following command:

```bash
flask db migrate -m "Initial migration"
```

This command generates a new migration script in the `migrations/versions` directory and the `-m` flag allows you to provide a message describing the migration. The message is useful for tracking changes and understanding the purpose of each migration.

Open the generated file and review the changes. It should contain the SQL to create the `user` table based on our `User` model. This is possible because Flask-Migrate uses SQLAlchemy's reflection capabilities to generate the migration script based on the model definitions instead of writing raw SQL or manually creating the schema.

It is a good practice to review the generated migration script before applying it to your database. This way, you can ensure that the changes are correct and will not cause any issues as in some cases Alembic might not generate the migration script as expected and you might need to modify it manually, so remember to always review the generated migration scripts.

## Applying the Migration

After reviewing the migration script, to apply the migration and create the table in your Neon Postgres database, run:

```bash
flask db upgrade
```

This command executes the migration script and creates the `user` table in your database.

The output that you should see after running the `flask db upgrade` command should look something like this:

```
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running upgrade  -> 1e48b6167844, Initial migration
```

This indicates that the migration was successful and the `user` table was created in your database.

## Making Schema Changes

Now that the initial migration is complete, let's make some changes to the schema. We'll add a new field to the `User` model and create a new migration to apply the changes.

Let's modify our `User` model to add a new field. Update the `User` class in `app.py`:

```python {5}
class User(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String(100), nullable=False)
    email = db.Column(db.String(120), unique=True, nullable=False)
    age = db.Column(db.Integer)

    def __repr__(self):
        return f'<User {self.name}>'
```

With the new `age` field added to the `User` model, we need to create a new migration to apply this change.

```bash
flask db migrate -m "Add age to User model"
```

Alembic will generate a new migration script that includes the necessary SQL to add the `age` column to the `user` table.

Review the generated migration script, then apply it:

```bash
flask db upgrade
```

The `age` column should now be added to the `user` table in your database. If you want to revert the changes, you can run `flask db downgrade` to roll back the last migration.

If you were to check the `user` table in your database, you should see that the `age` column has been added:

```sql
SELECT * FROM user;
```

This will return the data from the `user` table including the new `age` column.

## Renaming Columns

As your application evolves, you may need to rename columns in your database schema.

Such changes need to be handled carefully to avoid data loss or corruption. You also need to make sure that the application code is updated to reflect the new column names otherwise it might not be backwards compatible leading to issues.

To rename a column, you'll need to use SQLAlchemy's `alter_column` operation. Let's rename the `age` column to `years_old`:

1. Create a new migration:

```bash
flask db migrate -m "Rename age to years_old"
```

2. As the `alter_column` operation is not directly supported by Flask-Migrate, you'll need to modify the generated migration script manually.

Open the generated migration file and modify it:

```python
from alembic import op
import sqlalchemy as sa

def upgrade():
    op.alter_column('user', 'age', new_column_name='years_old')

def downgrade():
    op.alter_column('user', 'years_old', new_column_name='age')
```

3. Apply the migration:

```bash
flask db upgrade
```

The `age` column should now be renamed to `years_old` in your database. Remember to be cautious when renaming columns, as it can have implications on your application code and queries.

## Working with Indexes

Adding indexes can improve query performance. To learn more about indexing, refer to the [Neon documentation](/docs/postgres/indexes).

Let's add an index to the `email` column.

Open the `User` model in `app.py` and add the `index=True` parameter to the `email` column:

```python {6}
class User(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String(100), nullable=False)
    email = db.Column(db.String(120), unique=True, nullable=False)
    age = db.Column(db.Integer)
    email = db.Column(db.String(120), unique=True, nullable=False, index=True)

    def __repr__(self):
        return f'<User {self.name}>'
```

Create a new migration with a descriptive migration message:

```bash
flask db migrate -m "Add index to email column"
```

Review the generated migration script, it should contain an `op.create_index` operation for the `email` column:

```python {4}
def upgrade():
    with op.batch_alter_table('user', schema=None) as batch_op:
        batch_op.drop_constraint('user_email_key', type_='unique')
        batch_op.create_index(batch_op.f('ix_user_email'), ['email'], unique=True)
```

Finally, apply the newly created migration:

```bash
flask db upgrade
```

The `email` column should now have an index in your database, which can improve query performance when searching by email.

## Migrations in CI/CD Pipeline

Automating database migrations in your Continuous Integration pipeline can help with catching potential issues early.

By using Neon's branching feature, you can test your migrations safely without affecting your production database while ensuring that your application code and database schema changes are always in sync.

Here's an example of how you can automate migration testing using GitHub Actions and Neon branches:

```yaml
name: Test Migrations

on:
  pull_request:
    branches: [main]

jobs:
  test-migrations:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create Neon Branch
        uses: neondatabase/create-branch-action@v5
        id: create-branch
        with:
          project_id: ${{ secrets.NEON_PROJECT_ID }}
          branch_name: migrate-${{ github.sha }}
          api_key: ${{ secrets.NEON_API_KEY }}

      - name: Run migrations on Neon branch
        env:
          DATABASE_URL: ${{ steps.create-branch.outputs.db_url }}
        run: flask db upgrade

      - name: Run tests
        env:
          DATABASE_URL: ${{ steps.create-branch.outputs.db_url }}
        run: pytest

      - name: Clean up Neon branch
        if: always()
        uses: neondatabase/delete-branch-action@v3
        with:
          project_id: ${{ secrets.NEON_PROJECT_ID }}
          branch: ${{ steps.create-branch.outputs.branch_id }}
          api_key: ${{ secrets.NEON_API_KEY }}
```

This workflow does the following:

1. Fetches the latest code from the pull request.

2. Installs the specified Python version.

3. Installs the required Python packages listed in `requirements.txt`.

4. Uses the [official Neon GitHub action](/docs/guides/branching-github-actions) to create a new branch in your Neon project. This allows you to test migrations in isolation.

5. Applies any pending database migrations to the newly created Neon branch.

6. Executes your test suite against the updated database schema in the Neon branch.

7. Deletes the temporary Neon branch after the workflow completes, regardless of success or failure to make sure that no resources are left behind.

Using Neon's branching feature in your CI pipeline offers several advantages:

- You can test your migrations and schema changes in a separate branch without affecting your production or staging databases.
- Catch migration issues before they reach your production branch or production environment.
- Ensures that your database schema changes are always tested alongside your application code changes.
- Allows you to run your full test suite against the updated schema without risk to existing data.

## Conclusion

Managing database migrations is an important part of maintaining and evolving your Flask application. With Flask-Migrate and Neon Postgres, you have powerful tools at your disposal to handle schema changes efficiently and safely. Remember to always test your migrations thoroughly and have a solid backup strategy in place.

One thing that you should get in the habit of doing is to always review the generated migration scripts before applying them to your database. This way you can ensure that the changes that are about to be applied are correct and that they will not cause any issues. As well as that, you should use meaningful names for your migrations so that you can easily identify what each migration does.

## Additional Resources

- [Flask-Migrate Documentation](https://flask-migrate.readthedocs.io/en/latest/)
- [SQLAlchemy Migrations](https://docs.sqlalchemy.org/en/20/)
- [Alembic Documentation](https://alembic.sqlalchemy.org/en/latest/)
- [Neon Documentation](/docs)


# Developing a Scalable Flask Application with Neon Postgres

---
title: Developing a Scalable Flask Application with Neon Postgres
subtitle: Learn how to build a scalable Flask application with Neon Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-09-14T00:00:00.000Z'
updatedOn: '2024-09-14T00:00:00.000Z'
---

Building scalable web applications requires careful planning and the right tools. Flask is a Python web framework well-suited for building small to large web applications. It provides flexibility and extensibility, making it a popular choice for developers.

In this guide, we'll walk through developing a Flask application that uses Neon Postgres. We'll cover setting up the project structure, defining models, creating routes, and handling database migrations. We'll also explore frontend development using Tailwind CSS for responsive styling.

## Prerequisites

- Python 3.7 or later installed
- [Node.js 18](https://nodejs.org/en) or later
- A [Neon](https://console.neon.tech/signup) account for Postgres hosting
- Basic familiarity with Flask and SQLAlchemy

## Project Setup

1. Create a new directory and set up a virtual environment:

   ```bash
   mkdir flask-neon-app
   cd flask-neon-app
   python -m venv venv
   source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
   ```

2. Install required packages:

   ```bash
   pip install flask flask-sqlalchemy psycopg2-binary python-dotenv flask-migrate
   ```

   Create a `requirements.txt` file for package management:

   ```bash
   pip freeze > requirements.txt
   ```

   This file will help you manage dependencies and ensure consistent environments across development and deployment.

3. Create a `.env` file for environment variables:

   ```
   DATABASE_URL=postgresql://user:password@your-neon-host:5432/your-database
   ```

   Replace `user`, `password`, `your-neon-host`, and `your-database` with your Neon Postgres credentials.

## Application Structure

For small applications, you can keep all code in a single file. However, as your application grows, it's best to organize your code into separate modules. It is a good practice to separate models, routes, services, and utilities into different directories.

A typical Flask application structure might look like this:

```
flask-neon-app/
├── app/
│   ├── __init__.py
│   ├── models/
│   ├── routes/
│   ├── services/
│   ├── static/
│   │   └── css/
│   ├── templates/
│   └── utils/
├── config.py
├── requirements.txt
├── run.py
└── tailwind.config.js
```

This structure separates concerns and makes it easier to maintain and scale your application. The `app` directory contains the core application logic, while other files and directories handle configuration, dependencies, and frontend assets.

## Database Configuration

You can think of the `app/__init__.py` file as the entry point for your Flask application. It initializes the Flask app, sets up the database connection, and registers blueprints for routing.

By adding the `__init__.py` file, you transform the `app` directory into a Python package. This allows you to import modules from the `app` package in other files.

The `app` directory name is a common convention for Flask applications, but you can choose a different name if you prefer based on your project needs.

With that in mind, let's set up the database connection and initialize the Flask app. In `app/__init__.py`, add the following code:

```python
from flask import Flask
from flask_sqlalchemy import SQLAlchemy
from flask_migrate import Migrate
from dotenv import load_dotenv
import os

load_dotenv()

db = SQLAlchemy()
migrate = Migrate()

def create_app():
    app = Flask(__name__)
    app.config['SQLALCHEMY_DATABASE_URI'] = os.getenv('DATABASE_URL')
    app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False

    db.init_app(app)
    migrate.init_app(app, db)

    # Register blueprints here
    from app.routes import user_routes
    app.register_blueprint(user_routes.user_bp)

    return app
```

This setup initializes Flask, SQLAlchemy, and Flask-Migrate. It loads the database URL from the environment variables and disables SQLAlchemy's modification tracking for better performance.

Blueprints are a way to organize related routes and views in Flask applications. We will cover blueprints and routes in the next sections.

To learn more about Flask-Migrate, check out the [Managing database migrations and schema changes with Flask and Neon Postgres](/guides/flask-database-migrations) guide.

## Model Definition

In web applications, models represent the data structure and relationships in your database. In the context of Flask and SQLAlchemy, models are Python classes that map to database tables and also allow you to define the schema and relationships between tables.

As an example of a typical model definition, let's create a `User` model in `app/models/user.py`:

```python
from app import db
from datetime import datetime

class User(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(80), unique=True, nullable=False)
    email = db.Column(db.String(120), unique=True, nullable=False)
    created_at = db.Column(db.DateTime, default=datetime.utcnow)

    def __repr__(self):
        return f'<User {self.username}>'

    def to_dict(self):
        return {
            'id': self.id,
            'username': self.username,
            'email': self.email,
            'created_at': self.created_at.isoformat()
        }
```

This model defines a User with `username`, `email`, and `created_at` fields. The `__repr__` method provides a string representation of the model instance for debugging purposes and logging. The `to_dict` method allows easy serialization of the model to JSON which is useful when returning data from API endpoints.

## Route Creation with Blueprints

In Flask, blueprints are a good way to organize your application into components. They allow you to group related routes, view functions, templates, and static files. Blueprints help in structuring large applications allowing you to separate different functional areas of your project.

If you're familiar with Laravel, blueprints in Flask serve a similar purpose to Laravel's controllers and route groups, allowing you to logically organize your routes and associated functionality.

Some of the main benefits of using blueprints in Flask include grouping related routes together, organizing your application into modular components, and avoiding naming conflicts between different parts of your application.

Here's an expanded example of how to use blueprints in a Flask application, in a file named `app/routes/user_routes.py` for user-related routes:

```python
from flask import Blueprint, jsonify, request, render_template, redirect, url_for
from app.models.user import User
from app import db

# Create a blueprint named 'user' with a URL prefix '/user'
user_bp = Blueprint('user', __name__, url_prefix='/user')

# Route for creating a new user (HTML form submission)
@user_bp.route('/create', methods=['POST'])
def create_user():
    data = request.form
    new_user = User(username=data['username'], email=data['email'])
    db.session.add(new_user)
    db.session.commit()
    return redirect(url_for('user.list_users'))

# Route for displaying all users (HTML)
@user_bp.route('/list', methods=['GET'])
def list_users():
    users = User.query.all()
    return render_template('users.html', users=users)

# API route for retrieving all users (JSON)
@user_bp.route('/api/list', methods=['GET'])
def get_users_api():
    users = User.query.all()
    return jsonify([user.to_dict() for user in users])

# Route for displaying a single user (HTML)
@user_bp.route('/<int:user_id>', methods=['GET'])
def get_user(user_id):
    user = User.query.get_or_404(user_id)
    return render_template('user_detail.html', user=user)

# API route for retrieving a single user (JSON)
@user_bp.route('/api/<int:user_id>', methods=['GET'])
def get_user_api(user_id):
    user = User.query.get_or_404(user_id)
    return jsonify(user.to_dict())
```

Let's break down this code:

1. We import necessary modules and create a blueprint named 'user' with a URL prefix '/user'. This prefix will be prepended to all routes defined within this blueprint which helps in organizing routes into logical groups. If you have multiple blueprints, each can have its own URL prefix.

2. `create_user()`: This route handles POST requests to create a new user. It reads form data, creates a new `User` instance, adds it to the database session, and commits the transaction. It then redirects to the `list_users()` route.

3. `list_users()`: This route displays all users in HTML format. It queries all users from the database and renders a template with the user data.

4. `get_users_api()`: This API route returns all users in JSON format. It queries all users from the database, converts them to dictionaries using the `to_dict()` method, and returns a JSON response.

5. `get_user()`: This route displays details of a single user in HTML format. It retrieves a user by ID or returns a 404 error if the user doesn't exist.

6. `get_user_api()`: This API route returns details of a single user in JSON format. It retrieves a user by ID or returns a 404 error if the user doesn't exist.

To use this blueprint in your main Flask application, you would register it like this in `app/__init__.py` which we've seen earlier:

```python
from flask import Flask
from app.routes.user_routes import user_bp

app = Flask(__name__)
app.register_blueprint(user_bp)
```

This structure allows you to organize related routes together, making your application more modular and easier to maintain as it grows.

## Frontend with Tailwind CSS and Templates

Tailwind CSS is a utility-first CSS framework that allows you to rapidly build custom user interfaces. It provides low-level utility classes that let you build completely custom designs without ever leaving your HTML.

To integrate Tailwind CSS with Flask templates, you can follow these steps:

1. Install Tailwind CSS:

   ```bash
   npm init -y
   npm install tailwindcss
   npx tailwindcss init
   ```

   This initializes a new Node.js project, installs Tailwind CSS, and creates a basic Tailwind configuration file.

2. Update the Tailwind CSS configuration file `tailwind.config.js` to include your HTML templates:

   ```javascript
   module.exports = {
     content: ['./app/templates/**/*.html'],
     theme: {
       extend: {},
     },
     plugins: [],
   };
   ```

   This configuration tells Tailwind to scan your HTML templates for classes to include in the final CSS output. The `extend` key allows you to customize Tailwind's default theme.

3. Create `app/static/css/main.css`:

   ```css
   @tailwind base;
   @tailwind components;
   @tailwind utilities;
   ```

   This file imports Tailwind's base styles, component classes, and utility classes. It serves as the entry point for Tailwind to inject its styles.

4. Compile Tailwind CSS:

   ```bash
   npx tailwindcss -i ./app/static/css/main.css -o ./app/static/css/output.css --watch
   ```

   This command compiles your Tailwind CSS file and watches for changes. This allows you to keep your CSS file as small as possible by only including the styles you use in your templates, which is important for performance as your application grows, and unnecessary styles can slow down your site.

   The `--watch` flag ensures that the CSS is recompiled whenever you make changes to your HTML files. This is useful for rapid development and live reloading.

5. Create a base template in `app/templates/base.html`:

   ```html
   <!doctype html>
   <html lang="en">
     <head>
       <meta charset="UTF-8" />
       <meta name="viewport" content="width=device-width, initial-scale=1.0" />
       <title>{% block title %}Flask Neon App{% endblock %}</title>
       <link rel="stylesheet" href="{{ url_for('static', filename='css/output.css') }}" />
     </head>
     <body class="bg-gray-100">
       <nav class="bg-blue-500 p-4 text-white">
         <div class="container mx-auto">
           <a href="/" class="text-2xl font-bold">Flask Neon App</a>
         </div>
       </nav>
       <main class="container mx-auto mt-8">{% block content %}{% endblock %}</main>
     </body>
   </html>
   ```

   This base template sets up the basic structure of your HTML pages. It includes:
   - A title block that can be overridden in child templates
   - A link to the compiled Tailwind CSS file
   - A simple navigation bar with Tailwind classes for styling
   - A main content area with a block that child templates can fill thanks to Jinja2 template inheritance

6. Create `app/templates/users.html`:

   ```html
   {% extends "base.html" %} {% block title %}Users{% endblock %} {% block content %}
   <h1 class="mb-4 text-3xl font-bold">Users</h1>
   <div class="rounded bg-white p-4 shadow-md">
     <form action="{{ url_for('user.create_user') }}" method="post" class="mb-4">
       <input type="text" name="username" placeholder="Username" required class="mr-2 border p-2" />
       <input type="email" name="email" placeholder="Email" required class="mr-2 border p-2" />
       <button type="submit" class="bg-blue-500 rounded px-4 py-2 text-white">Add User</button>
     </form>
     <ul>
       {% for user in users %}
       <li class="mb-2">{{ user.username }} ({{ user.email }})</li>
       {% endfor %}
     </ul>
   </div>
   {% endblock %}
   ```

   This template extends the base template using `{% extends "base.html" %}` and provides specific content for the users page. It includes:
   - A form for adding new users, styled with Tailwind classes
   - A list of existing users, also styled with Tailwind
   - Jinja2 template syntax for dynamic content (e.g., `{% for user in users %}`)

Tailwind CSS and Jinja2 templates give you the flexibility to create your frontend design while keeping your codebase organized and maintainable. This approach allows you to build responsive and visually appealing web applications with ease.

## Database Migrations

Database migrations are an important part of managing your application's database schema over time. They allow you to evolve your database structure incrementally, keeping it in sync with your application's models. We will be using Flask-Migrate, which is an extension for Flask that handles SQLAlchemy database migrations using Alembic, makes this process straightforward.

Here's a quick guide to setting up and using Flask-Migrate for managing database migrations:

1. Initialize the migration repository:

   ```bash
   flask db init
   ```

   This command creates a new migration repository. It sets up a `migrations` directory with the necessary files for managing your migrations.

2. Create a migration:

   ```bash
   flask db migrate -m "Initial migration"
   ```

   This command creates a new migration script based on the changes detected in your models. Unlike some other migration tools, Flask-Migrate automatically detects changes to your models and generates the migration script for you having to write it manually. However, it's always a good idea to review the generated migration script to ensure it reflects the intended changes.

   The `-m` flag allows you to provide a brief description of the migration, which is helpful for tracking changes over time.

3. Apply the migration:

   ```bash
   flask db upgrade
   ```

   This command applies the migration to your database, making the necessary schema changes.

It's important to review the generated migration scripts before applying them, especially in production environments. While Flask-Migrate is generally good at detecting changes, complex modifications might require manual adjustments to the migration scripts.

For more advanced usage, Flask-Migrate provides additional commands:

- `flask db downgrade`: Reverts the last migration
- `flask db current`: Displays the current revision of the database
- `flask db history`: Shows the migration history

You should commit your migration files to version control so that all developers and deployment environments can maintain consistent database schemas.

To learn more about managing database migrations with Flask and Neon Postgres, check out the [Managing database migrations and schema changes with Flask and Neon Postgres](/guides/flask-database-migrations) guide.

## Scalability Considerations

Besides the above steps, as your Flask application grows, you can consider a few strategies to improve performance and scalability. Here are some best practices to keep in mind:

1. Connection pooling is a technique used to manage database connections efficiently. Instead of opening and closing a new connection for each database operation, a pool of reusable connections is maintained.

   Neon Postgres supports connection pooling, which can significantly improve your application's performance by reducing the overhead of creating new connections.

   To use connection pooling with Neon:

   ```python
   # Update your DATABASE_URL to use the pooled connection string
   app.config['SQLALCHEMY_DATABASE_URI'] = 'postgresql://user:password@pooler.address:5432/database'
   ```

   Refer to the [Neon documentation on connection pooling](/docs/connect/connection-pooling) for detailed instructions.

2. For performance optimization, consider caching frequently accessed data. Caching reduces the load on your database and speeds up response times for users.

   Implement caching for frequently accessed data using Flask-Caching:

   ```python
   from flask_caching import Cache

   cache = Cache()

   def create_app():
       # ... previous code ...
       cache.init_app(app, config={'CACHE_TYPE': 'simple'})

       return app

   @user_bp.route('/api/users', methods=['GET'])
   @cache.cached(timeout=60)  # Cache for 60 seconds
   def get_users_api():
       users = User.query.all()
       return jsonify([user.to_dict() for user in users])
   ```

   This example uses in-memory caching ('simple'). For production, you can switch to a caching backend like Redis.

3. Asynchronous processing allows your application to handle time-consuming tasks without blocking the main request-response cycle. You can use Celery, a distributed task queue, to run background tasks asynchronously.

   To integrate [Celery with Flask](https://flask.palletsprojects.com/en/2.3.x/patterns/celery/), you need to install the `celery` package and configure it in your Flask application:

   ```bash
   pip install celery
   ```

   ```python
   from celery import Celery

   celery = Celery(__name__)

   def create_app():
       # ... previous code ...
       celery.conf.update(app.config)

       return app

   @celery.task
   def send_email(user_id):
       user = User.query.get(user_id)
       # Send email to user

   # To call the task
   send_email.delay(user_id)
   ```

   This example defines a Celery task to send an email to a user asynchronously. You can run Celery workers to process these tasks in the background.

4. Rate limiting helps prevent abuse of your API and ensures fair usage among clients. It's an important security measure for public APIs.

   Implement rate limiting using Flask-Limiter:

   ```python
   from flask_limiter import Limiter
   from flask_limiter.util import get_remote_address

   limiter = Limiter(key_func=get_remote_address)

   def create_app():
       # ... previous code ...
       limiter.init_app(app)

       return app

   @user_bp.route('/api/users', methods=['GET'])
   @limiter.limit("5 per minute")
   def get_users_api():
       users = User.query.all()
       return jsonify([user.to_dict() for user in users])
   ```

   This example limits the `/api/users` endpoint to 5 requests per minute per IP address. You can adjust the limit based on your application's needs and resources.

## Conclusion

By following these practices, you've set up a scalable Flask application with Neon Postgres, including a responsive frontend using Tailwind CSS. This structure allows for easy expansion and maintenance as your project grows.

As a next step, consider adding authentication, authorization, and error handling to your application. These features are essential for securing your application and providing a good user experience.

You should also consider testing your application to ensure its reliability and performance. Unit tests, integration tests, and end-to-end tests can help you catch bugs early and maintain code quality. Testing your application with Neon's branching feature can help you test new features in isolation before deploying them to production.

## Additional Resources

- [Flask-Migrate Documentation](https://flask-migrate.readthedocs.io/en/latest/)
- [SQLAlchemy Migrations](https://docs.sqlalchemy.org/en/20/)
- [Alembic Documentation](https://alembic.sqlalchemy.org/en/latest/)
- [Neon Documentation](/docs)


# Testing Flask Applications with Neon's Database Branching

---
title: Testing Flask Applications with Neon's Database Branching
subtitle: Leveraging Realistic Production Data for Robust Testing with Flask and Neon Branching
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-09-15T00:00:00.000Z'
updatedOn: '2024-09-15T00:00:00.000Z'
---

[Flask](https://flask.palletsprojects.com/) is a popular Python micro-framework widely used for building web applications. It includes powerful tools for automated testing, with [pytest](https://docs.pytest.org/) being a preferred option due to its simplicity and effectiveness.

Testing with realistic data is crucial as it helps ensure that your application performs well under real-world conditions. Neon's database branching feature offers a unique solution by allowing you to test with actual production data without affecting your live database, thus maintaining data integrity and security.

## Understanding Flask Testing Approaches

In Flask applications, you would commonly use an in-memory SQLite database for testing. This method is favored because it allows for starting with a clean state for each test run by applying all database migrations and seeders. This setup is also great for parallel testing, as tests run quickly and do not interfere with each other.

However, testing with SQLite can differ significantly from your production environment, which might use a different database system, such as PostgreSQL. These differences can affect your application's behavior and lead to unexpected issues in production. This is one of the reasons why testing with real data can provide a more accurate finding of how your application will perform in its live environment.

## Neon Branching

Neon offers a database [branching feature](/docs/introduction/branching) that allows you to create isolated branches of your database for development, testing, and more.

A branch in Neon is a copy-on-write clone of your data that can be made from the current database state or any past state. This means you can have an exact copy of your production data at a specific point in time to use for testing.

Neon's branching is particularly useful in continuous integration and delivery pipelines, helping you be more productive by reducing the setup time needed for test environments.

This allows you to test with realistic data scenarios without the overhead of maintaining multiple separate databases. For more information on how to use Neon branching, refer to the [Neon documentation](/docs/introduction/branching).

Certainly! I'll rewrite this section with more in-depth explanations and remove the #### headings. Here's an improved version:

## Setting Up Your Testing Environment

Now that we've covered the benefits of testing Flask applications with Neon's database branching, let's walk through setting up a Flask project with a PostgreSQL database and writing tests using pytest.

### Prerequisites

Before you begin, ensure you have the following:

- Python 3.8 or higher installed on your machine
- A [Neon account](https://console.neon.tech/signup) with a project created
- Basic familiarity with Flask and SQLAlchemy

### Installation and Configuration

To set up your testing environment with Neon and Flask, follow these steps:

1. Configure Database Connection:

   After creating your Neon account and a new database branch, obtain the connection details from the Neon dashboard. Create a `.env` file with the Neon database connection parameters:

   ```env
   DATABASE_URL=postgresql://user:password@your-neon-hostname.neon.tech:5432/dbname
   ```

   Replace `user`, `password`, `your-neon-hostname`, and `dbname` with your Neon database details.

2. Install Required Packages:

   Install Flask, SQLAlchemy, pytest, and other necessary packages:

   ```bash
   pip install flask flask-sqlalchemy psycopg2-binary python-dotenv pytest
   ```

   Freeze the requirements for easy replication:

   ```bash
   pip freeze > requirements.txt
   ```

### Creating a Migration and Model

As we briefly mentioned earlier, you can use SQLAlchemy for database operations in Flask applications. Along with Flask-Migrate, you can manage database migrations effectively.

1. Set Up Flask-Migrate:

   Install and initialize Flask-Migrate:

   ```bash
   pip install Flask-Migrate
   ```

   In your main application file, initialize Flask-Migrate with your Flask app and database instance:

   ```python
   from flask_migrate import Migrate

   migrate = Migrate(app, db)
   ```

   This setup allows you to manage database migrations using Flask-Migrate.

2. Create a Model:

   In `models.py`, define a `Question` model:

   ```python
   from flask_sqlalchemy import SQLAlchemy

   db = SQLAlchemy()

   class Question(db.Model):
       id = db.Column(db.Integer, primary_key=True)
       title = db.Column(db.String(100), nullable=False)
       description = db.Column(db.Text, nullable=False)
   ```

3. Generate and Run Migrations:

   Create and apply the initial migration:

   ```bash
   flask db init
   flask db migrate -m "Initial migration"
   flask db upgrade
   ```

### Creating a Questions Route

In your main Flask application file, add a route to handle fetching questions from the database:

```python
from flask import jsonify
from models import Question

@app.route('/questions')
def get_questions():
    questions = Question.query.all()
    return jsonify([{
        'id': q.id,
        'title': q.title,
        'description': q.description
    } for q in questions])
```

This route fetches all questions from the database and returns them as JSON. You can expand this route to include additional functionality as needed.

If you don't have any questions in your database yet, you can add some manually or create a seed script to populate the database with test data.

To verify the setup, run the Flask development server:

```bash
flask run
```

If everything is set up correctly, you should be able to access the `/questions` route and see the questions returned as JSON.

### Writing a pytest Test for the Questions Route

The standard convention for naming test files is to prefix them with `test_`. This allows pytest to automatically discover and run the tests.

In this case, if your Flask application is in a file named `app.py`, create a file named `test_app.py` in the same directory:

```python
import pytest
from app import app, db
from models import Question

@pytest.fixture
def client():
    app.config['TESTING'] = True
    with app.test_client() as client:
        with app.app_context():
            db.create_all()
            yield client
            db.session.remove()
            db.drop_all()

def test_get_questions(client):
    # Add a test question
    question = Question(title='Test Question', description='This is a test')
    db.session.add(question)
    db.session.commit()

    response = client.get('/questions')
    assert response.status_code == 200
    data = response.get_json()
    assert len(data) == 1
    assert data[0]['title'] == 'Test Question'
```

Here we define a test fixture to set up and tear down the test environment. The `test_get_questions` function tests the `/questions` route by adding a test question to the database, making a request to the route, and asserting the response. This simple test verifies that the route returns the expected data.

### Running the Tests

With the simple test in place, you can now run the tests using pytest:

```bash
pytest
```

This setup provides a foundation for testing Flask applications with Neon Postgres, which you can expand upon for more complex applications and comprehensive test suites.

## Using Neon Branching with Flask

You should never run tests against your production database, as it can lead to data corruption and security risks. This is where Neon branching comes in handy.

Neon's branching feature enables you to create isolated database environments, which is ideal for testing changes without impacting the production database.

This can be particularly useful when testing complex features or changes that require realistic data scenarios. Especially when there are schema changes or data migrations involved, Neon branching provides a safe and efficient way to validate your application's behavior on a copy of your production data.

### Creating a Neon Branch

1. **Log In to Neon Dashboard:**
   - Access your Neon dashboard by logging in at [Neon's official website](https://neon.tech).

2. **Select Your Database:**
   - Navigate to the database project that you are using for your production environment.

3. **Create a New Branch:**
   - Click on "Branches" in the sidebar menu.
   - Click on "Create Branch."
   - Name your new branch (e.g., "testing-branch") and specify if it should be created from the current state of the database or from a specific point in time. This creates a copy-on-write clone of your database.
   - Wait for the branch to be fully provisioned, which usually takes just a few seconds.

### Integrating Neon Branching with Flask Testing

Go back to your Flask project and integrate the Neon branch into your testing setup:

1. **Update Environment Configuration:**
   - Once your branch is created, obtain the get details (hostname, database name, username, and password) from the Neon dashboard.
   - Create a new environment file for testing, such as `.env.test`, and configure it to use the Neon testing branch:

     ```env
     DATABASE_URL=postgresql://user:password@your-neon-testing-hostname.neon.tech:5432/dbname
     ```

2. **Update Test Configuration:**
   - Modify your `test_app.py` file to use the testing environment:

     ```python
     import os
     from dotenv import load_dotenv

     # Load test environment variables
     load_dotenv('.env.test')

     # Use the DATABASE_URL from the test environment
     app.config['SQLALCHEMY_DATABASE_URI'] = os.getenv('DATABASE_URL')
     ```

3. **Run Tests:**
   - With the testing branch configured, you can run your tests against the isolated database environment:

     ```bash
     pytest
     ```

   - Examine the output from pytest to ensure your application behaves as expected against the testing branch. This approach allows you to test changes in a controlled environment that mirrors your production setup instead of using an in-memory SQLite database.

In addition to running tests locally, you can automate the testing process by integrating Neon branching with your CI/CD pipeline. Neon provides a GitHub Actions workflow that simplifies the process of creating and managing database branches for testing. For more information, refer to the [Neon Branching GitHub Actions Guide](/docs/guides/branching-github-actions).

## Managing Neon Branches with `neonctl` CLI

With the `neonctl` CLI tool, managing your Neon database branches becomes more efficient and straightforward. You can create, list, obtain connection strings, and delete branches using simple commands.

### Installing `neonctl`

Before you can start using `neonctl`, you need to install it on your local machine. Follow the installation instructions provided in the [Neon CLI documentation](/docs/reference/cli-install) to set up `neonctl` on your system.

### Using `neonctl` to Manage Branches

Once `neonctl` is installed, you can use it to interact with your Neon database branches. Here are the basic commands for managing branches:

#### 1. [Creating a Branch](/docs/reference/cli-branches#create)

To create a new branch, use the `neonctl branches create` command:

```bash
neonctl branches create --project-id PROJECT_ID --parent PARENT_BRANCH_ID --name BRANCH_NAME
```

Replace `PROJECT_ID`, `PARENT_BRANCH_ID`, and `BRANCH_NAME` with the appropriate values for your Neon project. This command will create a new branch based on the specified parent branch.

#### 2. [Listing Branches](/docs/reference/cli-branches#list)

To list all branches in your Neon project, use the `neonctl branches list` command:

```bash
neonctl branches list --project-id PROJECT_ID
```

Replace `PROJECT_ID` with your Neon project ID. This command will display a list of all branches along with their IDs, names, and other relevant information.

#### 3. [Obtaining Connection String](/docs/reference/cli-connection-string)

Once you've created a branch, you'll need to obtain the connection string to configure your Laravel application. Use the `neonctl connection-string` command:

```bash
neonctl connection-string BRANCH_ID
```

Replace `BRANCH_ID` with the ID of the branch you want to connect to. This command will output the connection string that you can use to configure your Laravel `.env` file.

#### 4. [Deleting a Branch](/docs/reference/cli-branches#delete)

After you've finished testing with a branch, you can delete it using the `neonctl branches delete` command:

```bash
neonctl branches delete BRANCH_ID
```

Replace `BRANCH_ID` with the ID of the branch you want to delete. This command will remove the branch from your Neon project, ensuring that resources are not left unused.

## Conclusion

Testing Flask applications with Neon's database branching offers a solution that lets you test changes with realistic production data without affecting your live database.

By using realistic production data in a controlled testing environment, you can confidently validate your changes without risking your live application's integrity.

Neon's branching feature provides isolation, efficiency, flexibility, and simplicity, making it a valuable tool for streamlining the testing process.

## Additional Resources

- [Flask Documentation](https://flask.palletsprojects.com/)
- [pytest Documentation](https://docs.pytest.org/)
- [SQLAlchemy Documentation](https://docs.sqlalchemy.org/)
- [Neon Branching GitHub Actions Guide](/docs/guides/branching-github-actions)

<NeedHelp />


# Full Text Search using tsvector with Neon Postgres

---
title: Full Text Search using tsvector with Neon Postgres
subtitle: A step-by-step guide describing how to implement full text search with tsvector in Postgres
author: vkarpov15
enableTableOfContents: true
createdAt: '2024-09-17T13:24:36.612Z'
updatedOn: '2024-09-17T13:24:36.612Z'
---

The `tsvector` type enables you to use full text search on your text content in Postgres. Full text search allows you to search text content in a more flexible way than using `LIKE`. Full text search also supports features like _stemming_, which means searching for the word "run" will match variations like "ran" and "running".

## Steps

- Set up a table with a `tsvector` column
- Execute your first full text search
- Search for multiple words
- Rank the results
- Create a GIN index

## Set up a table with a `tsvector` column

To set up full text search, you need to create a column of type `tsvector` that will enable full text search. You can run the following `CREATE TABLE` statement in the [Neon SQL Editor](/docs/get-started/query-with-neon-sql-editor) or from a client such as [psql](/docs/connect/query-with-psql-editor) that is connected to Neon. This statement will create a table with a column `searchable` of type `tsvector`.

```sql
CREATE TABLE documents (
  id SERIAL PRIMARY KEY,
  title TEXT,
  body TEXT,
  searchable tsvector
);
```

Next, insert two new rows into the `documents` table. The [to_tsvector()] (https://www.postgresql.org/docs/current/textsearch-controls.html) function takes in a language and the text content to tokenize. In the following example, the text content is the `title` and `body` columns concatenated together.

```sql
INSERT INTO documents (title, body, searchable)
  VALUES (
    'PostgreSQL Full-Text Search',
    'This is an introduction to full-text search in PostgreSQL.',
    to_tsvector('english', 'PostgreSQL Full-Text Search This is an introduction to full-text search in PostgreSQL.')
  );

INSERT INTO documents (title, body, searchable)
  VALUES (
    'My Mashed Potatoes Recipe',
    'These amazing homemade mashed potatoes are perfectly rich and creamy, full of great flavor, easy to make, and always a crowd fave!',
    to_tsvector('english', 'My Mashed Potatoes Recipe These amazing homemade mashed potatoes are perfectly rich and creamy, full of great flavor, easy to make, and always a crowd fave!')
  );
```

Once you have inserted the new rows, try running `SELECT * FROM documents;`. You will see that the data stored in the `searchable` column looks like the following.

```
'full':3,12 'full-text':2,11 'introduct':9 'postgresql':1,16 'search':5,14 'text':4,13
```

Internally, `to_tsvector()` uses a parser to break the text content into tokens for easier searching.

## Execute your first full text search

You can execute a full text search query using a `WHERE` clause with `@@ to_tsquery('english', 'content here')` as shown below. The following query returns the "mashed potatoes" row because, although the word "flavorful" does not appear in that row's text content, the word "flavor" does. And "flavor" matches "flavorful".

```sql
SELECT
    *
  FROM documents
  WHERE searchable @@ to_tsquery('english', 'flavorful');
```

Similarly, the following query returns the "PostgeSQL" row, even though the word "searching" does not appear in that row's text content, but "search" does.

```sql
SELECT
    *
  FROM documents
  WHERE searchable @@ to_tsquery('english', 'searching');
```

The `@@` operator is a special operator which compares the `tsvector` value stored in the `searchable` column with the `tsquery` value provided in the query. The `tsquery` type is different from the `tsvector` type. For example, if you run the following command, Postgres will return the string "searching".

```sql
SELECT to_tsquery('english', 'searching');
```

## Search for multiple words

If you try using `to_tsquery()` with multiple words, like `to_tsquery('english', 'searching text');`, Postgres will throw the following error.

```
ERROR: syntax error in tsquery: "searching text" (SQLSTATE 42601)
```

That's because the input to `to_tsquery()` must be tokens separated by `tsquery` operators like `&`. The correct way to search for "searching" and "text" would be `to_tsquery('english', 'searching & text');`. To make full text search easier to work with, Postgres also has a `phraseto_tsquery()` function that converts text into a `tsquery` with no need for operators. The following query will successfully return the "PostgreSQL" row.

```sql
SELECT
    *
  FROM documents
  WHERE searchable @@ phraseto_tsquery('english', 'searching text');
```

`tsquery` also supports negations. For example, the following query will search for rows whose text content matches "searching" and does **not** match "text".

```sql
SELECT
    *
  FROM documents
  WHERE searchable @@ to_tsquery('english', 'searching & !text');
```

Postgres also supports a `websearch_to_tsquery()` function, which uses an alternative syntax that doesn't require putting operators between all tokens. `websearch_to_tsquery()` supports negations by prefixing a token with `-`. The following query also searches for rows whose text content matches "searching" and does **not** match "text".

```sql
SELECT
    *
  FROM documents
  WHERE searchable @@ websearch_to_tsquery('english', 'searching -text');
```

## Rank the results

Postgres provides two functions for ranking the results, allowing you to sort by which results are the best match. The following statement sorts rows that match "searching text" using the `ts_rank()` function, which counts the number of tokens that match.

```sql
SELECT
    id,
    title,
    ts_rank(searchable, websearch_to_tsquery('english', 'searching text')) AS rank
  FROM documents
  WHERE searchable @@ websearch_to_tsquery('english', 'searching text')
  ORDER BY rank DESC;
```

To see how sorting works in practice, insert two more rows as follows. The first row contains 6 tokens that match "search" and "text", so that row should show up first.

```sql
INSERT INTO documents (title, body, searchable)
  VALUES (
    'PostgreSQL Text Search',
    'A comprehensive, searchable guide in plain text format. Covers full text search in PostgreSQL',
    to_tsvector('english', 'PostgreSQL Text Search A comprehensive, searchable guide in plain text format. Covers full text search in PostgreSQL')
  );
```

Running the `ts_rank()` `SELECT` statement with these two new rows outputs rows in the following order. "PostgreSQL Text Search" appears first because it has the most occurrences of tokens that match "search" and "text".

```sql
2	PostgreSQL Text Search	0.34941113
1	PostgreSQL Full-Text Search	0.3054688
```

Postgres also has a `ts_rank_cd()` function which uses an alternative ranking algorithm based on _cover density_. `ts_rank_cd()` also takes proximity of matching tokens into consideration, so the "PostgreSQL Text Search" row will rank slightly lower with `ts_rank_cd()` because there's more words between the matching tokens.

```sql
SELECT
    id,
    title,
    ts_rank_cd(searchable, websearch_to_tsquery('english', 'searching text')) AS rank
  FROM documents
  WHERE searchable @@ websearch_to_tsquery('english', 'searching text')
  ORDER BY rank DESC;
```

```
1	PostgreSQL Full-Text Search	0.21666667
2	PostgreSQL Text Search	0.21428572
```

## Create a GIN index

[GIN indexes](https://www.postgresql.org/docs/current/gin-intro.html) allow you to index your `tsvector` properties, which can make your full text search queries faster as your data grows. Just be careful, [GIN indexes can slow down your updates](https://pganalyze.com/blog/gin-index). Below is how you can create a GIN index on the `searchable` column.

```sql
CREATE INDEX searchable_idx ON documents USING GIN(searchable);
```

To test out the GIN index, let's first insert 100 copies of the "mashed potatoes" document. Sometimes Postgres decides to skip using indexes and use a sequential scan instead when a query matches most of the table.

```sql
DO $$
BEGIN
  FOR i IN 1..100 LOOP
    INSERT INTO documents (title, body, searchable)
    VALUES (
      'My Mashed Potatoes Recipe',
      'These amazing homemade mashed potatoes are perfectly rich and creamy, full of great flavor, easy to make, and always a crowd fave!',
      to_tsvector('english', 'My Mashed Potatoes Recipe These amazing homemade mashed potatoes are perfectly rich and creamy, full of great flavor, easy to make, and always a crowd fave!')
    );
  END LOOP;
END $$;
```

Next, you can run an `EXPLAIN ANALYZE` query (or just click the **Explain** button in the Neon SQL Editor) to confirm that Postgres is using your GIN index.

```sql
EXPLAIN ANALYZE
SELECT
    id, title
  FROM documents
WHERE searchable @@ to_tsquery('english', 'search');
```

The `EXPLAIN ANALYZE` query should produce output that resembles the following. The `Bitmap Index Scan on searchable_idx` means that Postgres is using a GIN index rather than a sequential scan to answer the query.

```
Bitmap Heap Scan on documents  (cost=8.54..13.10 rows=2 width=29) (actual time=0.021..0.022 rows=2 loops=1)
  Recheck Cond: (searchable @@ '''search'''::tsquery)
  Heap Blocks: exact=1
  ->  Bitmap Index Scan on searchable_idx  (cost=0.00..8.54 rows=2 width=0) (actual time=0.009..0.009 rows=2 loops=1)
        Index Cond: (searchable @@ '''search'''::tsquery)
Planning Time: 0.095 ms
Execution Time: 0.105 ms
```


# Geospatial Search in Postgres

---
title: Geospatial Search in Postgres
subtitle: A step-by-step guide describing how to use PostGIS for geospatial search in Postgres
author: vkarpov15
enableTableOfContents: true
createdAt: '2025-02-16T13:24:36.612Z'
updatedOn: '2025-02-16T13:24:36.612Z'
---

Geospatial queries enable you to perform searches on geographical data stored in Postgres.
With the [PostGIS extension](https://postgis.net/), you can query for which rows are within 10 kilometers of a given point or which rows are within a given geospatial rectangle.
This makes Postgres a great option for applications involving maps, location-based services, and geographic analysis.

## Steps

- Install and enable PostGIS
- Create a table with a geometry column
- Insert and retrieve geospatial data
- Perform geospatial queries using `ST_DWithin`
- Sort locations by distance using `ST_Distance`
- Find locations within a polygon using `ST_MakeEnvelope`
- Index using GiST indexes

### Install and enable PostGIS

Before using geospatial queries, you need to install the [PostGIS extension](https://postgis.net/).
PostGIS adds geospatial data types like `GEOMETRY` and `GEOGRAPHY`, geospatial query functions like `ST_DWithin` and `ST_MakeEnvelope`, and GiST indexes.
In Neon, PostGIS is already installed, you just need to enable it using the following command.

```sql
CREATE EXTENSION IF NOT EXISTS postgis;
```

### Create a table with a geometry column

To store geospatial data, create a table with a geometry column.
The `GEOMETRY` type can be used to store points, lines, or polygons.
The following command creates a `geom` column that stores latitude/longitude points.

```sql
CREATE TABLE locations (
  id SERIAL PRIMARY KEY,
  name TEXT NOT NULL,
  geom GEOMETRY(Point, 4326) -- WGS 84 spatial reference system
);
```

The `4326` magic number is a Spatial Reference System Identifier (SRID) that represents the WGS 84 coordinate system.
In PostGIS, `4326` indicates that `geom` represents degrees of latitude and longitude, rather than meters or feet.

### Insert and retrieve geospatial data

You can insert geographic points using the `ST_GeomFromText` function as follows.
The two rows inserted below contain the approximate latitude and longitude of New York and San Francisco.

```sql
INSERT INTO locations (name, geom)
VALUES ('New York', ST_GeomFromText('POINT(-74.006 40.7128)', 4326)),
       ('San Francisco', ST_GeomFromText('POINT(-122.4194 37.7749)', 4326));
```

You can retrieve all stored locations using the following.

```sql
SELECT id, name, ST_AsText(geom) FROM locations;
```

### Perform geospatial queries using `ST_DWithin`

You can find all locations within a certain distance of a point using `ST_DWithin`.
The following query finds all locations that are within 4000 kilometers of New York, which is just New York.

```sql
SELECT name FROM locations
WHERE ST_DWithin(
  geom,
  ST_GeomFromText('POINT(-74.006 40.7128)', 4326),
  4000000, -- in meters
  true -- use_spheroid
);
```

The `use_spheroid` parameter ensures that `ST_DWithin` interprets the 3rd parameter as meters, and uses a more accurate spheroid model of the Earth which accounts for Earth's flattening at the poles.

San Francisco is approximately 4100 kilometers from New York, so if you instead query for all points that are within 4200 kilometers of New York then Postgres will return both New York and San Francisco.

```sql
SELECT name FROM locations
WHERE ST_DWithin(
  geom,
  ST_GeomFromText('POINT(-74.006 40.7128)', 4326),
  4200000, -- in meters
  true -- use_spheroid
);
```

### Sort locations by distance using `ST_Distance`

To sort locations based on their proximity to a given point, use `ST_Distance`.
The following query sorts locations by their distance from Miami, New York is closer.

```sql
SELECT name, ST_Distance(geom, ST_GeomFromText('POINT(-80.1918 25.7617)', 4326)) AS distance
FROM locations
ORDER BY distance ASC;
```

The following query sorts locations by their distance from Seattle, San Francisco is closer.

```sql
SELECT name, ST_Distance(geom, ST_GeomFromText('POINT(-122.3321 47.6062)', 4326)) AS distance
FROM locations
ORDER BY distance ASC;
```

### Find locations within a polygon using `ST_MakeEnvelope`

PostGIS provides support for more sophisticated queries than simple distances.
PostGIS has functions to check if lines and polygons intersect, contain, or touch each other.
For example, the following query finds all locations within a rectangle longitude -124 to -119 and latitude 36 to 39.
This rectangle roughly covers Northern California, including San Francisco.

```sql
SELECT name FROM locations
WHERE geom && ST_MakeEnvelope(-124, 36, -119, 39, 4326);
```

Below is what the `ST_MakeEnvelope` rectangle looks like on a map.

![Northern California Rectangle](https://codebarbarian-images.s3.us-east-1.amazonaws.com/ca-rectangle.png)

### Index using GiST indexes

For large datasets, geospatial queries can be slow without an index.
PostGIS supports the GiST index for efficient geospatial lookups.
You can create a GiST index on locations' `geom` property using the following command.

```sql
CREATE INDEX locations_gist ON locations USING GIST(geom);
```


# How to use self-hosted runners with GitHub Actions

---
author: paul-scanlon
enableTableOfContents: true
createdAt: '2025-03-01T00:00:00.000Z'
updatedOn: '2025-03-01T00:00:00.000Z'
title: How to use self-hosted runners with GitHub Actions
subtitle: Take full control of your GitHub Action's runner environment with DigitalOcean
---

In this guide, I'll walk you through setting up a Linux server on a DigitalOcean Droplet with Ubuntu installed, which you can use as a self-hosted runner for GitHub Actions.

## What is a self-hosted runner?

[Self-hosted runners](https://docs.github.com/en/actions/hosting-your-own-runners/managing-self-hosted-runners/about-self-hosted-runners#about-self-hosted-runners) work similarly to GitHub's default runners, but with the key difference that you manage the server yourself. While the default runners are convenient, they come with some limitations—most notably, they timeout after six hours.

This can be a challenge for long-running jobs, particularly for users of our [Neon Twin](docs/guides/neon-twin-intro) workflow who may encounter issues with large databases. In these scenarios, setting up your own self-hosted runner is a more reliable solution.

<CTA title="Create a Neon Twin" description="A Neon Twin is a full or partial clone of your production or staging database, providing developers and teams with isolated, sandboxed environments that closely mirror production. <br><br>Learn how to create a Twin <a href='/docs/guides/neon-twin-intro'>here</a>." isIntro />

GitHub's default runners come with several preinstalled packages and dependencies, which you can review in the [GitHub runner-images repository README](https://github.com/actions/runner-images?tab=readme-ov-file#package-managers-usage). The default runner image also includes specific user permissions. To set up an effective self-hosted runner, you'll need to manually configure these packages, dependencies, and permissions. But don't worry—I'll guide you through each step.

## Getting started with Digital Ocean

If you don't have a Digital Ocean account already, create one [here](https://cloud.digitalocean.com/registrations/new). You will need to enter payment details before continuing to the next step.

### Create a Droplet

From the navigation list on the left hand side select **Droplets**, then click **Create Droplet**.

![Screenshot of Digital Ocean Create Droploet](/guides/images/gihub-actions-self-hosted-runners/gihub-actions-self-hosted-runners-create-droplet.jpg)

On the next screen you'll have a number of options to choose from. In this example I'll be deploying the Droplet to Digital Ocean's New York **Datacenter** and using **Ubuntu** for the Droplet **image**.

![Screenshot of Digital Ocean Droplet Config - Datacenter](/guides/images/gihub-actions-self-hosted-runners/gihub-actions-self-hosted-runners-droplet-config-1.jpg)

Scroll down to the next section and choose the **Droplet Size** and **CPU Options**. In ths example, I've chosen to use a **Shared CPU** and The smallest **Disk size**.

![Screenshot of Digital Ocean Droplet Config - CPU Size](/guides/images/gihub-actions-self-hosted-runners/gihub-actions-self-hosted-runners-droplet-config-2.jpg)

The next step is to select an authentication method. Choose **Password** and set a password that you'll use to log in as the `root` user.

![Screenshot of Digital Ocean Droplet Config - Auth Method](/guides/images/gihub-actions-self-hosted-runners/gihub-actions-self-hosted-runners-auth-method.jpg)

The final step is to give your Droplet a name—I've chosen `self-hosted-actions-runner`. This name will appear in the GitHub UI, which I'll explain in a later step. Once you're ready, click **Create Droplet**.

![Screenshot of Digital Ocean Droplet Config- Droplet Name](/guides/images/gihub-actions-self-hosted-runners/gihub-actions-self-hosted-runners-droplet-name.jpg)

After your Droplet is created, it will appear in the DigitalOcean UI. From there, you can copy the **IP address**, which you'll need for the next step.

![Screenshot of Digital Ocean Droplet Config- Droplet IP](/guides/images/gihub-actions-self-hosted-runners/gihub-actions-self-hosted-runners-droplet-ip.jpg)

## Configure Droplet

Now that your Droplet is created, run the following command in your terminal to log in as the `root` user. You'll be prompted for your password—enter it to complete the login.

```shell
ssh root@<Your Droplet's IP address>
```

## Create a new user

It's generally not recommended to provide external services with `root` access to your server. In this step, you'll create a new user named `runneruser` and grant it the necessary privileges to function as a self-hosted runner for GitHub Actions.

In your terminal, run the following command to create the `runneruser` and set a password.

```shell
adduser runneruser
```

You'll be asked to re-enter the password. For the following prompts, you can simply press `ENTER` to accept the default values for fields like `Full Name`, `Room Number`, `Work Phone`, and so on. When prompted, press `Y` to confirm that all the information is correct.

### Add user to sudo group

The `runneruser` needs to be added to the `sudo` group and granted permission to install packages without being prompted for a password.

To add the `runneruser` to the sudo group, run the following command in your terminal.

```shell
adduser runneruser sudo
```

Next you need to update the permissions. To do this run the following in the terminal.

```shell
visudo
```

### Granting privileges

Scroll down until you find the following section, then update the configuration to grant `runneruser` permission to execute specific `sudo` commands.

In this guide, I’m allowing `runneruser` to:

- Use the `apt` command
- Run a specific command to install the `postgresql-common` package
- Execute these commands without being prompted for a password using `NOPASSWD`

The exact permissions you need to grant will depend on the specific requirements of your Action.

```shell {3}
# Allow members of group sudo to execute any command
%sudo   ALL=(ALL:ALL) ALL
runneruser ALL=(ALL) NOPASSWD: /usr/bin/apt, /usr/share/postgresql-common/pgdg/apt.postgresql.org.sh
```

To exit `visudo`, press `^X` and confirm the changes. Once you're done, log out as the `root` user by typing `exit` in your terminal.

## Log in as runneruser

Now that the `runneruser` has been created, you can log in to configure the system for the self-hosted runner. Run the following command in your terminal to log in as `runneruser`.

```shell
ssh runneruser@<Your Droplet's IP address>
```

After logging in, got back to GitHub and navigate to **Settings** > **Actions** > **Runners**, then click **New self-hosted runner**.

From the options select **Linux** under the **Runner Image** section and **x64** in the **Architecture** section.

![Screenshot of GitHub - Self-hosted Runners](/guides/images/gihub-actions-self-hosted-runners/gihub-actions-self-hosted-runners-github-runner-instructions.jpg)

Follow the **Download** and **Configure** steps. When you reach the final step, **Create the runner and start the configuration experience**, press **Enter** to accept the default options. However, skip the last step, which runs `./run.sh`—I'll explain why next.

### Running the Self-Hosted Runner

While running `./run.sh` is the simplest way to start your self-hosted runner on your Droplet, it operates as a foreground process. This means that once you close your terminal, the runner stops. If you intend to use the runner for long-running workflows, you’ll need to ensure it runs in the background.

To do this, you can set up your self-hosted runner to run as a service. This involves an additional setup step, and the method for starting the service is slightly different, but it guarantees the runner continues to operate in the background even after you close your terminal.

For further details, refer to the GitHub documentation: [Configuring the self-hosted runner application as a service](https://docs.github.com/en/actions/hosting-your-own-runners/managing-self-hosted-runners/configuring-the-self-hosted-runner-application-as-a-service#installing-the-service). Just keep in mind, you'll need to change directories before running any of the commands listed on that page.

To change the directory, run the following in your terminal:

```shell
cd actions-runner
```

After completing these steps, refresh the GitHub page. Your Droplet should now appear in the UI with an **Idle** status.

![Screenshot of GitHub - Runner Idle](/guides/images/gihub-actions-self-hosted-runners/gihub-actions-self-hosted-runners-github-runner-idle.jpg)

## Creating a test GitHub Action

This test Action installs Postgres using [APT](<https://en.wikipedia.org/wiki/APT_(software)>), then echoes both the `DEV_DATABASE_URL`'s Postgres version and the version of Postgres installed in the Action environment.

To use this Action, you'll need to create an environment variable named `DEV_DATABASE_URL` with a valid Postgres connection string and add it to your GitHub Repository Secrets.

To do this navigate to **Settings** > **Secrets and variables** > **Actions** and add the environment variable under **Repository secrets**.

![Screenshot of GitHub - Repository secrets](/guides/images/gihub-actions-self-hosted-runners/gihub-actions-self-hosted-runners-github-screts.jpg)

One key detail to note is the line: `runs-on: self-hosted`. This tells GitHub that the Action should run on your self-hosted runner rather than the default shared infrastructure.

```yml
name: Self Hosted Runner

on:
  workflow_dispatch:

env:
  DEV_DATABASE_URL: ${{ secrets.DEV_DATABASE_URL }}
  PG_VERSION: 16

jobs:
  check-pg-version:
    runs-on: self-hosted

    steps:
      - name: Install PostgreSQL Common Package
        run: |
          sudo apt update
          sudo apt install -y postgresql-common

      - name: Install PostgreSQL
        run: |
          sudo apt update
          yes '' | sudo /usr/share/postgresql-common/pgdg/apt.postgresql.org.sh
          sudo apt install -y postgresql-${{ env.PG_VERSION }}

      - name: Set PostgreSQL binary path
        run: echo "POSTGRES=/usr/lib/postgresql/${{ env.PG_VERSION }}/bin" >> $GITHUB_ENV

      - name: PostgreSQL version
        run: |
          "$POSTGRES/psql" "$DEV_DATABASE_URL" -c "SELECT version();"

      - name: Check psql Version
        run: |
          $POSTGRES/psql --version
```

## Finished

While this Action may seem somewhat complex, it effectively demonstrates how the steps within the workflow align with the permissions set on the Droplet.

Only the commands explicitly allowed for `runneruser` are permitted to run. If you need to add more steps to your Action that require extra permissions, you’ll need to update `runneruser`'s sudo privileges accordingly.

That said, with a self-hosted runner, you have full control. You can deploy a high-performance Droplet capable of running longer and faster than GitHub's default shared infrastructure runner, helping to overcome any limitations you may have encountered.


# Setting up GitHub Codespaces with Neon Database Branching for Pull Requests

---
title: Setting up GitHub Codespaces with Neon Database Branching for Pull Requests
subtitle: Learn how to create separate development environments for each pull request using GitHub Codespaces and Neon's Postgres branching
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-08-18T00:00:00.000Z'
updatedOn: '2024-08-18T00:00:00.000Z'
---

When working on a team project, it's useful to have separate environments for each new feature or bug fix. This helps prevent conflicts and makes it easier to test changes. In this guide, we'll show you how to set up a process that creates a new development environment for each pull request. We'll use GitHub Codespaces for the coding environment and Neon's Postgres branching for the database.

By the end of this guide, you'll have a setup that automatically creates a new Codespace and a new database branch for each pull request. This means each change can be tested separately, making it easier to find and fix problems.

## What you'll need

Before we start, make sure you have:

- A GitHub account that can use Codespaces
- A [Neon](https://console.neon.tech/signup) account and project
- A Neon API key (you can learn how to get one [here](/docs/manage/api-keys#create-an-api-key))
- Basic knowledge of Git, GitHub Actions, and CI/CD

## Creating a new project

This process will work with any language or framework, but for this guide, we'll use [Laravel](/guides/laravel-overview).

Let's start by making a new Laravel project and putting it on GitHub.

1. First, create a new Laravel project:

```bash
composer create-project laravel/laravel codespaces-neon-demo
cd codespaces-neon-demo
```

This command creates a new Laravel project in a folder called `codespaces-neon-demo` then uses `cd` to access the new project.

2. Next, set up Git for this project:

```bash
git init
git add .
git commit -m "First commit: New Laravel project"
```

The above commands will initialize a new Git repository, add all the project files to it, and create the first commit.

3. Now, create a new repository on GitHub and upload your code:

```bash
git remote add origin https://github.com/<yourusername>/codespaces-neon-demo.git
git branch -M main
git push -u origin main
```

Replace `yourusername` with your actual GitHub username. These commands connect your local repository to GitHub and upload your code.

## Setting up GitHub Codespaces

Now we'll set up GitHub Codespaces. This will define the development environment that will be created for each pull request.

1. Make a new folder in your project for the Codespaces configuration:

```bash
mkdir .devcontainer
```

2. In this new folder, create a file called `devcontainer.json`:

```json
{
  "name": "Laravel Codespaces",
  "image": "mcr.microsoft.com/devcontainers/php:8.2",
  "customizations": {
    "vscode": {
      "extensions": [
        "felixfbecker.php-debug",
        "bmewburn.vscode-intelephense-client",
        "mikestead.dotenv",
        "amiralizadeh9480.laravel-extra-intellisense"
      ]
    }
  },
  "forwardPorts": [8000],
  "postCreateCommand": "cp .env.example .env && composer install && php artisan key:generate && .devcontainer/setup-db.sh",
  "features": {
    "ghcr.io/devcontainers/features/node:1": {}
  }
}
```

This file tells GitHub Codespaces how to set up the development environment. Here's what each part does:

- `"name"`: This is just a name for the Codespace.
- `"image"`: This specifies the base Docker image to use. We're using a pre-built image with PHP 8.2.
- `"customizations"`: This section lists VS Code extensions to install.
- `"forwardPorts"`: This tells Codespaces which ports to make available.
- `"postCreateCommand"`: This runs commands after the Codespace is created. It installs PHP dependencies, generates an application key, and runs a setup script for the database.
- `"features"`: This adds Node.js to the environment.

## Setting up Neon Postgres

Now let's connect our project to a Neon Postgres database.

1. Go to the [Neon Console](https://console.neon.tech) and create a new project.

2. After creating the project, you'll see a connection string. Copy the details as you'll need them later.

3. Open the `.env` file in your Laravel project and update the database settings:

```env
DB_CONNECTION=pgsql
DB_HOST=your-neon-hostname.neon.tech
DB_PORT=5432
DB_DATABASE=your_database_name
DB_USERNAME=your_username
DB_PASSWORD=your_password
```

Replace the placeholders with the details from your Neon connection string.

4. Run the database migrations:

```bash
php artisan migrate
```

This command creates the necessary tables in your Neon database.

## Setting up GitHub Actions for Neon Branching

Now we'll set up GitHub Actions to create and delete Neon database branches automatically. First, we need to add your Neon API key to your GitHub repository:

1. In your GitHub repository, go to "Settings", then "Secrets and variables", then "Actions".
2. Click "New repository secret".
3. Name it `NEON_API_KEY` and paste your [Neon API key](/docs/manage/api-keys#create-an-api-key) as the value.
4. Click "Add secret".

Next, we'll create two GitHub Actions workflows: one to create a new Neon branch when a pull request is opened, and another to delete the branch when the pull request is closed.

### Workflow to Create a Branch

If you don't already have a `.github/workflows` directory, create one:

```bash
mkdir -p .github/workflows
```

Then create a file in this directory called `create-neon-branch.yml` with the following content:

```yaml
name: Create Neon Branch

on:
  pull_request:
    types: [opened, reopened]

jobs:
  create-branch:
    runs-on: ubuntu-latest
    steps:
      - uses: neondatabase/create-branch-action@v5
        with:
          project_id: your-neon-project-id
          branch_name: pr-${{ github.event.pull_request.number }}
          username: your-database-username
          api_key: ${{ secrets.NEON_API_KEY }}
        id: create-branch
      - run: echo ${{ steps.create-branch.outputs.db_url }}
      - run: echo ${{ steps.create-branch.outputs.branch_id }}
```

Replace `your-neon-project-id` and `your-database-username` with your actual Neon project ID and database username.

This workflow does the following:

- It runs when a pull request is opened or reopened thanks to the `on` section.
- It uses Neon's official action to create a new database branch.
- The branch name is based on the pull request number.
- It outputs the new branch's database URL and ID.

### Workflow to Delete a Branch

With the workflow to create a branch set up, let's create another one to delete the branch when the pull request is closed.

Create another file at `.github/workflows/delete-neon-branch.yml`:

```yaml
name: Delete Neon Branch

on:
  pull_request:
    types: [closed]

jobs:
  delete-branch:
    runs-on: ubuntu-latest
    steps:
      - uses: neondatabase/delete-branch-action@v3
        with:
          project_id: your-neon-project-id
          branch: pr-${{ github.event.pull_request.number }}
          api_key: ${{ secrets.NEON_API_KEY }}
```

Again, replace `your-neon-project-id` with your actual Neon project ID.

This workflow:

- Runs when a pull request is closed.
- Uses Neon's action to delete the database branch associated with the pull request.

## Configuring Codespaces to Use Neon Branches

Now we need to tell Codespaces how to connect to the right database branch. Create a file called `setup-db.sh` in the `.devcontainer` directory:

```bash
#!/bin/bash

PR_NUMBER=$(echo $GITHUB_REF | sed 's/refs\/pull\/\([0-9]*\).*/\1/')

if [ -n "$PR_NUMBER" ]; then
    BRANCH_NAME="pr-$PR_NUMBER"

    # Use GitHub CLI to get the branch details
    BRANCH_DETAILS=$(gh api /repos/$GITHUB_REPOSITORY/actions/runs/$GITHUB_RUN_ID/jobs -H "Accept: application/vnd.github.v3+json" | jq -r '.jobs[] | select(.name == "create-branch") | .steps[] | select(.name == "Create Neon Branch") | .outputs.db_url')

    if [ -n "$BRANCH_DETAILS" ]; then
        # Parse the connection string
        DB_HOST=$(echo $BRANCH_DETAILS | sed -n 's/.*@\(.*\):.*/\1/p')
        DB_NAME=$(echo $BRANCH_DETAILS | sed -n 's/.*\/\(.*\)?.*/\1/p')
        DB_USER=$(echo $BRANCH_DETAILS | sed -n 's/.*:\/\/\(.*\):.*/\1/p')
        DB_PASSWORD=$(echo $BRANCH_DETAILS | sed -n 's/.*:\/\/.*:\(.*\)@.*/\1/p')

        # Update the .env file
        sed -i "s/DB_HOST=.*/DB_HOST=$DB_HOST/" .env
        sed -i "s/DB_DATABASE=.*/DB_DATABASE=$DB_NAME/" .env
        sed -i "s/DB_USERNAME=.*/DB_USERNAME=$DB_USER/" .env
        sed -i "s/DB_PASSWORD=.*/DB_PASSWORD=$DB_PASSWORD/" .env

        echo "Updated .env file with PR-specific database details"
    else
        echo "No branch details found for PR $PR_NUMBER"
    fi
else
    echo "This is not a PR environment, using default database settings"
fi

# Run database migrations
php artisan migrate --force
```

The script does the following:

- It checks if we're in a pull request environment.
- If we are, it gets the details of the newly created database branch.
- It updates the `.env` file with these details.
- Finally, it runs database migrations.

Make sure to make this script executable:

```bash
chmod +x .devcontainer/setup-db.sh
```

After setting up these files, commit and push your changes to GitHub:

```bash
git add .
git commit -m "Add GitHub Actions and Codespaces configuration"
git push origin main
```

With everything set up, you can now create a new branch in your project, open a pull request, and see the new Codespace and database branch in action.

## How to Use This Setup

With everything set up, here's how you would use this in your development process:

1. Create a new branch in your project and make your changes:
   - Create and switch to a new branch: `git checkout -b feature-branch-name`
   - Make your changes to the code
   - Commit your changes: `git add .` and `git commit -m "Description of changes"`
   - Push your branch to GitHub: `git push -u origin feature-branch-name`

2. Open a pull request with your changes:
   - Go to your repository on GitHub
   - Click on "Pull requests" then "New pull request"
   - Select your feature branch as the compare branch
   - Click "Create pull request"
   - Fill in the title and description, then click "Create pull request"

3. GitHub Actions will automatically create a new Neon database branch for your pull request:
   - This happens automatically when the pull request is opened
   - You can check the "Actions" tab in your GitHub repository to see the progress
   - Once complete, you'll see a new branch in your Neon console named `pr-[number]`

4. Open a Codespace for this pull request:
   - On the pull request page, click the "Code" dropdown
   - Select "Open with Codespaces"
   - Click "New codespace"
   - Wait for the Codespace to build and start

5. Test your changes in the isolated environment:
   - The Codespace is now connected to your PR-specific database branch
   - Run your application: `php artisan serve`
   - Run tests: `php artisan test`
   - Make additional changes if needed, commit, and push

6. Review and merge the pull request:
   - Once you're satisfied with the changes, request a review if required
   - Reviewers can open their own Codespaces to test the changes
   - When ready, merge the pull request on GitHub

7. Automatic cleanup:
   - When the pull request is closed (either merged or declined), GitHub Actions will automatically delete the associated Neon database branch
   - You can verify this in your Neon console

## Keeping Things Secure

It's important to keep your project and its data safe:

1. Never share your Neon API key. Always use GitHub Secrets to store it.
2. Be careful about what information you put in public repositories.
3. Regularly change your API keys and check who has access to what.

## Conclusion

By setting up GitHub Codespaces with Neon database branching, you've created a system that gives each pull request its own complete development environment. This can help your team work more effectively by making it easier to test changes and avoid conflicts.

This workflow can be adapted to work with other languages and frameworks. You can also add more steps to the GitHub Actions workflows to suit your specific needs like running tests, deploying to staging environments, or sending notifications.

## Where to Learn More

- [GitHub Codespaces Documentation](https://docs.github.com/en/codespaces)
- [Neon Documentation](/docs)
- [GitHub Actions Documentation](https://docs.github.com/en/actions)

<NeedHelp />


# Implementing Database Migrations in Go Applications with Neon

---
title: Implementing Database Migrations in Go Applications with Neon
subtitle: Learn how to manage database schema changes in Go applications using Neon's serverless Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2025-02-22T00:00:00.000Z'
updatedOn: '2025-02-22T00:00:00.000Z'
---

Database migrations are essential for managing schema evolution in applications as they grow and change over time. When working with Go applications and Neon's serverless Postgres, implementing a good migration strategy allows you to have smooth deployments and database changes without disruption.

This guide will walk you through implementing and managing database migrations for Go applications using Neon Postgres, covering everything from basic concepts to advanced production deployment strategies.

## Prerequisites

Before diving into database migrations, make sure you have:

- [Go](https://golang.org/dl/) 1.18 or later installed
- A [Neon](https://console.neon.tech/signup) account and project
- Basic understanding of SQL and database schemas
- Familiarity with Go programming

## Understanding Database Migrations

Database migrations provide a structured approach to evolve your database schema over time. Each migration represents a discrete change to your database structure, such as adding a table, modifying a column, or creating an index.

Some of the main benefits of using database migrations include:

- Track changes to your schema alongside your application code.
- Easily set up new development or production databases.
- Apply changes incrementally with the ability to roll back.
- Multiple developers can make schema changes without conflicts.
- Integrate database changes into your CI/CD pipeline.

Without migrations, managing database schema changes becomes increasingly challenging as applications grow and team sizes increase.

## Migration Tools for Go

As with many programming languages, Go has a rich ecosystem of tools for managing database migrations. These tools help you create, apply, and roll back migrations in a controlled and repeatable manner.

Let's explore the most popular options:

### golang-migrate

[golang-migrate](https://github.com/golang-migrate/migrate) is one of the most widely used migration tools in the Go ecosystem. It provides:

- Support for multiple databases including Postgres
- Migration files in SQL or Go formats
- CLI tools for migration management
- A Go library for programmatic migration control
- Versioned migrations with up/down operations

### Other Options

While we'll focus on golang-migrate in this guide, other notable migration tools include:

- **[sql-migrate](https://github.com/rubenv/sql-migrate)**: Uses goose-style migrations with support for multiple databases
- **[goose](https://github.com/pressly/goose)**: A migration tool with support for Go or SQL migrations
- **[atlas](https://github.com/ariga/atlas)**: A newer tool that provides declarative schema migrations
- **[dbmate](https://github.com/amacneil/dbmate)**: A database migration tool that's language-agnostic

## Setting Up golang-migrate

Let's set up golang-migrate to work with your Neon Postgres database. It can be used both from the command line and programmatically within your Go code. We'll cover both approaches in this guide.

Let's start by installing the golang-migrate CLI.

### Install the Migration Tool

First, install the golang-migrate CLI. You can do this using `go install`:

```bash
go install -tags 'postgres' github.com/golang-migrate/migrate/v4/cmd/migrate@latest
```

Or alternatively, you can also use a package manager or follow the [installation instructions](https://github.com/golang-migrate/migrate/blob/master/cmd/migrate/README.md) from the golang-migrate repository depending on your operating system.

### Project Structure

Create a structure for your migrations in your Go project:

```
your-go-project/
├── cmd/
│   └── main.go
├── internal/
│   └── app/
├── migrations/
│   ├── 000001_create_users_table.up.sql
│   ├── 000001_create_users_table.down.sql
│   ├── 000002_add_user_roles.up.sql
│   └── 000002_add_user_roles.down.sql
└── go.mod
```

The `migrations` directory will store all your migration files. Each migration consists of two files:

- `NNNNNN_name.up.sql`: Contains SQL to apply the migration
- `NNNNNN_name.down.sql`: Contains SQL to revert the migration

### Creating Your First Migration

Next, let's create your first migration to establish a users table:

```bash
# Create the migrations directory if it doesn't exist
mkdir -p migrations

# Create the migration files
migrate create -ext sql -dir migrations -seq create_users_table
```

This command creates two files:

- `migrations/000001_create_users_table.up.sql`
- `migrations/000001_create_users_table.down.sql`

Now, edit the up migration file with the SQL to create your users table:

```sql
-- migrations/000001_create_users_table.up.sql
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    email VARCHAR(255) UNIQUE NOT NULL,
    name VARCHAR(100) NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Add an index on email for faster lookups
CREATE INDEX idx_users_email ON users(email);

-- Add a trigger to update the updated_at timestamp
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ language 'plpgsql';

CREATE TRIGGER update_users_updated_at
    BEFORE UPDATE ON users
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();
```

And edit the down migration to reverse these changes:

```sql
-- migrations/000001_create_users_table.down.sql
DROP TRIGGER IF EXISTS update_users_updated_at ON users;
DROP FUNCTION IF EXISTS update_updated_at_column();
DROP TABLE IF EXISTS users;
```

A few important points about this migration:

1. We're creating a table with a primary key and various constraints
2. We're adding an index for performance optimization
3. We're creating a trigger to automatically update the `updated_at` timestamp
4. The down migration reverses all changes in the correct order

Notice how the down migration drops objects in reverse order compared to how they were created in the up migration. This is important to avoid dependency issues when rolling back.

## Connecting to Neon Postgres

To run migrations against your Neon database, you'll need to construct a proper connection string. Neon provides a secure, TLS-enabled connection:

```
postgresql://[user]:[password]@[neon_hostname]/[dbname]?sslmode=require&channel_binding=require
```

Replace the placeholders with your actual Neon connection details, which you can find in the Neon Console under your project's connection settings.

For convenience, you might want to store this connection string in an environment variable:

```bash
export NEON_DB_URL="postgresql://user:password@ep-example-123456.us-east-2.aws.neon.tech/neondb?sslmode=require&channel_binding=require"
```

## Running Migrations

With your migration files created and your connection string ready, you can now run migrations against your Neon database.

### Running Migrations from CLI

To apply all pending migrations:

```bash
migrate -database "${NEON_DB_URL}" -path ./migrations up
```

To roll back the most recent migration:

```bash
migrate -database "${NEON_DB_URL}" -path ./migrations down 1
```

To migrate to a specific version:

```bash
migrate -database "${NEON_DB_URL}" -path ./migrations goto 2
```

To check the current migration version:

```bash
migrate -database "${NEON_DB_URL}" -path ./migrations version
```

Having the ability to run migrations from the command line is useful for local development and debugging. However, for production deployments, let's look at how to run migrations programmatically from your Go code.

### Running Migrations from Go Code

For many applications, you'll want to run migrations programmatically from your Go code, especially during application startup or as part of CI/CD processes.

Create a `migrations.go` file in your project:

```go
package migrations

import (
	"errors"
	"log"

	"github.com/golang-migrate/migrate/v4"
	_ "github.com/golang-migrate/migrate/v4/database/postgres"
	_ "github.com/golang-migrate/migrate/v4/source/file"
)

// RunMigrations applies database migrations to the specified database
func RunMigrations(dbURL, migrationsPath string) error {
	m, err := migrate.New("file://"+migrationsPath, dbURL)
	if err != nil {
		return err
	}

	if err := m.Up(); err != nil && !errors.Is(err, migrate.ErrNoChange) {
		return err
	}

	srcErr, dbErr := m.Close()
	if srcErr != nil {
		return srcErr
	}
	if dbErr != nil {
		return dbErr
	}

	log.Println("Migrations applied successfully")
	return nil
}
```

Then call this function during your application startup:

```go
package main

import (
	"log"
	"os"

	"your-module/migrations"
)

func main() {
	dbURL := os.Getenv("NEON_DB_URL")
	if dbURL == "" {
		log.Fatal("NEON_DB_URL environment variable is not set")
	}

	err := migrations.RunMigrations(dbURL, "./migrations")
	if err != nil {
		log.Fatalf("Failed to run migrations: %v", err)
	}

	// Continue with application startup...
}
```

This approach allows you to run migrations as part of your application's initialization process. You can also integrate this code into your CI/CD pipeline to ensure migrations are applied consistently across environments.

## Creating Additional Migrations

As your application evolves, you'll need to create additional migrations to modify your database schema. Let's create a second migration to add user roles:

```bash
migrate create -ext sql -dir migrations -seq add_user_roles
```

Edit the up migration:

```sql
-- migrations/000002_add_user_roles.up.sql
CREATE TABLE roles (
    id SERIAL PRIMARY KEY,
    name VARCHAR(50) UNIQUE NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE user_roles (
    user_id INT NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    role_id INT NOT NULL REFERENCES roles(id) ON DELETE CASCADE,
    PRIMARY KEY (user_id, role_id)
);

-- Insert default roles
INSERT INTO roles (name) VALUES ('user'), ('admin'), ('editor');

-- Add a default role for existing users
INSERT INTO user_roles (user_id, role_id)
SELECT u.id, r.id FROM users u, roles r WHERE r.name = 'user';
```

And the down migration:

```sql
-- migrations/000002_add_user_roles.down.sql
DROP TABLE IF EXISTS user_roles;
DROP TABLE IF EXISTS roles;
```

This migration shows a few common patterns:

1. Creating related tables with foreign key constraints
2. Seeding tables with initial data
3. Migrating existing data to maintain consistency

## Best Practices for Migrations

When working with migrations in Go applications and Neon Postgres, follow these best practices:

### 1. Keep Migrations Small and Focused

Each migration should do one thing and do it well. Small, focused migrations are:

- Easier to review
- Quicker to apply
- Simpler to roll back if needed
- Less likely to cause problems

For example, split adding a table and populating it with data into separate migrations when possible.

### 2. Make Migrations Idempotent When Possible

Idempotent migrations can be applied multiple times without changing the result. Use conditionals in your SQL to make migrations more robust:

```sql
-- Check if the index exists before creating it
DO $$
BEGIN
    IF NOT EXISTS (
        SELECT 1 FROM pg_indexes WHERE indexname = 'idx_users_email'
    ) THEN
        CREATE INDEX idx_users_email ON users(email);
    END IF;
END
$$;
```

### 3. Use Transactions for Safety

Ensure your migrations run within transactions to maintain database consistency. If a migration fails partway through, all changes should be rolled back.

The golang-migrate tool automatically wraps each migration in a transaction by default, but you can also explicitly include transactions in your SQL:

```sql
BEGIN;

-- Migration steps here

COMMIT;
```

There has been a feature request to add support for transactions in the golang-migrate tool, you can track the progress [here](https://github.com/golang-migrate/migrate/issues/196) but for now, you would need to handle transactions manually in your SQL.

### 4. Test Migrations Before Applying to Production

Always test migrations in a non-production environment first. Ideally, have a staging environment that mirrors production as closely as possible.

You can achieve this by setting up a separate Neon branch to test migrations before applying them to your production branch. You can learn more about Neon branches in the [Neon documentation](/docs/introduction/branching).

### 5. Version Control Your Migrations

Store migrations in version control alongside your application code to make sure that:

- Changes are tracked
- Code reviewers can see database changes
- Migration history is preserved

### 6. Never Edit Existing Migrations

Once a migration has been applied to any environment, consider it immutable. If you need to change something, create a new migration. This avoids inconsistencies between environments.

## Integrating with CI/CD Pipelines

Automating migrations as part of your CI/CD pipeline ensures database changes are applied consistently across environments.

### Example GitHub Actions Workflow

Here's an example GitHub Actions workflow that runs migrations during deployment:

```yaml
name: Deploy with Migrations

on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Go
        uses: actions/setup-go@v4
        with:
          go-version: '1.20'

      - name: Install migrate
        run: |
          curl -L https://github.com/golang-migrate/migrate/releases/download/v4.18.2/migrate.linux-amd64.tar.gz | tar xvz
          sudo mv migrate /usr/bin/migrate
          which migrate

      - name: Run migrations
        run: migrate -database "${NEON_DB_URL}" -path ./migrations up
        env:
          NEON_DB_URL: ${{ secrets.NEON_DB_URL }}

      # Continue with application deployment...
```

The workflow will trigger on pushes to the main branch, then perform the following steps:

1. Fetch the repository code
2. Prepare the Go environment
3. Download and install the `migrate` tool
4. Apply migrations to the database using the connection string stored in a GitHub secret
5. Continue with the deployment process

Running your database migrations directly on your production database can be risky. For a safer approach, let's look at how to test migrations on a Neon branch before deploying them to production.

### Running Migrations on a Neon Branch

For a more robust approach, you can use Neon's branching capabilities to test migrations before applying them to your production database.

Neon has a set of [GitHub Actions](/docs/guides/branching-github-actions) that allow you to create, delete, and compare branches programmatically. Here's an extended GitHub Actions workflow that uses Neon's branching actions to spin up a temporary branch for testing migrations:

```yaml
name: Test and Deploy Migrations

on:
  pull_request:
    types: [opened, synchronize]
    paths:
      - 'migrations/**'
  push:
    branches: [main]
    paths:
      - 'migrations/**'

jobs:
  migration-test:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
      contents: read
    steps:
      - uses: actions/checkout@v3

      - name: Set up Go
        uses: actions/setup-go@v4
        with:
          go-version: '1.20'

      - name: Install migrate
        run: |
          curl -L https://github.com/golang-migrate/migrate/releases/download/v4.18.2/migrate.linux-amd64.tar.gz | tar xvz
          sudo mv migrate /usr/bin/migrate

      # Create a temporary branch for testing migrations
      - name: Create Neon branch for testing
        id: create-branch
        uses: neondatabase/create-branch-action@v5
        with:
          project_id: ${{ vars.NEON_PROJECT_ID }}
          parent: main
          branch_name: migration-test-${{ github.run_id }}
          username: ${{ vars.NEON_DB_USER }}
          api_key: ${{ secrets.NEON_API_KEY }}

      - name: Run migrations on test branch
        run: |
          migrate -database "${{ steps.create-branch.outputs.db_url }}" -path ./migrations up
          echo "Migrations applied successfully to test branch"

      # Run tests against the migrated schema
      - name: Run database schema tests
        run: |
          go test ./tests/db/... -db-url="${{ steps.create-branch.outputs.db_url }}"

      # For pull requests, generate a schema diff
      - name: Generate schema diff
        if: github.event_name == 'pull_request'
        uses: neondatabase/schema-diff-action@v1
        with:
          project_id: ${{ vars.NEON_PROJECT_ID }}
          compare_branch: migration-test-${{ github.run_id }}
          base_branch: main
          api_key: ${{ secrets.NEON_API_KEY }}
          database: ${{ vars.NEON_DB_NAME || 'neondb' }}
          username: ${{ vars.NEON_DB_USER }}

      # Clean up the test branch
      - name: Delete test branch
        if: always()
        uses: neondatabase/delete-branch-action@v3
        with:
          project_id: ${{ vars.NEON_PROJECT_ID }}
          branch: migration-test-${{ github.run_id }}
          api_key: ${{ secrets.NEON_API_KEY }}

  # Only run on push to main
  deploy-production:
    needs: migration-test
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Install migrate
        run: |
          curl -L https://github.com/golang-migrate/migrate/releases/download/v4.18.2/migrate.linux-amd64.tar.gz | tar xvz
          sudo mv migrate /usr/bin/migrate

      # Run migrations on production database
      - name: Run migrations on production
        run: migrate -database "${NEON_PROD_DB_URL}" -path ./migrations up
        env:
          NEON_PROD_DB_URL: ${{ secrets.NEON_PROD_DB_URL }}

      # Continue with application deployment...
```

This extended workflow does the following:

1. Triggers on both pull requests affecting migration files and pushes to the main branch
2. Creates a temporary branch specifically for testing migrations using Neon's create-branch-action
3. Runs migrations on the test branch to verify they apply correctly
4. Executes schema tests to ensure the migrated schema works as expected
5. Generates a schema diff for pull requests, providing reviewers with a clear view of the proposed changes
6. Cleans up by deleting the test branch after testing
7. Deploys to production only when changes are pushed to the main branch, after successful testing

This approach provides several benefits:

- Migrations are thoroughly tested before being applied to production
- Pull request reviewers can see exactly what schema changes are being proposed
- Failed migrations don't affect your production database
- The workflow is fully automated as part of your CI/CD pipeline

To use this workflow, you'll need to set up the following GitHub repository secrets and variables:

- **Secrets**:
  - `NEON_API_KEY`: Your Neon API key
  - `NEON_PROD_DB_URL`: Production database connection string

- **Variables**:
  - `NEON_PROJECT_ID`: Your Neon project ID
  - `NEON_DB_USER`: Database username
  - `NEON_DB_NAME`: Database name (defaults to 'neondb' if not specified)

You can add more steps to this workflow depending on your specific deployment needs, such as building and deploying your application after successful migrations.

## Working with Multiple Environments

Most applications require different database configurations for development, testing, staging, and production environments.

### Environment-Specific Configurations

Manage environment-specific database URLs using environment variables or configuration files:

```go
package config

import (
	"os"
	"fmt"
)

// GetDatabaseURL returns the appropriate database URL for the current environment
func GetDatabaseURL() string {
	env := os.Getenv("APP_ENV")
	if env == "" {
		env = "development" // Default
	}

	switch env {
	case "production":
		return os.Getenv("NEON_PROD_DB_URL")
	case "staging":
		return os.Getenv("NEON_STAGING_DB_URL")
	case "test":
		return os.Getenv("NEON_TEST_DB_URL")
	default:
		return os.Getenv("NEON_DEV_DB_URL")
	}
}
```

## Conclusion

Database migrations are a critical part of managing application evolution. When working with Go applications and Neon Postgres, a well-implemented migration strategy ensures that your schema changes are version-controlled and applied consistently across environments.

The combination of Go's strong tooling, the flexibility of golang-migrate, and Neon's powerful Postgres capabilities provides an excellent foundation for managing database schema changes throughout your application's lifecycle.

## Additional Resources

- [golang-migrate documentation](https://github.com/golang-migrate/migrate)
- [Neon Documentation](/docs)
- [PostgreSQL Alter Table documentation](https://www.postgresql.org/docs/current/ddl-alter.html)
- [PostgreSQL schema design best practices](https://www.postgresql.org/docs/current/ddl-schemas.html)

<NeedHelp />


# Using GORM with Neon Postgres

---
title: Using GORM with Neon Postgres
subtitle: Learn how to use GORM, Go's most popular ORM, with Neon's serverless Postgres for efficient database operations
author: bobbyiliev
enableTableOfContents: true
createdAt: '2025-02-15T00:00:00.000Z'
updatedOn: '2025-02-15T00:00:00.000Z'
---

[GORM](https://gorm.io/) is Go's most popular ORM library, providing a developer-friendly interface to interact with databases. When combined with Neon's serverless Postgres, it creates a great foundation for building scalable Go applications with minimal database management overhead.

This guide walks you through the process of integrating GORM with Neon Postgres, we will cover everything that you need to know to get started with GORM and Neon Postgres.

## Prerequisites

Before getting started, make sure you have:

- [Go](https://golang.org/dl/) 1.18 or later installed
- A [Neon](https://console.neon.tech/signup) account
- Basic familiarity with Go and SQL

## Setting Up Your Environment

### Create a Neon Project

If you don't have one already, create a Neon project:

1. Navigate to the [Projects page](https://console.neon.tech/app/projects) in the Neon Console
2. Click **New Project**
3. Specify your project settings and click **Create Project**

Save your connection details including your password. You'll need these when configuring your Go application.

### Initialize Your Go Project

Let's begin by setting up your project structure. In Go, projects are organized as modules, which manage dependencies and package versioning. A module is initialized with a unique module path that distinguishes your project in the Go ecosystem.

Start by creating a new directory for your project and initializing a Go module:

```bash
mkdir neon-gorm-example
cd neon-gorm-example
go mod init example.com/neon-gorm
```

This creates a `go.mod` file that will track your project's dependencies. The `example.com/neon-gorm` is the module path and should be replaced with your own domain or GitHub repository if you plan to publish your code.

### Install Required Packages

Now we need to install the packages our application will depend on. For this guide, we'll need two main packages:

1. **GORM** - The ORM library that provides a developer-friendly interface to interact with the database
2. **GORM Postgres Driver** - The database driver that allows GORM to connect to Postgres databases

Run the following commands to install these packages:

```bash
go get -u gorm.io/gorm
go get -u gorm.io/driver/postgres
```

These commands fetch the latest versions of the packages and add them to your project's `go.mod` file. The `-u` flag ensures you get the most recent version of each package.

## Connecting to Neon with GORM

### Basic Connection Setup

Now let's establish a connection to your Neon Postgres database using GORM. This is an essential step that initializes the database connection that we'll use throughout our application.

Create a new file named `main.go` with the following code:

```go
package main

import (
	"fmt"
	"log"

	"gorm.io/driver/postgres"
	"gorm.io/gorm"
	"gorm.io/gorm/logger"
)

func main() {
	// Connection string for Neon Postgres
	dsn := "postgresql://[user]:[password]@[neon_hostname]/[dbname]?sslmode=require&channel_binding=require"

	// Connect to the database
	db, err := gorm.Open(postgres.Open(dsn), &gorm.Config{
		Logger: logger.Default.LogMode(logger.Info), // Set to Info level for development
	})
	if err != nil {
		log.Fatalf("Failed to connect to database: %v", err)
	}

	// Get the underlying SQL DB object
	sqlDB, err := db.DB()
	if err != nil {
		log.Fatalf("Failed to get DB object: %v", err)
	}

	// Verify connection
	if err := sqlDB.Ping(); err != nil {
		log.Fatalf("Failed to ping DB: %v", err)
	}

	fmt.Println("Successfully connected to Neon Postgres database!")
}
```

In this code, we're performing several important steps:

1. Defining a DSN - this is the connection string that contains all the information needed to connect to your Neon database
2. Using `gorm.Open()` to establish a connection with the Postgres driver
3. Configuring GORM's logger to show SQL queries during development, which helps with debugging
4. Getting the underlying `*sql.DB` object to access lower-level database functions
5. Verifying the connection is active by pinging the database

Make sure to replace `[user]`, `[password]`, `[neon_hostname]`, and `[dbname]` with your actual Neon database credentials. The `?sslmode=require&channel_binding=require` part of the connection string ensures secure communication with your Neon database.

Replace `[user]`, `[password]`, `[neon_hostname]`, and `[dbname]` with your actual Neon connection details. You can find these by clicking the **Connect** button on your Neon **Project Dashboard**.

### Connection Pooling and Configuration

Connection pooling is a technique that maintains a set of reusable database connections. This significantly improves performance by avoiding the overhead of establishing a new database connection for each operation.

#### Neon Connection Pooling

Neon provides a **built-in connection pooler**, powered by PgBouncer, to efficiently manage database connections. This pooler reduces connection overhead by reusing a limited number of persistent Postgres connections while supporting thousands of client sessions.

Instead of each request opening a new database connection, the pooler transparently distributes queries across existing backend connections, improving performance and scalability. To use it, simply enable connection pooling in the Neon console and update your connection string to include `-pooler` in the hostname.

This approach helps applications handle high concurrency while minimizing latency and resource consumption. However, since Neon's pooler operates in **transaction pooling mode**, session-based features like `LISTEN/NOTIFY`, `SET search_path`, and server-side prepared statements are not supported. For operations that require session persistence, it's best to use a direct (non-pooled) connection. You can find more details in the [Neon connection pooling documentation](/docs/connect/connection-pooling).

#### Configuring Connection Pooling in GORM

When using Go with Neon, GORM offers built-in connection pooling that works seamlessly with Neon's pooler. By configuring settings like `SetMaxOpenConns` and `SetConnMaxIdleTime`, developers can fine-tune how connections are managed within their application before they reach the database layer.

Since Neon already optimizes pooling on the database side, applications should maintain a moderate number of open connections to avoid excessive connection churn.

The recommended approach is to use a pooled connection string for normal queries and switch to a direct connection for migration tasks that require session state. For guidance on configuring connection pooling in Go, refer to the [GORM connection documentation](https://gorm.io/docs/generic_interface.html#Connection-Pool).

## Defining Models

In GORM, models are Go structs that represent tables in your database. Each field in the struct maps to a column in the table, and GORM uses struct tags (annotations enclosed in backticks) to configure how these fields are handled in the database.

Let's create a simple blogging application with `User` and `Post` models. These models will define the structure of our database tables and establish relationships between them.

```go
package main

import (
	"time"

	"gorm.io/gorm"
)

// User represents a user in the system
type User struct {
	ID        uint           `gorm:"primaryKey"`
	CreatedAt time.Time
	UpdatedAt time.Time
	DeletedAt gorm.DeletedAt `gorm:"index"`
	Name      string         `gorm:"size:255;not null"`
	Email     string         `gorm:"size:255;not null;uniqueIndex"`
	Posts     []Post         `gorm:"foreignKey:UserID"`
}

// Post represents a blog post
type Post struct {
	ID        uint           `gorm:"primaryKey"`
	CreatedAt time.Time
	UpdatedAt time.Time
	DeletedAt gorm.DeletedAt `gorm:"index"`
	Title     string         `gorm:"size:255;not null"`
	Content   string         `gorm:"type:text"`
	UserID    uint           `gorm:"not null"`
	User      User           `gorm:"foreignKey:UserID"`
}
```

Let's examine the key components of these models:

1. **Basic Fields**: `ID`, `CreatedAt`, `UpdatedAt`, and `DeletedAt` are standard fields in GORM models. They handle primary keys, timestamps, and soft deletion.

2. **Field Tags**: The struct tags like `gorm:"size:255;not null"` define constraints and properties for each field:
   - `primaryKey`: Designates a field as the table's primary key
   - `size:255`: Sets the column's maximum length
   - `not null`: Ensures the field cannot be empty
   - `uniqueIndex`: Creates a unique index on the column
   - `type:text`: Specifies the SQL data type

3. **Relationships**: The `Posts` field in the User model and the `User` field in the Post model establish a one-to-many relationship. The `foreignKey` tag specifies which field serves as the foreign key.

By default, GORM will pluralize struct names to create table names (e.g., "User" becomes "users"), but you can customize this using the `TableName` method or the `gorm:"tableName:custom_name"` tag. This is similar to how other ORMs like Sequelize and Elequent work.

GORM provides sensible defaults for table names (pluralized struct names) and column names (field names), but you can customize these using struct tags.

## Automatic Migrations

Migrations are a way to manage database schema changes over time. GORM provides a convenient `AutoMigrate` feature that automatically creates tables, indexes, constraints, and foreign keys based on your model definitions.

While this automation is extremely useful during development, it's worth noting that for production environments, you'll typically want more control over schema changes. We'll cover structured migrations for production later in this guide, but for now, let's see how to use GORM's automatic migrations for development.

Here's how to set up automatic migrations:

```go
func main() {
	// Connection setup (as shown above)

	// Auto-migrate the schema
	err = db.AutoMigrate(&User{}, &Post{})
	if err != nil {
		log.Fatalf("Failed to migrate database: %v", err)
	}

	fmt.Println("Database migrated successfully!")
}
```

When you run this code, GORM will:

1. Check if the tables exist, and create them if they don't
2. Add missing columns to existing tables
3. Create indexes and constraints
4. Establish foreign key relationships between tables

The `AutoMigrate` function works by comparing your Go struct definitions to the actual database schema and making necessary changes to align them. It accepts a list of model struct pointers and returns an error if something goes wrong.

Note that `AutoMigrate` only adds things that are missing—it won't delete columns or tables that exist in the database but not in your models. This is a safety feature to prevent accidental data loss.

This will create the necessary tables if they don't exist and update them to match your model definitions.

## Basic CRUD Operations

Now that we have our models and database connection set up, let's perform basic Create, Read, Update, and Delete (CRUD) operations:

### Creating Records

Now that we have our models defined and tables created, let's start adding data to our database. GORM makes it straightforward to create new records with the `Create` method.

Let's add a user and a blog post to our database:

```go
// Create a new user
user := User{
	Name:  "John Doe",
	Email: "john@example.com",
}
result := db.Create(&user)
if result.Error != nil {
	log.Fatalf("Failed to create user: %v", result.Error)
}
fmt.Printf("Created user with ID: %d\n", user.ID)

// Create a post for the user
post := Post{
	Title:   "Getting Started with GORM and Neon",
	Content: "GORM makes it easy to work with databases in Go...",
	UserID:  user.ID,
}
result = db.Create(&post)
if result.Error != nil {
	log.Fatalf("Failed to create post: %v", result.Error)
}
fmt.Printf("Created post with ID: %d\n", post.ID)
```

Here's what happens in this code:

1. We create a new `User` struct instance with basic information
2. We pass a pointer to this struct to the `db.Create()` method, which inserts it into the database
3. GORM automatically handles generating the primary key ID, timestamps, and other default values
4. After successful creation, the user's generated ID is populated in the `user.ID` field
5. We then create a `Post` struct, setting the `UserID` field to establish the relationship with our user
6. We insert the post into the database using the same `Create` method

Notice how GORM returns a `result` object that contains an `Error` field. Always check this field to ensure your database operations succeeded. The result object also provides other useful information like the number of rows affected by the operation.

### Reading Records

Reading data from the database is one of the most common operations in any application. GORM provides several methods for retrieving data, from simple lookups to complex queries.

Let's look at different ways to retrieve data from our database:

```go
// Retrieve a user by ID
var retrievedUser User
result = db.First(&retrievedUser, user.ID)
if result.Error != nil {
	log.Fatalf("Failed to retrieve user: %v", result.Error)
}
fmt.Printf("Retrieved user: %s (%s)\n", retrievedUser.Name, retrievedUser.Email)

// Retrieve a user with their posts
var userWithPosts User
result = db.Preload("Posts").First(&userWithPosts, user.ID)
if result.Error != nil {
	log.Fatalf("Failed to retrieve user with posts: %v", result.Error)
}
fmt.Printf("User %s has %d posts\n", userWithPosts.Name, len(userWithPosts.Posts))

// Find users with specific criteria
var users []User
result = db.Where("name LIKE ?", "%John%").Find(&users)
if result.Error != nil {
	log.Fatalf("Failed to find users: %v", result.Error)
}
fmt.Printf("Found %d users with 'John' in their name\n", len(users))
```

Let's break down these query operations:

1. **Simple Retrieval**: The `First` method retrieves the first record that matches the condition. In our first example, we're finding a user by their ID, which should return exactly one record since IDs are unique.

2. **Eager Loading with Preload**: The `Preload` method allows us to load related records in a single query. In our second example, we load a user and all their posts in one go. This is more efficient than performing separate queries for the user and their posts.

3. **Conditional Queries with Where**: The `Where` method allows us to specify conditions for our queries. In our third example, we're using the SQL `LIKE` operator to find users whose names contain "John". The `?` is a placeholder that helps prevent SQL injection attacks.

GORM provides many other query methods that we haven't covered here, such as:

- `Last`: Retrieves the last record matching the condition
- `Take`: Retrieves a record without any specified order
- `Pluck`: Retrieves a single column from the database as a slice
- `Count`: Returns the number of records matching the condition

All these methods return a result object that contains an `Error` field, which should be checked to ensure the query was successful.

### Updating Records

GORM provides several methods for updating records in the database. You can update a single field, multiple fields, or even use more complex update operations.

Let's look at how to update our user and post records:

```go
// Update a user's email
result = db.Model(&user).Update("email", "johndoe@example.com")
if result.Error != nil {
	log.Fatalf("Failed to update user: %v", result.Error)
}

// Update multiple fields at once
result = db.Model(&post).Updates(Post{
	Title:   "Updated: Getting Started with GORM and Neon",
	Content: "Updated content about GORM and Neon...",
})
if result.Error != nil {
	log.Fatalf("Failed to update post: %v", result.Error)
}
```

Here's what's happening in these update operations:

1. **Single Field Update**: The `Update` method allows us to change a single column's value. In the first example, we're updating the user's email address. We use the `Model` method to specify which record to update (based on its primary key).

2. **Multiple Field Update**: The `Updates` method allows us to change multiple columns at once. We provide a struct with the fields we want to update. Note that GORM will only update non-zero fields by default, which means fields with their zero values (empty string, 0, false, etc.) won't be updated unless you use `Updates` with a map.

GORM also offers other update methods:

- **Update with conditions**: `db.Model(&User{}).Where("name = ?", "john").Update("name", "jane")`
- **Batch updates**: `db.Table("users").Where("role = ?", "admin").Update("active", true)`
- **Raw SQL updates**: `db.Exec("UPDATE users SET name = ? WHERE age > ?", "Jane", 20)`

When updating records, GORM automatically sets the `UpdatedAt` field to the current time if your model includes this field. This helps track when records were last modified.

### Deleting Records

GORM provides two types of deletion: soft deletion and hard deletion. Soft deletion marks records as deleted without actually removing them from the database, while hard deletion permanently removes records.

Let's see how to perform both types of deletions:

```go
// Soft delete a post (with GORM's DeletedAt field)
result = db.Delete(&post)
if result.Error != nil {
	log.Fatalf("Failed to delete post: %v", result.Error)
}

// Hard delete a post (permanently remove from database)
result = db.Unscoped().Delete(&post)
if result.Error != nil {
	log.Fatalf("Failed to permanently delete post: %v", result.Error)
}
```

Here's what's happening in these deletion operations:

1. **Soft Deletion**: When we call `Delete` on a model that has a `DeletedAt` field (like our models do thanks to `gorm.Model`), GORM performs a soft delete. This doesn't actually remove the record from the database; instead, it sets the `DeletedAt` field to the current time. Subsequent queries will automatically exclude these "deleted" records unless you explicitly include them.

2. **Hard Deletion**: The `Unscoped` method tells GORM to ignore the soft delete mechanism and perform a true deletion, permanently removing the record from the database. This is used when you really want to delete data, not just hide it.

Soft deletion is particularly useful for:

- Keeping an audit trail of records
- Allowing data to be restored if deleted accidentally
- Maintaining referential integrity in related data
- Meeting regulatory requirements that prohibit true deletion

By default, GORM queries won't return soft-deleted records. If you need to include them, you can use the `Unscoped` method: `db.Unscoped().Where("name = ?", "John").Find(&users)`.

## Advanced GORM Features

With the basics covered, let's explore some advanced features of GORM that can help you build more robust and efficient applications.

### Transactions

Transactions are a way to group multiple database operations into a single unit of work. They ensure that either all operations succeed or none of them do, maintaining data consistency. This is especially important when you have multiple related changes that need to happen together.

GORM provides comprehensive support for database transactions:

```go
// Begin a transaction
tx := db.Begin()

// Perform operations within the transaction
user := User{Name: "Transaction User", Email: "tx@example.com"}
if err := tx.Create(&user).Error; err != nil {
	tx.Rollback() // Rollback if there's an error
	log.Fatalf("Failed to create user in transaction: %v", err)
}

post := Post{
	Title:   "Post in Transaction",
	Content: "This post is created in a transaction",
	UserID:  user.ID,
}
if err := tx.Create(&post).Error; err != nil {
	tx.Rollback() // Rollback if there's an error
	log.Fatalf("Failed to create post in transaction: %v", err)
}

// Commit the transaction
if err := tx.Commit().Error; err != nil {
	log.Fatalf("Failed to commit transaction: %v", err)
}
```

Let's break down how transactions work in GORM:

1. **Begin a Transaction**: The `Begin` method starts a new transaction and returns a transaction object.
2. **Perform Operations**: Use the transaction object (instead of the regular db object) to perform database operations. All operations will be part of the transaction.
3. **Rollback or Commit**: If any operation fails, call `Rollback` to cancel all changes. If all operations succeed, call `Commit` to permanently apply the changes to the database.

Transactions are essential in scenarios like:

- Creating a user and their profile simultaneously
- Transferring funds between accounts
- Processing an order with multiple line items

They help maintain data integrity by ensuring that related operations either all succeed or all fail, preventing partial updates that could leave your database in an inconsistent state.

For a more elegant and concise approach, GORM provides a transaction helper method that automatically handles the begin, commit, and rollback operations:

```go
err := db.Transaction(func(tx *gorm.DB) error {
	user := User{Name: "Transaction User", Email: "tx@example.com"}
	if err := tx.Create(&user).Error; err != nil {
		return err
	}

	post := Post{
		Title:   "Post in Transaction",
		Content: "This post is created in a transaction",
		UserID:  user.ID,
	}
	if err := tx.Create(&post).Error; err != nil {
		return err
	}

	return nil
})

if err != nil {
	log.Fatalf("Transaction failed: %v", err)
}
```

This approach is generally preferred for several reasons:

1. **Simplified Error Handling**: You simply return an error from the closure function, and GORM automatically rolls back the transaction if an error is returned.
2. **Cleaner Code**: The transaction logic is encapsulated in a single function, making the code more readable.
3. **Automatic Resource Management**: GORM ensures that the transaction is properly closed whether it succeeds or fails, preventing resource leaks.

With this method, you focus on the business logic inside the transaction rather than managing the transaction lifecycle. If the function returns nil, the transaction is committed; if it returns an error, the transaction is rolled back automatically.

### Raw SQL and Complex Queries

While GORM's built-in query methods are powerful and cover most common scenarios, sometimes you need more control or have complex requirements that are best expressed in raw SQL. GORM provides several ways to work with raw SQL while still benefiting from its safety features and result handling.

Here are different approaches to executing raw SQL and complex queries:

```go
// Execute raw SQL
var result []map[string]interface{}
db.Raw("SELECT u.name, COUNT(p.id) as post_count FROM users u LEFT JOIN posts p ON u.id = p.user_id GROUP BY u.name").Scan(&result)

// Combined with GORM methods
var users []User
db.Raw("SELECT * FROM users WHERE name = ?", "John").Scan(&users)

// Complex queries
var userStats []struct {
	UserName  string
	PostCount int
}
db.Table("users").
	Select("users.name as user_name, COUNT(posts.id) as post_count").
	Joins("left join posts on posts.user_id = users.id").
	Where("users.deleted_at IS NULL").
	Group("users.name").
	Having("COUNT(posts.id) > ?", 1).
	Find(&userStats)
```

Let's examine these different approaches:

1. **Raw SQL with Generic Results**: The first example executes a raw SQL query and scans the results into a slice of maps. This is useful when you don't have a predefined struct for the result or need flexibility in handling different result shapes.

2. **Raw SQL with Model Mapping**: The second example shows how you can execute raw SQL but still map the results to your model structs. GORM handles the mapping between column names and struct fields.

3. **Query Builder API**: The third example demonstrates GORM's query builder API, which provides a fluent interface for constructing complex queries. This approach offers:
   - Type safety and IDE auto-completion
   - SQL injection protection with parameter placeholders
   - Readability for complex queries
   - The ability to build queries dynamically based on conditions

When should you use raw SQL versus GORM's query builder?

- **Use raw SQL when**: You have complex queries that are difficult to express with the query builder, or when you're optimizing performance with database-specific features.
- **Use the query builder when**: You want type safety, need to build queries dynamically, or prefer a more Go-idiomatic approach.

In either case, GORM handles parameter binding to protect against SQL injection, making both approaches secure when used correctly.

### Hooks

Hooks (also known as callbacks) are functions that are called at specific stages of the database operation lifecycle. They allow you to inject custom logic before or after these operations, such as validation, data transformation, or triggering side effects.

GORM provides a comprehensive set of hooks for various operations, you can find the full list of hooks in the [GORM documentation](https://gorm.io/docs/hooks.html). But let's look at a couple of common hooks:

```go
// Define hooks in your model
type User struct {
	// ... fields as defined earlier
	Password string `gorm:"size:255;not null"`
}

// BeforeCreate is called before a record is created
func (u *User) BeforeCreate(tx *gorm.DB) (err error) {
	// For demonstration purposes - in real apps use proper password hashing!
	u.Password = "hashed_" + u.Password
	return
}

// AfterFind is called after a record is retrieved
func (u *User) AfterFind(tx *gorm.DB) (err error) {
	// Custom logic after a user is found
	return
}
```

Hooks are a great way to keep your business logic consistent and avoid duplicating code across your application. By defining these methods directly on your model structs, the behavior is encapsulated with the data it operates on.

## Structured Migrations for Production

While `AutoMigrate` is convenient for development, production systems need more controlled migration management. In production environments, you need precise control over when and how database changes occur, with the ability to roll back changes if something goes wrong.

[Golang-migrate](https://github.com/golang-migrate/migrate) is a popular migration tool for Go applications that provides version-controlled, reversible database migrations. Let's set up proper migrations using this tool:

1. Install the migrate CLI:

   ```bash
   go install -tags 'postgres' github.com/golang-migrate/migrate/v4/cmd/migrate@latest
   ```

   This installs the migration tool globally on your system, allowing you to run migrations from the command line.

2. Create the `migrations` directory:

   ```bash
   mkdir -p migrations
   ```

   This directory will store all your migration files in a structured, version-controlled format.

3. Create your first migration:

   ```bash
   migrate create -ext sql -dir migrations -seq create_users_table
   ```

   This creates two files: an "up" migration for applying changes and a "down" migration for reverting them. The `-seq` flag ensures migrations are numbered sequentially, which helps maintain the order of execution.

4. Edit the up migration (`migrations/000001_create_users_table.up.sql`):

   ```sql
   CREATE TABLE users (
       id SERIAL PRIMARY KEY,
       created_at TIMESTAMP NOT NULL DEFAULT NOW(),
       updated_at TIMESTAMP NOT NULL DEFAULT NOW(),
       deleted_at TIMESTAMP,
       name VARCHAR(255) NOT NULL,
       email VARCHAR(255) NOT NULL UNIQUE
   );

   CREATE INDEX idx_users_deleted_at ON users(deleted_at);
   ```

   The "up" migration contains SQL to create new database objects or modify existing ones. Here, we're creating a users table with the necessary columns and an index.

5. Edit the down migration (`migrations/000001_create_users_table.down.sql`):

   ```sql
   DROP TABLE IF EXISTS users;
   ```

   The "down" migration contains SQL to undo the changes made by the corresponding "up" migration. This ensures you can roll back changes if needed. In this case, we're dropping the users table.

6. Create a migration for the posts table:

   ```bash
   migrate create -ext sql -dir migrations -seq create_posts_table
   ```

   Now we'll create the second migration for the posts table, which depends on the users table created in the first migration.

7. Edit the up migration (`migrations/000002_create_posts_table.up.sql`):

   ```sql
   CREATE TABLE posts (
       id SERIAL PRIMARY KEY,
       created_at TIMESTAMP NOT NULL DEFAULT NOW(),
       updated_at TIMESTAMP NOT NULL DEFAULT NOW(),
       deleted_at TIMESTAMP,
       title VARCHAR(255) NOT NULL,
       content TEXT,
       user_id INTEGER NOT NULL REFERENCES users(id)
   );

   CREATE INDEX idx_posts_deleted_at ON posts(deleted_at);
   ```

   Note the foreign key reference to the users table (`REFERENCES users(id)`). This ensures referential integrity between the two tables.

8. Edit the down migration (`migrations/000002_create_posts_table.down.sql`):

   ```sql
   DROP TABLE IF EXISTS posts;
   ```

   Again, the down migration simply drops the table to reverse the changes.

9. Run the migrations:

   ```bash
   export POSTGRESQL_URL="postgresql://[user]:[password]@[neon_hostname]/[dbname]?sslmode=require&channel_binding=require"
   migrate -database ${POSTGRESQL_URL} -path migrations up
   ```

   This command applies all pending migrations to your database. The `up` argument tells the tool to apply migrations that haven't been applied yet. You can also use `down` to reverse migrations, or specify a version number to migrate to a specific version.

The migrate tool keeps track of which migrations have been applied in a special table called `schema_migrations` in your database. This ensures that migrations are only applied once and in the correct order.

For more complex applications, you might want to create a Go function to run migrations programmatically. This approach offers several advantages:

1. Migrations can be run as part of your application startup
2. The same code can be used in different environments (development, staging, production)
3. You can implement custom logic around migrations, like waiting for the database to be ready

Here's how to create a Go function to run migrations programmatically:

```go
package main

import (
	"log"

	"github.com/golang-migrate/migrate/v4"
	_ "github.com/golang-migrate/migrate/v4/database/postgres"
	_ "github.com/golang-migrate/migrate/v4/source/file"
)

func runMigrations() {
	m, err := migrate.New(
		"file://migrations",
		"postgresql://[user]:[password]@[neon_hostname]/[dbname]?sslmode=require&channel_binding=require",
	)
	if err != nil {
		log.Fatalf("Failed to create migration instance: %v", err)
	}

	if err := m.Up(); err != nil && err != migrate.ErrNoChange {
		log.Fatalf("Failed to run migrations: %v", err)
	}

	log.Println("Migrations completed successfully")
}

func main() {
	runMigrations()

	// Continue with your application...
}
```

This function:

1. Creates a new migration instance that reads migration files from the local filesystem
2. Configures the database connection using your Neon credentials
3. Runs all pending migrations with `m.Up()`
4. Handles errors appropriately, distinguishing between actual errors and the "no changes" case

You would typically call this function early in your application's startup process, before initializing your GORM instance. This ensures that your database schema is up-to-date before your application starts using it.

For more advanced scenarios, you might want to add features like:

- Checking database connectivity before running migrations
- Adding retry logic for transient connection issues
- Implementing a "migrate and seed" function for development environments
- Adding version reporting to track which migrations have been applied

## Performance Optimization with Neon

When working with Neon and GORM, consider these performance optimization techniques:

### Efficient Querying

One of the most effective ways to improve performance is to be selective about what data you retrieve from the database. Fetching only the specific fields you need reduces the amount of data transferred between Neon and your application, resulting in faster queries and less memory usage.

Let's look at how to query efficiently with GORM:

```go
var users []struct {
	ID    uint
	Name  string
	Email string
}
db.Model(&User{}).Select("id", "name", "email").Where("id > ?", 10).Find(&users)
```

In this example, instead of selecting all fields with `Find(&users)`, we're using the `Select` method to specify exactly which columns we want. This has several benefits:

1. **Reduced Data Transfer**: Only the specified columns are fetched, reducing network bandwidth usage.
2. **Improved Query Performance**: The database can optimize the query better when it knows exactly which columns to return.
3. **Lower Memory Usage**: Your application only stores the data it actually needs.

Notice that we're also using an anonymous struct that contains only the fields we're interested in, rather than using the full `User` model. This is another optimization that ensures we're not allocating memory for fields we don't need.

### Batch Processing

When working with large datasets, processing all the data at once can lead to performance issues, including high memory usage and long-running queries. Instead, you can use batch processing to handle large datasets in smaller, more manageable chunks.

GORM provides the `FindInBatches` method to simplify this pattern:

```go
// Find in batches
db.Model(&User{}).Where("active = ?", true).FindInBatches(&results, 100, func(tx *gorm.DB, batch int) error {
	for _, result := range results {
		// Process result...
	}
	return nil
})
```

Here's what this code does:

1. We start a query on the User model for active users
2. Instead of fetching all results at once, we use `FindInBatches` to retrieve them in batches of 100 records
3. For each batch, GORM calls the provided callback function with the batch results
4. Inside the callback, we process each result individually

Batch processing is particularly important when dealing with operations that might affect millions of records, such as data migrations, report generation, or bulk updates. It's also useful when you need to perform complex processing on each record that might be resource-intensive.

For more information on batch processing and other advanced querying techniques, refer to the [GORM documentation](https://gorm.io/docs/advanced_query.html#FindInBatches).

### Indexing

Proper indexing is essential for query performance. With GORM, you can define indexes in your models:

```go
type User struct {
	ID      uint   `gorm:"primaryKey"`
	Name    string `gorm:"index:idx_name_email,unique"`
	Email   string `gorm:"index:idx_name_email,unique"`
	Address string `gorm:"index"`
}
```

For more complex indexing requirements, use migrations as shown in the previous section.

For more information on indexing and optimizing database performance, refer to the [Neon indexing documentation](/postgresql/postgresql-indexes).

## Complete Application Example

Let's put everything together in a complete application example:

```go
package main

import (
	"fmt"
	"log"
	"time"

	"gorm.io/driver/postgres"
	"gorm.io/gorm"
	"gorm.io/gorm/logger"
)

// User model
type User struct {
	gorm.Model
	Name     string `gorm:"size:255;not null"`
	Email    string `gorm:"size:255;not null;uniqueIndex"`
	Password string `gorm:"size:255;not null"`
	Posts    []Post `gorm:"foreignKey:UserID"`
}

// BeforeCreate hook for User
func (u *User) BeforeCreate(tx *gorm.DB) (err error) {
	// Simulate password hashing
	u.Password = "hashed_" + u.Password
	return
}

// Post model
type Post struct {
	gorm.Model
	Title   string `gorm:"size:255;not null"`
	Content string `gorm:"type:text"`
	UserID  uint   `gorm:"not null"`
	User    User   `gorm:"foreignKey:UserID"`
}

func main() {
	// Connection string for Neon Postgres
	dsn := "postgresql://[user]:[password]@[neon_hostname]/[dbname]?sslmode=require&channel_binding=require"

	// Connect to the database
	db, err := gorm.Open(postgres.Open(dsn), &gorm.Config{
		Logger: logger.Default.LogMode(logger.Info),
	})
	if err != nil {
		log.Fatalf("Failed to connect to database: %v", err)
	}

	// Get the underlying SQL DB object
	sqlDB, err := db.DB()
	if err != nil {
		log.Fatalf("Failed to get DB object: %v", err)
	}

	// Configure connection pool
	sqlDB.SetMaxIdleConns(5)
	sqlDB.SetMaxOpenConns(10)
	sqlDB.SetConnMaxLifetime(time.Hour)
	sqlDB.SetConnMaxIdleTime(30 * time.Minute)

	// Auto-migrate the schema
	err = db.AutoMigrate(&User{}, &Post{})
	if err != nil {
		log.Fatalf("Failed to migrate database: %v", err)
	}

	// Create a new user
	user := User{
		Name:     "John Doe",
		Email:    "john@example.com",
		Password: "secret123",
	}

	result := db.Create(&user)
	if result.Error != nil {
		log.Fatalf("Failed to create user: %v", result.Error)
	}
	fmt.Printf("Created user with ID: %d\n", user.ID)

	// Create posts for the user
	posts := []Post{
		{Title: "First Post", Content: "Content of first post", UserID: user.ID},
		{Title: "Second Post", Content: "Content of second post", UserID: user.ID},
	}

	result = db.Create(&posts)
	if result.Error != nil {
		log.Fatalf("Failed to create posts: %v", result.Error)
	}

	// Retrieve user with posts
	var userWithPosts User
	result = db.Preload("Posts").First(&userWithPosts, user.ID)
	if result.Error != nil {
		log.Fatalf("Failed to retrieve user with posts: %v", result.Error)
	}

	fmt.Printf("Retrieved user: %s (%s)\n", userWithPosts.Name, userWithPosts.Email)
	fmt.Printf("User has %d posts:\n", len(userWithPosts.Posts))

	for i, post := range userWithPosts.Posts {
		fmt.Printf("  %d. %s: %s\n", i+1, post.Title, post.Content)
	}

	// Use transactions for related operations
	err = db.Transaction(func(tx *gorm.DB) error {
		// Update user's email
		if err := tx.Model(&user).Update("email", "john.doe@example.com").Error; err != nil {
			return err
		}

		// Update first post's title
		if err := tx.Model(&posts[0]).Update("title", "Updated: First Post").Error; err != nil {
			return err
		}

		return nil
	})

	if err != nil {
		log.Fatalf("Transaction failed: %v", err)
	}

	fmt.Println("Transaction completed successfully")

	// Query with raw SQL
	var stats []struct {
		UserName  string
		PostCount int
	}

	db.Raw(`
		SELECT u.name as user_name, COUNT(p.id) as post_count
		FROM users u
		LEFT JOIN posts p ON u.id = p.user_id
		WHERE u.deleted_at IS NULL
		GROUP BY u.name
	`).Scan(&stats)

	for _, stat := range stats {
		fmt.Printf("User %s has written %d posts\n", stat.UserName, stat.PostCount)
	}
}
```

Save this code in a file named `main.go`, run `go mod tidy` to download the necessary dependencies, and run it with `go run main.go`. This application puts everything we've covered into practice: connecting to the database, defining models, performing CRUD operations, using transactions, and executing raw SQL queries.

## Conclusion

GORM with Neon Postgres provides a great combination for building scalable Go applications. GORM's developer-friendly API simplifies database interactions, while Neon's serverless architecture ensures your database scales according to demand.

By following the steps in this guide, you can build robust applications that efficiently interact with your Neon database. As your application grows, you can leverage additional GORM features such as plugins, hooks, and more advanced querying techniques to meet your evolving needs.

## Additional Resources

- [GORM Documentation](https://gorm.io/docs/)
- [Neon Documentation](/docs)
- [Go Database/SQL Documentation](https://golang.org/pkg/database/sql/)
- [Effective Go](https://golang.org/doc/effective_go)

<NeedHelp />


# Creating a Secure Authentication System with Go, JWT, and Neon Postgres

---
title: Creating a Secure Authentication System with Go, JWT, and Neon Postgres
subtitle: Learn how to build a secure authentication system using Go, JWT tokens, and Neon Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2025-03-29T00:00:00.000Z'
updatedOn: '2025-03-29T00:00:00.000Z'
---

Authentication is the foundation of web applications, it ensures that users are who they claim to be. In this guide, you'll learn how to create a secure authentication system using Go, JSON Web Tokens (JWT), and Neon Postgres.

We'll focus on the essential concepts and patterns for implementing a robust authentication system, including user registration, secure password storage, token-based authentication, and protected routes.

## Prerequisites

To follow the steps in this guide, you will need:

- [Go](https://go.dev/dl/) 1.20 or later installed
- A [Neon](https://console.neon.tech/signup) account
- Basic familiarity with SQL, Go programming, and authentication concepts

## Understanding JWT in Our Go Authentication System

Before we dive into the implementation details, let's understand how JSON Web Tokens (JWT) work and why they're a popular choice for authentication systems.

JWT provides a compact, self-contained way to securely transmit information as a JSON object. In our Go authentication system, we'll use JWTs to maintain user sessions without server-side storage.

### JWT Structure

A JWT consists of three parts encoded in `Base64URL` format and separated by dots:

```go
Header.Payload.Signature
```

For example:

```
eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwiZXhwIjoxNjgwMDAwMDAwfQ.8Gj_9bJjAqQ-5j3iCKMzVnlg-d1Kk-fXnOKC1Vt2fGc
```

1. The header identifies the algorithm used for signing:

   ```go
   // In Go, the header is typically handled by the JWT library
   token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims)
   ```

2. The payload contains claims about the user like ID, roles, and expiration time

   ```go
   // Creating claims in Go
   claims := jwt.MapClaims{
       "sub": user.ID.String(),
       "username": user.Username,
       "exp": time.Now().Add(15 * time.Minute).Unix(),
   }
   ```

3. The signature verifies the token hasn't been tampered with
   ```go
   // Signing the token with our secret
   tokenString, err := token.SignedString([]byte(jwtSecret))
   ```

### How Our JWT Flow Works

To understand how JWT fits into our Go authentication system, let's walk through the flow of a user logging in and accessing protected routes:

1. When a user successfully authenticates, our Go service:
   - Validates credentials against Neon Postgres
   - Creates JWT with appropriate claims and expiration
   - Signs the token with a secret key

2. The client:
   - Stores the JWT (typically in `localStorage` or a secure cookie)
   - Includes the token in the `Authorization` header for subsequent requests

   ```
   Authorization: Bearer eyJhbGciOiJIUzI1Ni...
   ```

3. Our middleware:
   - Extracts the JWT from the request header
   - Validates the signature using our secret key
   - Checks that the token hasn't expired
   - Extracts the user identity from claims
   - Adds the user ID to the request context

4. Since the token contains all necessary user information, our server can authenticate requests without maintaining session state or additional database queries.

The security of this system relies on keeping the signing key secret and using short-lived access tokens. If a token is compromised, it's only valid for a limited time, reducing the risk of unauthorized access.

## Create a Neon project

First, let's create a Neon project to store our authentication data.

1. Navigate to the [Neon Console](https://console.neon.tech/app/projects) and click New Project.
2. Give your project a name, such as "auth-system".
3. Choose your preferred region.
4. Click Create Project.

Once your project is created, you'll receive a connection string that looks like this:

```
postgres://[user]:[password]@[hostname]/[dbname]?sslmode=require&channel_binding=require
```

Save this connection string, you'll need it to connect your Go application to the Neon database.

## Set up the database schema

Now we'll create a database schema that securely stores user information and authentication tokens. Connect to your Neon database and run the following SQL to create the necessary tables:

```sql
-- Create users table
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    email VARCHAR(255) UNIQUE NOT NULL,
    username VARCHAR(50) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    last_login TIMESTAMP
);

-- Create refresh_tokens table
CREATE TABLE refresh_tokens (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    token VARCHAR(255) UNIQUE NOT NULL,
    expires_at TIMESTAMP NOT NULL,
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    revoked BOOLEAN NOT NULL DEFAULT FALSE
);

-- Create indexes for fast lookups
CREATE INDEX idx_refresh_tokens_token ON refresh_tokens(token);
```

This schema includes several key features for security and performance:

- Using **UUIDs** for primary keys instead of sequential integers, making it harder to guess or enumerate IDs
- Storing only **password hashes**, never plain-text passwords
- Creating a separate table for **refresh tokens** with an expiration date
- Using a **token revocation flag** to invalidate tokens when needed
- Including appropriate **indexes** for performance optimization

## Create the Go application structure

Let's set up a new Go application with the necessary dependencies. Create a new directory and initialize a Go module:

```bash
mkdir auth-system
cd auth-system
go mod init github.com/yourusername/auth-system
```

Make sure to replace `yourusername` with your GitHub username or organization name. This will be the base path for your Go modules.

Install the essential packages:

```bash
go get github.com/jackc/pgx/v5          # PostgreSQL driver
go get github.com/gorilla/mux           # HTTP router
go get github.com/golang-jwt/jwt/v5     # JWT library
go get golang.org/x/crypto/bcrypt       # Password hashing
go get github.com/google/uuid           # UUID generation
go get github.com/joho/godotenv         # Load environment variables
```

For this guide, let's focus on the key components we'll need:

1. Database connection
2. User model and repository
3. Authentication service
4. HTTP handlers
5. Middleware for route protection

Let's start with a connection to our Neon database:

```go
// db/db.go
package db

import (
    "database/sql"
    "log"

    _ "github.com/jackc/pgx/v5/stdlib"
)

// Connect establishes a connection to the Postgres database
func Connect(connectionString string) (*sql.DB, error) {
    db, err := sql.Open("postgres", connectionString)
    if err != nil {
        return nil, err
    }

    // Test the connection
    if err := db.Ping(); err != nil {
        return nil, err
    }

    log.Println("Connected to the database successfully")
    return db, nil
}
```

This simple function connects to our Neon Postgres database and verifies the connection with a ping.

## Implement password handling

Let's create functions to hash passwords during registration and verify them during login:

```go
// auth/passwords.go
package auth

import (
    "golang.org/x/crypto/bcrypt"
)

// HashPassword creates a bcrypt hash from a plain-text password
func HashPassword(password string) (string, error) {
    // The cost determines how computationally expensive the hash is
    // Higher is more secure but slower (default is 10)
    hashedBytes, err := bcrypt.GenerateFromPassword([]byte(password), bcrypt.DefaultCost)
    if err != nil {
        return "", err
    }
    return string(hashedBytes), nil
}

// VerifyPassword checks if the provided password matches the stored hash
func VerifyPassword(hashedPassword, providedPassword string) error {
    return bcrypt.CompareHashAndPassword([]byte(hashedPassword), []byte(providedPassword))
}
```

Bcrypt is used for password hashing because:

1. It's slow by design, making brute-force attacks impractical
2. It automatically includes a salt to protect against rainbow table attacks
3. It has an adjustable cost factor to increase security as hardware gets faster
4. It's a one-way function that can't be reversed to obtain the original password

When a user registers, we'll hash their password before storing it. When they log in, we'll compare their provided password against the stored hash.

Now, let's create a simple user model and repository to interact with our database:

```go
// models/user.go
package models

import (
    "database/sql"
    "time"

    "github.com/google/uuid"
)

// User represents a user in our system
type User struct {
    ID           uuid.UUID
    Email        string
    Username     string
    PasswordHash string
    CreatedAt    time.Time
    LastLogin    *time.Time
}

// UserRepository handles database operations for users
type UserRepository struct {
    db *sql.DB
}

// NewUserRepository creates a new user repository
func NewUserRepository(db *sql.DB) *UserRepository {
    return &UserRepository{db: db}
}

// CreateUser adds a new user to the database
func (r *UserRepository) CreateUser(email, username, passwordHash string) (*User, error) {
    user := &User{
        ID:           uuid.New(),
        Email:        email,
        Username:     username,
        PasswordHash: passwordHash,
        CreatedAt:    time.Now(),
    }

    query := `
        INSERT INTO users (id, email, username, password_hash, created_at)
        VALUES ($1, $2, $3, $4, $5)
    `

    _, err := r.db.Exec(query, user.ID, user.Email, user.Username, user.PasswordHash, user.CreatedAt)
    if err != nil {
        return nil, err
    }

    return user, nil
}

// GetUserByEmail retrieves a user by their email address
func (r *UserRepository) GetUserByEmail(email string) (*User, error) {
    query := `SELECT id, email, username, password_hash, created_at, last_login FROM users WHERE email = $1`

    var user User
    var lastLogin sql.NullTime

    err := r.db.QueryRow(query, email).Scan(
        &user.ID,
        &user.Email,
        &user.Username,
        &user.PasswordHash,
        &user.CreatedAt,
        &lastLogin,
    )

    if err != nil {
        return nil, err
    }

    if lastLogin.Valid {
        user.LastLogin = &lastLogin.Time
    }

    return &user, nil
}

// GetUserByID retrieves a user by their ID
func (r *UserRepository) GetUserByID(id uuid.UUID) (*User, error) {
    query := `SELECT id, email, username, password_hash, created_at, last_login FROM users WHERE id = $1`

    var user User
    var lastLogin sql.NullTime

    err := r.db.QueryRow(query, id).Scan(
        &user.ID,
        &user.Email,
        &user.Username,
        &user.PasswordHash,
        &user.CreatedAt,
        &lastLogin,
    )

    if err != nil {
        return nil, err
    }

    if lastLogin.Valid {
        user.LastLogin = &lastLogin.Time
    }

    return &user, nil
}
```

This simple repository provides methods to create new users and retrieve existing users by email, which we'll need for our authentication logic. The `User` struct represents the core user data we'll store in the database.

Additionally, we store the `last_login` timestamp to track user activity along with the creation timestamp.

## Create the JWT authentication system

With the database and user handling in place, let's implement the core of our authentication system using JWT. We'll create a service that handles login verification and token generation:

```go
// auth/service.go
package auth

import (
    "database/sql"
    "errors"
    "time"

    "github.com/golang-jwt/jwt/v5"
    "github.com/yourusername/auth-system/models"
)

var (
    ErrInvalidCredentials = errors.New("invalid credentials")
    ErrInvalidToken       = errors.New("invalid token")
    ErrExpiredToken       = errors.New("token has expired")
    ErrEmailInUse         = errors.New("email already in use")
)

// AuthService provides authentication functionality
type AuthService struct {
    userRepo         *models.UserRepository
    refreshTokenRepo *models.RefreshTokenRepository
    jwtSecret        []byte
    accessTokenTTL   time.Duration
}

// NewAuthService creates a new authentication service
func NewAuthService(userRepo *models.UserRepository, refreshTokenRepo *models.RefreshTokenRepository, jwtSecret string, accessTokenTTL time.Duration) *AuthService {
    return &AuthService{
        userRepo:         userRepo,
        refreshTokenRepo: refreshTokenRepo,
        jwtSecret:        []byte(jwtSecret),
        accessTokenTTL:   accessTokenTTL,
    }
}

// Register creates a new user with the provided credentials
func (s *AuthService) Register(email, username, password string) (*models.User, error) {
    // Check if user already exists
    _, err := s.userRepo.GetUserByEmail(email)
    if err == nil {
        return nil, ErrEmailInUse
    }

    // Only proceed if the error was "user not found"
    if !errors.Is(err, sql.ErrNoRows) {
        return nil, err
    }

    // Hash the password
    hashedPassword, err := HashPassword(password)
    if err != nil {
        return nil, err
    }

    // Create the user
    user, err := s.userRepo.CreateUser(email, username, hashedPassword)
    if err != nil {
        return nil, err
    }

    return user, nil
}

// Login authenticates a user and returns an access token
func (s *AuthService) Login(email, password string) (string, error) {
    // Get the user from the database
    user, err := s.userRepo.GetUserByEmail(email)
    if err != nil {
        return "", ErrInvalidCredentials
    }

    // Verify the password
    if err := VerifyPassword(user.PasswordHash, password); err != nil {
        return "", ErrInvalidCredentials
    }

    // Generate an access token
    token, err := s.generateAccessToken(user)
    if err != nil {
        return "", err
    }

    return token, nil
}

// generateAccessToken creates a new JWT access token
func (s *AuthService) generateAccessToken(user *models.User) (string, error) {
    // Set the expiration time
    expirationTime := time.Now().Add(s.accessTokenTTL)

    // Create the JWT claims
    claims := jwt.MapClaims{
        "sub":      user.ID.String(),      // subject (user ID)
        "username": user.Username,         // custom claim
        "email":    user.Email,            // custom claim
        "exp":      expirationTime.Unix(), // expiration time
        "iat":      time.Now().Unix(),     // issued at time
    }

    // Create the token with claims
    token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims)

    // Sign the token with our secret key
    tokenString, err := token.SignedString(s.jwtSecret)
    if err != nil {
        return "", err
    }

    return tokenString, nil
}

// ValidateToken verifies a JWT token and returns the claims
func (s *AuthService) ValidateToken(tokenString string) (jwt.MapClaims, error) {
    // Parse the token
    token, err := jwt.Parse(tokenString, func(token *jwt.Token) (interface{}, error) {
        // Validate the signing method
        if _, ok := token.Method.(*jwt.SigningMethodHMAC); !ok {
            return nil, ErrInvalidToken
        }
        return s.jwtSecret, nil
    })

    if err != nil {
        if errors.Is(err, jwt.ErrTokenExpired) {
            return nil, ErrExpiredToken
        }
        return nil, ErrInvalidToken
    }

    // Extract and validate claims
    if claims, ok := token.Claims.(jwt.MapClaims); ok && token.Valid {
        return claims, nil
    }

    return nil, ErrInvalidToken
}
```

This authentication service handles three key functions:

1. **Login**: Verifies a user's credentials and issues an access token
2. **Token Generation**: Creates a JWT with appropriate claims and expiration
3. **Token Validation**: Verifies that a token is valid and not expired

Now let's create HTTP handlers to expose these authentication features via an API:

```go
// handlers/auth.go
package handlers

import (
    "encoding/json"
    "errors"
    "net/http"

    "github.com/yourusername/auth-system/auth"
)

// AuthHandler contains HTTP handlers for authentication
type AuthHandler struct {
    authService *auth.AuthService
}

// NewAuthHandler creates a new auth handler
func NewAuthHandler(authService *auth.AuthService) *AuthHandler {
    return &AuthHandler{
        authService: authService,
    }
}

// RegisterRequest represents the registration payload
type RegisterRequest struct {
    Email    string `json:"email"`
    Username string `json:"username"`
    Password string `json:"password"`
}

// RegisterResponse contains the user data after successful registration
type RegisterResponse struct {
    ID       string `json:"id"`
    Email    string `json:"email"`
    Username string `json:"username"`
}

// Register handles user registration
func (h *AuthHandler) Register(w http.ResponseWriter, r *http.Request) {
    // Parse the request body
    var req RegisterRequest
    if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
        http.Error(w, "Invalid request payload", http.StatusBadRequest)
        return
    }

    // Validate input
    if req.Email == "" || req.Username == "" || req.Password == "" {
        http.Error(w, "Email, username, and password are required", http.StatusBadRequest)
        return
    }

    // Call the auth service to register the user
    user, err := h.authService.Register(req.Email, req.Username, req.Password)
    if err != nil {
        if errors.Is(err, auth.ErrEmailInUse) {
            http.Error(w, "Email already in use", http.StatusConflict)
            return
        }

        http.Error(w, "Error creating user", http.StatusInternalServerError)
        return
    }

    // Return the created user (without sensitive data)
    response := RegisterResponse{
        ID:       user.ID.String(),
        Email:    user.Email,
        Username: user.Username,
    }

    w.Header().Set("Content-Type", "application/json")
    w.WriteHeader(http.StatusCreated)
    json.NewEncoder(w).Encode(response)
}

// LoginRequest represents the login payload
type LoginRequest struct {
    Email    string `json:"email"`
    Password string `json:"password"`
}

// LoginResponse contains the JWT token after successful login
type LoginResponse struct {
    Token string `json:"token"`
}

// Login handles user login
func (h *AuthHandler) Login(w http.ResponseWriter, r *http.Request) {
    // Parse the request body
    var req LoginRequest
    if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
        http.Error(w, "Invalid request payload", http.StatusBadRequest)
        return
    }

    // Attempt to login
    token, err := h.authService.Login(req.Email, req.Password)
    if err != nil {
        if errors.Is(err, auth.ErrInvalidCredentials) {
            http.Error(w, "Invalid credentials", http.StatusUnauthorized)
        } else {
            http.Error(w, "Internal server error", http.StatusInternalServerError)
        }
        return
    }

    // Return the token
    response := LoginResponse{Token: token}
    w.Header().Set("Content-Type", "application/json")
    json.NewEncoder(w).Encode(response)
}
```

This handler exposes a simple login endpoint that accepts an email and password, verifies the credentials, and returns a JWT token on success.

## Implement refresh tokens

Short-lived access tokens are more secure, but they require users to log in frequently.

To improve user experience while maintaining security, we can implement a refresh token system. This essentially creates a two-tier authentication system, where a long-lived refresh token is used to obtain short-lived access tokens.

The refresh token can be revoked if needed allowing for better control over user sessions.

First, let's add support for refresh tokens to our database operations:

```go
// models/refresh_token.go
package models

import (
    "database/sql"
    "time"

    "github.com/google/uuid"
)

// RefreshToken represents a refresh token in the system
type RefreshToken struct {
    ID        uuid.UUID
    UserID    uuid.UUID
    Token     string
    ExpiresAt time.Time
    CreatedAt time.Time
    Revoked   bool
}

// RefreshTokenRepository handles database operations for refresh tokens
type RefreshTokenRepository struct {
    db *sql.DB
}

// NewRefreshTokenRepository creates a new refresh token repository
func NewRefreshTokenRepository(db *sql.DB) *RefreshTokenRepository {
    return &RefreshTokenRepository{db: db}
}

// CreateRefreshToken creates a new refresh token for a user
func (r *RefreshTokenRepository) CreateRefreshToken(userID uuid.UUID, ttl time.Duration) (*RefreshToken, error) {
    // Generate a unique token identifier
    tokenID := uuid.New()
    expiresAt := time.Now().Add(ttl)

    token := &RefreshToken{
        ID:        tokenID,
        UserID:    userID,
        Token:     tokenID.String(), // Use the UUID as the token
        ExpiresAt: expiresAt,
        CreatedAt: time.Now(),
        Revoked:   false,
    }

    query := `
        INSERT INTO refresh_tokens (id, user_id, token, expires_at, created_at, revoked)
        VALUES ($1, $2, $3, $4, $5, $6)
    `

    _, err := r.db.Exec(query, token.ID, token.UserID, token.Token, token.ExpiresAt, token.CreatedAt, token.Revoked)
    if err != nil {
        return nil, err
    }

    return token, nil
}

// GetRefreshToken retrieves a refresh token by its token string
func (r *RefreshTokenRepository) GetRefreshToken(tokenString string) (*RefreshToken, error) {
    query := `
        SELECT id, user_id, token, expires_at, created_at, revoked
        FROM refresh_tokens
        WHERE token = $1
    `

    var token RefreshToken
    err := r.db.QueryRow(query, tokenString).Scan(
        &token.ID,
        &token.UserID,
        &token.Token,
        &token.ExpiresAt,
        &token.CreatedAt,
        &token.Revoked,
    )

    if err != nil {
        return nil, err
    }

    return &token, nil
}

// RevokeRefreshToken marks a refresh token as revoked
func (r *RefreshTokenRepository) RevokeRefreshToken(tokenString string) error {
    query := `
        UPDATE refresh_tokens
        SET revoked = true
        WHERE token = $1
    `

    _, err := r.db.Exec(query, tokenString)
    return err
}
```

Now let's extend our `AuthService` to handle refresh tokens:

```go
// auth/service.go (existing methods)

// LoginWithRefresh authenticates a user and returns both access and refresh tokens
func (s *AuthService) LoginWithRefresh(email, password string, refreshTokenTTL time.Duration) (accessToken string, refreshToken string, err error) {
    // Get the user from the database
    user, err := s.userRepo.GetUserByEmail(email)
    if err != nil {
        return "", "", ErrInvalidCredentials
    }

    // Verify the password
    if err := VerifyPassword(user.PasswordHash, password); err != nil {
        return "", "", ErrInvalidCredentials
    }

    // Generate an access token
    accessToken, err = s.generateAccessToken(user)
    if err != nil {
        return "", "", err
    }

    // Create a refresh token
    token, err := s.refreshTokenRepo.CreateRefreshToken(user.ID, refreshTokenTTL)
    if err != nil {
        return "", "", err
    }

    return accessToken, token.Token, nil
}

// RefreshAccessToken creates a new access token using a refresh token
func (s *AuthService) RefreshAccessToken(refreshTokenString string) (string, error) {
    // Retrieve the refresh token
    token, err := s.refreshTokenRepo.GetRefreshToken(refreshTokenString)
    if err != nil {
        return "", ErrInvalidToken
    }

    // Check if the token is valid
    if token.Revoked {
        return "", ErrInvalidToken
    }

    // Check if the token has expired
    if time.Now().After(token.ExpiresAt) {
        return "", ErrExpiredToken
    }

    // Get the user
    user, err := s.userRepo.GetUserByID(token.UserID)
    if err != nil {
        return "", err
    }

    // Generate a new access token
    accessToken, err := s.generateAccessToken(user)
    if err != nil {
        return "", err
    }

    return accessToken, nil
}
```

The main benefit of refresh tokens is that they:

1. Allow access tokens to be short-lived (e.g., 15 minutes), which reduces the risk if they're leaked
2. Enable longer sessions without requiring frequent logins
3. Can be revoked server-side if needed, such as on logout or if a security breach is detected

Let's add an HTTP handler for refreshing tokens:

```go
// handlers/auth.go (existing methods)

// RefreshRequest represents the refresh token payload
type RefreshRequest struct {
    RefreshToken string `json:"refresh_token"`
}

// RefreshResponse contains the new access token
type RefreshResponse struct {
    Token string `json:"token"`
}

// RefreshToken handles access token refresh
func (h *AuthHandler) RefreshToken(w http.ResponseWriter, r *http.Request) {
    // Parse the request body
    var req RefreshRequest
    if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
        http.Error(w, "Invalid request payload", http.StatusBadRequest)
        return
    }

    // Attempt to refresh the token
    token, err := h.authService.RefreshAccessToken(req.RefreshToken)
    if err != nil {
        if errors.Is(err, auth.ErrInvalidToken) || errors.Is(err, auth.ErrExpiredToken) {
            http.Error(w, "Invalid or expired refresh token", http.StatusUnauthorized)
        } else {
            http.Error(w, "Internal server error", http.StatusInternalServerError)
        }
        return
    }

    // Return the new access token
    response := RefreshResponse{Token: token}
    w.Header().Set("Content-Type", "application/json")
    json.NewEncoder(w).Encode(response)
}
```

The additional `RefreshToken` method allows clients to obtain a new access token using a valid refresh token. This endpoint is useful for maintaining user sessions without requiring frequent logins.

## Create protected routes

Now let's create middleware to protect routes that require authentication:

```go
// middleware/auth.go
package middleware

import (
    "context"
    "net/http"
    "strings"

    "github.com/google/uuid"
    "github.com/yourusername/auth-system/auth"
)

// Key type for context values
type contextKey string

const (
    // UserIDKey is the key for user ID in the request context
    UserIDKey contextKey = "userID"
)

// AuthMiddleware checks JWT tokens and adds user info to the request context
func AuthMiddleware(authService *auth.AuthService) func(http.Handler) http.Handler {
    return func(next http.Handler) http.Handler {
        return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
            // Extract token from Authorization header
            authHeader := r.Header.Get("Authorization")
            if authHeader == "" {
                http.Error(w, "Authorization header required", http.StatusUnauthorized)
                return
            }

            // Check Bearer token format
            parts := strings.Split(authHeader, " ")
            if len(parts) != 2 || parts[0] != "Bearer" {
                http.Error(w, "Invalid authorization format", http.StatusUnauthorized)
                return
            }

            tokenString := parts[1]

            // Validate the token
            claims, err := authService.ValidateToken(tokenString)
            if err != nil {
                http.Error(w, "Invalid or expired token", http.StatusUnauthorized)
                return
            }

            // Extract user ID from claims
            userIDStr, ok := claims["sub"].(string)
            if !ok {
                http.Error(w, "Invalid token claims", http.StatusUnauthorized)
                return
            }

            userID, err := uuid.Parse(userIDStr)
            if err != nil {
                http.Error(w, "Invalid user ID in token", http.StatusUnauthorized)
                return
            }

            // Add user ID to request context
            ctx := context.WithValue(r.Context(), UserIDKey, userID)

            // Call the next handler with the enhanced context
            next.ServeHTTP(w, r.WithContext(ctx))
        })
    }
}

// GetUserID retrieves the user ID from the request context
func GetUserID(r *http.Request) (uuid.UUID, bool) {
    userID, ok := r.Context().Value(UserIDKey).(uuid.UUID)
    return userID, ok
}
```

This middleware extracts the JWT token from the `Authorization` header, validates it, and adds the user ID to the request context. This allows subsequent handlers to access the authenticated user's identity.

Now we can create a protected endpoint that requires authentication:

```go
// handlers/user.go
package handlers

import (
    "encoding/json"
    "net/http"

    "github.com/yourusername/auth-system/middleware"
    "github.com/yourusername/auth-system/models"
)

// UserHandler contains HTTP handlers for user-related endpoints
type UserHandler struct {
    userRepo *models.UserRepository
}

// NewUserHandler creates a new user handler
func NewUserHandler(userRepo *models.UserRepository) *UserHandler {
    return &UserHandler{
        userRepo: userRepo,
    }
}

// UserResponse represents the user data returned to clients
type UserResponse struct {
    ID       string  `json:"id"`
    Email    string  `json:"email"`
    Username string  `json:"username"`
}

// Profile returns the authenticated user's profile
func (h *UserHandler) Profile(w http.ResponseWriter, r *http.Request) {
    // Get user ID from request context (set by auth middleware)
    userID, ok := middleware.GetUserID(r)
    if !ok {
        http.Error(w, "Unauthorized", http.StatusUnauthorized)
        return
    }

    // Get user from database
    user, err := h.userRepo.GetUserByID(userID)
    if err != nil {
        http.Error(w, "User not found", http.StatusNotFound)
        return
    }

    // Return user profile (excluding sensitive data)
    response := UserResponse{
        ID:       user.ID.String(),
        Email:    user.Email,
        Username: user.Username,
    }

    w.Header().Set("Content-Type", "application/json")
    json.NewEncoder(w).Encode(response)
}
```

The auth middleware handles a few key tasks:

1. Extracting the JWT token from the Authorization header
2. Validating the token signature and expiration
3. Adding the authenticated user's ID to the request context
4. Rejecting requests with invalid or missing tokens

To wire everything up, we need to register our routes with the appropriate middleware:

```go
// main.go

package main

import (
	"log"
	"net/http"
	"os"
	"time"

	"github.com/yourusername/auth-system/auth"
	"github.com/yourusername/auth-system/db"
	"github.com/yourusername/auth-system/handlers"
	"github.com/yourusername/auth-system/middleware"
	"github.com/yourusername/auth-system/models"
	"github.com/gorilla/mux"
	"github.com/joho/godotenv"
)

// loadEnv loads environment variables from .env file
func loadEnv() {
    // Load .env file if it exists
    if err := godotenv.Load(); err != nil {
        log.Println("No .env file found, using environment variables")
    }

    // Check required variables
    requiredVars := []string{"DATABASE_URL", "JWT_SECRET"}
    for _, v := range requiredVars {
        if os.Getenv(v) == "" {
            log.Fatalf("Required environment variable %s is not set", v)
        }
    }
}

func main() {
    // Load environment variables
    loadEnv()

    // Connect to the database
    database, err := db.Connect(os.Getenv("DATABASE_URL"))
    if err != nil {
        log.Fatalf("Failed to connect to database: %v", err)
    }

    r := mux.NewRouter()

    // Create repositories
    userRepo := models.NewUserRepository(database)
    refreshTokenRepo := models.NewRefreshTokenRepository(database)

    // Create services
    authService := auth.NewAuthService(userRepo, refreshTokenRepo, os.Getenv("JWT_SECRET"), 15*time.Minute)

    // Create handlers
    authHandler := handlers.NewAuthHandler(authService)
    userHandler := handlers.NewUserHandler(userRepo)

    // Public routes
    r.HandleFunc("/api/auth/register", authHandler.Register).Methods("POST")
    r.HandleFunc("/api/auth/login", authHandler.Login).Methods("POST")
    r.HandleFunc("/api/auth/refresh", authHandler.RefreshToken).Methods("POST")

    // Protected routes
    protected := r.PathPrefix("/api").Subrouter()
    protected.Use(middleware.AuthMiddleware(authService))

    protected.HandleFunc("/profile", userHandler.Profile).Methods("GET")

    port := os.Getenv("PORT")
    if port == "" {
        port = "8080"
    }
    log.Printf("Server starting on port %s", port)
    log.Fatal(http.ListenAndServe(":"+port, r))
}
```

## Test and deploy the application

Before testing our authentication system, we need to set up environment variables and start the application.

### Setting environment variables

Create a `.env` file in the root of your project with the following variables:

```
# Database connection
DATABASE_URL=postgres://[user]:[password]@[hostname]/[dbname]?sslmode=require&channel_binding=require&channel_binding=require

# JWT configuration
JWT_SECRET=your-very-secure-jwt-secret-key
REFRESH_SECRET=your-very-secure-refresh-secret-key
ACCESS_TOKEN_EXPIRY=15m
REFRESH_TOKEN_EXPIRY=7d

# Server configuration
PORT=8080
```

Replace the `DATABASE_URL` with your actual Neon connection string. The JWT secrets should be strong, random strings in production (at least 32 characters). For testing purposes, you can use simpler values.

### Starting the application

To start the application, first, make sure you've built your Go binary:

```bash
go build -o auth-server main.go
```

Then, run the binary:

```bash
./auth-server
```

Alternatively, you can use `go run`:

```bash
go run main.go
```

You should see output similar to:

```
2025/03/30 12:34:56 Connected to the database successfully
2025/03/30 12:34:56 Server starting on port 8080
```

The server is now running and ready to accept requests.

### Testing with curl

Now let's test our authentication system using curl commands:

1. First, register a new user:

```bash
curl -X POST http://localhost:8080/api/auth/register \
  -H "Content-Type: application/json" \
  -d '{
    "email": "user@example.com",
    "username": "testuser",
    "password": "SecureP@ssw0rd!"
  }'
```

Expected response:

```json
{
  "id": "a1b2c3d4-e5f6-7890-a1b2-c3d4e5f67890",
  "email": "user@example.com",
  "username": "testuser"
}
```

2. Next, log in to get access and refresh tokens:

```bash
curl -X POST http://localhost:8080/api/auth/login \
  -H "Content-Type: application/json" \
  -d '{
    "email": "user@example.com",
    "password": "SecureP@ssw0rd!"
  }'
```

Expected response:

```json
{
  "access_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
  "refresh_token": "c3d4e5f6-7890-a1b2-c3d4-e5f67890a1b2"
}
```

3. Save the access token and use it to access a protected endpoint:

```bash
export ACCESS_TOKEN="eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."

curl -X GET http://localhost:8080/api/profile \
  -H "Authorization: Bearer $ACCESS_TOKEN"
```

Expected response:

```json
{
  "id": "a1b2c3d4-e5f6-7890-a1b2-c3d4e5f67890",
  "email": "user@example.com",
  "username": "testuser"
}
```

4. When your access token expires, refresh it using the refresh token:

```bash
export REFRESH_TOKEN="c3d4e5f6-7890-a1b2-c3d4-e5f67890a1b2"

curl -X POST http://localhost:8080/api/auth/refresh \
  -H "Content-Type: application/json" \
  -d '{
    "refresh_token": "'$REFRESH_TOKEN'"
  }'
```

Expected response:

```json
{
  "access_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."
}
```

5. You can also test an invalid token to see the authentication fail:

```bash
curl -X GET http://localhost:8080/api/profile \
  -H "Authorization: Bearer invalid-token"
```

Expected response:

```
Invalid or expired token
```

These tests verify that our authentication system is working correctly.

You can use tools like [Postman](https://www.postman.com/) or [Insomnia](https://insomnia.rest/) for more advanced API testing with a graphical interface.

## Summary

In this guide, you built a secure authentication system using Go, JWT, and Neon Postgres. The system includes secure password hashing, token-based authentication, refresh token support, middleware-protected routes, and basic rate limiting to prevent brute-force attacks. Security headers were also added to protect against common web vulnerabilities.

By using Neon Postgres as the database, you gain the scalability and performance of a serverless Postgres platform, without sacrificing the reliability and flexibility developers expect from PostgreSQL. It's an ideal foundation for authentication systems that need to scale securely and efficiently.

## Additional Resources

- [Neon Documentation](/docs)
- [Go Documentation](https://go.dev/doc/)
- [JWT Introduction](https://jwt.io/introduction)
- [OWASP Authentication Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/Authentication_Cheat_Sheet.html)

<NeedHelp />


# Graph Queries in Postgres

---
title: Graph Queries in Postgres
subtitle: A step-by-step guide describing how to use ltree and pgRouting for analyzing graph data in Postgres
author: vkarpov15
enableTableOfContents: true
createdAt: '2025-02-28T13:24:36.612Z'
updatedOn: '2025-02-28T13:24:36.612Z'
---

Graph databases are used to store and analyze data that is connected in a network-like structure.
For example, cities connected by roads, people in a social network, or category hierarchies where categories have sub-categories.
While there are dedicated graph databases, extensions like [ltree](https://www.postgresql.org/docs/current/ltree.html) and [pgRouting](https://pgrouting.org/) add graph functionality to Postgres.

## Steps

- Enable the `ltree` and `pgrouting` extensions
- Create a table to store hierarchical data
- Insert and retrieve hierarchical data
- Perform hierarchical queries using ltree
- Create a table to store network data
- Insert and query network data

## Enable ltree and pgRouting

`ltree` adds a new `LTREE` type to Postgres for storing hierarchies, like categories.
In Neon, `ltree` is already installed, you just need to enable it using the following command.

```sql
CREATE EXTENSION IF NOT EXISTS ltree;
```

`pgrouting` is a separate extension that can be used for routing and shortest path calculations in network data, like finding the shortest path between two cities.

```sql
CREATE EXTENSION IF NOT EXISTS pgrouting;
```

## Create a table to store hierarchical data

With `ltree`, you can store hierarchical relationships such as category trees, organizational charts, or file directories.
You can create a table to store a category tree using the following command.

```sql
CREATE TABLE categories (
  id SERIAL PRIMARY KEY,
  name TEXT NOT NULL,
  path LTREE
);
```

## Insert and retrieve hierarchical data

The `ltree` type allows inserting hierarchical paths:

```sql
INSERT INTO categories (name, path) VALUES
  ('Electronics', 'Electronics'),
  ('Laptops', 'Electronics.Laptops'),
  ('Gaming Laptops', 'Electronics.Laptops.Gaming');
```

You can retrieve all subcategories under a given path using `<@` as follows.

```sql
SELECT * FROM categories WHERE path <@ 'Electronics.Laptops';
```

| id  | name           | path                       |
| --- | -------------- | -------------------------- |
| 2   | Laptops        | Electronics.Laptops        |
| 3   | Gaming Laptops | Electronics.Laptops.Gaming |

## Create a table to store network data

With `pgrouting`, you can model roads, social networks, or any graph-like data structure.
For example, the following table stores a graph of roads that have an associated `cost`.
Each road has a source and a target, and an associated `cost`.

```sql
CREATE TABLE roads (
  id SERIAL PRIMARY KEY,
  source INT,
  target INT,
  cost FLOAT
);
```

## Insert and query network data

Insert edges representing connections between nodes using the following command.

```sql
INSERT INTO roads (source, target, cost) VALUES
  (1, 2, 4.0),
  (2, 3, 3.0),
  (3, 4, 2.5),
  (1, 4, 10.0);
```

You can then find the minimal cost path between two nodes using the `pgr_dijkstra()` function, which is an implementation of [Dijkstra's Algorithm](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm).

```sql
SELECT * FROM pgr_dijkstra(
  'SELECT id, source, target, cost FROM roads',
  1, 4, false
);
```

The above query returns the following, which shows the shortest path from node 1 to node 4 is by visiting each node in order (1, 2, 3, 4) with an aggregate cost of 9.5.

| seq | path_seq | node | edge | cost | agg_cost |
| --- | -------- | ---- | ---- | ---- | -------- |
| 1   | 1        | 1    | 1    | 4    | 0        |
| 2   | 2        | 2    | 2    | 3    | 4        |
| 3   | 3        | 3    | 3    | 2.5  | 7        |
| 4   | 4        | 4    | 4    | 0    | 9.5      |


# Dynamic Routing with Hasura and Neon

---
title: Dynamic Routing with Hasura and Neon
subtitle: Leverage Neon's branching with Hasura's dynamic routing for powerful development, testing, and preview environments.
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-04-20T00:00:00.000Z'
updatedOn: '2025-04-20T00:00:00.000Z'
---

Managing different database environments for development, testing, staging, and production can be complex. Traditional methods often involve provisioning separate database instances, managing complex data synchronization scripts, or dealing with slow snapshot restores. Neon's serverless Postgres brings efficient, Git-like branching to your database, while Hasura provides an instant GraphQL API layer.

This guide demonstrates how to combine the power of [Neon's database branching](/docs/introduction/branching) with [Hasura's Dynamic Database Routing](https://hasura.io/docs/2.0/databases/database-config/dynamic-db-connection/) feature. This combination allows you to create isolated database environments instantly using Neon branches and dynamically route GraphQL requests from Hasura to the appropriate branch based on request context (like HTTP headers or session variables), streamlining your development, testing, and preview workflows. By leveraging Neon's branching and Hasura's dynamic routing, you can effectively consolidate your infrastructure, serving multiple development, testing, or preview environments from only one Neon project and one Hasura instance.

## Prerequisites

Before you start, ensure you have the following:

- **A Neon Account:** Sign up for a free Neon account at [neon.tech](https://console.neon.tech/signup).
- **A Neon Project:** You need to have a Neon project. If you do not have one, create it via [pg.new](https://pg.new)
- **A Hasura Instance:** A running Hasura instance (v2.x or later). This can be Hasura Cloud Professional or Enterprise tiers, or a self-hosted Enterprise instance. Dynamic routing is not available in the free tier.

## Understanding the core concepts

### Neon Branching

Neon allows you to create branches of your Postgres database almost instantly. Key features include:

- **Copy-on-write:** Branches are lightweight clones. They initially share the parent's data without duplication. Storage costs only increase for the _changes_ (deltas) made within a branch.
- **Isolation:** Each branch operates independently. Changes made in one branch do not affect the parent or other branches. This is perfect for development, testing, or running experiments without impacting production data.
- **Speed:** Creating a branch takes only a few seconds.
- **Management:** Branches can be created and managed via the Neon Console, Neon API, or Neon CLI.
- **Connection string:** Each branch gets its own unique connection string, allowing applications to connect directly to it.

Think of Neon branching like Git branching, but for your database.

### Hasura Dynamic Database Routing

Available in Hasura Cloud Professional/Enterprise and Self-Hosted Enterprise, this feature allows Hasura to route GraphQL requests to different database connections based on request parameters. It uses two main components:

- **Connection set:** A named collection of database connections (including the primary database and potentially read replicas or other databases/branches) that Hasura can choose from.
- **Connection template:** A template written in [Kriti templating language](https://hasura.io/docs/2.0/api-reference/kriti-templating/) that defines the logic for selecting a connection from the Connection Set for a given GraphQL request.

The Kriti template has access to request context variables like:

- `$.request.headers`: HTTP headers from the client request (e.g., `x-hasura-branch-name`).
- `$.request.session`: Hasura session variables (e.g., `x-hasura-role`, `x-hasura-user-id`).
- `$.request.query`: Information about the GraphQL query (e.g., `operation_type`, `operation_name`).

The template must resolve to one of the predefined connection identifiers:

- `$.primary`: The main database connection defined for the source.
- `$.read_replicas`: A randomly chosen read replica (if configured).
- `$.connection_set.<member_name>`: A specific connection defined within the connection set.
- `$.default`: The default Hasura behavior (route to read replicas for queries/subscriptions, primary for mutations, or just primary if no replicas).

## Conceptual overview

Here's a high-level overview of how to set up dynamic routing with Neon and Hasura:

1.  **Create Neon branches:** For each environment you need (e.g., `dev`, `staging`, `feature-x`), create a corresponding branch in your Neon project. Obtain the connection string for each branch.
2.  **Configure Hasura data source:** Add your _primary_ Neon database as a data source in Hasura.
3.  **Define connection set:** In the Hasura data source configuration, add the connection strings of your Neon branches to the Connection set, giving each a unique, descriptive name (e.g., `dev_branch`, `staging_branch`, `feature_x_branch`).
4.  **Implement connection template:** Write a Kriti template that inspects the incoming GraphQL request (e.g., checks for a specific header like `x-hasura-branch-name`) and resolves to the appropriate member name in the Connection set (e.g., `$.connection_set.dev_branch`).
5.  **Route Requests:** Send GraphQL requests to Hasura with the necessary context (e.g., the `x-hasura-branch-name` header) to route them to the desired Neon branch.

We shall discuss the implementation in detail in the next section.

## Step-by-step implementation

### Create Neon branches

You can create branches using the [Neon Console](/docs/introduction/branching/#create-a-branch), [API](/docs/guides/branching-neon-api), or [CLI](/docs/guides/branching-neon-cli). For detailed instructions, follow [Neon's Create a branch guide](/docs/manage/branches#create-a-branch) to set up branches for your development and feature environments.

Copy the connection strings for each branch you create; you will need them later.

### Configure Hasura data source

If you haven't already, add your Neon database as a data source in Hasura. Follow the step by-step guide on [Connect from Hasura Cloud to Neon](/docs/guides/hasura) to set up the primary connection.

### Define the connection set in Hasura

Now, add your Neon branches to the connection set for the data source you just configured:

1.  Go to the `Hasura Console -> Data -> Manage`.
2.  Click "Edit" next to your data source.

        ![Edit Data Source](/docs/guides/hasura/edit-data-source.png)

3.  Navigate to the `Dynamic Routing` tab.
4.  Under "Available Connections for Templating", click `+ Add Connection`.
    ![Add Connections for Templating](/docs/guides/hasura/add-connection-for-templating.png)
5.  In the modal:
    - **Connection name:** Enter a unique, lowercase name (e.g., `dev_branch`). This name will be used in the Kriti template.
    - **Connect Database via:** Select `Database URL`.
    - **Database URL:** Paste the connection string for your `dev` Neon branch which you copied earlier in the [Create Neon Branches](#create-neon-branches) section.
      ![Add Connection Modal](/docs/guides/hasura/add-connection-modal.png)
    - Click `Add Connection`.

      <Admonition type="tip">
      To enhance security and manageability, consider using environment variables in Hasura instead of hardcoding the connection string. To do this, navigate to **Hasura Project settings** > **Env vars** > **New env var** and create a new variable (e.g., `NEON_DATABASE_URL_DEV_BRANCH`) with your connection string as its value.
          
          ![Create Environment Variable](/docs/guides/hasura/create-env-var.png)

      Then, in the connection modal, select **Connect database via Environment variable** and enter the variable name you created. This approach keeps your connection string secure and simplifies future updates.
      </Admonition>

6.  Repeat step 5 for other branches, e.g., `staging_branch`, `feature_x_branch`, etc., using their respective connection strings.

You should now see `dev_branch`, `feature_x_branch` and `staging_branch` listed under "Available Connections for Templating".

    ![Available Connections for Templating](/docs/guides/hasura/available-connections-for-templating.png)

### Create the connection template

This template defines the routing logic. We'll create a template that routes requests based on an `x-hasura-branch-name` HTTP header.

1.  In the `Dynamic Routing` tab for your data source, find the "Connection Template" section.
2.  Select `Custom Template`.
    ![Custom Template Kriti](/docs/guides/hasura/custom-template-kriti.png)
3.  Enter the following Kriti template in the editor:

    ```json
    {{ if ($.request.headers?["x-hasura-branch-name"] == "dev")}}
        {{$.connection_set.dev_branch}}
    {{ elif ($.request.headers?["x-hasura-branch-name"] == "feature-x")}}
        {{$.connection_set.feature_x_branch}}
    {{ elif ($.request.headers?["x-hasura-branch-name"] == "staging")}}
        {{$.connection_set.staging_branch}}
    {{ else }}
        {{$.default}}
    {{ end }}
    ```

4.  Click `Update Connection Template` to save it.

#### Explanation of the template

- `$.request.headers?["x-hasura-branch-name"]`: Accesses the header value. The `?` handles cases where the header might be missing.
- `== "dev"` / `== "feature-x"` / `== "staging"`: Checks if the header matches the expected branch name.
- `{{$.connection_set.dev_branch}}`: If the header matches 'dev', the template resolves to use the connection named `dev_branch` from the set.
- `{{$.connection_set.feature_x_branch}}`: If the header matches 'feature-x', route to that connection.
- `{{$.connection_set.staging_branch}}`: If the header matches 'staging', route to that connection.
- The `else` block provides fallback behavior: mutations go to the default (primary) branch, while other operations follow the default routing. You can adjust this fallback as needed.

<Admonition type="note">
You can utilize any attribute from the request context within your Kriti template. For instance, you can use session variables to route requests based on user roles or IDs.

Here's an example of how you might check the request session:

    ```json
    {{ if ($.request.session.x-hasura-role == "manager")}}
        {{$.connection_set.manager_connection}}
    {{ elif ($.request.session.x-hasura-role == "employee")}}
        {{$.connection_set.employee_connection}}
    {{ else }}
        {{$.default}}
    {{ end }}
    ```

This approach enables more complex routing logic by directing requests according to the user's role.

For more advanced routing logic, or for information on dynamically creating and updating connection templates, refer to [Hasura's Dynamic Routing for Databases](https://hasura.io/docs/2.0/databases/database-config/dynamic-db-connection/).

</Admonition>

### Testing the connection template with Hasura Validate

Hasura provides a convenient way to test your connection template directly within the Console. This simulates a GraphQL request based on the context you provide (headers, session variables, etc.).

1.  You can find the **Validate** button in the **Dynamic Routing** tab of your data source configuration.
    ![Validate Dynamic Routing](/docs/guides/hasura/validate-dynamic-routing.png)

2.  **Simulate Request Context:**
    This modal allows you to define the context (`$.request`) that your Kriti template will evaluate against.
    - **Test Routing to `dev_branch`:**
      - In the **Headers** section, click `+ Add`.
      - Enter `x-hasura-branch-name` as the header key and `dev` as the value.
      - Leave **Operation Type** as `Query`.
      - Click the `► Validate` button at the _bottom right_ of the modal.
      - **Expected Result:** The output below should show `Routing to: $.connection_set. Value: dev_branch`.
        ![Validate Dynamic Routing to dev_branch](/docs/guides/hasura/validate-dynamic-routing-dev-branch.png)

    - **Test Fallback Routing (Query):**
      - Remove the `x-hasura-branch-name` header
      - Click `► Validate`.
      - **Expected Result:** Based on our template's fallback logic, this should show `Routing to: $.default`.

This validation provides a quick and safe way to confirm your routing logic works as expected under different conditions before applying it to live traffic.

### Update your application code

Now that the connection template is validated, you're ready to leverage Hasura's dynamic routing with your Neon branches. Send the `x-hasura-branch-name` header along with your GraphQL requests, setting its value to match the target branch identifier (e.g., `dev`, `feature-x`, `staging`). This mechanism provides precise control and can be easily incorporated into your application code or automated within CI/CD processes to manage environments effectively.

## Read replicas and routing

Neon allows you to create [Read Replicas](/docs/introduction/read-replicas) for your database branches, which are separate compute endpoints designed for handling read-only traffic. Hasura's Dynamic Routing feature allows you to leverage these replicas strategically using connection templates.

### Creating read replicas in Neon

First, create the necessary read replicas for your Neon branches by following the [Create and manage Read Replicas guide](/docs/guides/read-replica-guide).

Note that replicas can be added to any branch, including the primary. Once a replica is created, copy its connection string, which you'll need for the next step.

### Configuring read replicas in Hasura

To configure read replicas for your primary Neon data source within Hasura, follow these steps:

1.  Go to the `Hasura Console -> Data -> Manage`.
2.  Locate your primary data source and click the "Edit" button next to it.
3.  Navigate to the `Connection Details` tab.
4.  Scroll down to the "Read Replicas" section and click `+ Add New Read Replica`.
    ![Add Read Replica](/docs/guides/hasura/add-read-replica.png)
5.  In the modal, paste the connection string **you copied earlier for your Neon read replica** into the `Database URL` field, then click `Add Read Replica` (within the modal).
6.  (Optional) Repeat step 5 if you have multiple read replicas to add for this primary source.
7.  **Finally, ensure you click** the main `Update Connection` button at the bottom of the page to save these changes to the data source configuration.
    ![Hasura Update Connection](/docs/guides/hasura/update-connection-read-replica.png)

### Kriti variables for replica routing

Within your Kriti connection template, you have access to specific variables that control how Hasura interacts with read replicas (which are typically configured in the main Data Source 'Connection Settings' for the primary connection):

- **`{{$.primary}}`**: Explicitly routes the request to the primary read-write connection defined for the data source, bypassing any read replicas.
- **`{{$.read_replicas}}`**: Routes the request to a _randomly chosen_ read replica from the list configured in the main 'Connection Settings'. This is only valid for `query` and `subscription` operations. Using it for `mutation` operations will result in an error.
- **`{{$.default}}`**: Routes the request according to Hasura's default behavior:
  - If read replicas _are_ configured: Routes `query`/`subscription` operations to a random read replica and `mutation` operations to the `primary` connection.
  - If read replicas _are not_ configured: Routes all operations to the `primary` connection.
- **`{{$.connection_set.<member_name>}}`**: Routes the request to a specific named connection (like a Neon branch connection string) defined in your Dynamic Routing connection set, bypassing default replica logic.

### Implementing routing logic in Kriti

You can use conditional logic within your Kriti template to decide when to utilize read replicas. The primary use case is often within the fallback logic (when a specific branch isn't targeted via a header). The `default` connection in Hasura will automatically route to a read replica if one is configured.

Here's an example of how you might implement this in your Kriti template:

    ```json
    {{ if ($.request.headers?["x-hasura-branch-name"] == "dev")}}
        {{$.connection_set.dev_branch}}
    {{ elif ($.request.headers?["x-hasura-branch-name"] == "feature-x")}}
        {{$.connection_set.feature_x_branch}}
    {{ elif ($.request.headers?["x-hasura-branch-name"] == "staging")}}
        {{$.connection_set.staging_branch}}
    {{ elif ($.request.query.operation_type == "mutation")}}
        {{$.primary}}
    {{ elif ($.request.headers?["no-stale-read"] == "true")}}
        {{$.primary}}
    {{ else }}
        {{$.default}}
    {{ end }}
    ```

#### Explanation of the fallback logic:

1.  **Mutations:** Always directed to `{{$.primary}}` for write capability.
2.  **Fresh reads:** If the `no-stale-read: true` header is present (for queries/subscriptions), route to `{{$.primary}}` to bypass potential replication lag on replicas.
3.  **Standard reads:** For all other queries/subscriptions in the fallback scenario, route to `{{$.default}}`. This directs Hasura to use one of the read replicas configured in the main connection settings. If no replicas are configured there, Hasura falls back gracefully to the primary connection.
4.  **Branch-specific reads:** If a specific branch is targeted via the `x-hasura-branch-name` header, route to that branch connection.

## Considerations and limitations

- **Schema consistency:** Hasura derives the GraphQL schema _only_ from the `primary` connection. All branches in your connection set _should_ have a compatible schema. Hasura does not verify schema consistency across the connection set members. Mismatched schemas can lead to runtime errors when a request is routed to a branch with an incompatible schema.
- **Migrations:** Hasura CLI migrations can only be applied directly to the `primary` connection defined in the source configuration.
- **Event triggers:** Hasura Event Triggers are typically configured on the primary database and will only fire for mutations executed on the `primary` connection. Mutations routed to other branches in the connection set will _not_ trigger these events.

## Conclusion

Combining Neon's instant database branching with Hasura's dynamic routing offers a powerful and flexible way to manage multiple database environments for development, testing, and previews. By creating lightweight, isolated Neon branches and using Hasura's connection templates to intelligently route requests based on context, you can significantly streamline your workflows, improve developer productivity, and ensure safer testing without the overhead of managing multiple full databases and GraphQL instances.

## Resources

- [Neon Branching](/docs/introduction/branching)
- [Neon Read Replica](/docs/introduction/read-replicas)
- [Hasura Dynamic Database Connection Routing](https://hasura.io/docs/2.0/databases/database-config/dynamic-db-connection/)
- [Hasura Kriti Templating Specification](https://hasura.io/docs/2.0/api-reference/kriti-templating/)
- [Hasura Read Replicas](https://hasura.io/docs/2.0/databases/database-config/read-replicas/)


# Getting started with the HONC stack

---
title: Getting started with the HONC stack
subtitle: Building a serverless Task API with Hono, Drizzle, Neon, and Cloudflare
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-05-14T00:00:00.000Z'
updatedOn: '2025-05-14T00:00:00.000Z'
---

The [HONC stack](https://honc.dev/) - an acronym for **H**ono, **O**RM (Drizzle), **N**eon, and **C**loudflare - is a modern toolkit for building lightweight, type-safe, and edge-enabled data APIs. It's designed for developers seeking to build fast, serverless applications with a strong emphasis on scalability and a great developer experience.

This guide will walk you through building a simple Task management API using the HONC stack. You'll learn how to:

- Initialize a HONC project using `create-honc-app`.
- Define your database schema with Drizzle ORM.
- Use Neon as your serverless Postgres database.
- Create API endpoints using the Hono framework.
- Run your application locally and deploy it to Cloudflare Workers.
- Utilize the built-in Fiberplane API playground for easy testing.

By the end, you'll have a functional serverless API and a solid understanding of how the HONC components work together.

## Prerequisites

Before you begin, ensure you have the following:

- **Node.js:** Version `22.15` or later installed on your machine. You can download it from [nodejs.org](https://nodejs.org/).
- **Neon account:** A free Neon account. If you don't have one, sign up at [Neon](https://console.neon.tech/signup).
- **Cloudflare account:** A free Cloudflare account, which you'll need for deployment. Sign up at [Cloudflare](https://dash.cloudflare.com/sign-up).

<Steps>

## Initialize your HONC project

The easiest way to start a HONC project is by using the [`create-honc-app`](https://github.com/fiberplane/create-honc-app) CLI tool.

1.  Open your terminal and run the following command:

    ```bash
    npm create honc-app@latest
    ```

      <Admonition type="note" title="Node.js version">
      Use Node.js version `22.15` or later. Older versions may cause project initialization issues. Check your version with:
      ```bash
      node -v
      ```
      </Admonition>

2.  The CLI will guide you through the setup process. Here's an example interaction:

    ```text shouldWrap
    npm create honc-app@latest

    > npx
    > create-honc-app

     __  __     ______     __   __     ______
    /\ \_\ \   /\  __ \   /\ "-.\ \   /\  ___\
    \ \  __ \  \ \ \/\ \  \ \ \-.  \  \ \ \____
     \ \_\ \_\  \ \_____\  \ \_\\"\_\  \ \_____\
      \/_/\/_/   \/_____/   \/_/ \/_/   \/_____/


    ┌  🪿 create-honc-app
    │
    ◇  Where should we create your project? (./relative-path)
    │  ./honc-task-api
    │
    ◇  Which template do you want to use?
    │  Neon template
    │
    ◇  Do you need an OpenAPI spec?
    │  Yes
    │
    ◇  The selected template uses Neon, do you want the create-honc-app to set up the connection string for you?
    │  Yes
    │
    ◇  Do you want to install dependencies?
    │  Yes
    │
    ◇  Do you want to initialize a git repository and stage all the files?
    │  Yes
    |
    ◆  Template set up successfully
    │
    ◇  Setting up Neon:
    │
    │  In order to connect to your database project and retrieve the connection key, you'll need to authenticate with Neon.
    │
    │  The connection URI will be written to your .dev.vars file as DATABASE_URL. The token itself will *NOT* be stored anywhere after this session is complete.
    │
    ◇  Awaiting authentication in web browser. Auth URL:
    │
    │  https://oauth2.neon.tech/oauth2/auth?response_type=code&client_id=create-honc-app&state=[...]&scope=[...]&redirect_uri=[...]&code_challenge=[...]&code_challenge_method=S256
    │
    ◆  Neon authentication successful
    │
    ◇  Select a Neon project to use:
    │  Create a new project
    │
    ◇  What is the name of the project?
    │  honc-task-api
    │
    ◆  Project created successfully: honc-task-api on branch: main
    │
    ◇  Select a project branch to use:
    │  main
    │
    ◇  Select a database you want to connect to:
    │  neondb
    │
    ◇  Select which role to use to connect to the database:
    │  neondb_owner
    │
    ◇  Writing connection string to .dev.vars file
    │
    ◆  Neon connection string written to .dev.vars file
    │
    ◆  Dependencies installed successfully
    │
    ◆  Git repository initialized and files staged successfully
    │
    └  🪿 HONC app created successfully in ./honc-task-api!
    ```

    Here's a breakdown of the options:
    - **Where to create your project:** Specify the directory for your new project. Here, we used `./honc-task-api`.
    - **Template:** Choose the Neon template for this guide.
    - **OpenAPI spec:** Opt-in to generate an OpenAPI spec for your API.
    - **Neon connection string:** Allow the CLI to set up the connection string for you.
    - **Install dependencies:** Yes, to install the required packages.
    - **Git repository:** Yes, to initialize a git repository and stage all files.
    - **Neon authentication:** Follow the link to authenticate with Neon. This will allow the CLI to set up your database connection.
      ![Neon authentication prompt](/docs/guides/honc-neon-auth.png)
    - **Create a new project:** Choose to create a new Neon project or use an existing one. Here, we created a new one.
    - **Project name:** Provide a name for your Neon project (e.g., `honc-task-api`) if creating a new one.
    - **Project branch:** Select the main branch for your Neon project.
    - **Database:** Choose the default database (e.g., `neondb`).
    - **Role:** Select the `neondb_owner` role for database access.
    - **Connection string:** The CLI will write the connection string to a `.dev.vars` file in your project directory.
    - **Setup**: The CLI will set up the project, install dependencies, and initialize a git repository.

3.  Navigate into your new project directory.

    ```bash
    cd honc-task-api
    ```

4.  Open the project in your favorite code editor.

## Confirm Neon connection

If you chose to let `create-honc-app` set up the connection string, your Neon `DATABASE_URL` should already be in the `.dev.vars` file in your project root. This file is used by [Wrangler](https://developers.cloudflare.com/workers/wrangler/) (Cloudflare's CLI) for local development and is gitignored by default.

Verify its content:

```ini
// .dev.vars
DATABASE_URL="postgresql://neondb_owner:..."
```

If you didn't use the CLI for setup, copy `.dev.vars.example` to `.dev.vars`. Then, manually add your Neon project's `DATABASE_URL` to the `.dev.vars` file. You can find your connection string in the Neon console. Learn more: [Connect from any application](/docs/connect/connect-from-any-app)

## Define database schema with Drizzle

The `create-honc-app` template comes with an example schema (for `users`) in `src/db/schema.ts`. You need to modify this to define a `tasks` table.

1.  Open `src/db/schema.ts`. Remove the existing `users` schema definition. Add the following schema definition for `tasks`:

    ```typescript
    import { pgTable, serial, text, boolean, timestamp } from 'drizzle-orm/pg-core';

    export type NewUser = typeof users.$inferInsert; // [!code --]
    export const users = pgTable('users', {
      // [!code --]
      id: uuid('id').defaultRandom().primaryKey(), // [!code --]
      name: text('name').notNull(), // [!code --]
      email: text('email').notNull(), // [!code --]
      settings: jsonb('settings'), // [!code --]
      createdAt: timestamp('created_at').defaultNow().notNull(), // [!code --]
      updatedAt: timestamp('updated_at').defaultNow().notNull(), // [!code --]
    }); // [!code --]

    export const tasks = pgTable('tasks', {
      // [!code ++]
      id: serial('id').primaryKey(), // [!code ++]
      title: text('title').notNull(), // [!code ++]
      description: text('description'), // [!code ++]
      completed: boolean('completed').default(false).notNull(), // [!code ++]
      createdAt: timestamp('created_at').defaultNow().notNull(), // [!code ++]
      updatedAt: timestamp('updated_at').defaultNow().notNull(), // [!code ++]
    }); // [!code ++]

    export type Task = typeof tasks.$inferSelect; // [!code ++]
    export type NewTask = typeof tasks.$inferInsert; // [!code ++]
    ```

    The tasks table schema defines the structure for storing tasks. It includes:
    - A unique, auto-incrementing integer `id`.
    - `title` and `description` fields.
    - A `completed` status.
    - `createdAt` and `updatedAt` timestamps to track creation and modification times.

    For type safety when interacting with tasks (e.g., selecting or inserting), `Task` and `NewTask` types are exported. These types are inferred from the schema and can be used throughout the application.

## Generate and apply database migrations

With the schema updated, generate and apply database migrations.

1.  **Generate migrations:**

    ```bash
    npm run db:generate
    ```

    This creates SQL migration files in the `drizzle` folder.

2.  **Apply migrations:**
    ```bash
    npm run db:migrate
    ```
    This applies the migrations to your Neon database. Your `tasks` table should now exist. You can verify this in the **Tables** section of your Neon project console.
    ![Neon console - Tasks table](/docs/guides/honc-neon-tasks-table.png)

## Adapt API endpoints for tasks

The `src/index.ts` file generated by `create-honc-app` will contain Hono routes and Zod schemas for a sample `users` API. You need to adapt this foundation to create a RESTful API for managing our `tasks`. This involves defining how clients can interact with our tasks data through standard HTTP methods (`GET`, `POST`, `PUT`, `DELETE`).

1.  Open `src/index.ts`. You'll see code with `UserSchema`, an `apiRouter` instance for `/api/users`, Zod validators, and `describeRoute` for OpenAPI documentation.

2.  **Modify Zod schemas:**
    First, define the expected structure of task data for API requests and responses using Zod. This ensures type safety and provides a clear contract. Find the existing `UserSchema` and related definitions and replace them with schemas for `Task` (how a task looks when retrieved) and `NewTask` (how a new task looks when being created).

    ```typescript
    // ... import statements and middleware for database connection

    const UserSchema = z // [!code --]
      .object({
        // [!code --]
        id: z.number().openapi({
          // [!code --]
          example: 1, // [!code --]
        }), // [!code --]
        name: z.string().openapi({
          // [!code --]
          example: 'Nikita', // [!code --]
        }), // [!code --]
        email: z.string().email().openapi({
          // [!code --]
          example: 'nikita@neon.tech', // [!code --]
        }), // [!code --]
      }) // [!code --]
      .openapi({ ref: 'User' }); // [!code --]

    const TaskSchema = z // [!code ++]
      .object({
        // [!code ++]
        id: z.string().openapi({
          // [!code ++]
          description: 'The unique identifier for the task.', // [!code ++]
          example: '1', // [!code ++]
        }), // [!code ++]
        title: z.string().openapi({
          // [!code ++]
          description: 'The title of the task.', // [!code ++]
          example: 'Learn HONC', // [!code ++]
        }), // [!code ++]
        description: z.string().nullable().optional().openapi({
          // [!code ++]
          description: 'A detailed description of the task.', // [!code ++]
          example: 'Build a complete task API with the HONC Stack', // [!code ++]
        }), // [!code ++]
        completed: z.boolean().openapi({
          // [!code ++]
          description: 'Indicates if the task is completed.', // [!code ++]
          example: false, // [!code ++]
        }), // [!code ++]
        createdAt: z.string().datetime().openapi({
          // [!code ++]
          description: 'The date and time when the task was created.', // [!code ++]
          example: new Date().toISOString(), // [!code ++]
        }), // [!code ++]
        updatedAt: z.string().datetime().openapi({
          // [!code ++]
          description: 'The date and time when the task was last updated.', // [!code ++]
          example: new Date().toISOString(), // [!code ++]
        }), // [!code ++]
      }) // [!code ++]
      .openapi({ ref: 'Task' }); // [!code ++]

    const NewTaskSchema = z // [!code ++]
      .object({
        // [!code ++]
        title: z.string().min(1, 'Title cannot be empty').openapi({
          // [!code ++]
          example: 'Deploy to Cloudflare', // [!code ++]
        }), // [!code ++]
        description: z.string().nullable().optional().openapi({
          // [!code ++]
          example: 'Finalize deployment steps for the task API.', // [!code ++]
        }), // [!code ++]
      }) // [!code ++]
      .openapi({ ref: 'NewTask' }); // [!code ++]
    ```

    Here's a breakdown of the Zod schemas:
    - `TaskSchema` defines the full structure of a task for API responses.
    - `NewTaskSchema` defines the structure for creating a new task.
    - The `.openapi({ ref: "..." })` annotations are used to generate OpenAPI documentation.

3.  **Adapt API router:**
    The `apiRouter` groups related routes. We'll modify the one for `/api/users` to handle `/api/tasks`.
    - Locate where `app.route` is defined for `/api/users` and change it to `/api/tasks`:

      ```typescript
        app
        .get(
          "/",
          describeRoute({...})
        )
        .route("/api/users", apiRouter); // [!code --]
        .route("/api/tasks", apiRouter); // [!code ++]
      ```

    - Inside `apiRouter`, modify the CRUD operations. For each route:
      - `describeRoute` adds OpenAPI documentation.
      - `zValidator` validates request parameters or JSON bodies.
      - The `async` handler interacts with the database via Drizzle.

    Here's the adapted `apiRouter` code for tasks with CRUD operations:

    ```typescript shouldWrap
    // In src/index.ts, adapt the apiRouter for tasks

    const apiRouter = new Hono<{ Bindings: Bindings; Variables: Variables }>();

    apiRouter // [!code --]
      .get( // [!code --]
        "/", // [!code --]
        describeRoute({...}) // [!code --]
      ) // [!code --]
      .post( // [!code --]
        "/", // [!code --]
        describeRoute({...}), // [!code --]
        zValidator( // [!code --]
          "json", // [!code --]
          // ... Zod schema for POST (users) ... // [!code --]
        ) // [!code --]
      ) // [!code --]
      .get( // [!code --]
        "/:id", // [!code --]
        describeRoute({...}), // [!code --]
        zValidator( // [!code --]
          "param", // [!code --]
          // ... Zod schema for GET by ID (users) ... // [!code --]
        ) // [!code --]
      ); // [!code --]

    apiRouter // [!code ++]
      .get( // [!code ++]
        "/", // [!code ++]
        describeRoute({ // [!code ++]
          summary: "List all tasks", // [!code ++]
          description: "Retrieves a list of all tasks, ordered by creation date.", // [!code ++]
          responses: { // [!code ++]
            200: { // [!code ++]
              content: { // [!code ++]
                "application/json": { schema: resolver(z.array(TaskSchema)) }, // [!code ++]
              }, // [!code ++]
              description: "Tasks fetched successfully", // [!code ++]
            }, // [!code ++]
          }, // [!code ++]
        }), // [!code ++]
        async (c) => { // [!code ++]
          const db = c.get("db"); // [!code ++]
          const tasks = await db // [!code ++]
            .select() // [!code ++]
            .from(schema.tasks) // [!code ++]
            .orderBy(desc(schema.tasks.createdAt)); // [!code ++]
          return c.json(tasks, 200); // [!code ++]
        }, // [!code ++]
      ) // [!code ++]
      .post( // [!code ++]
        "/", // [!code ++]
        describeRoute({ // [!code ++]
          summary: "Create a new task", // [!code ++]
          description: "Adds a new task to the list.", // [!code ++]
          responses: { // [!code ++]
            201: { // [!code ++]
              content: { // [!code ++]
                "application/json": { // [!code ++]
                  schema: resolver(TaskSchema), // [!code ++]
                }, // [!code ++]
              }, // [!code ++]
              description: "Task created successfully", // [!code ++]
            }, // [!code ++]
            400: { // [!code ++]
              description: "Invalid input for task creation", // [!code ++]
            }, // [!code ++]
          }, // [!code ++]
        }), // [!code ++]
        zValidator("json", NewTaskSchema), // [!code ++]
        async (c) => { // [!code ++]
          const db = c.get("db"); // [!code ++]
          const { title, description } = c.req.valid("json"); // [!code ++]
          const newTaskPayload: schema.NewTask = { // [!code ++]
            title, // [!code ++]
            description: description || null, // [!code ++]
            completed: false, // [!code ++]
          }; // [!code ++]
          const [insertedTask] = await db // [!code ++]
            .insert(schema.tasks) // [!code ++]
            .values(newTaskPayload) // [!code ++]
            .returning(); // [!code ++]
          return c.json(insertedTask, 201); // [!code ++]
        }, // [!code ++]
      ) // [!code ++]
      .get( // [!code ++]
        "/:id", // [!code ++]
        describeRoute({ // [!code ++]
          summary: "Get a single task by ID", // [!code ++]
          responses: { // [!code ++]
            200: { // [!code ++]
              content: { "application/json": { schema: resolver(TaskSchema) } }, // [!code ++]
              description: "Task fetched successfully", // [!code ++]
            }, // [!code ++]
            404: { description: "Task not found" }, // [!code ++]
            400: { description: "Invalid ID format" }, // [!code ++]
          }, // [!code ++]
        }), // [!code ++]
        zValidator( // [!code ++]
          "param", // [!code ++]
          z.object({ // [!code ++]
            id: z.string().openapi({ // [!code ++]
              param: { name: "id", in: "path" }, // [!code ++]
              example: "1", // [!code ++]
              description: "The ID of the task to retrieve", // [!code ++]
            }), // [!code ++]
          }), // [!code ++]
        ), // [!code ++]
        async (c) => { // [!code ++]
          const db = c.get("db"); // [!code ++]
          const { id } = c.req.valid("param"); // [!code ++]
          const [task] = await db // [!code ++]
            .select() // [!code ++]
            .from(schema.tasks) // [!code ++]
            .where(eq(schema.tasks.id, Number(id))); // [!code ++]
          if (!task) { // [!code ++]
            return c.json({ error: "Task not found" }, 404); // [!code ++]
          } // [!code ++]
          return c.json(task, 200); // [!code ++]
        }, // [!code ++]
      ) // [!code ++]
      .put( // [!code ++]
        "/:id", // [!code ++]
        describeRoute({ // [!code ++]
          summary: "Update a task's completion status", // [!code ++]
          description: "Toggles or sets the completion status of a specific task.", // [!code ++]
          responses: { // [!code ++]
            200: { // [!code ++]
              content: { "application/json": { schema: resolver(TaskSchema) } }, // [!code ++]
              description: "Task updated successfully", // [!code ++]
            }, // [!code ++]
            404: { description: "Task not found" }, // [!code ++]
            400: { description: "Invalid input or ID format" }, // [!code ++]
          }, // [!code ++]
        }), // [!code ++]
        zValidator( // [!code ++]
          "param", // [!code ++]
          z.object({ // [!code ++]
            id: z.string().openapi({ // [!code ++]
              param: { name: "id", in: "path" }, // [!code ++]
              example: "1", // [!code ++]
              description: "The ID of the task to update.", // [!code ++]
            }), // [!code ++]
          }), // [!code ++]
        ), // [!code ++]
        zValidator( // [!code ++]
          "json", // [!code ++]
          z // [!code ++]
            .object({ // [!code ++]
              completed: z.boolean().openapi({ // [!code ++]
                example: true, // [!code ++]
                description: "The new completion status of the task.", // [!code ++]
              }), // [!code ++]
            }) // [!code ++]
        ), // [!code ++]
        async (c) => { // [!code ++]
          const db = c.get("db"); // [!code ++]
          const { id } = c.req.valid("param"); // [!code ++]
          const { completed } = c.req.valid("json"); // [!code ++]
          const [updatedTask] = await db // [!code ++]
            .update(schema.tasks) // [!code ++]
            .set({ updatedAt: sql`NOW()`, completed }) // [!code ++]
            .where(eq(schema.tasks.id, Number(id))) // [!code ++]
            .returning(); // [!code ++]
          if (!updatedTask) { // [!code ++]
            return c.json({ error: "Task not found" }, 404); // [!code ++]
          } // [!code ++]
          return c.json(updatedTask, 200); // [!code ++]
        }, // [!code ++]
      ) // [!code ++]
      .delete( // [!code ++]
        "/:id", // [!code ++]
        describeRoute({ // [!code ++]
          summary: "Delete a task", // [!code ++]
          description: "Removes a specific task from the list.", // [!code ++]
          responses: { // [!code ++]
            200: { // [!code ++]
              content: { // [!code ++]
                "application/json": { // [!code ++]
                  schema: resolver( // [!code ++]
                    z.object({ message: z.string(), id: z.string() }), // [!code ++]
                  ), // [!code ++]
                }, // [!code ++]
              }, // [!code ++]
              description: "Task deleted successfully", // [!code ++]
            }, // [!code ++]
            404: { description: "Task not found" }, // [!code ++]
            400: { description: "Invalid ID format" }, // [!code ++]
          }, // [!code ++]
        }), // [!code ++]
        zValidator( // [!code ++]
          "param", // [!code ++]
          z.object({ // [!code ++]
            id: z.string().openapi({ // [!code ++]
              param: { name: "id", in: "path" }, // [!code ++]
              example: "1", // [!code ++]
              description: "The ID of the task to delete.", // [!code ++]
            }), // [!code ++]
          }), // [!code ++]
        ), // [!code ++]
        async (c) => { // [!code ++]
          const db = c.get("db"); // [!code ++]
          const { id } = c.req.valid("param"); // [!code ++]
          const [deletedTask] = await db // [!code ++]
            .delete(schema.tasks) // [!code ++]
            .where(eq(schema.tasks.id, Number(id))) // [!code ++]
            .returning({ id: schema.tasks.id }); // [!code ++]
          if (!deletedTask) { // [!code ++]
            return c.json({ error: "Task not found" }, 404); // [!code ++]
          } // [!code ++]
          return c.json( // [!code ++]
            { message: "Task deleted successfully", id: deletedTask.id }, // [!code ++]
            200, // [!code ++]
          ); // [!code ++]
        }, // [!code ++]
      ); // [!code ++]
    ```

    **Breakdown of the API endpoints:**
    - **`GET /` (List tasks):** Fetches all tasks from the `schema.tasks` table using `db.select()`. It orders them by `createdAt` in descending order so newer tasks appear first. The response is a JSON array of `TaskSchema` objects.
    - **`POST /` (Create task):**
      - Validates the incoming JSON request body against `NewTaskSchema` (requires `title`, `description` is optional).
      - If valid, it constructs a `newTaskPayload` (setting `completed` to `false` by default).
      - Inserts the new task into `schema.tasks` using `db.insert().values().returning()` to get the newly created task (including its auto-generated ID and timestamps).
      - Returns the created task (matching `TaskSchema`) with a `201 Created` status.
    - **`GET /:id` (Get task by ID):**
      - Fetches a single task from `schema.tasks` where the `id` matches.
      - Returns the task if found, or a `404 Not Found` error.
    - **`PUT /:id` (Update task):**
      - Validates the `id` path parameter.
      - Validates the incoming JSON request body against `z.object({ completed: z.boolean() })`.
      - Updates the task's `completed` status and `updatedAt` timestamp in `schema.tasks`.
    - **`DELETE /:id` (Delete task):**
      - Validates the `id` path parameter.
      - Deletes the task with the matching `id` from `schema.tasks`.
      - Returns a success message with the ID of the deleted task, or a `404 Not Found`.

## Run and test locally

Run your HONC application locally using Wrangler:

1.  In your terminal, at the root of your project:

    ```bash
    npm run dev
    ```

    This starts a local server, typically at `http://localhost:8787`.

2.  **Test your API endpoints:**
    You can use tools like cURL, Postman, or the Fiberplane API Playground (see next section).
    - **Create a task:**

      ```bash
      curl -X POST -H "Content-Type: application/json" -d '{"title":"Learn HONC","description":"Build a task API"}' http://localhost:8787/api/tasks
      ```

      A successful response should return the created task with a unique ID.

      ```json
      {
        "id": 1,
        "title": "Learn HONC",
        "description": "Build a task API",
        "completed": false,
        "createdAt": "2025-05-14T09:17:25.392Z",
        "updatedAt": "2025-05-14T09:17:25.392Z"
      }
      ```

      You can also verify if the task was added to your database by checking your project in the Neon console. The task should appear in the `tasks` table.
      ![Neon console - Tasks table with new task](/docs/guides/honc-neon-tasks-table-new-task.png)

    - **List all tasks:**

      ```bash
      curl http://localhost:8787/api/tasks
      ```

      A successful response should return an array of tasks.

      ```json
      [
        {
          "id": 1,
          "title": "Learn HONC",
          "description": "Build a task API",
          "completed": false,
          "createdAt": "2025-05-14T09:17:25.392Z",
          "updatedAt": "2025-05-14T09:17:25.392Z"
        }
      ]
      ```

    - **Get a specific task (replace `TASK_ID` with an actual ID from the list):**

      ```bash
      curl http://localhost:8787/api/tasks/TASK_ID
      ```

      For example, if the ID is `1`:

      ```bash
      curl http://localhost:8787/api/tasks/1
      ```

      A successful response should return the task with ID `1`.

      ```json
      {
        "id": 1,
        "title": "Learn HONC",
        "description": "Build a task API",
        "completed": false,
        "createdAt": "2025-05-14T09:17:25.392Z",
        "updatedAt": "2025-05-14T09:17:25.392Z"
      }
      ```

    - **Update a task (replace `TASK_ID`):**

      ```bash
      curl -X PUT -H "Content-Type: application/json" -d '{"completed":true}' http://localhost:8787/api/tasks/TASK_ID
      ```

      For example, if the ID is `1`:

      ```bash
      curl -X PUT -H "Content-Type: application/json" -d '{"completed":true}' http://localhost:8787/api/tasks/1
      ```

      A successful response should return the updated task.

      ```json
      {
        "id": 1,
        "title": "Learn HONC Stack",
        "description": "Build a task API",
        "completed": true,
        "createdAt": "2025-05-14T09:17:25.392Z",
        "updatedAt": "2025-05-14T09:17:25.392Z"
      }
      ```

    - **Delete a task (replace `TASK_ID`):**

      ```bash
      curl -X DELETE http://localhost:8787/api/tasks/TASK_ID
      ```

      For example, if the ID is `1`:

      ```bash
      curl -X DELETE http://localhost:8787/api/tasks/1
      ```

      A successful response should return a message confirming deletion.

      ```json
      { "message": "Task deleted successfully", "id": 1 }
      ```

<Admonition type="info" title="Interactive Testing with Fiberplane API Playground">

The `create-honc-app` boilerplate includes integration with the [**Fiberplane API Playground**](https://fiberplane.com/blog/hono-native-playground/), an in-browser tool designed for interacting with your HONC API during development.

To access it, simply ensure your local development server is running via `npm run dev`. Once the server is active, open your web browser and navigate to [`localhost:8787/fp`](http://localhost:8787/fp).

Within the playground, you'll find a visual exploration of your API. It reads your `/openapi.json` spec (generated by `hono-openapi` if enabled) to display all your defined API endpoints, such as `/api/tasks` or `/api/tasks/{id}`, within a user-friendly interface. This allows for easy request crafting; you can select an endpoint and fill in necessary parameters, path variables, and request bodies directly within the UI.

This is incredibly useful for quick testing and debugging cycles during development, reducing the frequent need for external tools like Postman or cURL.

![Fiberplane API Playground showing API endpoints for the HONC Task API](/docs/guides/honc-fiberplane-api-playground.png)

</Admonition>

## Deploy to Cloudflare Workers

Deploy your application globally via Cloudflare's edge network.

1.  **Set `DATABASE_URL` secret in Cloudflare:**
    Your deployed Worker needs the Neon database connection string.

    ```bash
    npx wrangler secret put DATABASE_URL
    ```

    Paste your Neon connection string when prompted.

    ```bash
    npx wrangler secret put DATABASE_URL
    ⛅️ wrangler 4.14.4
    -------------------
    ✔ Enter a secret value: … ************************************************************************************************************************
    🌀 Creating the secret for the Worker "honc-task-api"
    ✔ There doesn't seem to be a Worker called "honc-task-api". Do you want to create a new Worker with that name and add secrets to it? … yes
    🌀 Creating new Worker "honc-task-api"...
    ✨ Success! Uploaded secret DATABASE_URL
    ```

    > Steps may vary based on your Cloudflare account and login status. Ensure you are logged in if prompted.

2.  **Deploy:**

    ```bash
    npm run deploy
    ```

    Wrangler will deploy your application to Cloudflare Workers. The output will show the deployment status and the URL of your deployed Worker.

        ```bash
        npm run deploy
        > deploy
        > wrangler deploy --minify src/index.ts
        ⛅️ wrangler 4.14.4
        -------------------
        Total Upload: 505.17 KiB / gzip: 147.10 KiB
        Worker Startup Time: 32 ms
        No bindings found.
        Uploaded honc-task-api (13.49 sec)
        Deployed honc-task-api triggers (3.50 sec)
          https://honc-task-api.[xxx].workers.dev
        Current Version ID: b0c90b17-f10a-4807-xxxx
        ```

</Steps>

## Summary

Congratulations! You've successfully adapted the `create-honc-app` boilerplate to build a serverless Task API using the HONC stack. You've defined a schema with Drizzle, created Hono endpoints with Zod validation, tested locally using tools like cURL and the integrated Fiberplane API Playground, and learned how to deploy to Cloudflare Workers.

The HONC stack offers a streamlined, type-safe, and performant approach to building modern edge APIs.

You can find the source code for the application described in this guide on GitHub.

<DetailIconCards>
    <a href="https://github.com/neondatabase-labs/honc-example" description="HONC Stack" icon="github">
      HONC Task API
    </a>
</DetailIconCards>

## Resources

- **HONC:** [honc.dev](https://honc.dev), [create-honc-app GitHub](https://github.com/fiberplane/create-honc-app)
- **Fiberplane API Playground:** [Hono-native API Playground, powered by OpenAPI](https://fiberplane.com/blog/hono-native-playground/), [Features](https://fiberplane.com/docs/features/playground/)
- **Hono:** [hono.dev](https://hono.dev)
- **Drizzle ORM:** [orm.drizzle.team](https://orm.drizzle.team)
- **Neon:** [neon.tech/docs](/docs)
- **Cloudflare Workers:** [developers.cloudflare.com/workers](https://developers.cloudflare.com/workers/)

<NeedHelp/>


# Building Internal Tools Using Neon, StackAuth, and Vercel’s Free Plans

---
title: Building Internal Tools Using Neon, StackAuth, and Vercel’s Free Plans
subtitle: Get secure, host, and deploy internal tools in minutes, for free
author: sam-harri
enableTableOfContents: true
createdAt: '2025-07-08T00:00:00.000Z'
---

Almost every tech company, from small startups to Fortune 500 enterprises, relies on internal tools. Larger organizations often have dedicated services and structured procedures in place, but for many others, internal tools can be messy, often ending up as either insecure setups hosted behind a VPN or overly complex solutions cobbled together from a surprising number of AWS services.

It shouldn't feel like you're managing a full-fledged production application or rack up monthly bills for basic functionalities, especially without compromising on security.

[This template](https://github.com/sam-harri/internal_tooling_neon_stack), built with Neon, StackAuth, and Vercel’s free plans, gives you a secure and scalable starting point for internal tools. It comes preconfigured with a Postgres database, API routes, authentication, authorization, and a built-in admin panel - everything you need to get an application running in minutes.

![Tool Overview](/guides/images/internal-tool-template/internal-tool-template.gif)

## Getting Started with the Template

Using the template, you can get your internal tool up and running in minutes, and all you need are Neon, Stack Auth, and Vercel accounts.

```bash
git clone https://github.com/sam-harri/internal_tooling_neon_stack.git
```

Then open that new project in your editor of choice, and install the dependencies:

```bash
npm install
```

Next, set up the environment variables by copying over the `.env.example` to a new `.env` file. Fill these in with the connection string from your Neon database project in the console, and the Stack Auth keys from the Auth tab.

Since the template comes with some admin logic already, you’ll need to sync your database by applying the schema:

```bash
npx drizzle-kit push
```

## Set Up Roles and Claim Your Project

Head to the Auth tab in the Neon console and claim the project with your Stack Auth account. From there, create two project roles: `admin` and `user`, where `user` is contained within `admin`.

Your Project Permissions section should look like this afterward:

![Project Permissions settings in the Neon Console Auth tab](/guides/images/internal-tool-template/project-permissions.png)

Once that’s done, boot up the project locally, log in, and head over to `/setup` to claim your admin privileges. You can then deploy your app with Vercel. Don’t forget to add your Vercel-provided domain to the allowed domains in Neon’s Auth tab, or your custom domain if you're using one.

## Authentication and Access Control with StackAuth

You obviously don't want just anyone to be able to access your internal tools. Luckily, Neon comes with an auth solution supporting social and password sign-on, role-based access control, and much more.

In this template, only authorized users can access the tools. The application admin can whitelist domains, allowing anyone with an email from a specific domain (e.g. `yourcompany.tld`) and a verified email to sign up and get access.

Likewise, you can grant and revoke access to specific external email addresses, perfect for contractors or partners. Emails can also be explicitly blocked even if their domain is whitelisted. Managing access is straightforward via the built-in admin panel at `/tools/admin`. Admins can assign or revoke privileges to other users, giving you complete control over your application's access policies.

These rules are meant to cover most of the common scenarios you might face, but since it’s a template, you can easily update it to add features like regex validation or subdomain whitelisting.

![Admin Panel Overview](/guides/images/internal-tool-template/admin-page.png)

## Database and Logic with Neon and Next.js

Internal tools inherently require business logic and data storage. Neon provides a free serverless Postgres database, along with all the most popular Postgres extensions, and offers a serverless database driver that’s perfect for single-shot queries in Next.js serverless functions.

For business logic, you can use Next’s server actions and API routes without the overhead of managing and deploying a separate API—ideal for simple internal tools. Vercel makes hosting your Next.js app painless, even allowing you to attach a custom domain so that you can host it at something like `yourtool.yourcompany.tld`.

## Customize the App for Your Team

Customization is designed to be simple. Just fill out your company-specific details in `config/app.ts`, then add your custom tools to the `app/tools` directory and declare them within the configuration file.

To get started, play around with `app/tools/tool1/page.tsx`.

## Get Started

[Clone the template](https://github.com/sam-harri/internal_tooling_neon_stack), connect your Neon and StackAuth accounts, and deploy to Vercel. Your next internal tool is just a few commands away!


# Using Postgres as a Key-Value Store with hstore and JSONB

---
title: Using Postgres as a Key-Value Store with hstore and JSONB
subtitle: A step-by-step guide describing how to use hstore and JSONB for storing key-value pairs in Postgres
author: vkarpov15
enableTableOfContents: true
createdAt: '2025-04-15T13:24:36.612Z'
updatedOn: '2025-04-15T13:24:36.612Z'
---

PostgreSQL is well known for its relational features, but it also offers powerful tools for working with key-value pairs. If you want to store flexible, schema-less data in a Postgres column, you can use either the [`hstore` extension](https://www.postgresql.org/docs/current/hstore.html) or the built-in [JSONB type](https://www.postgresql.org/docs/current/datatype-json.html).

## Steps

- [Install and enable `hstore`](#install-and-enable-hstore)
- [Create a table with an `hstore` column](#create-a-table-with-an-hstore-column)
- [Insert and query key-value data with `hstore`](#insert-and-query-key-value-data-with-hstore)
- [Using `JSONB`](#using-jsonb)
- [Create a table with a `JSONB` column](#create-a-table-with-a-jsonb-column)
- [Insert and query key-value data with `JSONB`](#insert-and-query-key-value-data-with-jsonb)
- [Index key-value data for performance](#index-key-value-data-for-performance)
- [`hstore` vs `JSONB`](#hstore-vs-jsonb)

### Install and enable `hstore`

To create columns with the `hstore` type, you need to install the extension. In Neon, `hstore` is already installed, you just need to enable it with the following command.

```sql
CREATE EXTENSION IF NOT EXISTS hstore;
```

### Create a table with an `hstore` column

You can store key-value pairs in a column using the `hstore` type. Here’s an example of a products table with an `attributes` column of type `hstore`.

```sql
CREATE TABLE products (
  id SERIAL PRIMARY KEY,
  name TEXT NOT NULL,
  attributes HSTORE
);
```

### Insert and query key-value data with `hstore`

Each row in the `attributes` column can store a map of keys and values as strings as follows. Keep in mind that `hstore` can only store string keys and string values: you can't store numbers or objects as `hstore` values.

```sql
INSERT INTO products (name, attributes)
VALUES ('Backpack', 'color => blue, size => large'),
       ('Jacket', 'color => red, waterproof => yes');
```

You can then query based on the `attributes` row's keys and values using operators like `->` and `?` as follows.

```sql
-- Get products with a 'color' key
SELECT * FROM products WHERE attributes ? 'color';

-- Get products where color is 'blue'
SELECT * FROM products WHERE attributes -> 'color' = 'blue';
```

### Using `JSONB`

PostgreSQL’s `JSONB` type stores structured JSON data in a binary format. It supports indexing and nesting, making it great for more complex data structures.

### Create a table with a `JSONB` column

The following command creates a similar products table where `attributes` has type `JSONB` instead of `hstore`.

```sql
CREATE TABLE products_json (
  id SERIAL PRIMARY KEY,
  name TEXT NOT NULL,
  attributes JSONB
);
```

### Insert and query key-value data with `JSONB`

You can then insert products with `attributes` using JSON syntax as follows.

```sql
INSERT INTO products_json (name, attributes)
VALUES ('Backpack', '{"color": "blue", "size": "large"}'),
       ('Jacket', '{"color": "red", "waterproof": true}');
```

You can then query the `attributes` row by keys and values using operators like `?`, `@>`, and `->>`.

```sql
-- Get products with a 'waterproof' key
SELECT * FROM products_json WHERE attributes ? 'waterproof';

-- Get products where color is 'blue'
SELECT * FROM products_json
WHERE attributes @> '{"color": "blue"}';

-- Get products where color is 'blue' using `->>`
SELECT * FROM products_json
WHERE attributes->>'color' = 'blue';
```

### Index key-value data for performance

[GIN indexes](https://www.postgresql.org/docs/current/gin-intro.html) allow you to index `hstore` and `JSONB` properties, which can make your queries faster as your data grows. The following command shows how you can create a GIN index on the `attributes` property for both tables - GIN indexes work on both `hstore` and `JSONB` rows.

```sql
-- hstore
CREATE INDEX idx_hstore_attrs ON products USING GIN (attributes);

-- JSONB
CREATE INDEX idx_jsonb_attrs ON products_json USING GIN (attributes);
```

### `hstore` vs `JSONB`

Both `hstore` and `JSONB` offer an alternative to external key-value stores and enable you to keep your key-value data in the same place as the rest of your data. Whether you choose `hstore` or `JSONB` depends on your needs:

- If you need nested data, arrays, or any data type beyond strings, use `JSONB`.
- If you need extra performance and are certain you don't need more complex data types, `hstore` can be faster for very simple key-value lookups and use less disk space.


# Getting started with LangGraph + Neon

---
title: Getting started with LangGraph + Neon
subtitle: A step-by-step guide to building AI agents with LangGraph and Neon
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-02-21T00:00:00.000Z'
updatedOn: '2025-02-21T00:00:00.000Z'
---

This guide demonstrates how to integrate LangGraph with Neon. [LangGraph](https://www.langchain.com/langgraph) is a library in the [LangChain](https://www.langchain.com/langchain) ecosystem that simplifies the development of complex, multi-agent LLM applications by using a directed graph structure for efficient coordination and state management.

This guide walks through building a simple [ReAct (Reason + Act)](https://arxiv.org/abs/2210.03629) agent using LangGraph that interacts with Neon to create a database and perform SQL queries. It builds upon the concepts demonstrated in the [prebuilt ReAct agent from LangGraph](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent).

## Prerequisites

Before you begin, make sure you have the following prerequisites:

- **Python 3.10 or higher:** This guide requires Python 3.10 or a later version. If you don't have it installed, download it from [python.org](https://www.python.org/downloads/).

- **Neon account and API key:**
  - Sign up for a free Neon account at [neon.tech](https://console.neon.tech/signup).
  - After signing up, get your Neon API Key from the [Neon console](https://console.neon.tech/app/settings/profile). This API key is needed to authenticate your application with Neon.

- **Google API key:**
  - This guide utilizes the `gemini-2.0-flash` model from Google. You'll need a Google API key to proceed. If you don't already have one, get an API key from the [Google AI Studio](https://aistudio.google.com/apikey).
  - The free tier is sufficient for the example in this guide.

## LangGraph basics

Before building your AI agent workflows, it's important to understand the core concepts behind LangGraph.

### What is LangGraph?

LangGraph is an open‐source orchestration framework for building stateful, multi-actor applications with large language models (LLMs). It enables you to create complex agentic workflows by modeling your application as a graph of interconnected nodes. With LangGraph, you can:

- **Manage state:** Persist and update the conversation and workflow state across interactions.
- **Customize workflows:** Design flexible control flows—including conditional branching, loops, and hierarchical structures—to suit your specific use case.
- **Stream agent reasoning:** Deliver real-time, token-by-token streaming of agent actions to enhance user experience.

### Key components of LangGraph

- **Nodes:** The fundamental units of work. Each node represents a distinct operation (such as an LLM call or tool execution) that processes inputs and updates the application state.
- **Edges:** These define the transitions between nodes, enabling the workflow to branch conditionally or iterate as needed.
- **State persistence:** A central persistence layer that saves the state of your workflow, allowing you to resume or “time-travel” across interactions.
- **Tools:** Integration points for external functionalities (like web search, code execution, or API calls) that extend the agent’s capabilities beyond simple text generation.
- **Human-in-the-loop workflows:** Built-in mechanisms that allow human oversight, ensuring that agent decisions can be validated, corrected, or enhanced before proceeding.

By leveraging these powerful components, LangGraph empowers you to build reliable, scalable, and highly customizable AI agent applications—from prototyping to production.

## Why Neon for AI Agents?

Neon's architecture is particularly well-suited for AI agent development, offering several key advantages:

- **One-Second Provisioning:** Neon databases can be provisioned in about a second. This is _critical_ for AI agents that need to dynamically create databases. Traditional databases, with provisioning times often measured in minutes, create a significant bottleneck. Neon's speed keeps agents operating efficiently.

- **Scale-to-Zero and Serverless Pricing:** Neon's serverless architecture automatically scales databases down to zero when idle, and you only pay for active compute time. This is cost-effective for AI agent workflows, which often involve unpredictable workloads and many short-lived database instances. It enables "database-per-agent" or "database-per-session" patterns without incurring prohibitive costs.

- **Agent-Friendly API:** Neon provides a simple REST API for managing databases, roles, branches, and various other Neon platform operations. This API is easy for AI agents (and human developers) to interact with programmatically, allowing agents to manage their own database infrastructure without complex tooling.

## Building a LangGraph agent with Neon tools integration

Let's build a LangGraph agent that can provision a Neon database and interact with it using SQL queries. This agent will use the LangGraph framework to manage the agent's state and workflow, while functions derived from Neon's API will handle the database operations.

### Setting up the project

Create a new Python project directory and navigate to it:

```bash
mkdir langgraph-neon-example
cd langgraph-neon-example
```

Creating a virtual environment is strongly recommended to manage project dependencies in isolation. Use `venv` to create a virtual environment within your project directory:

```bash
python3 -m venv venv
source venv/bin/activate   # For macOS/Linux. On Windows, use `venv\Scripts\activate`
```

### Installing required libraries

Next, install the necessary Python libraries for this project. Create a file named `requirements.txt` in your project directory and add the following dependencies:

```
python-dotenv
neon-api
psycopg2-binary
langchain
langgraph
langchain-google-genai
```

<Admonition type="note">
`neon-api` is the [Python wrapper for Neon's API](https://github.com/neondatabase/neon-api-python).
</Admonition>

Install these libraries using pip:

```bash
pip install -r requirements.txt
```

### Configuring API keys in `.env`

For secure API key management, create a `.env` file in your project directory and add your API keys as environment variables:

```env
GOOGLE_API_KEY=YOUR_GOOGLE_API
NEON_API_KEY=YOUR_NEON_API_KEY
```

**Replace the placeholders** `YOUR_GOOGLE_API_KEY` and `YOUR_NEON_API_KEY` with the actual API keys you obtained in the [Prerequisites](#prerequisites) section.

<Admonition type="note">
    It is crucial to add `.env` to your `.gitignore` file if you are using Git for version control. This prevents your API keys from being inadvertently exposed in your code repository.
</Admonition>

### Creating the `main.py` script

Now, let's create the `main.py` script that defines the LangGraph agent and its interactions with Neon.

```python
import os

import psycopg2
from dotenv import load_dotenv
from langchain_core.tools import tool
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import SystemMessage
from langgraph.prebuilt import create_react_agent
from neon_api import NeonAPI
from psycopg2.extras import RealDictCursor

load_dotenv()

neon_client = NeonAPI(
    api_key=os.environ["NEON_API_KEY"],
)


@tool
def create_database(project_name: str) -> str:
    """
    Creates a new Neon project. (this takes less than 500ms)
    Args:
        project_name: Name of the project to create
    Returns:
        the connection URI for the new project
    """
    try:
        project = neon_client.project_create(project={"name": project_name}).project
        connection_uri = neon_client.connection_uri(
            project_id=project.id, database_name="neondb", role_name="neondb_owner"
        ).uri

        return f"Project/database created, connection URI: {connection_uri}"
    except Exception as e:
        return f"Failed to create project: {str(e)}"


@tool
def run_sql_query(connection_uri: str, query: str) -> str:
    """
    Runs an SQL query in the Neon database.
    Args:
        connection_uri: The connection URI for the Neon database
        query: The SQL query to execute
    Returns:
        the result of the SQL query
    """
    conn = psycopg2.connect(connection_uri)
    cur = conn.cursor(cursor_factory=RealDictCursor)
    try:
        cur.execute(query)
        conn.commit()
        try:
            records = cur.fetchall()
            return f"Query result: {records}"
        except psycopg2.ProgrammingError:
            return f"Query executed successfully"
    except Exception as e:
        conn.rollback()
        return f"Failed to execute SQL query: {str(e)}"
    finally:
        cur.close()
        conn.close()


available_tools = [create_database, run_sql_query]

system_prompt = SystemMessage(
    f"You are a helpful AI assistant. You will be assisting users with all of your available tools. You can help users by using the following tools: {', '.join([f"\n{tool.name}: {tool.description}" for tool in available_tools])}."
)

model = ChatGoogleGenerativeAI(model="gemini-2.0-flash")
agent_graph = create_react_agent(
    model=model,
    tools=available_tools,
    prompt=system_prompt,
)

inputs = {
    "messages": [
        (
            "user",
            "Create a new Neon project called langgraph and create a table named users. Add 10 sample records to the table. Then print the records as a markdown table.",
        )
    ]
}
result = agent_graph.invoke(inputs)

print("Step by Step execution : ")
for message in result["messages"]:
    print(message.pretty_repr())
```

Let's examine the code step by step to understand each component.

#### Import necessary libraries

```python
import os

import psycopg2
from dotenv import load_dotenv
from langchain_core.tools import tool
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import SystemMessage
from langgraph.prebuilt import create_react_agent
from neon_api import NeonAPI
from psycopg2.extras import RealDictCursor

load_dotenv()

neon_client = NeonAPI(
    api_key=os.environ["NEON_API_KEY"],
)
```

This section imports all the required Python libraries such as `os`, `psycopg2`, `dotenv`, and LangChain modules. These libraries are essential for interacting with Neon, managing environment variables, and creating LangGraph agents.

#### Define `create_database` tool

```python
@tool
def create_database(project_name: str) -> str:
    """
    Creates a new Neon project. (this takes less than 500ms)
    Args:
        project_name: Name of the project to create
    Returns:
        the connection URI for the new project
    """
    try:
        project = neon_client.project_create(project={"name": project_name}).project
        connection_uri = neon_client.connection_uri(
            project_id=project.id, database_name="neondb", role_name="neondb_owner"
        ).uri

        return f"Project/database created, connection URI: {connection_uri}"
    except Exception as e:
        return f"Failed to create project: {str(e)}"
```

This Python function, decorated with LangChain's `@tool`, allows LangGraph agents to create a Neon project and retrieve its connection URI. It takes a `project_name` as input and uses `neon_client` to interact with the Neon API. If the project is created successfully, the function returns the connection URI; otherwise, it returns an error message.

#### Define `run_sql_query` tool

```python
@tool
def run_sql_query(connection_uri: str, query: str) -> str:
    """
    Runs an SQL query in the Neon database.
    Args:
        connection_uri: The connection URI for the Neon database
        query: The SQL query to execute
    Returns:
        the result of the SQL query
    """
    conn = psycopg2.connect(connection_uri)
    cur = conn.cursor(cursor_factory=RealDictCursor)
    try:
        cur.execute(query)
        conn.commit()
        try:
            records = cur.fetchall()
            return f"Query result: {records}"
        except psycopg2.ProgrammingError:
            return f"Query executed successfully"
    except Exception as e:
        conn.rollback()
        return f"Failed to execute SQL query: {str(e)}"
    finally:
        cur.close()
        conn.close()
```

Similarily, `run_sql_query` decorated with LangChain's `@tool`, executes an SQL query on a Neon database. It takes a `connection_uri` and a `query` as input, connects to the database using `psycopg2`, and runs the query. If the query returns results, they are fetched and returned; otherwise, a success message is provided. In case of an error, the function rolls back the transaction and returns an error message.

#### Define agent setup and graph invocation

```python
available_tools = [create_database, run_sql_query]

system_prompt = SystemMessage(
    f"You are a helpful AI assistant. You will be assisting users with all of your available tools. You can help users by using the following tools: {', '.join([f"\n{tool.name}: {tool.description}" for tool in available_tools])}."
)

model = ChatGoogleGenerativeAI(model="gemini-2.0-flash")
agent_graph = create_react_agent(
    model=model,
    tools=available_tools,
    prompt=system_prompt,
)

inputs = {
    "messages": [
        (
            "user",
            "Create a new Neon project called langgraph and create a table named users. Add 10 sample records to the table. Then print the records as a markdown table.",
        )
    ]
}
result = agent_graph.invoke(inputs)

print("Step by Step execution : ")
for message in result["messages"]:
    print(message.pretty_repr())
```

This is the core part of the script where the LangGraph agent is set up and invoked. Let's break down this section:

- `available_tools = [create_database, run_sql_query]`: Creates a list of tools that will be made available to the agent. This list includes the `create_database` and `run_sql_query` functions defined earlier.
- `system_prompt = SystemMessage(...)`: Defines the system message for the AI agent. This message is crucial as it sets the agent's persona and provides instructions on how to use the available tools. It dynamically lists the tools and their descriptions in the prompt, instructing the agent on its capabilities.
- `model = ChatGoogleGenerativeAI(model="gemini-2.0-flash")`: Initializes the language model that will power the agent. Here, `ChatGoogleGenerativeAI` is used to specify Google's Gemini `gemini-2.0-flash` model.
- `agent_graph = create_react_agent(...)`: This line is where the LangGraph agent is created using the `create_react_agent` function.
  - `model=model`: Specifies the language model (`gemini-2.0-flash`) to be used by the agent for reasoning and generating responses.
  - `tools=available_tools`: Passes the list of tools (`create_database`, `run_sql_query`) that the agent can use.
  - `prompt=system_prompt`: Sets the system prompt that defines the agent's behavior and instructions.
- `inputs = {"messages": [...]}`: Defines the input to the agent. In this case, it's a user message asking the agent to perform a series of database tasks: create a Neon project, create a table named `users`, add 10 sample records, and then print these records as a Markdown table.
- `result = agent_graph.invoke(inputs)`: Invokes the LangGraph agent with the specified input. This starts the agent's execution, processing the user's request and orchestrating the use of tools to fulfill the task. The `invoke` method runs the agent and returns the final state, which includes the conversation history and the outcomes of the agent's actions.
- `for message in result["messages"]: print(message.pretty_repr())`: Iterates through the messages in the `result["messages"]` list, which contains the history of the agent's conversation and actions. `message.pretty_repr()` is used to print each message in a human-readable format, showing the step-by-step execution of the agent's thought process and actions.

#### Visualizing the agent's graph (optional)

You can visualize the agent's graph using LangGraph's built-in visualization tools. To do this, add the following code snippet at the end of the `main.py` script:

```python
with open("graph.png", "wb") as f:
    f.write(agent_graph.get_graph().draw_mermaid_png())
```

<img width="200" height="200" src="/docs/guides/langgraph-graph.png" alt="LangGraph Agent Graph" />

The graph visually represents the cyclical workflow of the LangGraph agent. Let's break down the key components of the graph:

- **`__start__` Node:** This is the entry point of the graph. Execution begins here when a task is initiated. It represents the starting point of the agent's workflow.

- **`agent` Node:** This node represents the core reasoning component of the agent, powered by the Gemini model.
  - **Decision Point:** The `agent` node is responsible for processing user input and deciding the next course of action. It determines whether to:
    - **Engage tools:** If the task requires database operations (like creating a project or running SQL queries), the agent decides to use the available tools. This is represented by the dotted line leading to the `tools` node.
    - **Respond directly:** If the agent can directly answer the user or has completed the task without needing further tool use, it can proceed to the `__end__` node. This is represented by the dotted line leading directly to the `__end__` node.

- **`tools` Node:** This node is activated when the `agent` node decides to use a tool.
  - **Tool execution:** Within the `tools` node, the appropriate tool (either `create_database` or `run_sql_query` in this example) is executed based on the agent's decision.
  - **Feedback loop:** After executing the tool and obtaining results, the workflow loops back to the `agent` node (solid line). This allows the agent to process the tool's output, reason further, and decide on the next step based on the new information. This loop is central to the ReAct (Reason and Act) pattern, enabling iterative problem-solving.

- **`__end__` Node:** This is the termination point of the graph. When the workflow reaches this node, it signifies that the agent has completed its task or conversation according to the defined termination conditions.

### Running the agent

You can now run the `main.py` script to execute the LangGraph agent. Run the following command in your terminal:

```bash
python main.py
```

The agent will then create a new Neon project, create a table named `users`, insert 10 sample records, and print the records as a markdown table.

Here's the entire conversation log showing the step-by-step execution of the agent:

```text shouldWrap
Step by Step execution :
================================ Human Message =================================

Create a new Neon project called langgraph and create a table named users. Add 10 sample records to the table. Then print the records as a markdown table.
================================== Ai Message ==================================

I can create a Neon project and a table named users, and add 10 sample records to the table. However, I cannot automatically print the records as a markdown table. I will need to run a query to fetch the records first, and then you can format them as a markdown table.

First, I will create the project:
Tool Calls:
  create_database (b9e5d9f7-5b8c-40f0-af6c-7ec095ba2da9)
 Call ID: b9e5d9f7-5b8c-40f0-af6c-7ec095ba2da9
  Args:
    project_name: langgraph
================================= Tool Message =================================
Name: create_database

Project/database created, connection URI: postgresql://neondb_owner:npg_HCFnoIvx5L9g@ep-broad-water-a53lox4z.us-east-2.aws.neon.tech/neondb?sslmode=require&channel_binding=require
================================== Ai Message ==================================

OK. I've created the project and the connection URI is postgresql://neondb_owner:npg_HCFnoIvx5L9g@ep-broad-water-a53lox4z.us-east-2.aws.neon.tech/neondb?sslmode=require&channel_binding=require. Now, I will create the table and add the records.
Tool Calls:
  run_sql_query (c3346333-b024-4fc5-99ba-d745e0108bb8)
 Call ID: c3346333-b024-4fc5-99ba-d745e0108bb8
  Args:
    connection_uri: postgresql://neondb_owner:npg_HCFnoIvx5L9g@ep-broad-water-a53lox4z.us-east-2.aws.neon.tech/neondb?sslmode=require&channel_binding=require
    query: CREATE TABLE users (id INT PRIMARY KEY, name VARCHAR(255), email VARCHAR(255));
================================= Tool Message =================================
Name: run_sql_query

Query executed successfully
================================== Ai Message ==================================
Tool Calls:
  run_sql_query (4be2ae12-adfe-45ed-bba3-d321073902ef)
 Call ID: 4be2ae12-adfe-45ed-bba3-d321073902ef
  Args:
    connection_uri: postgresql://neondb_owner:npg_HCFnoIvx5L9g@ep-broad-water-a53lox4z.us-east-2.aws.neon.tech/neondb?sslmode=require&channel_binding=require
    query: INSERT INTO users (id, name, email) VALUES (1, 'John Doe', 'john.doe@example.com'), (2, 'Jane Smith', 'jane.smith@example.com'), (3, 'Robert Jones', 'robert.jones@example.com'), (4, 'Emily Brown', 'emily.brown@example.com'), (5, 'Michael Davis', 'michael.davis@example.com'), (6, 'Jessica Wilson', 'jessica.wilson@example.com'), (7, 'Christopher Garcia', 'christopher.garcia@example.com'), (8, 'Ashley Rodriguez', 'ashley.rodriguez@example.com'), (9, 'Matthew Williams', 'matthew.williams@example.com'), (10, 'Brittany Miller', 'brittany.miller@example.com');
================================= Tool Message =================================
Name: run_sql_query

Query executed successfully
================================== Ai Message ==================================
Tool Calls:
  run_sql_query (f6484943-0dcc-4059-b794-2dc83ae31b1a)
 Call ID: f6484943-0dcc-4059-b794-2dc83ae31b1a
  Args:
    connection_uri: postgresql://neondb_owner:npg_HCFnoIvx5L9g@ep-broad-water-a53lox4z.us-east-2.aws.neon.tech/neondb?sslmode=require&channel_binding=require
    query: SELECT * FROM users;
================================= Tool Message =================================
Name: run_sql_query

Query result: [RealDictRow({'id': 1, 'name': 'John Doe', 'email': 'john.doe@example.com'}), RealDictRow({'id': 2, 'name': 'Jane Smith', 'email': 'jane.smith@example.com'}), RealDictRow({'id': 3, 'name': 'Robert Jones', 'email': 'robert.jones@example.com'}), RealDictRow({'id': 4, 'name': 'Emily Brown', 'email': 'emily.brown@example.com'}), RealDictRow({'id': 5, 'name': 'Michael Davis', 'email': 'michael.davis@example.com'}), RealDictRow({'id': 6, 'name': 'Jessica Wilson', 'email': 'jessica.wilson@example.com'}), RealDictRow({'id': 7, 'name': 'Christopher Garcia', 'email': 'christopher.garcia@example.com'}), RealDictRow({'id': 8, 'name': 'Ashley Rodriguez', 'email': 'ashley.rodriguez@example.com'}), RealDictRow({'id': 9, 'name': 'Matthew Williams', 'email': 'matthew.williams@example.com'}), RealDictRow({'id': 10, 'name': 'Brittany Miller', 'email': 'brittany.miller@example.com'})]
================================== Ai Message ==================================

| id | name               | email                        |
| -- | ------------------ | ---------------------------- |
| 1  | John Doe           | john.doe@example.com         |
| 2  | Jane Smith          | jane.smith@example.com        |
| 3  | Robert Jones        | robert.jones@example.com       |
| 4  | Emily Brown         | emily.brown@example.com        |
| 5  | Michael Davis       | michael.davis@example.com      |
| 6  | Jessica Wilson      | jessica.wilson@example.com     |
| 7  | Christopher Garcia  | christopher.garcia@example.com |
| 8  | Ashley Rodriguez    | ashley.rodriguez@example.com   |
| 9  | Matthew Williams    | matthew.williams@example.com   |
| 10 | Brittany Miller     | brittany.miller@example.com    |
```

Notice how with just a single user input, the agent orchestrates multiple tools to create a Neon project, provision a database, create a table, insert records, and query the records. The agent finally formats the query results as a markdown table as requested.

### Verifying the agent's actions

You can verify the successful completion of the task by checking the [Neon Console](https://console.neon.tech/). The `langgraph` project should have been created, and the `users` table should contain 10 sample records.

![Output in Neon console](/docs/guides/langgraph-neon-console.png)

**Congratulations!** You have successfully built and executed a LangGraph agent that interacts with tools (Neon API here) to perform actions based on user input. This example serves as a foundation for creating more complex AI agents and workflows, enabling you to automate a wide range of tasks and processes.

You can find the source code for the application described in this guide on GitHub.

<DetailIconCards>
    <a href="https://github.com/neondatabase-labs/langgraph-neon-example" description="LangGraph + Neon AI agent example" icon="github">AI Agent with LangGraph and Neon</a>
</DetailIconCards>

## Conclusion

This guide has provided an introductory exploration into building AI agents using LangGraph and Neon. You have now seen how to construct an agent capable of performing database operations within Neon, driven by natural language commands. This example illustrates the fundamental structure of LangGraph, demonstrating its approach to managing workflows through interconnected [Nodes](https://langchain-ai.github.io/langgraph/concepts/low_level/#nodes) and [Edges](https://langchain-ai.github.io/langgraph/concepts/low_level/#edges).

While this guide covers the basics, LangGraph offers a range of features for developing more advanced applications. To expand your understanding and capabilities, it is recommended to further investigate several key aspects of the framework. [Checkpointers](https://langchain-ai.github.io/langgraph/concepts/persistence) provide a mechanism for state persistence, enabling agents to retain context across sessions and resume operations. The use of [Command](https://langchain-ai.github.io/langgraph/how-tos/command) objects allows for control over workflow and state updates within nodes, enhancing agent responsiveness.

For optimizing application performance and user experience, LangGraph supports [Streaming](https://langchain-ai.github.io/langgraph/concepts/streaming), which can provide real-time outputs and token-by-token updates during agent execution. Understanding the [Recursion Limit](https://langchain-ai.github.io/langgraph/concepts/low_level/#recursion-limit) is important for managing the execution depth of complex workflows and ensuring predictable behavior. Furthermore, [Human-in-the-Loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop) workflows, facilitated by the `interrupt` function and `Command` objects, offer options for integrating human oversight into agent processes, which may be necessary for applications requiring validation or control. Consider adding it for `DELETE` and `UPDATE` operations to the `run_sql_query` tool to enhance the agent's capabilities.

## Resources

- [LangGraph GitHub](https://github.com/langchain-ai/langgraph)
- [LangGraph Documentation](https://python.langchain.com/docs/langgraph)
- [LangGraph Conceptual Guide](https://langchain-ai.github.io/langgraph/concepts)
- [LangGraph Glossary](https://langchain-ai.github.io/langgraph/concepts/low_level/#langgraph-glossary)
- [Neon Documentation](/docs)
- [Neon API Reference](https://api-docs.neon.tech/reference/getting-started-with-neon-api)
- [Neon API keys](/docs/manage/api-keys#creating-api-keys)

<NeedHelp/>


# Implementing Fine-Grained Authorization in Laravel with Neon Postgres

---
title: Implementing Fine-Grained Authorization in Laravel with Neon Postgres
subtitle: Learn how to set up and utilize Laravel's powerful authorization features to create a secure and flexible application using Neon's high-performance database.
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-07-14T00:00:00.000Z'
updatedOn: '2024-07-14T00:00:00.000Z'
---

Laravel provides an authorization system that allows developers to implement fine-grained access control in their applications. While Laravel's built-in features are powerful, some projects require even more advanced role-based access control (RBAC). This is where third-party packages like Spatie's Laravel Permission come into play.

In this guide, we'll walk through the process of setting up fine-grained authorization in a Laravel application using Neon Postgres. We'll start with Laravel's native authorization features, including Gates and Policies, and then expand our implementation to incorporate Spatie's Laravel Permission package for more sophisticated RBAC capabilities.

By the end of this tutorial, you'll have a good understanding of how to create a flexible and secure authorization system that can scale with your application's needs.

## Prerequisites

Before we begin, make sure you have the following:

- PHP 8.1 or higher installed on your system
- [Composer](https://getcomposer.org/) for managing PHP dependencies
- A [Neon](https://console.neon.tech/signup) account for database hosting
- Basic knowledge of Laravel and its authentication system

## Setting up the Project

Let's start by creating a new Laravel project and configuring it to use Neon Postgres.

### Creating a New Laravel Project

Open your terminal and run the following command to create a new Laravel project:

```bash
composer create-project laravel/laravel laravel-auth-demo
cd laravel-auth-demo
```

This will create a new Laravel project in a directory named `laravel-auth-demo` and navigate you into the project directory.

### Connecting to Neon Database

Update your `.env` file with your Neon database credentials:

```env
DB_CONNECTION=pgsql
DB_HOST=your-neon-hostname.neon.tech
DB_PORT=5432
DB_DATABASE=your_database_name
DB_USERNAME=your_username
DB_PASSWORD=your_password
```

Make sure to replace the placeholders with your actual Neon database details.

## Understanding Laravel's Authorization System

Laravel's authorization system is built on two main concepts: [Gates](https://laravel.com/docs/11.x/authorization#gates) and [Policies](https://laravel.com/docs/11.x/authorization#creating-policies).

- **Gates** are Closure-based approaches to authorization. They provide a simple, Closure-based method of authorizing actions. Gates are ideal for simple checks that don't necessarily relate to a specific model or resource. They can be thought of as general-purpose authorization checks that can be used throughout your application.

- **Policies** are classes that organize authorization logic around a particular model or resource. They encapsulate the logic for authorizing actions on a specific type of model. Policies are particularly useful when you have multiple authorization checks related to a single model, as they help keep your authorization logic organized and maintainable.

For fine-grained authorization, we'll primarily focus on Policies, as they provide a more structured and scalable approach for complex applications. Policies allow you to group related authorization logic together, making it easier to manage and understand the permissions associated with each model in your application.

That said, Gates can still play an important role in your authorization strategy. They're great for defining broader, application-wide permissions that aren't tied to a specific model. You might use Gates for actions like "access admin dashboard" or "manage site settings".

It's worth noting that Laravel's authorization system is deeply integrated with its authentication system. This means you can easily check a user's permissions within your controllers, views, and even database queries. Whether you're using Gates or Policies, you'll find that Laravel provides a consistent and intuitive API for performing authorization checks throughout your application.

## Implementing Fine-Grained Authorization

Let's implement a fine-grained authorization system for a blog application where users can create, read, update, and delete posts.

### Setting up the Post Model and Migration

For the purpose of this guide, we'll create a `Post` model with basic fields like `title`, `content`, and `is_published`. We'll also associate each post with a user.

First, let's create a `Post` model with its migration:

```bash
php artisan make:model Post -m
```

Update the migration file in `database/migrations` to define the structure of our `posts` table:

```php
public function up()
{
    Schema::create('posts', function (Blueprint $table) {
        $table->id();
        $table->foreignId('user_id')->constrained()->onDelete('cascade');
        $table->string('title');
        $table->text('content');
        $table->boolean('is_published')->default(false);
        $table->timestamps();
    });
}
```

Run the migration:

```bash
php artisan migrate
```

This will create a `posts` table in your Neon Postgres database.

### Creating a Policy for Posts

Now, let's create a policy for the `Post` model:

```bash
php artisan make:policy PostPolicy --model=Post
```

This command creates a new policy class in `app/Policies/PostPolicy.php`. Let's update it with our authorization logic:

```php
<?php

namespace App\Policies;

use App\Models\Post;
use App\Models\User;
use Illuminate\Auth\Access\HandlesAuthorization;

class PostPolicy
{
    use HandlesAuthorization;

    public function viewAny(User $user)
    {
        return true; // Allow all users to view the list of posts
    }

    public function view(User $user, Post $post)
    {
        return true; // Allow all users to view individual posts
    }

    public function create(User $user)
    {
        return true; // Allow all authenticated users to create posts
    }

    public function update(User $user, Post $post)
    {
        return $user->id === $post->user_id; // Allow only the author to update the post
    }

    public function delete(User $user, Post $post)
    {
        return $user->id === $post->user_id; // Allow only the author to delete the post
    }

    public function publish(User $user, Post $post)
    {
        // Allow only the author or an admin to publish the post
        return $user->id === $post->user_id || $user->is_admin;
    }
}
```

Rundown of the `PostPolicy` class:

1. `viewAny` method: This allows all users to view the list of posts. This could be useful for a public blog where anyone can see the list of available posts.

2. `view` method: Similar to `viewAny`, this allows all users to view individual posts. Again, this is suitable for a public blog where post content is accessible to everyone.

3. `create` method: This permits all authenticated users to create posts. It assumes that any logged-in user should be able to write a post.

4. `update` method: This method only allows the author of the post to update it. It compares the ID of the current user with the user ID associated with the post.

5. `delete` method: Similar to `update`, this method restricts deletion to the author of the post. This ensures that users can only delete their own posts.

6. `publish` method: This introduces a more complex authorization rule. It allows either the author of the post or an admin user to publish the post. This is useful for blogs where posts might need approval before being made public.

Each method in this policy corresponds to a specific action that can be performed on a Post model. The methods return boolean values: `true` if the action is allowed, and `false` if it's not.

This policy provides a fine-grained control over post-related actions, ensuring that users can only perform actions they're authorized to do. It's a good example of how policies can encapsulate complex authorization logic in a clean, readable way.

### Registering the Policy

By default, Laravel 11.x and later versions automatically discover policies. However, if you're using an older version, you might need to manually register the policy in the `AuthServiceProvider`.

To automatically discover policies, your policies should be in the `app/Policies` directory and follow the naming convention of `ModelNamePolicy`. Laravel will automatically associate the policy with the corresponding model.

To manually register the policy, add the following line to the `boot` method of your `AuthServiceProvider`:

```php
use App\Models\Post;
use App\Policies\PostPolicy;

public function boot(): void
{
    Gate::policy(Post::class, PostPolicy::class);
}
```

This line tells Laravel to use the `PostPolicy` class for authorization checks related to the `Post` model.

### Implementing Role-Based Access Control

To support our `is_admin` flag and implement basic role-based access control, let's update our `users` table.

To do that, create a new migration to add the `is_admin` column:

```bash
php artisan make:migration add_is_admin_to_users_table
```

Update the migration file:

```php
public function up()
{
    Schema::table('users', function (Blueprint $table) {
        $table->boolean('is_admin')->default(false);
    });
}

public function down()
{
    Schema::table('users', function (Blueprint $table) {
        $table->dropColumn('is_admin');
    });
}
```

Run the migration to add the `is_admin` column to the `users` table:

```bash
php artisan migrate
```

This column will allow us to differentiate between regular users and administrators.

### Using Authorization in Controllers

Now that we have our policy set up, let's use it in a controller. Create a new `PostController`:

```bash
php artisan make:controller PostController --resource
```

Update the `PostController` with authorization checks:

```php
<?php

namespace App\Http\Controllers;

use App\Models\Post;
use Illuminate\Http\Request;

class PostController extends Controller
{
    public function __construct()
    {
        $this->authorizeResource(Post::class, 'post');
    }

    public function index()
    {
        $posts = Post::all();
        return view('posts.index', compact('posts'));
    }

    public function show(Post $post)
    {
        return view('posts.show', compact('post'));
    }

    public function create()
    {
        return view('posts.create');
    }

    public function store(Request $request)
    {
        $validatedData = $request->validate([
            'title' => 'required|max:255',
            'content' => 'required',
        ]);

        $post = auth()->user()->posts()->create($validatedData);

        return redirect()->route('posts.show', $post);
    }

    public function edit(Post $post)
    {
        return view('posts.edit', compact('post'));
    }

    public function update(Request $request, Post $post)
    {
        $validatedData = $request->validate([
            'title' => 'required|max:255',
            'content' => 'required',
        ]);

        $post->update($validatedData);

        return redirect()->route('posts.show', $post);
    }

    public function destroy(Post $post)
    {
        $post->delete();

        return redirect()->route('posts.index');
    }

    public function publish(Post $post)
    {
        $this->authorize('publish', $post);

        $post->update(['is_published' => true]);

        return redirect()->route('posts.show', $post);
    }
}
```

The `__construct` method uses the `authorizeResource` method to automatically authorize resource controller methods. We've also added a `publish` method with a manual authorization check.

Alternatively, you can use the `authorize` method within each controller method to perform authorization checks manually. This method takes the name of the policy method to call and the model to authorize against:

```php
$this->authorize('publish', $post);
```

This line checks if the current user is authorized to publish the post. If not, Laravel will throw an `AuthorizationException` preventing the action from being executed.

### Using Authorization in Views

In your Blade views, you can use the `@can` directive to conditionally show or hide elements based on the user's permissions. For example, in `resources/views/posts/show.blade.php`:

```html
<h1>{{ $post->title }}</h1>
<p>{{ $post->content }}</p>

@can('update', $post)
<a href="{{ route('posts.edit', $post) }}">Edit Post</a>
@endcan @can('delete', $post)
<form action="{{ route('posts.destroy', $post) }}" method="POST">
  @csrf @method('DELETE')
  <button type="submit">Delete Post</button>
</form>
@endcan @can('publish', $post) @if(!$post->is_published)
<form action="{{ route('posts.publish', $post) }}" method="POST">
  @csrf
  <button type="submit">Publish Post</button>
</form>
@endif @endcan
```

Rundown of the Blade view:

1. The view starts by displaying the post's title and content, which are accessible to all users as per our policy.

2. `@can('update', $post)` directive: This checks if the current user is authorized to update the post. If true, it displays an "Edit Post" link. This corresponds to the `update` method in our `PostPolicy`.

3. `@can('delete', $post)` directive: Similar to the update check, this verifies if the user can delete the post. If authorized, it shows a delete form with a submit button. This uses the `delete` method from our policy.

4. `@can('publish', $post)` directive: This checks if the user can publish the post, corresponding to the `publish` method in our policy.

5. Inside the publish check, there's an additional `@if(!$post->is_published)` condition. This ensures the publish button only appears if the post isn't already published.

6. Each form includes a `@csrf` directive for CSRF protection, which is a security feature in Laravel to prevent cross-site request forgery attacks.

7. The delete form also includes `@method('DELETE')`, which is Laravel's way of spoofing HTTP methods that aren't supported by HTML forms (like DELETE, PUT, PATCH).

By using these directives, you can conditionally display elements based on the user's permissions, providing a tailored experience for each user based on their role and authorization level.

## Integrating Spatie's Laravel Permission for Advanced RBAC

While Laravel's built-in authorization system provides you with a good foundation for managing permissions and policies, you might require more complex role and permission structures.

Spatie's Laravel Permission package provides a solution for implementing advanced RBAC (Role-Based Access Control) in your Laravel application.

### Installing Spatie Laravel Permission

First, let's install the package using Composer:

```bash
composer require spatie/laravel-permission
```

After installation, publish the package's configuration and migration files:

```bash
php artisan vendor:publish --provider="Spatie\Permission\PermissionServiceProvider"
```

This command will create a `config/permission.php` file and a migration file in your `database/migrations` directory.

Run the migrations to create the necessary tables in your Neon Postgres database:

```bash
php artisan migrate
```

This will create the permissions, roles, and model_has_roles tables in your database.

### Configuring the Package

The package's configuration file is located at `config/permission.php`. For most use cases, the default configuration works well. However, you can customize it based on your needs. For example, you can change the table names or add a cache expiration time.

### Setting Up Roles and Permissions

Let's create some roles and permissions for our blog application. We'll do this in a seeder for easy setup and testing.

Create a new seeder:

```bash
php artisan make:seeder RolesAndPermissionsSeeder
```

Update the seeder file (`database/seeders/RolesAndPermissionsSeeder.php`):

```php
<?php

namespace Database\Seeders;

use Illuminate\Database\Seeder;
use Spatie\Permission\Models\Role;
use Spatie\Permission\Models\Permission;

class RolesAndPermissionsSeeder extends Seeder
{
    public function run()
    {
        // Reset cached roles and permissions
        app()[\Spatie\Permission\PermissionRegistrar::class]->forgetCachedPermissions();

        // Create permissions
        Permission::create(['name' => 'view posts']);
        Permission::create(['name' => 'create posts']);
        Permission::create(['name' => 'edit posts']);
        Permission::create(['name' => 'delete posts']);
        Permission::create(['name' => 'publish posts']);
        Permission::create(['name' => 'unpublish posts']);

        // Create roles and assign permissions
        $role = Role::create(['name' => 'writer']);
        $role->givePermissionTo(['view posts', 'create posts', 'edit posts', 'delete posts']);

        $role = Role::create(['name' => 'editor']);
        $role->givePermissionTo(['view posts', 'create posts', 'edit posts', 'delete posts', 'publish posts', 'unpublish posts']);

        $role = Role::create(['name' => 'admin']);
        $role->givePermissionTo(Permission::all());
    }
}
```

Update your `DatabaseSeeder.php` to include this new seeder:

```php
public function run()
{
    $this->call([
        RolesAndPermissionsSeeder::class,
    ]);
}
```

Run the seeder:

```bash
php artisan db:seed
```

This will create roles for `writer`, `editor`, and `admin`, along with permissions for viewing, creating, editing, deleting, publishing, and unpublishing posts.

### Updating the User Model

To use the package, your `User` model should use the `HasRoles` trait. Update your `app/Models/User.php`:

```php {1-1}
use Spatie\Permission\Traits\HasRoles;

class User extends Authenticatable
{
    use HasFactory, Notifiable, HasRoles;

    // ...
}
```

This trait provides methods for assigning and checking roles and permissions for users in your application.

### Implementing RBAC in Controllers

Now, let's update our `PostController` to use the new permissions:

```php {10-17}
<?php

namespace App\Http\Controllers;

use App\Models\Post;
use Illuminate\Http\Request;

class PostController extends Controller
{
    public function __construct()
    {
        $this->middleware('permission:view posts')->only('index', 'show');
        $this->middleware('permission:create posts')->only('create', 'store');
        $this->middleware('permission:edit posts')->only('edit', 'update');
        $this->middleware('permission:delete posts')->only('destroy');
        $this->middleware('permission:publish posts')->only('publish');
    }

    // ... other methods remain the same

    public function publish(Post $post)
    {
        $post->update(['is_published' => true]);
        return redirect()->route('posts.show', $post);
    }
}
```

The `__construct` method now uses middleware to check permissions for each controller action. This ensures that only users with the appropriate permissions can access the corresponding methods.

### Using RBAC in Blade Templates

You can use the package's directives in your Blade templates to show or hide elements based on the user's roles and permissions:

```html
@can('edit posts')
<a href="{{ route('posts.edit', $post) }}">Edit Post</a>
@endcan @role('admin')
<a href="{{ route('admin.dashboard') }}">Admin Dashboard</a>
@endrole @hasanyrole('writer|editor')
<a href="{{ route('posts.create') }}">Create New Post</a>
@endhasanyrole
```

You can also use the `@canany` directive to check if the user has any of the specified permissions:

```html
@canany(['edit posts', 'delete posts'])
<form action="{{ route('posts.destroy', $post) }}" method="POST">
  @csrf @method('DELETE')
  <button type="submit">Delete Post</button>
</form>
@endcanany
```

### Dynamic Role and Permission Assignment

Besides seeding roles and permissions, you can also assign roles and permissions dynamically based on user actions. For example, you might assign the `writer` role to users who have published a certain number of posts.

Here's an example of how you can assign roles and permissions dynamically in your controllers:

```php
public function assignRole(User $user, Request $request)
{
    $validatedData = $request->validate([
        'role' => 'required|exists:roles,name',
    ]);

    $user->assignRole($validatedData['role']);

    return back()->with('success', 'Role assigned successfully');
}

public function revokeRole(User $user, Request $request)
{
    $validatedData = $request->validate([
        'role' => 'required|exists:roles,name',
    ]);

    $user->removeRole($validatedData['role']);

    return back()->with('success', 'Role revoked successfully');
}
```

Besides the `removeRole` and `assignRole` methods, the package provides other methods for managing roles and permissions, such as `syncRoles`, `givePermissionTo`, and `revokePermissionTo` for more advanced use cases.

### Optimizing RBAC Performance with Neon Postgres

When working with RBAC, especially in larger applications, you might encounter performance issues due to the increased number of database queries.

Here are some tips to optimize performance when using Spatie Laravel Permission with Neon Postgres:

1. **Caching**: Enable caching in the package's configuration to reduce database queries:

   ```php
   // In config/permission.php
   'cache' => [
       'expiration_time' => \DateInterval::createFromDateString('24 hours'),
       'key' => 'spatie.permission.cache',
       'model_key' => 'name',
       'store' => 'default',
   ],
   ```

2. **Eager Loading**: When fetching users with their roles and permissions, use eager loading:

   ```php
   $users = User::with('roles', 'permissions')->get();
   ```

3. **Indexing**: Ensure that the `model_id` and `model_type` columns in the `model_has_roles` and `model_has_permissions` tables are properly indexed. For more information on indexing, refer to the [Neon Postgres documentation](/docs/postgres/indexes).

4. **Minimize Permission Checks**: Instead of checking individual permissions, consider using roles or permission groups to reduce the number of checks you do on each request.

5. **Use Database-Level Permissions**: For very large-scale applications, consider implementing some permissions at the database level using [Neon Postgres's role-based access control features](/blog/the-non-obviousness-of-postgres-roles).

## Conclusion

In this guide, we've implemented a fine-grained authorization system in Laravel using Policies and Gates. We've covered creating and registering policies, implementing role-based access control, using authorization in controllers and views, and testing our authorization rules.

This implementation provides a solid foundation for a secure application, but there are always ways to enhance and expand its functionality.

For more complex applications, Spatie's Laravel Permission package provides a flexible way to implement advanced RBAC in your Laravel application.

## Additional Resources

- [Laravel Authorization Documentation](https://laravel.com/docs/11.x/authorization)
- [Laravel Policies](https://laravel.com/docs/11.x/authorization#creating-policies)
- [Laravel Gates](https://laravel.com/docs/11.x/authorization#gates)
- [Spatie Laravel Permission Documentation](https://spatie.be/docs/laravel-permission)
- [Neon Postgres Documentation](/docs)


# Building a CRUD API with Laravel and Sanctum

---
title: Building a CRUD API with Laravel and Sanctum
subtitle: Learn how to create a robust, secure CRUD API using Laravel and Laravel Sanctum for authentication
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-07-01T00:00:00.000Z'
updatedOn: '2024-07-01T00:00:00.000Z'
---

Laravel is a powerful PHP framework that allows developers to easily build web applications and APIs.

In this guide, we'll walk through the process of creating a CRUD (Create, Read, Update, Delete) API using Laravel, and we'll implement authentication using [Laravel Sanctum](https://laravel.com/docs/11.x/sanctum).

By the end of this tutorial, you'll have a fully functional API that allows authenticated users to perform CRUD operations on a resource. We'll use a 'Task' model as our example resource and implement the necessary endpoints to manage tasks.

## Prerequisites

Before we begin, ensure you have the following:

- PHP 8.1 or higher installed on your system
- [Composer](https://getcomposer.org/) for managing PHP dependencies
- A [Neon](https://console.neon.tech/signup) account for database hosting
- Basic knowledge of Laravel and RESTful API principles

## Setting up the Project

Let's start by creating a new Laravel project and setting up the necessary components.

### Creating a New Laravel Project

Open your terminal and run the following command to create a new Laravel project:

```bash
composer create-project laravel/laravel laravel-crud-api
cd laravel-crud-api
```

This will create a new Laravel project in a directory named `laravel-crud-api` and install all the necessary dependencies.

### Setting up the Database

Update your `.env` file with your Neon database credentials:

```env
DB_CONNECTION=pgsql
DB_HOST=your-neon-hostname.neon.tech
DB_PORT=5432
DB_DATABASE=your_database_name
DB_USERNAME=your_username
DB_PASSWORD=your_password
```

Make sure to replace `your-neon-hostname`, `your_database_name`, `your_username`, and `your_password` with your actual database credentials.

### Installing Laravel Sanctum

Laravel Sanctum provides a featherweight authentication system for SPAs and simple APIs. To install it, all you need to do is use the following command:

```bash
php artisan install:api
```

If you get asked to run all pending migrations, type `yes` and press Enter. Else, run the migrations to create the necessary tables:

```bash
php artisan migrate
```

This will create the necessary tables for Sanctum to work.

## Creating the Task Model and Migration

As mentioned earlier, for our CRUD API, we'll use a 'Task' model as our example resource. This model will have fields such as title, description, status, due date, and priority.

Let's create it along with its migration file:

```bash
php artisan make:model Task -m
```

Once created, open the newly created migration file in `database/migrations` and update it to include the necessary columns for the 'tasks' table:

```php
public function up()
{
    Schema::create('tasks', function (Blueprint $table) {
        $table->id();
        $table->foreignId('user_id')->constrained()->onDelete('cascade');
        $table->string('title');
        $table->text('description')->nullable();
        $table->enum('status', ['pending', 'in_progress', 'completed'])->default('pending');
        $table->date('due_date')->nullable();
        $table->integer('priority')->default(1);
        $table->timestamps();
    });
}
```

We've defined a foreign key `user_id` to associate each task with a user. This allows us to implement user-specific tasks and ensure that each task belongs to a specific user. The `constrained()` method creates a foreign key constraint that references the `id` column of the `users` table. The `onDelete('cascade')` method ensures that when a user is deleted, all associated tasks are also deleted.

Run the migration to create the 'tasks' table:

```bash
php artisan migrate
```

After that, update the `app/Models/Task.php` model file to include the necessary fields in the `$fillable` array:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class Task extends Model
{
    use HasFactory;

    protected $fillable = ['title', 'description', 'status', 'due_date', 'priority'];

    public function user()
    {
        return $this->belongsTo(User::class);
    }
}
```

As a reference, the `fillable` property specifies which fields can be mass-assigned when creating or updating a model. This helps protect against mass assignment vulnerabilities and ensures that only the specified fields can be modified.

## Implementing API Authentication

Before we create our CRUD endpoints, let's set up authentication using Laravel Sanctum. This will allow users to register, log in, and access protected routes in our API.

### Creating the User Registration and Login Controllers

By default, Laravel comes with a few route groups like `web`, `api`, and `auth`. We'll use the `api` group for our API routes which will be protected by Sanctum.

Start by creating a controller called `AuthController` to handle user registration and login using the artisan command:

```bash
php artisan make:controller Api/AuthController
```

Then, update the `app/Http/Controllers/Api/AuthController.php` file and add the necessary methods:

```php
<?php

namespace App\Http\Controllers\Api;

use App\Http\Controllers\Controller;
use App\Models\User;
use Illuminate\Http\Request;
use Illuminate\Support\Facades\Hash;
use Illuminate\Validation\ValidationException;

class AuthController extends Controller
{
    public function register(Request $request)
    {
        $request->validate([
            'name' => 'required|string|max:255',
            'email' => 'required|string|email|max:255|unique:users',
            'password' => 'required|string|min:8|confirmed',
        ]);

        $user = User::create([
            'name' => $request->name,
            'email' => $request->email,
            'password' => Hash::make($request->password),
        ]);

        $token = $user->createToken('auth_token')->plainTextToken;

        return response()->json([
            'access_token' => $token,
            'token_type' => 'Bearer',
        ]);
    }

    public function login(Request $request)
    {
        $request->validate([
            'email' => 'required|email',
            'password' => 'required',
        ]);

        $user = User::where('email', $request->email)->first();

        if (! $user || ! Hash::check($request->password, $user->password)) {
            throw ValidationException::withMessages([
                'email' => ['The provided credentials are incorrect.'],
            ]);
        }

        $token = $user->createToken('auth_token')->plainTextToken;

        return response()->json([
            'access_token' => $token,
            'token_type' => 'Bearer',
        ]);
    }

    public function logout(Request $request)
    {
        $request->user()->currentAccessToken()->delete();

        return response()->json(['message' => 'Logged out successfully']);
    }
}
```

Rundown of the methods in the `AuthController`:

- `register`: Handles user registration. Validates the request data, creates a new user, and returns an access token.
- `login`: Handles user login. Validates the request data, checks the user credentials, and returns an access token.
- `logout`: Logs out the authenticated user by deleting the current access token.

### Setting up Authentication Routes

Update `routes/api.php` to include the authentication routes within the `api` route group:

```php
<?php

use App\Http\Controllers\Api\AuthController;
use Illuminate\Support\Facades\Route;

Route::post('/register', [AuthController::class, 'register']);
Route::post('/login', [AuthController::class, 'login']);

Route::middleware('auth:sanctum')->group(function () {
    Route::post('/logout', [AuthController::class, 'logout']);
    // We'll add our task routes here later
});
```

The `auth:sanctum` middleware protects the routes by requiring a valid access token. This ensures that only authenticated users can access the protected routes.

The `/register` and `/login` routes are public and do not require authentication. Users can register and log in to obtain an access token.

### Issuing API Tokens

To issue API tokens, we need to update the `User` model to use the `HasApiTokens` trait. Laravel ships with a default `User` model located at `app/Models/User.php`.

Let's update the `app/Models/User.php` file and add the `HasApiTokens` trait:

```php
<?php

// Existing imports
use Laravel\Sanctum\HasApiTokens;

class User extends Authenticatable
{
    // Add the HasApiTokens trait here
    use HasApiTokens, HasFactory, Notifiable;

    // Existing code
}
```

After adding the `HasApiTokens` trait, you can use the `createToken` method to generate an access token for a user. We've used this method in the `AuthController` to issue tokens during registration and login.

While we're here, let's also update the `User` model to include a relationship with the `Task` model:

```php
public function tasks()
{
    return $this->hasMany(Task::class);
}
```

This relationship allows us to retrieve all tasks associated with a user and create new tasks for a user, simplifying the task management process.

### Testing the Authentication Endpoints

To test the authentication endpoints, you can use a tool like [Postman](https://www.postman.com/) or [Insomnia](https://insomnia.rest/).

For the sake of simplicity, you can use the `curl` command in your terminal.

Let's start by testing the registration route and try to register a new user:

```bash
curl -X POST http://laravel-crud-api.test/api/register \
    -H 'Content-Type: application/json' \
    -d '{
        "name": "John Doe",
        "email": "john@example.com",
        "password": "password",
        "password_confirmation": "password"
    }'
```

> Note: Replace `laravel-crud-api.test` with your Laravel project URL.

The response should include an access token like this:

```json
{
  "access_token": "1|eyJ...your_access_token_here",
  "token_type": "Bearer"
}
```

To log in with the registered user:

```bash
curl -X POST http://laravel-crud-api.test/api/login \
    -H 'Content-Type: application/json' \
    -d '{
        "email": "john@example.com",
        "password": "password"
    }'
```

This will return another access token:

```json
{
  "access_token": "1|eyJ...your_new_access_token_here",
  "token_type": "Bearer"
}
```

With the access token, you can now access the protected routes. To log out, use the `/logout` route:

```bash
curl -X POST http://laravel-crud-api.test/api/logout \
    -H 'Authorization: Bearer <your_access_token_here>'
```

Replace `<your_access_token_here>` with the access token you received during login. This will log out the user and delete the access token and you will get a response like:

```json
{
  "message": "Logged out successfully"
}
```

## Implementing CRUD Operations

Now that we have authentication set up, let's create our CRUD operations for the Task model.

### Creating the TaskController

Generate a new controller for handling task operations:

```bash
php artisan make:controller Api/TaskController --api
```

The `--api` flag generates a controller with the necessary methods for a RESTful API. This will create a new controller file at `app/Http/Controllers/Api/TaskController.php` and will include methods like `index`, `store`, `show`, `update`, and `destroy`.

After that, update `app/Http/Controllers/Api/TaskController.php` and populate those methods with the necessary logic:

```php
<?php

namespace App\Http\Controllers\Api;

use App\Http\Controllers\Controller;
use App\Models\Task;
use Illuminate\Http\Request;

class TaskController extends Controller
{
    public function index()
    {
        $tasks = Task::all();
        return response()->json($tasks);
    }

    public function store(Request $request)
    {
        $request->validate([
            'title' => 'required|string|max:255',
            'description' => 'nullable|string',
            'status' => 'required|in:pending,in_progress,completed',
            'due_date' => 'nullable|date',
            'priority' => 'required|integer|min:1|max:5',
        ]);

        $task = $request->user()->tasks()->create($request->all());
        return new TaskResource($task);
    }

    public function show(Task $task)
    {
        return response()->json($task);
    }

    public function update(Request $request, Task $task)
    {
        $this->authorize('update', $task);

        $validated = $request->validate([
            'title' => 'sometimes|required|string|max:255',
            'description' => 'nullable|string',
            'status' => 'sometimes|required|in:pending,in_progress,completed',
            'due_date' => 'nullable|date',
            'priority' => 'sometimes|required|integer|min:1|max:5',
        ]);

        $task->update($validated);

        return new TaskResource($task);
    }

    public function destroy(Task $task)
    {
        $task->delete();
        return response()->json(null, 204);
    }
}
```

> Note: We'll create the `TaskResource` class later to transform the task model into a JSON response.

Rundown of the methods in the `TaskController`:

- `index`: Fetches all tasks. Returns a JSON response with all tasks. We'll update this method to use API Resources later.
- `store`: Creates a new task. Validates the request data, creates a new task, and returns the task as JSON.
- `show`: Fetches a single task. Returns a JSON response with the specified task.
- `update`: Updates a task. Validates the request data, updates the task, and returns the updated task as JSON.
- `destroy`: Deletes a task. Deletes the specified task and returns a 204 No Content response.

### Adding Task Routes

Once we have the `TaskController` set up, let's add the task routes to `routes/api.php` to include the task routes within the authenticated group:

```php
// Include the TaskController at the top:
use App\Http\Controllers\Api\TaskController;

Route::middleware('auth:sanctum')->group(function () {
    Route::post('/logout', [AuthController::class, 'logout']);

    // Add the task routes here:
    Route::apiResource('tasks', TaskController::class);
});
```

The `Route::apiResource` method automatically generates the necessary routes for a RESTful resource. This will create routes for `tasks` with the appropriate HTTP verbs and route names.

You can use the `php artisan route:list` command to see a list of all registered routes.

## Implementing API Resources

To provide a consistent and customizable way of transforming our models into JSON responses, let's use Laravel's API Resources.

A resource class represents a single model that needs to be transformed into a JSON structure. It allows you to customize the data that is returned when a model is converted to JSON rather than returning the entire model instance.

Generate a new resource for the Task model:

```bash
php artisan make:resource TaskResource
```

Open the `app/Http/Resources/TaskResource.php` file and update it as follows:

```php
<?php

namespace App\Http\Resources;

use Illuminate\Http\Request;
use Illuminate\Http\Resources\Json\JsonResource;

class TaskResource extends JsonResource
{
    public function toArray(Request $request): array
    {
        return [
            'id' => $this->id,
            'title' => $this->title,
            'description' => $this->description,
            'status' => $this->status,
            'due_date' => $this->due_date,
            'priority' => $this->priority,
            'created_at' => $this->created_at,
            'updated_at' => $this->updated_at,
        ];
    }
}
```

Here, we've defined the fields that should be included in the JSON response for a task and how they should be formatted. For more complex transformations, you can customize the `toArray` method as needed.

Now, update the `TaskController` to use this resource when returning task data instead of returning the raw model:

```php
<?php

namespace App\Http\Controllers\Api;

use App\Http\Resources\TaskResource;
use App\Http\Controllers\Controller;
use App\Models\Task;
use Illuminate\Http\Request;

class TaskController extends Controller
{
    public function index()
    {
        $tasks = Task::all();
        // Change this line to use the TaskResource:
        return TaskResource::collection($tasks);
    }

    public function store(Request $request)
    {
        $validated = $request->validate([
            'title' => 'required|string|max:255',
            'description' => 'nullable|string',
            'status' => 'required|in:pending,in_progress,completed',
            'due_date' => 'nullable|date',
            'priority' => 'required|integer|min:1|max:5',
        ]);

        $task = $request->user()->tasks()->create($validated);

        return new TaskResource($task);
    }

    public function show(Task $task)
    {
        return new TaskResource($task);
    }

    public function update(Request $request, Task $task)
    {
        $request->validate([
            'title' => 'required|string|max:255',
            'description' => 'nullable|string',
            'status' => 'required|in:pending,in_progress,completed',
            'due_date' => 'nullable|date',
            'priority' => 'required|integer|min:1|max:5',
        ]);

        $task->update($request->all());
        return new TaskResource($task);
    }

    public function destroy(Task $task)
    {
        $task->delete();
        return response()->json(null, 204);
    }
}
```

Now, when you fetch tasks, create a new task, or update an existing task, the response will be formatted according to the `TaskResource` class.

## Testing the API Endpoints

To test the new Task API, you can again use `curl` or a tool like Postman or Insomnia.

Let's first create a new task:

```bash
curl -X POST http://laravel-crud-api.test/api/tasks \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer <your_access_token_here>' \
    -d '{
        "title": "New Task",
        "description": "Task description",
        "status": "pending",
        "due_date": "2024-12-31",
        "priority": 2
    }'
```

As a response, you should see the newly created task:

```json
{
  "id": 1,
  "title": "New Task",
  "description": "Task description",
  "status": "pending",
  "due_date": "2024-12-31",
  "priority": 2,
  "created_at": "2024-07-01T00:00:00.000000Z",
  "updated_at": "2024-07-01T00:00:00.000000Z"
}
```

You can then fetch all tasks:

```bash
curl -X GET http://laravel-crud-api.test/api/tasks \
    -H 'Authorization: Bearer <your_access_token_here>'
```

This will return a list of tasks in JSON format.

## Adding Pagination

To improve performance and reduce payload size, we can add pagination to the task list. Laravel provides a simple way to paginate query results using the `paginate` method when fetching data. That way the response will include only a subset of tasks per page instead of the entire collection which can be large depending on the number of entries in the database.

To do that, update the `index` method in `TaskController` and change the `all` method to `paginate` followed by the number of items per page:

```php
public function index()
{
    $tasks = Task::paginate(15);
    return TaskResource::collection($tasks);
}
```

This pagination method will limit the number of tasks returned to 15 per page, significantly reducing the payload size for large datasets.

The response will now include additional pagination metadata such as the total number of items, the number of pages, and links to the next and previous pages, allowing for easy navigation through the entire collection of tasks.

## Implementing Request Classes

Request classes allow you to encapsulate request validation logic within dedicated classes. This helps keep your controller clean and improves reusability.

To keep our controller clean and improve reusability, let's create dedicated request classes for validation for creating and updating tasks:

```bash
php artisan make:request StoreTaskRequest
php artisan make:request UpdateTaskRequest
```

Update `app/Http/Requests/StoreTaskRequest.php` to include the validation rules:

```php
<?php

namespace App\Http\Requests;

use Illuminate\Foundation\Http\FormRequest;

class StoreTaskRequest extends FormRequest
{
    public function authorize()
    {
        return true;
    }

    public function rules()
    {
        return [
            'title' => 'required|string|max:255',
            'description' => 'nullable|string',
            'status' => 'required|in:pending,in_progress,completed',
            'due_date' => 'nullable|date',
            'priority' => 'required|integer|min:1|max:5',
        ];
    }
}
```

Update `app/Http/Requests/UpdateTaskRequest.php` the same way:

```php
<?php

namespace App\Http\Requests;

use Illuminate\Foundation\Http\FormRequest;

class UpdateTaskRequest extends FormRequest
{
    public function authorize()
    {
        return true;
    }

    public function rules()
    {
        return [
            'title' => 'required|string|max:255',
            'description' => 'nullable|string',
            'status' => 'required|in:pending,in_progress,completed',
            'due_date' => 'nullable|date',
            'priority' => 'required|integer|min:1|max:5',
        ];
    }
}
```

Now, we are ready to update the `TaskController` to use these request classes instead of validating the request directly in the controller methods:

```php
// Include the request classes at the top:
use App\Http\Requests\StoreTaskRequest;
use App\Http\Requests\UpdateTaskRequest;

class TaskController extends Controller
{
    // ... other methods ...

    public function store(StoreTaskRequest $request)
    {
        $task = $request->user()->tasks()->create($request->validated());
        return new TaskResource($task);
    }

    public function update(UpdateTaskRequest $request, Task $task)
    {
        $task->update($request->validated());
        return new TaskResource($task);
    }

    // ... other methods ...
}
```

The end-user will still receive the same JSON response, but the validation logic is now encapsulated within the request classes. This makes the controller cleaner and easier to maintain.

## Testing the API

To ensure our API works as expected, Laravel provides a powerful testing suite out of the box.

To learn more about testing in Laravel along with Neon branding, check out the [Testing Laravel Applications with Neon's Database Branching](/guides/laravel-test-on-branch).

## Adding API Documentation

For better developer experience, it's important to have good API documentation.

You can use a third-party package called [Scribe](https://scribe.knuckles.wtf/) to generate your API documentation.

To install Scribe, run the following command:

```bash
composer require --dev knuckleswtf/scribe
```

Publish the configuration file:

```bash
php artisan vendor:publish --tag=scribe-config
```

Then update the `config/scribe.php` file to configure the documentation settings according to your preferences.

Generate the documentation:

```bash
php artisan scribe:generate
```

This will create a `public/docs` directory with the generated API documentation. You can access it by visiting `http://laravel-crud-api.test/docs`. The generated format will also include Postman collections and OpenAPI specifications.

## Implementing API Versioning

As your API evolves, you might need to introduce breaking changes. API versioning allows you to do this without affecting existing clients.

To implement a simple versioning strategy, you can prefix your API routes with a version number. This way, you can maintain backward compatibility while introducing new features in future versions.

To do that, create a new directory for v1 of your API:

```bash
mkdir app/Http/Controllers/Api/V1
```

Move your `TaskController.php` to this new directory and update its namespace:

```php
namespace App\Http\Controllers\Api\V1;
```

Then update your `routes/api.php` file to include versioning and the new namespace:

```php
// Update the TaskController import at the top:
use App\Http\Controllers\Api\V1\TaskController;

// Update the routes to include the version prefix:
Route::prefix('v1')->group(function () {
    Route::post('/register', [AuthController::class, 'register']);
    Route::post('/login', [AuthController::class, 'login']);

    Route::middleware('auth:sanctum')->group(function () {
        Route::post('/logout', [AuthController::class, 'logout']);
        Route::apiResource('tasks', \App\Http\Controllers\Api\V1\TaskController::class);
    });
});
```

Now, your API endpoints will be prefixed with `/api/v1`. This allows you to introduce breaking changes in future versions without affecting existing clients.

Later on, you can create a new version (e.g., `v2`) and update the routes accordingly to maintain backward compatibility.

## Implementing Caching

To improve performance, especially for frequently accessed and rarely changing data, it is a good idea to implement caching for our task list.

This can significantly reduce the response time and server load and reduce the number of database queries putting less pressure on the database.

As an example, let's implement that for the `index` method in `TaskController`:

```php
// Include the Cache facade at the top:
use Illuminate\Support\Facades\Cache;

public function index()
{
    // Use the Cache facade to store the tasks for one hour
    $tasks = Cache::remember('tasks', 3600, function () {
        return Task::paginate(15);
    });

    return TaskResource::collection($tasks);
}
```

This caches the task list for one hour. Remember to clear the cache when tasks are updated, created, or deleted.

## Conclusion

In this guide, we've walked through the process of building a simple CRUD API with Laravel, secured with Laravel Sanctum for authentication.

We've covered setting up the project, configuring the database with Neon, and implementing CRUD operations for a Task model. We also added essential features such as API versioning, API documentation, and caching to improve the API's performance, security, and maintainability.

By following these steps, you now have a fully functional API that allows authenticated users to manage tasks effectively. This can be used as the foundation for more complex applications and extended with additional features as needed.

As next steps you can think about adding more features to the API, such as search, filtering, sorting, and more advanced authentication and authorization mechanisms.

Additionally, it is a good idea to implement throttling to protect your API from abuse and to ensure fair usage.

## Additional Resources

- [Laravel Documentation](https://laravel.com/docs)
- [Laravel Sanctum Documentation](https://laravel.com/docs/sanctum)
- [Scribe Documentation](https://scribe.knuckles.wtf/laravel)
- [Neon Documentation](/docs)


# Getting Started with Laravel Events and Listeners

---
title: Getting Started with Laravel Events and Listeners
subtitle: Learn how to implement and utilize Laravel's event system with Neon
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-06-30T00:00:00.000Z'
updatedOn: '2024-06-30T00:00:00.000Z'
---

Laravel's event system provides a simple observer implementation, allowing you to subscribe and listen for various events that occur in your application.

This can be particularly useful for decoupling various parts of your application's logic and not blocking the main request flow. Queued listeners can also be used to handle time-consuming tasks asynchronously, improving the performance of your application.

In this guide, we'll walk through the process of setting up and using Laravel Events and Listeners, with a focus on database operations using Neon Postgres.

## Prerequisites

Before we begin, ensure you have the following:

- PHP 8.1 or higher installed on your system
- [Composer](https://getcomposer.org/) for managing PHP dependencies
- A [Neon](https://console.neon.tech/signup) account for database hosting
- Basic knowledge of Laravel and database operations

## Setting up the Project

Let's start by creating a new Laravel project and setting up the necessary components. If you already have a Laravel project set up, you can skip this section.

### Creating a New Laravel Project

Open your terminal and run the following command to create a new Laravel project:

```bash
composer create-project laravel/laravel laravel-events
cd laravel-events
```

This will create a new Laravel project in a directory named `laravel-events` with all the necessary dependencies installed.

### Setting up the Database

Once you have your Laravel project set up, you'll need to configure your Neon database connection. If you don't have a Neon account, you can sign up [here](https://console.neon.tech/signup).

Update your `.env` file with your Neon database credentials:

```env
DB_CONNECTION=pgsql
DB_HOST=your-neon-hostname.neon.tech
DB_PORT=5432
DB_DATABASE=your_database_name
DB_USERNAME=your_username
DB_PASSWORD=your_password
```

Make sure to replace `your-neon-hostname`, `your_database_name`, `your_username`, and `your_password` with your actual Neon database credentials.

## Creating a Model and Migration

For this tutorial, let's create a simple `Order` model that we'll use to demonstrate events and listeners.

To create the model and migration, run the following command:

```bash
php artisan make:model Order -m
```

This command creates both the `Order` model and a migration file for the `orders` table.

Open the newly created migration file in `database/migrations` and update the `up` method with the following content:

```php
public function up()
{
    Schema::create('orders', function (Blueprint $table) {
        $table->id();
        $table->string('customer_name');
        $table->decimal('total', 8, 2);
        $table->enum('status', ['pending', 'processing', 'completed', 'cancelled'])->default('pending');
        $table->timestamps();
    });
}
```

Run the migration to create the 'orders' table in your Neon database:

```bash
php artisan migrate
```

Next update the `Order` model in `app/Models/Order.php` to include the `customer_name`, `total`, and `status` to the `$fillable` property:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class Order extends Model
{
    use HasFactory;

    protected $fillable = ['customer_name', 'total', 'status'];
}
```

As a reference, the `$fillable` property specifies which attributes are mass-assignable, meaning they can be set using the `create` method on the model. This helps protect against mass assignment vulnerabilities in your application.

## Creating an Event

An event in Laravel is a simple class that represents something that has happened in your application. Events can be used to trigger actions or notify other parts of your application that something has occurred.

Now, let's create an event that will be triggered when an order is placed. Run the following command:

```bash
php artisan make:event OrderPlaced
```

This creates a new event class in `app/Events/OrderPlaced.php`. Update it with the following content:

```php
<?php

namespace App\Events;

use App\Models\Order;
use Illuminate\Broadcasting\Channel;
use Illuminate\Broadcasting\InteractsWithSockets;
use Illuminate\Broadcasting\PresenceChannel;
use Illuminate\Broadcasting\PrivateChannel;
use Illuminate\Contracts\Broadcasting\ShouldBroadcast;
use Illuminate\Foundation\Events\Dispatchable;
use Illuminate\Queue\SerializesModels;

class OrderPlaced
{
    use Dispatchable, InteractsWithSockets, SerializesModels;

    public $order;

    public function __construct(Order $order)
    {
        $this->order = $order;
    }
}
```

This event will carry the `Order` model instance, allowing listeners to access the order details. In a real application, you would perform additional actions, such as sending an email or updating other records in the database, when this event is triggered.

## Creating a Listener

A listener in Laravel is a class that listens for a specific event and performs actions in response to that event.

Now that we have an event, let's create a listener that will respond to this event.

Run the following command to create a new listener:

```bash
php artisan make:listener SendOrderConfirmation --event=OrderPlaced
```

This creates a new listener in `app/Listeners/SendOrderConfirmation.php`. Update it with the following content:

```php
<?php

namespace App\Listeners;

use App\Events\OrderPlaced;
use Illuminate\Contracts\Queue\ShouldQueue;
use Illuminate\Queue\InteractsWithQueue;
use Illuminate\Support\Facades\Log;

class SendOrderConfirmation implements ShouldQueue
{
    use InteractsWithQueue;

    public function handle(OrderPlaced $event)
    {
        // In a real application, you would send an email here
        Log::info('Order confirmation sent for Order #' . $event->order->id);

        // Additional actions related to order confirmation
        Log::info('Slack notification sent for Order #' . $event->order->id);
        Log::info('SMS notification sent for Order #' . $event->order->id);
        Log::info('Update inventory for Order #' . $event->order->id);
    }
}
```

A very important thing to note here is that we are implementing the `ShouldQueue` interface.

Here our listener implements the `ShouldQueue` interface, meaning it will be handled by [Laravel's queue system](https://laravel.com/docs/11.x/queues), which is beneficial for performance when dealing with time-consuming tasks.

If we didn't implement the `ShouldQueue` interface, the listener would be executed synchronously, which could slow down the response time of your application but could be useful for certain use cases where you need to perform actions synchronously rather than asynchronously.

Here, we're simply logging a message to the Laravel log file, but in a real application, you would send an email along with other actions related to order confirmation, which could be time-consuming.

## Registering the Event and Listener

Laravel 11.x and later versions automatically discover events and listeners, so you don't need to manually register them. If you are using an older version of Laravel, you can register your events and listeners in the `EventServiceProvider`.

For more information on registering events and listeners, refer to the [Laravel documentation](https://laravel.com/docs/events).

## Dispatching the Event

Now that we have set up our event and listener, let's create a simple controller to simulate an order placement. This controller will handle order creation and dispatch our event.

Run the following command to create a new controller:

```bash
php artisan make:controller OrderController
```

Open `app/Http/Controllers/OrderController.php` and add the following content:

```php
<?php

namespace App\Http\Controllers;

use App\Models\Order;
use App\Events\OrderPlaced;
use Illuminate\Http\Request;

class OrderController extends Controller
{
    public function store(Request $request)
    {
        $order = Order::create([
            'customer_name' => $request->customer_name,
            'total' => $request->total,
        ]);

        event(new OrderPlaced($order));

        return response()->json(['message' => 'Order placed successfully', 'order' => $order]);
    }
}
```

This controller creates a new order in the database and then dispatches the `OrderPlaced` event.

The `event` helper function is used to dispatch the event, passing the order instance to the event constructor.

This allows the listener to access the order details when the event is triggered rather than blocking the main request flow to perform the additional actions directly in the controller method itself.

We will use this controller to create a new order and trigger the event.

## Adding a Route

To use our new controller, let's add a route. Open `routes/api.php` and add the following line:

```php
Route::post('/orders', [App\Http\Controllers\OrderController::class, 'store']);
```

If you don't have a route file for API routes, you can create one by running the following command:

```bash
php artisan install:api
```

This will create a new `api.php` file in the `routes` directory.

## Testing the Event System

Now, let's test our event system. You can use a tool like Postman or `curl` to send a POST request to your `/api/orders` endpoint.

Using `curl`:

```bash
curl -X POST http://laravel-events.test/api/orders \
     -H "Content-Type: application/json" \
     -d '{"customer_name":"John Doe","total":99.99}'
```

Replace `laravel-events.test` with your actual application URL.

If everything is set up correctly, you should see a new order in your Neon database, but you won't see any log messages in your Laravel log file yet because the listener is queued and we haven't run the queue worker to process the queued jobs.

## Running Queued Jobs

As we mentioned earlier, the `SendOrderConfirmation` listener implements the `ShouldQueue` interface, meaning it will be handled by Laravel's queue system.

If you were to check your logs immediately after placing an order, you might not see the log messages from the listener. Instead, you can run the queue worker to process the queued jobs:

```bash
php artisan queue:work
```

This command starts the queue worker, which will process any queued jobs, including the order confirmation listener.

Once the queue worker is running, you should see the log messages from the listener and the following output:

```
$ php artisan queue:work

   INFO  Processing jobs from the [default] queue.

   App\Listeners\SendOrderConfirmation ....... RUNNING
   App\Listeners\SendOrderConfirmation ....... 1s DONE
```

In a different terminal window, you can place a new order using `curl` or Postman to see the listener in action.

If you were to remove the `ShouldQueue` interface from the listener, the actions would be executed synchronously, and you would see the log messages immediately after placing an order, but thanks to the queue system, the response time of your application is not affected.

The default `QUEUE_CONNECTION` in Laravel is `database`, which uses the database to manage the queue. This means that the queued jobs are stored in your Neon database and processed by the queue worker. You can change the queue connection in your `.env` file if you prefer a different queue driver like `redis` for example. To see all available queue drivers, refer to the [Laravel documentation](https://laravel.com/docs/11.x/queues#driver-prerequisites) or review the `config/queue.php` file within your Laravel project where you can configure the queue connection.

The jobs are queued in the `jobs` table in your database, which is usually created by default with new Laravel installations, or you can run the migration to create the table:

```bash
php artisan queue:table
php artisan migrate
```

## Using Database Transactions with Events

When working with database operations and events, it's important to understand how Laravel handles queued event listeners within database transactions. That way you can make sure that your data remains consistent and that your listeners are triggered at the right time.

Let's update our `OrderController` and `SendOrderConfirmation` listener to handle this correctly:

```php
<?php

namespace App\Http\Controllers;

use App\Models\Order;
use App\Events\OrderPlaced;
use Illuminate\Http\Request;
use Illuminate\Support\Facades\DB;

class OrderController extends Controller
{
    public function store(Request $request)
    {
        $order = DB::transaction(function () use ($request) {
            $order = Order::create([
                'customer_name' => $request->customer_name,
                'total' => $request->total,
            ]);

            event(new OrderPlaced($order));

            return $order;
        });

        return response()->json(['message' => 'Order placed successfully', 'order' => $order]);
    }
}
```

Now, let's update our `SendOrderConfirmation` listener to ensure it handles the event after the database transaction has been committed:

```php
<?php

namespace App\Listeners;

use App\Events\OrderPlaced;
use Illuminate\Contracts\Queue\ShouldQueue;
use Illuminate\Contracts\Events\ShouldHandleEventsAfterCommit;
use Illuminate\Queue\InteractsWithQueue;
use Illuminate\Support\Facades\Log;

class SendOrderConfirmation implements ShouldQueue, ShouldHandleEventsAfterCommit
{
    use InteractsWithQueue;

    public function handle(OrderPlaced $event)
    {
        // In a real application, you would send an email here
        Log::info('Order confirmation sent for Order #' . $event->order->id);
    }
}
```

By implementing the `ShouldHandleEventsAfterCommit` interface, we're telling Laravel to only process this listener after all open database transactions have been committed. This is crucial when your listener depends on database changes made within the transaction.

This approach ensures that:

1. The order is created in the database.
2. The `OrderPlaced` event is dispatched within the transaction.
3. The transaction is committed, saving the order to the Neon database.
4. Only after the transaction is successfully committed, the `SendOrderConfirmation` listener is processed.

This prevents potential issues where the listener might try to access data that hasn't been committed to the database yet, ensuring data consistency between your event processing and your Neon database state.

If your queue connection's `after_commit` configuration option is set to `true` in your `config/queue.php` file, all of your queued listeners will automatically wait for open database transactions to commit before they are processed, and you won't need to use the `ShouldHandleEventsAfterCommit` interface.

## Conclusion

In this guide, we've explored how to implement and utilize Laravel's event system, focusing on database operations with Neon as our database provider. We've covered creating and dispatching events, creating and registering listeners, and how to use database transactions with events.

Events and listeners provide a powerful way to decouple various aspects of your application, making your code more maintainable and scalable without blocking the main request flow for time-consuming tasks.

As a next step, you might want to look into implementing [Supervisor](https://laravel.com/docs/11.x/queues#supervisor-configuration) to manage your queue workers in a production environment and [Laravel Horizon](https://laravel.com/docs/11.x/horizon) for monitoring and managing your queues rather than using the `queue:work` command directly.

## Additional Resources

- [Laravel Events Documentation](https://laravel.com/docs/events)
- [Laravel Queues Documentation](https://laravel.com/docs/queues)
- [Neon Documentation](/docs)


# Building a Blog with Laravel, Livewire, and Laravel Breeze

---
title: Building a Blog with Laravel, Livewire, and Laravel Breeze
subtitle: Learn how to create a dynamic blog application using Laravel, Livewire, and Laravel Breeze for authentication and Neon.
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-06-30T00:00:00.000Z'
updatedOn: '2024-06-30T00:00:00.000Z'
---

Laravel is a powerful PHP framework that makes it easy to build web applications. When combined with Livewire, a full-stack framework for Laravel, you can create dynamic, reactive interfaces with minimal JavaScript. In this guide, we'll build a blog application using Laravel and Livewire, and we'll use Laravel Breeze to handle authentication, along with Neon Postgres.

By the end of this tutorial, you'll have a fully functional blog where users can create, read, update, and delete posts. We'll also implement comments and a simple tagging system.

## Prerequisites

Before we start, make sure you have the following:

- PHP 8.1 or higher installed on your system
- [Composer](https://getcomposer.org/) for managing PHP dependencies
- [Node.js](https://nodejs.org/) and [npm](https://www.npmjs.com/) for managing front-end assets
- A [Neon](https://console.neon.tech/signup) account for database hosting
- Basic knowledge of Laravel, Livewire, and Tailwind CSS

## Setting up the Project

Let's start by creating a new Laravel project and setting up the necessary components. We'll use Laravel Breeze for authentication, Livewire for building interactive components, and Tailwind CSS for styling.

### Creating a New Laravel Project

Open your terminal and run the following command to create a new Laravel project:

```bash
composer create-project laravel/laravel laravel-livewire-blog
cd laravel-livewire-blog
```

This command creates a new Laravel project in a directory named `laravel-livewire-blog` and installs all the necessary dependencies.

### Installing Laravel Breeze

[Laravel Breeze](https://laravel.com/docs/11.x/starter-kits) provides a minimal and simple starting point for building a Laravel application with authentication.

An alternative to Laravel Breeze is Laravel Jetstream, which provides more features out of the box, such as team management and two-factor authentication. However, for this tutorial, we'll use Laravel Breeze for its simplicity.

Let's install Laravel Breeze with the Blade views:

```bash
composer require laravel/breeze --dev
php artisan breeze:install blade
```

This command installs Breeze and sets up the necessary views and routes for authentication.

While in the terminal, also install the Livewire package:

```bash
composer require livewire/livewire
```

### Setting up the Database

Update your `.env` file with your Neon database credentials:

```env
DB_CONNECTION=pgsql
DB_HOST=your-neon-hostname.neon.tech
DB_PORT=5432
DB_DATABASE=your_database_name
DB_USERNAME=your_username
DB_PASSWORD=your_password
```

Make sure to replace `your-neon-hostname`, `your_database_name`, `your_username`, and `your_password` with your actual database details and save the file.

### Compiling Assets

Laravel Breeze uses Tailwind CSS for styling, so we need to compile the assets to generate the CSS file.

To compile the assets, run:

```bash
npm install
npm run dev
```

Keep the Vite development server running in the background as you continue with the next steps. This will automatically compile the assets when changes are made so you can see the updates in real-time.

## Creating the Blog Structure

Now that we have our basic setup, we are ready to create the structure for our blog, including models, migrations, and Livewire components, routes, policies, and views.

### Creating the Post Model and Migration

Models in Laravel are used to interact with the database using the Eloquent ORM. We'll create models for posts, comments, and tags, along with their respective migrations.

Run the following command to create a `Post` model with its migration:

```bash
php artisan make:model Post -m
```

Open the migration file in `database/migrations` and update it:

```php
public function up()
{
    Schema::create('posts', function (Blueprint $table) {
        $table->id();
        $table->foreignId('user_id')->constrained()->onDelete('cascade');
        $table->string('title');
        $table->string('slug')->unique();
        $table->text('content');
        $table->boolean('is_published')->default(false);
        $table->timestamp('published_at')->nullable();
        $table->timestamps();
    });
}
```

This migration creates a `posts` table with columns for the post title, content, publication status, and publication date. It also includes a foreign key to the `users` table for the post author. The `slug` column will be used to generate SEO-friendly URLs.

### Creating the Comment Model and Migration

Now, let's create a `Comment` model and its migration:

```bash
php artisan make:model Comment -m
```

The `comments` table will store the comments for each post, along with the user who made the comment, the post ID, and the comment content.

With that in mind, let's update the migration file:

```php
public function up()
{
    Schema::create('comments', function (Blueprint $table) {
        $table->id();
        $table->foreignId('user_id')->constrained()->onDelete('cascade');
        $table->foreignId('post_id')->constrained()->onDelete('cascade');
        $table->text('content');
        $table->timestamps();
    });
}
```

### Creating the Tag Model and Migration

To take this a step further, we can add a tagging system to our blog. This will allow us to categorize posts based on different topics.

```bash
php artisan make:model Tag -m
```

The `tags` table will store the tags that can be associated with posts. Update the migration file as follows:

```php
public function up()
{
    Schema::create('tags', function (Blueprint $table) {
        $table->id();
        $table->string('name')->unique();
        $table->timestamps();
    });
```

We'll also create a pivot table to manage the many-to-many relationship between posts and tags. The convention for naming this table is to combine the singular form of the related models in alphabetical order. In this case, the models are `Post` and `Tag`, so the pivot table will be named `post_tag`.

```bash
php artisan make:migration create_post_tag_table
```

Update the migration file as follows:

```php
    Schema::create('post_tag', function (Blueprint $table) {
        $table->id();
        $table->foreignId('post_id')->constrained()->onDelete('cascade');
        $table->foreignId('tag_id')->constrained()->onDelete('cascade');
        $table->unique(['post_id', 'tag_id']);
    });
}
```

We don't need to create a model for the pivot table, as it will be managed by Laravel's Eloquent ORM.

Now, run the migrations to create all the tables in the Neon database:

```bash
php artisan migrate
```

This command will create the `posts`, `comments`, `tags`, and `post_tag` tables in your database and keep track of the migrations that have been run. If you need to rollback the migrations, you can run `php artisan migrate:rollback` or if you were to add a new migration, you can run `php artisan migrate` and it will only run the new migrations.

### Updating the Models

Let's update our models to define the relationships. What we want to achieve is:

- A post **belongs** to a user
- A post **has many** comments
- A post can **have many** tags
- A comment **belongs to** a user
- A comment **belongs to** a post
- A tag can be associated with **many** posts

We already have that structure in our database, but we need to define these relationships in our models so we can access them easily in our application.

In `app/Models/Post.php` we define the relationships to the `User`, `Comment`, and `Tag` models:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class Post extends Model
{
    use HasFactory;

    protected $fillable = ['title', 'slug', 'content', 'is_published', 'published_at'];

    public function user()
    {
        // Using the `belongsTo` relationship to get the user who created the post
        return $this->belongsTo(User::class);
    }

    public function comments()
    {
        // Using the `hasMany` relationship to get all comments for a post
        return $this->hasMany(Comment::class);
    }

    public function tags()
    {
        // Using the `belongsToMany` relationship to get all tags associated with a post
        return $this->belongsToMany(Tag::class);
    }
}
```

In `app/Models/Comment.php` we define the relationships to the `User` and `Post` models so we can get the user and post associated with a comment:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class Comment extends Model
{
    use HasFactory;

    protected $fillable = ['content', 'user_id'];

    public function user()
    {
        return $this->belongsTo(User::class);
    }

    public function post()
    {
        return $this->belongsTo(Post::class);
    }
}
```

In `app/Models/Tag.php` we define the relationship to the `Post` model, this will allow us to get all posts associated with a tag:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class Tag extends Model
{
    use HasFactory;

    protected $fillable = ['name'];

    public function posts()
    {
        return $this->belongsToMany(Post::class);
    }
}
```

Finally, update `app/Models/User.php` to include the relationship with posts and comments, where a user can have many posts and many comments:

```php
public function posts()
{
    return $this->hasMany(Post::class);
}

public function comments()
{
    return $this->hasMany(Comment::class);
}
```

With these relationships defined, we can now easily access the related models and data using Eloquent.

### Seeding the Database

To populate the database with some sample data, let's create seeders for `Tag` models so we can associate tags with posts.

Create a seeder for the `Tag` model:

```bash
php artisan make:seeder TagSeeder
```

Update the seeder file in `database/seeders/TagSeeder.php`:

```php
<?php

namespace Database\Seeders;

use Illuminate\Database\Seeder;
use App\Models\Tag;

class TagSeeder extends Seeder
{
    public function run()
    {
        $tags = [
            'Postgres',
            'Neon',
            'Web Development',
            'Laravel',
            'PHP',
            'JavaScript',
            'Database',
            'Design',
            'UI/UX',
            'AI',
            'Machine Learning',
            'Cloud Computing',
            'DevOps',
            'Security',
        ];

        foreach ($tags as $tagName) {
            Tag::create(['name' => $tagName]);
        }
    }
}
```

Now, update the main `DatabaseSeeder` in `database/seeders/DatabaseSeeder.php` to include these new seeder:

```php
<?php

namespace Database\Seeders;

use Illuminate\Database\Seeder;

class DatabaseSeeder extends Seeder
{
    public function run()
    {
        $this->call([
            TagSeeder::class,
        ]);
    }
}
```

Finally, to seed your database with this sample data, run:

```bash
php artisan db:seed
```

This command will run the `TagSeeder` and populate the `tags` table with the sample tags which we can associate with posts later on when users create new posts.

## Implementing the Blog Functionality

Now that we have our models and migrations set up, we can go ahead and implement the blog functionality using Livewire components.

We will start by creating two Livewire components:

- `PostList` to display a list of blog posts
- `PostForm` to create and edit posts

### Creating the Post List Component

First, let's create a Livewire component to display the list of blog posts:

```bash
php artisan make:livewire PostList
```

This command creates a new Livewire component in the `app/Livewire` directory, along with a view file in `resources/views/livewire`.

Update `app/Livewire/PostList.php` to fetch the posts and handle search functionality:

```php
<?php

namespace App\Livewire;

use App\Models\Post;
use Livewire\Component;
use Livewire\WithPagination;

class PostList extends Component
{
    use WithPagination;

    public $search = '';

    public function updatingSearch()
    {
        $this->resetPage();
    }

    public function render()
    {
        $posts = Post::where('is_published', true)
            ->where(function ($query) {
                $query->where('title', 'ilike', '%' . $this->search . '%')
                    ->orWhere('content', 'ilike', '%' . $this->search . '%');
            })
            ->with('user', 'tags')
            ->latest('published_at')
            ->paginate(10);

        return view('livewire.post-list', [
            'posts' => $posts,
        ]);
    }
}
```

In the `render` method, we fetch the posts that are published and match the search query.

An important thing to note here is that we also eager load the `user` and `tags` relationships to avoid additional queries when accessing these relationships in the view.

To learn more about how to implement search functionality in Livewire, check out the [Building a Simple Real-Time Search with Laravel, Livewire, and Neon guide](/guides/laravel-livewire-simple-search).

Now, update the view in `resources/views/livewire/post-list.blade.php` to display the list of posts:

```html
<div>
  <div class="mb-4">
    <input
      wire:model.live.debounce.300ms="search"
      type="text"
      placeholder="Search posts..."
      class="focus:ring-blue-500 w-full rounded-lg border px-4 py-2 focus:outline-none focus:ring-2"
    />
  </div>

  <div class="space-y-4">
    @foreach($posts as $post)
    <div class="rounded-lg bg-white p-6 shadow-md">
      <h2 class="mb-2 text-2xl font-bold">
        <a href="{{ route('posts.show', $post) }}" class="text-blue-600 hover:text-blue-800"
          >{{ $post->title }}</a
        >
      </h2>
      <p class="text-gray-600 mb-2">By {{ $post->user->name }} on {{ $post->published_at }}</p>
      <p class="text-gray-700 mb-4">{{ Str::limit($post->content, 200) }}</p>
      <div class="flex flex-wrap gap-2">
        @foreach($post->tags as $tag)
        <span class="bg-blue-100 text-blue-800 rounded px-2.5 py-0.5 text-xs font-semibold"
          >{{ $tag->name }}</span
        >
        @endforeach
      </div>
    </div>
    @endforeach
  </div>

  <div class="mt-4">{{ $posts->links() }}</div>
</div>
```

This view displays the list of posts along with the post title, author, publication date, content, and tags. It also includes a search input field to filter the posts based on the search query.

### Creating the Post Form Component

Now, let's create a Livewire component for creating and editing posts.

```bash
php artisan make:livewire PostForm
```

Update `app/Livewire/PostForm.php` to handle post creation and editing:

```php
<?php

namespace App\Livewire;

use App\Models\Post;
use App\Models\Tag;
use Illuminate\Support\Str;
use Livewire\Component;

class PostForm extends Component
{
    public $post;
    public $title;
    public $content;
    public $tags;
    public $selectedTags = [];

    protected $rules = [
        'title' => 'required|min:5',
        'content' => 'required|min:10',
        'selectedTags' => 'array',
    ];

    public function mount($post = null)
    {
        if ($post) {
            $this->post = $post;
            $this->title = $post->title;
            $this->content = $post->content;
            $this->selectedTags = $post->tags->pluck('id')->toArray();
        }
    }

    public function save()
    {
        $this->validate();

        $isNew = !$this->post;

        if ($isNew) {
            $this->post = new Post();
            $this->post->user_id = auth()->id();
        }

        $this->post->title = $this->title;
        $this->post->slug = Str::slug($this->title);
        $this->post->content = $this->content;
        $this->post->is_published = true;
        $this->post->published_at = now();
        $this->post->save();

        $this->post->tags()->sync($this->selectedTags);

        session()->flash('message', $isNew ? 'Post created successfully.' : 'Post updated successfully.');

        return redirect()->route('posts.show', $this->post);
    }

    public function render()
    {
        $allTags = Tag::all();
        return view('livewire.post-form', [
            'allTags' => $allTags,
        ]);
    }
}
```

Rundown of the methods in the `PostForm` component:

- The `mount` method is used to set the initial values for the form fields when editing a post. The post data is passed to the component as a parameter.
- The `save` method is called when the form is submitted. It validates the form fields, creates a new post or updates an existing one, and redirects to the post detail page.
- The `render` method fetches all tags from the database and passes them to the view.
- In the `rules` property, we define the validation rules for the form fields.

After that, update the `resources/views/livewire/post-form.blade.php` view to display the post form:

```html
<div>
  <form wire:submit.prevent="save">
    <div class="mb-4">
      <label for="title" class="text-gray-700 mb-2 block font-bold">Title</label>
      <input
        wire:model="title"
        type="text"
        id="title"
        class="text-gray-700 w-full rounded-lg border px-3 py-2 focus:outline-none"
        required
      />
      @error('title') <span class="text-red-500">{{ $message }}</span> @enderror
    </div>

    <div class="mb-4">
      <label for="content" class="text-gray-700 mb-2 block font-bold">Content</label>
      <textarea
        wire:model="content"
        id="content"
        rows="6"
        class="text-gray-700 w-full rounded-lg border px-3 py-2 focus:outline-none"
        required
      ></textarea>
      @error('content') <span class="text-red-500">{{ $message }}</span> @enderror
    </div>

    <div class="mb-4">
      <label class="text-gray-700 mb-2 block font-bold">Tags</label>
      <div class="flex flex-wrap gap-2">
        @foreach($allTags as $tag)
        <label class="inline-flex items-center">
          <input
            type="checkbox"
            wire:model="selectedTags"
            value="{{ $tag->id }}"
            class="form-checkbox text-blue-600 h-5 w-5"
          />
          <span class="text-gray-700 ml-2">{{ $tag->name }}</span>
        </label>
        @endforeach
      </div>
    </div>

    <div>
      <button
        type="submit"
        class="bg-blue-500 hover:bg-blue-700 rounded px-4 py-2 font-bold text-white"
      >
        {{ $post ? 'Update Post' : 'Create Post' }}
      </button>
    </div>
  </form>
</div>
```

This view includes form fields for the post title, content, and tags. The tags are displayed as checkboxes, allowing the user to select multiple tags for the post when creating or editing it.

### Creating Routes and Controllers

Now that we have our Livewire components ready, let's create the necessary routes and controllers to handle the blog functionality.

Routes are defined in the `routes/web.php` file, and controllers are used to handle the logic for each route.

```php
<?php

use App\Http\Controllers\PostController;
use Illuminate\Support\Facades\Route;

Route::middleware(['auth'])->group(function () {
    // After the existing Breeze routes add the following routes:
    Route::get('/posts/create', [PostController::class, 'create'])->name('posts.create');
    Route::get('/posts/{post}/edit', [PostController::class, 'edit'])->name('posts.edit');
});

// Outside the middleware group, add a route to display posts publicly:
Route::get('/posts', [PostController::class, 'index'])->name('posts.index');
Route::get('/posts/{post}', [PostController::class, 'show'])->name('posts.show');
```

Next, create a controller which will handle the blog functionality for the above routes that we just defined:

```bash
php artisan make:controller PostController
```

The above command creates a new controller in the `app/Http/Controllers` directory.

Update `app/Http/Controllers/PostController.php` to include the necessary methods:

```php
<?php

namespace App\Http\Controllers;

use App\Models\Post;
use Illuminate\Support\Facades\Gate;

use Illuminate\Http\Request;

class PostController extends Controller
{
    public function index()
    {
        return view('posts.index');
    }

    public function show(Post $post)
    {
        return view('posts.show', compact('post'));
    }

    public function create()
    {
        return view('posts.create');
    }

    public function edit(Post $post)
    {
        if (Gate::denies('update', $post)) {
            abort(403);
        }
        return view('posts.edit', compact('post'));
    }
}
```

For all the methods, we return the corresponding views. The `edit` method also includes an authorization gate to check if the current user is authorized to edit the post which we will define later.

### Creating the Views

With the routes and controllers in place, let's create the views for the blog functionality. The views will include the layout, navigation, and content for the blog posts.

Let's start by creating a `resources/views/posts/index.blade.php` view to display the list of blog posts:

```html
<x-app-layout>
  <x-slot name="header">
    <h2 class="text-gray-800 text-xl font-semibold leading-tight">{{ __('Blog Posts') }}</h2>
  </x-slot>

  <div class="py-12">
    <div class="mx-auto max-w-7xl lg:px-8 sm:px-6">
      <div class="overflow-hidden bg-white shadow-sm sm:rounded-lg">
        <div class="border-gray-200 border-b bg-white p-6">@livewire('post-list')</div>
      </div>
    </div>
  </div>
</x-app-layout>
```

This view includes the `PostList` Livewire component to display the list of blog posts.

Next, create the `resources/views/posts/show.blade.php` view to display a single blog post:

```html
<x-app-layout>
  <x-slot name="header">
    <h2 class="text-gray-800 text-xl font-semibold leading-tight">{{ $post->title }}</h2>
  </x-slot>

  <div class="py-12">
    <div class="mx-auto max-w-7xl lg:px-8 sm:px-6">
      <div class="overflow-hidden bg-white shadow-sm sm:rounded-lg">
        <div class="border-gray-200 border-b bg-white p-6">
          <h1 class="mb-4 text-3xl font-bold">{{ $post->title }}</h1>
          <p class="text-gray-600 mb-4">By {{ $post->user->name }} on {{ $post->published_at }}</p>
          <div class="prose mb-6 max-w-none">{!! nl2br(e($post->content)) !!}</div>
          <div class="mb-6 flex flex-wrap gap-2">
            @foreach($post->tags as $tag)
            <span class="bg-blue-100 text-blue-800 rounded px-2.5 py-0.5 text-xs font-semibold"
              >{{ $tag->name }}</span
            >
            @endforeach
          </div>
          @can('update', $post)
          <a
            href="{{ route('posts.edit', $post) }}"
            class="bg-blue-500 hover:bg-blue-700 rounded px-4 py-2 font-bold text-white"
            >Edit Post</a
          >
          @endcan
        </div>
      </div>
    </div>
  </div>
</x-app-layout>
```

This view displays the post title, author, publication date, content, and tags. It also includes a link to edit the post if the current user is authorized to do so.

After that, create the `resources/views/posts/create.blade.php` and `resources/views/posts/edit.blade.php` views for creating and editing posts, respectively. These views will include the `PostForm` Livewire component, which we created earlier, and handle the form submission.

Create the `resources/views/posts/create.blade.php` view with the following content:

```html
<x-app-layout>
  <x-slot name="header">
    <h2 class="text-gray-800 text-xl font-semibold leading-tight">{{ __('Create New Post') }}</h2>
  </x-slot>

  <div class="py-12">
    <div class="mx-auto max-w-7xl lg:px-8 sm:px-6">
      <div class="overflow-hidden bg-white shadow-sm sm:rounded-lg">
        <div class="border-gray-200 border-b bg-white p-6">@livewire('post-form')</div>
      </div>
    </div>
  </div>
</x-app-layout>
```

Using the `@livewire` directive, we include the `PostForm` component to create a new post.

With the same structure, create the `resources/views/posts/edit.blade.php` view:

```html
<x-app-layout>
  <x-slot name="header">
    <h2 class="text-gray-800 text-xl font-semibold leading-tight">{{ __('Edit Post') }}</h2>
  </x-slot>

  <div class="py-12">
    <div class="mx-auto max-w-7xl lg:px-8 sm:px-6">
      <div class="overflow-hidden bg-white shadow-sm sm:rounded-lg">
        <div class="border-gray-200 border-b bg-white p-6">
          @livewire('post-form', ['post' => $post])
        </div>
      </div>
    </div>
  </div>
</x-app-layout>
```

This view includes the `PostForm` component with the post data passed as a parameter to edit the post. The form fields will be pre-filled with the existing post data when users edit one of their posts.

### Adding Authorization

As this will be a multi-user blog, we need to implement authorization to ensure that users can only edit their own posts. Our goal is to allow users to edit posts only if they are the authors of those posts.

Laravel provides a simple way to define authorization policies using policies and gates. Policies are classes that define the authorization logic for a particular model, while gates are more general-purpose authorization checks.

Let's create a policy for the `Post` model:

```bash
php artisan make:policy PostPolicy --model=Post
```

Update `app/Policies/PostPolicy.php` to define the authorization logic for updating and deleting posts:

```php
<?php

namespace App\Policies;

use App\Models\Post;
use App\Models\User;
use Illuminate\Auth\Access\HandlesAuthorization;

class PostPolicy
{
    use HandlesAuthorization;

    public function update(User $user, Post $post)
    {
        return $user->id === $post->user_id;
    }

    public function delete(User $user, Post $post)
    {
        return $user->id === $post->user_id;
    }
}
```

In the `PostPolicy` class, we define the `update` and `delete` methods to check if the current user is the author of the post. If the user is the author, the method returns `true`, allowing the user to update or delete the post. Otherwise, it returns `false` to deny access and prevent unauthorized actions.

### Implementing Comments

By now we have the basic functionality of our blog in place. If you were to visit the blog, you would see a list of posts, be able to view individual posts, and create new posts. However, a blog wouldn't be complete without the ability to add comments to posts!

Let's add the comment system to our blog posts! First, create a new Livewire component:

```bash
php artisan make:livewire CommentSection
```

Update `app/Livewire/CommentSection.php` to handle adding comments to a post:

```php
<?php

namespace App\Livewire;

use App\Models\Comment;
use Livewire\Component;

class CommentSection extends Component
{
    public $post;
    public $newComment;

    protected $rules = [
        'newComment' => 'required|min:3',
    ];

    public function mount($post)
    {
        $this->post = $post;
    }

    public function addComment()
    {
        $this->validate();

        $this->post->comments()->create([
            'user_id' => auth()->id(),
            'content' => $this->newComment,
        ]);

        $this->newComment = '';
        $this->post = $this->post->fresh(['comments.user']);
    }

    public function deleteComment($commentId)
    {
        $comment = Comment::find($commentId);

        if ($comment->user_id === auth()->id()) {
            $comment->delete();
            $this->post = $this->post->fresh(['comments.user']);
        }
    }

    public function render()
    {
        return view('livewire.comment-section');
    }
}
```

Here, we have the `CommentSection` Livewire component with methods to add and delete comments. The `addComment` method creates a new comment for the post, while the `deleteComment` method deletes a comment if the current user is the author of the comment. You can also see the `rules` property defining the validation rules for the comment content and create a policy for the `Comment` model to handle authorization instead of checking it in the component itself.

Next, update the view in `resources/views/livewire/comment-section.blade.php` to display comments and allow users to add new comments:

```html
<div>
  <h3 class="mb-4 text-2xl font-bold">Comments</h3>

  @foreach($post->comments as $comment)
  <div class="bg-gray-100 mb-4 rounded-lg p-4">
    <p class="text-gray-800">{{ $comment->content }}</p>
    <p class="text-gray-600 mt-2 text-sm">
      By {{ $comment->user->name }} on {{ $comment->created_at }} @if($comment->user_id ===
      auth()->id()) |
      <button wire:click="deleteComment({{ $comment->id }})" class="text-red-500 hover:underline">
        Delete
      </button>
      @endif
    </p>
  </div>
  @endforeach @auth
  <form wire:submit.prevent="addComment" class="mt-6">
    <div class="mb-4">
      <label for="newComment" class="text-gray-700 mb-2 block font-bold">Add a comment</label>
      <textarea
        wire:model="newComment"
        id="newComment"
        rows="3"
        class="text-gray-700 w-full rounded-lg border px-3 py-2 focus:outline-none"
        required
      ></textarea>
      @error('newComment') <span class="text-red-500">{{ $message }}</span> @enderror
    </div>
    <button
      type="submit"
      class="bg-blue-500 hover:bg-blue-700 rounded px-4 py-2 font-bold text-white"
    >
      Post Comment
    </button>
  </form>
  @else
  <p class="text-gray-600 mt-6">
    Please <a href="{{ route('login') }}" class="text-blue-500 hover:underline">log in</a> to leave
    a comment.
  </p>
  @endauth
</div>
```

After that, go back to the `resources/views/posts/show.blade.php` view and update it to include the comment section:

```html
<!-- Add this after the post content -->
<div class="mt-8">@livewire('comment-section', ['post' => $post])</div>
```

### Adding Navigation Links

Laravel Breeze provides a simple layout with a navigation menu that includes links for logging in and registering. Let's add links for creating new posts and logging out.

Update the existing `resources/views/layouts/navigation.blade.php` view to include links for creating new posts:

```html
<!-- Add this inside the navigation menu -->
<x-nav-link :href="route('posts.create')" :active="request()->routeIs('posts.create')">
  {{ __('Create Post') }}
</x-nav-link>
<x-nav-link :href="route('posts.index')" :active="request()->routeIs('posts.index')">
  {{ __('Blog') }}
</x-nav-link>
```

## Testing

To ensure our blog functionality works as expected, it's important to test the application.

To learn more about testing in Laravel along Neon, check out the [Testing Laravel Applications with Neon's Database Branching guide](/guides/laravel-test-on-branch).

## Conclusion

In this tutorial, we've built a fully functional blog application using Laravel, Livewire, and Laravel Breeze. We've implemented features such as user authentication, creating and editing blog posts, adding comments, and basic authorization.

This implementation provides a solid foundation for a blog, but there are always ways to improve and expand its functionality:

- Implement a more advanced authorization system with roles and permissions
- Add a rich text editor for post content
- Implement a more robust tagging system with the ability to create new tags
- Add a search functionality for posts
- Implement social sharing features
- Add an admin panel for managing posts, users, and comments

By combining the power of Laravel, the simplicity of Livewire, and the authentication scaffolding provided by Laravel Breeze, you can quickly create dynamic and interactive web applications that meet your users' needs.

## Additional Resources

- [Laravel Documentation](https://laravel.com/docs)
- [Livewire Documentation](https://laravel-livewire.com/docs)
- [Laravel Breeze Documentation](https://laravel.com/docs/8.x/starter-kits#laravel-breeze)
- [Tailwind CSS Documentation](https://tailwindcss.com/docs)
- [Neon Documentation](/docs)


# Building Dynamic Charts with Laravel, Livewire, and Neon Postgres

---
title: Building Dynamic Charts with Laravel, Livewire, and Neon Postgres
subtitle: Learn how to build dynamic charts with Laravel, Livewire, and Neon Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-10-20T00:00:00.000Z'
updatedOn: '2024-10-20T00:00:00.000Z'
---

Laravel is an amazing PHP framework for building web applications, while Livewire provides a simple way to build dynamic interfaces using PHP.

In this guide, we'll walk through the process of creating a dynamic analytics dashboard for a SaaS application using Laravel Breeze for authentication, Livewire Charts for data visualization, and Neon Postgres for data storage.

We'll build interactive charts that display key metrics such as daily active users, feature usage trends, and user signups vs. cancellations.

## Prerequisites

Before we begin, make sure you have:

- PHP 8.1 or higher installed
- Composer for managing PHP dependencies
- A [Neon](https://console.neon.tech/signup) account for Postgres hosting
- Basic familiarity with Laravel, Livewire, and Postgres

## Setting up the Project

1. Create a new Laravel project:

   ```bash
   composer create-project laravel/laravel saas-charts
   cd saas-charts
   ```

2. Install Laravel Breeze with Livewire:

   ```bash
   composer require laravel/breeze --dev
   php artisan breeze:install livewire
   ```

3. Install the Livewire Charts package which we'll use for data visualization:

   ```bash
   composer require asantibanez/livewire-charts
   ```

4. Install the Livewire Charts assets which include the necessary JavaScript and CSS files:

   ```bash
   php artisan livewire-charts:install
   ```

5. Set up your Neon Postgres connection in the `.env` file:

   ```env
   DB_CONNECTION=pgsql
   DB_HOST=your-neon-hostname.neon.tech
   DB_PORT=5432
   DB_DATABASE=your_database_name
   DB_USERNAME=your_username
   DB_PASSWORD=your_password
   ```

6. Run the migrations to set up the users table and other Breeze-related tables in your Neon Postgres database:

   ```bash
   php artisan migrate
   ```

   This will create the necessary tables for user authentication and session management.

## Additional Database Tables

Now that we have the `users` table set up by Breeze, let's create migrations for our additional SaaS analytics data.

For the purpose of this guide, we'll track feature usage and subscriptions. You can adjust these tables based on your specific application requirements.

1. Create migrations:

   ```bash
   php artisan make:migration create_feature_usage_table
   php artisan make:migration create_subscriptions_table
   ```

   Note that the naming convention for the migration files is important to make sure that your migrations are named correctly with the `create_` prefix followed by the table name.

2. Update the migration files:

   The above commands will create two migration files in the `database/migrations` directory. Update the migration files as follows:

   For the `create_feature_usage_table` we'll track the usage of different features by users, so we'll store the `user_id`, `feature_name`, and the `used_at` timestamp:

   ```php
   public function up()
   {
       Schema::create('feature_usage', function (Blueprint $table) {
           $table->id();
           $table->foreignId('user_id')->constrained()->onDelete('cascade');
           $table->string('feature_name');
           $table->timestamp('used_at');
           $table->timestamps();
       });
   }
   ```

   For the `create_subscriptions_table` we'll track user subscriptions, including the `user_id`, `plan`, `started_at`, and `ended_at` timestamps:

   ```php
   public function up()
   {
       Schema::create('subscriptions', function (Blueprint $table) {
           $table->id();
           $table->foreignId('user_id')->constrained()->onDelete('cascade');
           $table->string('plan');
           $table->timestamp('started_at');
           $table->timestamp('ended_at')->nullable();
           $table->timestamps();
       });
   }
   ```

3. With the migrations in place, run the migrations to create the tables in your Neon Postgres database:

   ```bash
   php artisan migrate
   ```

   This will create the `feature_usage` and `subscriptions` tables in your database.

## Creating Models

Laravel's Eloquent ORM provides a convenient way to interact with your database.

By defining models, we can represent and manipulate the data in the `FeatureUsage` and `Subscription` tables, which we created earlier through migrations.

In this step, we'll create models and set up relationships to ensure efficient data retrieval and interaction.

### Step 1: Generate the Models

Start by creating the `FeatureUsage` and `Subscription` models using Laravel's Artisan command:

```bash
php artisan make:model FeatureUsage
php artisan make:model Subscription
```

This will generate two model files in the `app/Models` directory corresponding to the `feature_usage` and `subscriptions` tables in your database.

### Step 2: Define Relationships in the Models

Now, let's update the model classes to define relationships between the tables. The `FeatureUsage` and `Subscription` models will be connected to the `User` model via foreign keys.

#### 2.1 `FeatureUsage` Model

In the `app/Models/FeatureUsage.php` file, define the relationship with the `User` model. Since each feature usage entry belongs to a specific user, we will use a `belongsTo` relationship:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class FeatureUsage extends Model
{
    use HasFactory;

    protected $table = 'feature_usage';

    protected $fillable = [
        'user_id',
        'feature_name',
        'used_at',
    ];

    public function user()
    {
        return $this->belongsTo(User::class);
    }

    protected $casts = [
        'used_at' => 'datetime',
    ];
}
```

The above defines the following:

- `fillable`: Specifies which attributes can be mass-assigned, in this case, `user_id`, `feature_name`, and `used_at`.
- `user()`: Defines a `belongsTo` relationship, meaning each `FeatureUsage` belongs to a single `User`.
- `casts`: Automatically casts the `used_at` column to a `datetime` object for easier manipulation in PHP.

#### 2.2 `Subscription` Model

In the `app/Models/Subscription.php` file, define relationships with both the `User` model and handle timestamps (`started_at` and `ended_at`) correctly. This indicates that each subscription belongs to a user and includes a `plan`:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class Subscription extends Model
{
    use HasFactory;

    protected $fillable = [
        'user_id',
        'plan',
        'started_at',
        'ended_at',
    ];

    public function user()
    {
        return $this->belongsTo(User::class);
    }

    protected $casts = [
        'started_at' => 'datetime',
        'ended_at' => 'datetime',
    ];
}
```

The above defines the following:

- `fillable`: Makes sure that the fields `user_id`, `plan`, `started_at`, and `ended_at` are mass assignable.
- `user()`: Defines a `belongsTo` relationship where each `Subscription` is linked to a specific `User`.
- `casts`: Automatically casts the `started_at` and `ended_at` columns to `datetime` objects.

### Step 3: Database Relationships

Once these relationships are defined, Eloquent provides methods to interact with related data. For example:

- Access a user's feature usages with `$user->featureUsages()`.
- Retrieve a user's subscriptions with `$user->subscriptions()`.
- Easily manipulate and retrieve data for timestamps (e.g., `started_at`, `ended_at`, and `used_at`).

## Building the Dashboard

We’ll create a simple SaaS dashboard that showcases our dynamic charts which will include daily active users, feature usage trends, and user signups vs. cancellations. This dashboard will use Livewire for interactivity and Tailwind CSS for styling along with the Livewire Charts package for creating the dynamic charts.

### Step 1: Create the Livewire Component

First, generate a new Livewire component for the dashboard:

```bash
php artisan make:livewire Dashboard
```

This will create both the `Dashboard` class in `app/Http/Livewire` and the a view in `resources/views/livewire/dashboard.blade.php`.

### Step 2: Update `Dashboard.php`

In the `app/Http/Livewire/Dashboard.php` file, we’ll render the dashboard view inside the main layout:

```php
<?php

namespace App\Livewire;

use Livewire\Component;

class Dashboard extends Component
{
    public function render()
    {
        return view('livewire.dashboard')->layout('layouts.app');
    }
}
```

Note that we're using the `layout('layouts.app')` method to specify the main layout file for the dashboard view. This layout file will contains the main structure of the dashboard.

### Step 3: Create the Dashboard View

Now, let’s update the `dashboard.blade.php` view with a grid layout that displays multiple charts, along with some Tailwind CSS styling to improve the design.

```blade
<div class="min-h-screen py-12 bg-gray-100">
    <div class="mx-auto max-w-7xl sm:px-6 lg:px-8">
        <!-- Dashboard Header -->
        <div class="flex items-center justify-between mb-10">
            <h1 class="text-4xl font-bold text-gray-800">SaaS Analytics Dashboard</h1>
            <button class="px-4 py-2 text-white transition duration-200 bg-indigo-600 rounded-lg hover:bg-indigo-700">
                Refresh Data
            </button>
        </div>

        <!-- Dashboard Grid -->
        <div class="grid grid-cols-1 gap-8 sm:grid-cols-2 lg:grid-cols-3">
            <!-- Daily Active Users Chart -->
            <div class="p-6 bg-white rounded-lg shadow-md">
                <h2 class="mb-4 text-xl font-semibold">Daily Active Users</h2>
                <livewire:daily-active-users-chart />
            </div>

            <!-- Feature Usage Trends Chart -->
            <div class="p-6 bg-white rounded-lg shadow-md">
                <h2 class="mb-4 text-xl font-semibold">Feature Usage Trends</h2>
                <livewire:feature-usage-trends-chart />
            </div>

            <!-- User Signups vs Cancellations Chart -->
            <div class="p-6 bg-white rounded-lg shadow-md">
                <h2 class="mb-4 text-xl font-semibold">User Signups vs. Cancellations</h2>
                <livewire:user-signups-vs-cancellations-chart />
            </div>
        </div>
    </div>
</div>
```

This view includes a header with a title and a refresh button, followed by a grid layout that displays the three charts: 'Daily Active Users', 'Feature Usage Trends', and 'User Signups vs. Cancellations'. We will create those charts components next.

## Setting up Routes

With the charts dashboard view and the Livewire component in place, let's set up the routes to display the dashboard.

```php
use App\Livewire\Dashboard;

Route::get('/charts', Dashboard::class)->middleware(['auth'])->name('dashboard');
```

This route will display the dashboard view when the `/charts` URL is accessed. The `auth` middleware ensures that only authenticated users can access the dashboard.

## Set Up Livewire Charts for the Dashboard

Now with everything in place, let's implement individual chart components.

The Livewire Charts package provides a wide range of chart types, including area charts, radar charts, and treemaps, offering flexibility to create various data visualizations.

We'll use `LivewireLineChart` for 'Daily Active Users', `LivewireColumnChart` for 'Feature Usage Trends', and `LivewirePieChart` for 'User Signups vs. Cancellations'. To get a full list of available chart types, check out the [Livewire Charts documentation](https://github.com/asantibanez/livewire-charts/).

### 2.1 Daily Active Users Chart

Create a Livewire component for the daily active users chart:

```bash
php artisan make:livewire DailyActiveUsersChart
```

In `app/Livewire/DailyActiveUsersChart.php`, define the logic to fetch the data:

```php
<?php

namespace App\Livewire;

use Livewire\Component;
use App\Models\FeatureUsage;
use Asantibanez\LivewireCharts\Models\LineChartModel;

class DailyActiveUsersChart extends Component
{
    public function render()
    {
        // Example query to fetch daily active users
        $activeUsers = FeatureUsage::selectRaw('DATE(used_at) as date, COUNT(DISTINCT user_id) as users')
            ->groupBy('date')
            ->get();

        // Prepare data for the chart
        $lineChartModel = (new LineChartModel())
            ->setTitle('Daily Active Users')
            ->setAnimated(true)
            ->setSmoothCurve()
            ->withOnPointClickEvent('onPointClick');

        foreach ($activeUsers as $activeUser) {
            $lineChartModel->addPoint($activeUser->date, $activeUser->users);
        }

        return view('livewire.daily-active-users-chart', [
            'lineChartModel' => $lineChartModel
        ]);
    }
}
```

Create the corresponding Blade view in `resources/views/livewire/daily-active-users-chart.blade.php`:

```blade
<div class="h-64">
    <livewire:livewire-line-chart
        :line-chart-model="$lineChartModel"
    />
</div>
```

### 2.2 Feature Usage Trends Chart

Create another Livewire component for feature usage trends:

```bash
php artisan make:livewire FeatureUsageTrendsChart
```

In `app/Livewire/FeatureUsageTrendsChart.php`, define the data logic:

```php
<?php

namespace App\Livewire;

use Livewire\Component;
use App\Models\FeatureUsage;
use Asantibanez\LivewireCharts\Models\ColumnChartModel;

class FeatureUsageTrendsChart extends Component
{
    public function render()
    {
        // Example query to fetch feature usage trends
        $featureUsages = FeatureUsage::selectRaw('feature_name, COUNT(*) as usage_count')
            ->groupBy('feature_name')
            ->get();

        // Prepare the chart data
        $columnChartModel = (new ColumnChartModel())
            ->setTitle('Feature Usage Trends')
            ->setAnimated(true);

        foreach ($featureUsages as $usage) {
            $columnChartModel->addColumn($usage->feature_name, $usage->usage_count, '#f6ad55');
        }

        return view('livewire.feature-usage-trends-chart', [
            'columnChartModel' => $columnChartModel
        ]);
    }
}
```

In `resources/views/livewire/feature-usage-trends-chart.blade.php`:

```blade
<div class="h-64">
    <livewire:livewire-column-chart
        :column-chart-model="$columnChartModel"
    />
</div>
```

### 2.3 User Signups vs. Cancellations Chart

Create a Livewire component for user signups vs. cancellations:

```bash
php artisan make:livewire UserSignupsVsCancellationsChart
```

In `app/Livewire/UserSignupsVsCancellationsChart.php`, define the data logic:

```php
<?php

namespace App\Livewire;

use Livewire\Component;
use App\Models\Subscription;
use Asantibanez\LivewireCharts\Models\PieChartModel;

class UserSignupsVsCancellationsChart extends Component
{
    public function render()
    {
        // Example query to fetch signups vs cancellations
        $signups = Subscription::whereNotNull('started_at')->count();
        $cancellations = Subscription::whereNotNull('ended_at')->count();

        // Prepare the chart data
        $pieChartModel = (new PieChartModel())
            ->setTitle('Signups vs Cancellations')
            ->addSlice('Signups', $signups, '#90cdf4')
            ->addSlice('Cancellations', $cancellations, '#fc8181');

        return view('livewire.user-signups-vs-cancellations-chart', [
            'pieChartModel' => $pieChartModel
        ]);
    }
}
```

In `resources/views/livewire/user-signups-vs-cancellations-chart.blade.php`:

```blade
<div class="h-64">
    <livewire:livewire-pie-chart
        :pie-chart-model="$pieChartModel"
    />
</div>
```

## Step 3: Add Chart Scripts

Include the chart scripts in your main layout file (`resources/views/layouts/app.blade.php`) by adding:

```blade
@livewireScripts
@livewireChartsScripts
```

This will load the necessary JavaScript files for Livewire and Livewire Charts to render the interactive charts on the dashboard.

## Step 4: Test the Dashboard

Run the server to access your charts dashboard if you haven't already:

```bash
php artisan serve
```

Navigate to the `/charts` route, and you should see the real-time interactive charts displayed on your dashboard.

### Seeding the Database with Sample Data

If you don't have any data yet, you can seed the database with sample data to test the charts. First, create a seeder that populates the `FeatureUsage` and `Subscription` tables with mock data:

1. Generate the seeder:

   ```bash
   php artisan make:seeder SampleDataSeeder
   ```

2. Open the newly created seeder file (`database/seeders/SampleDataSeeder.php`) and populate it with sample data for feature usage and subscriptions:

   ```php
   <?php

   namespace Database\Seeders;

   use Illuminate\Database\Seeder;
   use App\Models\FeatureUsage;
   use App\Models\Subscription;
   use App\Models\User;
   use Carbon\Carbon;

   class SampleDataSeeder extends Seeder
   {
       public function run()
       {
           // Create users
           $users = User::factory(10)->create();

           // Seed FeatureUsage data
           foreach ($users as $user) {
               for ($i = 0; $i < 5; $i++) {
                   FeatureUsage::create([
                       'user_id' => $user->id,
                       'feature_name' => 'Feature ' . rand(1, 5),
                       'used_at' => Carbon::now()->subDays(rand(0, 30)),
                   ]);
               }
           }

           // Seed Subscription data
           foreach ($users as $user) {
               Subscription::create([
                   'user_id' => $user->id,
                   'plan' => 'Basic',
                   'started_at' => Carbon::now()->subMonths(2),
                   'ended_at' => rand(0, 1) ? Carbon::now()->subMonth() : null,
               ]);
           }
       }
   }
   ```

3. Run the seeder to populate the database with test data:

   ```bash
   php artisan db:seed --class=SampleDataSeeder
   ```

Once the database is seeded, refresh the charts dashboard, and you should see the charts populated with real-time data.

For more information on seeding the database, check out the [Laravel documentation](https://laravel.com/docs/11.x/seeding).

## Optimizing Performance

When working with large datasets, you will have to make sure that your application is optimized for performance. This includes optimizing database queries, caching results, and using efficient algorithms.

We will cover some optimization techniques for improving the performance of your Neon Postgres application below but you should also check out the [Performance tips for Neon Postgres](/blog/performance-tips-for-neon-postgres) blog post for more specific tips.

### 1. Database Indexing for Frequently Queried Columns

Database indexing is a key technique to speed up query execution, especially for columns used frequently in `WHERE`, `JOIN`, and `ORDER BY` clauses. With indexes in place, the database can find records faster, making your queries more efficient.

This can be especially useful for tables like `FeatureUsage` and `Subscription`, where you might frequently query by `user_id`, `used_at`, `started_at`, and `ended_at`.

Here’s how to add indexes for the `FeatureUsage` and `Subscription` tables:

```php
Schema::table('feature_usage', function (Blueprint $table) {
    $table->index(['user_id', 'used_at']);  // Index on user_id and used_at to speed up queries
});

Schema::table('subscriptions', function (Blueprint $table) {
    $table->index(['user_id', 'started_at', 'ended_at']);  // Index on user_id, started_at, and ended_at for faster lookups
});
```

These indexes will optimize queries related to filtering or grouping by `user_id`, `used_at`, `started_at`, and `ended_at`, which are common in analytics.

To learn more about indexing in Neon Postgres, check out the [Neon documentation](/docs/postgres/indexes) on indexes.

### 2. Implement Caching for Expensive Queries

Caching is a great way to reduce the load on your database by storing the results of expensive queries and retrieving them from memory when needed. This avoids running the same query multiple times for data that doesn't change frequently.

Here's how you can cache the results of a query for daily active users for a specific time period:

```php
use Illuminate\Support\Facades\Cache;

// In your Livewire component
$dailyActiveUsers = Cache::remember('daily_active_users_' . $this->selectedDays, 60 * 5, function () {
    return FeatureUsage::selectRaw('DATE(used_at) as date, COUNT(DISTINCT user_id) as count')
        ->whereDate('used_at', '>=', now()->subDays($this->selectedDays))
        ->groupBy('date')
        ->orderBy('date')
        ->get();
});
```

Quick explanation of the code:

- `Cache::remember`: Caches the query result for 5 minutes (`60 * 5` seconds). If the data is already cached, it retrieves the result from the cache; otherwise, it runs the query and stores the result.
- This is useful for queries that don’t need real-time updates and can tolerate slight delays, such as historical data or reports.

The `Cache::remember` method is a convenient way to cache query results in Laravel. You can adjust the cache duration based on your application's requirements. However, be cautious with caching, as it can lead to stale data if not managed properly.

## Conclusion

In this guide, we've built a simple dynamic SaaS dashboard using Laravel Breeze for authentication, Livewire Charts for data visualization, and Neon Postgres for data storage. This setup provides a good starting point for tracking key metrics in your SaaS or web application.

To go further, consider the following next steps:

1. Implementing more detailed drill-down features for each chart.
2. Adding user-specific analytics for personalized insights.
3. Implementing real-time updates using Livewire's polling feature or websockets.

## Additional Resources

- [Laravel Documentation](https://laravel.com/docs)
- [Neon Documentation](/docs)
- [Livewire Documentation](https://livewire.laravel.com/)
- [Livewire Charts Documentation](https://github.com/asantibanez/livewire-charts/)

<NeedHelp />


# Building a Simple Real-Time Search with Laravel, Livewire, and Neon

---
title: Building a Simple Real-Time Search with Laravel, Livewire, and Neon
subtitle: Learn how to integrate Laravel with Postgres on Neon, using Laravel's Eloquent ORM and migrations for efficient database management.
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-06-29T00:00:00.000Z'
updatedOn: '2024-06-29T00:00:00.000Z'
---

Laravel is a powerful PHP framework known for its elegant syntax and feature-rich ecosystem. Livewire, a full-stack framework for Laravel, allows us to build dynamic interfaces with minimal JavaScript. Together, they provide a robust foundation for creating interactive web applications.

In this guide, we'll build a simple real-time search feature using Laravel, Livewire, and Neon. We'll set up a Laravel project, create a database schema, implement the search functionality with Livewire, and optimize performance with Neon. By the end of this tutorial, you'll have a working real-time search feature that leverages the strengths of Laravel, Livewire, and Neon.

## Prerequisites

Before we begin, you will need to have the following:

- PHP 8.1 or higher installed on your system
- [Composer](https://getcomposer.org/) for managing PHP dependencies
- [Node.js](https://nodejs.org/) and [npm](https://www.npmjs.com/) installed on your local machine for managing front-end assets
- A [Neon](https://console.neon.tech/signup) account
- Basic knowledge of Laravel, Livewire, and Tailwind CSS

## Setting up the Project

Before we dive into building the search functionality, let's set up a new Laravel project and configure the necessary components.

### Creating a New Laravel Project

Open your terminal and run the following command to create a new Laravel project:

```bash
composer create-project laravel/laravel real-time-search
cd real-time-search
```

This command will create a new Laravel project in a directory named `real-time-search`. Navigate to the project directory to continue with the setup.

### Installing and Configuring Livewire

Now that we have a Laravel project, let's install Livewire:

```bash
composer require livewire/livewire
```

Livewire will automatically register its service provider.

### Setting up Tailwind CSS

To use Tailwind CSS, we need to install and configure it as well.

```bash
npm install -D tailwindcss@latest postcss@latest autoprefixer@latest
npx tailwindcss init -p
```

Update your `tailwind.config.js` file to include Laravel and Livewire specific paths:

```javascript
/** @type {import('tailwindcss').Config} */
export default {
  content: ['./resources/**/*.blade.php', './resources/**/*.js', './resources/**/*.vue'],
  theme: {
    extend: {},
  },
  plugins: [],
};
```

The above configuration tells Tailwind to scan the specified files in the `resources` directory for classes to include in the compiled CSS.

This approach ensures that Tailwind's utility classes are available in your Laravel views and Livewire components and keeps your CSS bundle size minimal.

Next, add the `@tailwind` directives to your `resources/css/app.css` file:

```css
@tailwind base;
@tailwind components;
@tailwind utilities;
```

Finally, run the following command to compile your assets with Vite:

```bash
npm run dev
```

Leave the Vite development server running in the background to compile your assets and proceed with the next steps.

### Connecting to Neon Database

To connect your Laravel application to your Neon database, update your `.env` file with the Neon database credentials:

```env
DB_CONNECTION=pgsql
DB_HOST=your-neon-hostname.neon.tech
DB_PORT=5432
DB_DATABASE=your_database_name
DB_USERNAME=your_username
DB_PASSWORD=your_password
```

Make sure to replace the placeholders with your actual Neon database details.

## Building the Search Functionality

Now that our project is set up, let's build the search functionality. We'll create a simple product search feature that filters products based on their name and description.

### Creating the Database Schema

We'll create a simple `products` table for this example. Run the following command to create a migration:

```bash
php artisan make:model Product -m
```

Open the newly created migration file in `database/migrations` and update the `up` method to include the necessary columns:

```php
public function up()
{
    Schema::create('products', function (Blueprint $table) {
        $table->id();
        $table->string('name');
        $table->text('description');
        $table->decimal('price', 8, 2);
        $table->timestamps();
    });
}
```

For the sake of simplicity, we've included the `name`, `description`, and `price` columns in our `products` table.

Run the migration to create the `products` table in your database:

```bash
php artisan migrate
```

### Seeding Sample Data

Laravel provides a convenient way to seed your database with sample data. Let's create some sample data. Create a new seeder:

```bash
php artisan make:seeder ProductSeeder
```

Open `database/seeders/ProductSeeder.php` and add the following:

```php
<?php

namespace Database\Seeders;

use Illuminate\Database\Console\Seeds\WithoutModelEvents;
use Illuminate\Database\Seeder;

class ProductSeeder extends Seeder
{
    /**
     * Run the database seeds.
     */
    public function run(): void
    {
        $products = [
            ['name' => 'Laptop', 'description' => 'High-performance laptop', 'price' => 999.99],
            ['name' => 'Smartphone', 'description' => 'Latest model smartphone', 'price' => 699.99],
            ['name' => 'Headphones', 'description' => 'Noise-cancelling headphones', 'price' => 199.99],
            ['name' => 'Smartwatch', 'description' => 'Fitness tracking smartwatch', 'price' => 249.99],
            ['name' => 'Tablet', 'description' => '10-inch tablet', 'price' => 399.99],
            ['name' => 'Desktop', 'description' => 'High-performance desktop', 'price' => 1499.99],
            ['name' => 'Monitor', 'description' => '27-inch 4K monitor', 'price' => 499.99],
            ['name' => 'Keyboard', 'description' => 'Mechanical gaming keyboard', 'price' => 149.99],
            ['name' => 'Mouse', 'description' => 'Wireless mouse', 'price' => 49.99],
            ['name' => 'Printer', 'description' => 'Wireless all-in-one printer', 'price' => 199.99],
            ['name' => 'Scanner', 'description' => 'High-speed document scanner', 'price' => 299.99],
            ['name' => 'Projector', 'description' => '1080p home theater projector', 'price' => 799.99],
            ['name' => 'Camera', 'description' => 'Mirrorless camera', 'price' => 999.99],
            ['name' => 'Drone', 'description' => '4K camera drone', 'price' => 1199.99],
            ['name' => 'Gaming Console', 'description' => 'Next-gen gaming console', 'price' => 499.99],
            ['name' => 'VR Headset', 'description' => 'Wireless VR headset', 'price' => 299.99],
            ['name' => 'External Hard Drive', 'description' => '2TB external hard drive', 'price' => 99.99],
            ['name' => 'USB Flash Drive', 'description' => '128GB USB flash drive', 'price' => 29.99],
            ['name' => 'Wireless Router', 'description' => 'Dual-band wireless router', 'price' => 99.99],
            ['name' => 'Smart Speaker', 'description' => 'Voice-controlled smart speaker', 'price' => 79.99],
        ];

        foreach ($products as $product) {
            \App\Models\Product::create($product);
        }
    }
}

```

Update `database/seeders/DatabaseSeeder.php` to include the `ProductSeeder` class by adding it to the `run` method:

```php
public function run()
{
    $this->call([
        ProductSeeder::class,
    ]);
}
```

Run the seeder to populate the `products` table with sample data:

```bash
php artisan db:seed
```

This command will insert the sample products into the `products` table so we can test our search functionality. Note that the `db:seed` command will run all seeders by default, and if you run it multiple times, it will insert duplicate records.

### Implementing the Livewire Component

Next, let's create a Livewire component for our search functionality. Run the following command to generate a new Livewire component:

```bash
php artisan make:livewire ProductSearch
```

This command creates two files:

- `app/Livewire/ProductSearch.php`: The Livewire component class, which contains the search logic.
- `resources/views/livewire/product-search.blade.php`: The view file for the Livewire component.

Open `app/Livewire/ProductSearch.php` and update it with the following code which fetches products based on the search query:

```php
<?php

namespace App\Livewire;

use Livewire\Component;
use App\Models\Product;

class ProductSearch extends Component
{
    public $search = '';

    public function render()
    {
        $products = Product::where('name', 'like', '%' . $this->search . '%')
                           ->orWhere('description', 'like', '%' . $this->search . '%')
                           ->get();

        return view('livewire.product-search', [
            'products' => $products
        ]);
    }
}
```

Rundown of the code above:

- We start by defining a `$search` property that will be bound to the search input, and a `render` method that fetches products based on the search query.
- The `render` method queries the `products` table for records that match the search query in the `name` or `description` columns.
- We're using a simple `ILIKE` query to perform a case-insensitive search. You can customize the search logic based on your requirements.
- Next we get all matching products using the `get` method and pass them to the view. Alternatively, you can paginate the results for better performance using Laravel's `paginate` method.
- The `render` method returns the view `livewire.product-search` along with the `$products` variable.

Once you've updated the component class, let's create the view for this component. Open `resources/views/livewire/product-search.blade.php` and add the following content:

```html
<div>
  <div class="mb-4">
    <input
      wire:model.live.debounce.300ms="search"
      type="text"
      placeholder="Search products..."
      class="focus:ring-blue-500 w-full rounded-lg border px-4 py-2 focus:outline-none focus:ring-2"
    />
  </div>

  <div class="grid grid-cols-1 gap-4 lg:grid-cols-3 md:grid-cols-2">
    @forelse($products as $product)
    <div
      class="transform rounded-lg bg-white p-4 shadow transition duration-300 ease-in-out hover:scale-105"
    >
      <h3 class="text-lg font-semibold">{{ $product->name }}</h3>
      <p class="text-gray-600">{{ $product->description }}</p>
      <p class="text-blue-600 mt-2 font-bold">${{ number_format($product->price, 2) }}</p>
    </div>
    @empty
    <div class="rounded-lg bg-white p-4 text-center shadow">No products found.</div>
    @endforelse
  </div>
</div>
```

This view includes an input field for the search query and a grid to display the search results.

The `wire:model.live.debounce.300ms` attribute on the input field binds it to the `$search` property in our Livewire component, with a300ms debounce to reduce the number of database queries triggered by user input changes.

Using the `@forelse` directive, we loop through the `$products` collection and display each product's name, description, and price. If no products match the search query, we display a message indicating that no products were found.

### Updating the Layout

To use our new component, let's update the main layout. Open `resources/views/welcome.blade.php` and replace its content with:

```html
<!doctype html>
<html lang="{{ str_replace('_', '-', app()->getLocale()) }}">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Real-Time Search</title>
    @vite('resources/css/app.css') @livewireStyles
  </head>
  <body class="bg-gray-100">
    <div class="container mx-auto px-4 py-8">
      <h1 class="mb-8 text-center text-3xl font-bold">Real-Time Product Search</h1>
      @livewire('product-search')
    </div>

    @livewireScripts @vite('resources/js/app.js')
  </body>
</html>
```

This layout includes the necessary Livewire scripts and styles, as well as our `ProductSearch` component.

After updating the layout, make sure that your Vite development server is still running to compile the assets.

```bash
npm run dev
```

## Optimizing Search Performance with Neon

To optimize our search performance, we can leverage Neon's indexing capabilities.

Indexing the `name` and `description` columns will speed up search queries by allowing the database to quickly locate matching records.

Let's create an index on the `name` and `description` columns of our `products` table.

Create a new migration:

```bash
php artisan make:migration add_index_to_products_table
```

Open the new migration file and update the `up` and `down` methods to add and remove the index from the `products` table respectively:

```php
public function up()
{
    Schema::table('products', function (Blueprint $table) {
        $table->index(['name', 'description']);
    });
}

public function down()
{
    Schema::table('products', function (Blueprint $table) {
        $table->dropIndex(['name', 'description']);
    });
}
```

Run the migration:

```bash
php artisan migrate
```

This index will significantly improve the performance of our search queries, especially as the number of products grows.

## Testing the Search Functionality

To ensure our search functionality works as expected, let's write a simple test. Run the following command to create a test file:

```bash
php artisan make:test ProductSearchTest
```

Open the newly created test file in `tests/Feature/ProductSearchTest.php` and add the following test:

```php
<?php

namespace Tests\Feature;

use App\Livewire\ProductSearch;
use App\Models\Product;
use Illuminate\Foundation\Testing\RefreshDatabase;
use Livewire\Livewire;
use Tests\TestCase;

class ProductSearchTest extends TestCase
{
    // use RefreshDatabase;

    /** @test */
    public function it_can_search_products()
    {
        // If using RefreshDatabase trait, make sure to seed the database

        Livewire::test(ProductSearch::class)
            ->set('search', 'Laptop')
            ->assertSee('Laptop')
            ->assertDontSee('Phone');
    }
}
```

This test creates two products and then checks if the search functionality correctly filters the results.

> Make sure to only use the `RefreshDatabase` trait when running tests to avoid modifying your production database during testing.

Run the test with:

```bash
php artisan test
```

To learn more about testing in Laravel along with Neon branding, check out the [Testing Laravel Applications with Neon's Database Branching](/guides/laravel-test-on-branch).

## Conclusion

In this tutorial, we've built a real-time search feature using Laravel, Livewire, and Neon. We've leveraged Livewire's real-time capabilities to create a responsive search component, and utilized Neon's high-performance database to ensure quick and efficient queries.

This implementation provides a solid foundation for a search feature, but there are always ways to enhance and expand its functionality:

- Implement pagination for large result sets
- Add filters for more refined searches
- Incorporate full-text search capabilities for more accurate results
- Implement caching to further improve performance
- Use [Laravel Scout](https://laravel.com/docs/11.x/scout) for full-text search capabilities

By combining the power of Laravel, the simplicity of Livewire, and the performance of Neon, it's easy to create dynamic and responsive web applications that meet your users' needs.

## Additional Resources

- [Laravel Documentation](https://laravel.com/docs)
- [Livewire Documentation](https://laravel-livewire.com/docs)
- [Tailwind CSS Documentation](https://tailwindcss.com/docs)
- [Neon Documentation](/docs)

<NeedHelp />


# Building a TODO Application with Laravel, Livewire, and Volt

---
title: Building a TODO Application with Laravel, Livewire, and Volt
subtitle: Learn how to create a simple yet powerful TODO app using Laravel, Livewire, Volt, and Laravel Breeze for authentication
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-06-30T00:00:00.000Z'
updatedOn: '2024-06-30T00:00:00.000Z'
---

In this guide, we'll walk through the process of building a TODO application using [Laravel](https://laravel.com/), [Livewire](https://livewire.laravel.com/), and [Volt](https://livewire.laravel.com/docs/volt).

We'll use [Laravel Breeze](https://laravel.com/docs/11.x/starter-kits) for authentication and Neon Postgres as our database.

By the end of this tutorial, you'll have a simple yet fully functional TODO application that allows users to create, update, and delete tasks.

## Prerequisites

Before we begin, ensure you have the following:

- PHP 8.1 or higher installed on your system
- [Composer](https://getcomposer.org/) for managing PHP dependencies
- [Node.js](https://nodejs.org/) and npm for managing front-end assets
- A [Neon](https://console.neon.tech/signup) account for database hosting
- Basic knowledge of Laravel and Livewire

## Setting up the Project

Let's start by creating a new Laravel project and setting up the necessary components for our TODO application.

### Install the Laravel Installer

The Laravel installer is a command-line tool that simplifies the process of creating new Laravel projects. If you don't have it installed, run the following command:

```bash
composer global require laravel/installer
```

To verify the installation, run:

```bash
laravel --version
```

This should display the Laravel installer version, confirming that the installation was successful.

### Creating a New Laravel Project

Open your terminal and run the following command to create a new Laravel project using the Laravel installer:

```bash
laravel new laravel-todo-volt
```

Follow the on-screen instructions to create the project by selecting the following options:

- Starter Kit: 'Laravel Breeze'
- Breeze stack: 'Livewire (Volt Class API) with Alpine'
- Dark mode: Based on your preference
- Testing framework: Pest
- Database: PostgreSQL

This command creates a new Laravel project named `laravel-todo-volt` with the selected options and installs the necessary dependencies like Breeze, Livewire, and Volt.

After the project is created, navigate to the project directory:

```bash
cd laravel-todo-volt
```

This can also be done directly via `composer` instead of using the Laravel installer, but the Laravel installer provides an interactive setup process that simplifies the initial project setup rather than running multiple commands manually.

### Setting up the Database

Update your `.env` file with your Neon database credentials:

```env
DB_CONNECTION=pgsql
DB_HOST=your-neon-hostname.neon.tech
DB_PORT=5432
DB_DATABASE=your_database_name
DB_USERNAME=your_username
DB_PASSWORD=your_password
```

Make sure to replace `your-neon-hostname`, `your_database_name`, `your_username`, and `your_password` with your actual Neon database details.

Once you've updated the `.env` file, run the following command to create the default tables:

```bash
php artisan migrate
```

This will create the necessary tables in your Neon database.

### Compiling Assets

Laravel Breeze uses Vite for asset compilation. Run the following commands to install dependencies and compile assets:

```bash
npm install
npm run dev
```

Keep the Vite development server running in the background as you continue with the next steps.

## Creating the TODO Model and Migration

A model in Laravel represents a database table and allows you to interact with the table's data using Eloquent ORM. The migration file defines the structure of the database table and is used to create or modify the table.

The standard convention in Laravel is to create the model with singular naming and the migration with plural naming. For example, a `Todo` model would correspond to a `todos` table in the database, `User` model to `users` table, and so on.

Now, let's create the `Todo` model along with its migration:

```bash
php artisan make:model Todo -m
```

Open the newly created migration file in `database/migrations` and update it to include the necessary columns for the `todos` table:

```php
public function up()
{
    Schema::create('todos', function (Blueprint $table) {
        $table->id();
        $table->foreignId('user_id')->constrained()->onDelete('cascade');
        $table->string('title');
        $table->text('description')->nullable();
        $table->boolean('is_completed')->default(false);
        $table->timestamps();
    });
}
```

This migration creates a `todos` table with columns for the todo title, description, completion status, and a foreign key to the `users` table.

The `onDelete('cascade')` method ensures that todos are deleted when the corresponding user is deleted so that we don't have orphaned records left in the database.

Update the `app/Models/Todo.php` model file to include the relationship with the user and the fillable fields:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class Todo extends Model
{
    use HasFactory;

    protected $fillable = ['title', 'description', 'is_completed'];

    public function user()
    {
        return $this->belongsTo(User::class);
    }
}
```

The `$fillable` property specifies which fields can be mass-assigned when creating or updating a todo task. This helps protect against mass-assignment vulnerabilities and ensures that only the specified fields are allowed to be updated.

Also, update the `app/Models/User.php` file to include the relationship with todos:

```php
public function todos()
{
    return $this->hasMany(Todo::class);
}
```

This method defines a one-to-many relationship between the `User` and `Todo` models, allowing us to retrieve all todos associated with a user. For example, `Auth::user()->todos` will return all todos created by the authenticated user.

Now, run the migrations to create the `todos` table in your Neon database:

```bash
php artisan migrate
```

## Creating the Volt Component

Volt is a new addition to Livewire that allows you to define Livewire components using a class-based API.

This makes it easier to organize and manage your components, especially for larger applications with many components.

Let's create a Volt component for our TODO list:

```bash
php artisan make:volt TodoList
```

This command creates a new Volt component file at `resources/views/livewire/todolist.blade.php`. This single file will contain both the component's logic and its template compared to the traditional Livewire components where the logic is in a separate PHP file.

```php
<?php

use App\Models\Todo;
use Illuminate\Support\Facades\Auth;
use function Livewire\Volt\{state, computed, action};

state(['newTodoTitle' => '']);

$todos = computed(function () {
    return Auth::user()->todos()->latest()->get();
});

$addTodo = action(function () {
    $this->validate([
        'newTodoTitle' => 'required|min:3'
    ]);

    Auth::user()->todos()->create([
        'title' => $this->newTodoTitle,
    ]);

    $this->newTodoTitle = '';
});

$toggleComplete = action(function (Todo $todo) {
    $todo->update(['is_completed' => !$todo->is_completed]);
});

$deleteTodo = action(function (Todo $todo) {
    $todo->delete();
});

?>

<div>
    <h2 class="text-2xl font-semibold mb-4">Your TODO List</h2>

    <form wire:submit="addTodo" class="mb-4">
        <div class="flex">
            <input
                wire:model="newTodoTitle"
                type="text"
                class="flex-1 rounded-l-md border-gray-300 shadow-sm focus:border-indigo-300 focus:ring focus:ring-indigo-200 focus:ring-opacity-50 dark:bg-gray-700 dark:border-gray-600 dark:text-indigo-400 dark:focus:ring-indigo-500 dark:focus:border-indigo-500"
                placeholder="Enter a new TODO item"
            >
            <button type="submit" class="bg-blue-500 text-white px-4 py-2 rounded-r-md hover:bg-blue-600">Add</button>
        </div>
        @error('newTodoTitle') <span class="text-red-500 text-sm">{{ $message }}</span> @enderror
    </form>

    <ul class="space-y-2">
        @foreach($this->todos as $todo)
            <li class="flex items-center justify-between bg-white p-4 rounded-md shadow dark:bg-gray-800">
                <div class="flex items-center">
                    <input
                        type="checkbox"
                        wire:click="toggleComplete({{ $todo->id }})"
                        {{ $todo->is_completed ? 'checked' : '' }}
                        class="mr-2 rounded border-gray-300 text-indigo-600 shadow-sm focus:border-indigo-300 focus:ring focus:ring-indigo-200 focus:ring-opacity-50 dark:bg-gray-700 dark:border-gray-600 dark:text-indigo-400 dark:focus:ring-indigo-500 dark:focus:border-indigo-500"
                    >
                    <span class="{{ $todo->is_completed ? 'line-through text-gray-400' : '' }}">
                        {{ $todo->title }}
                    </span>
                </div>
                <button
                    wire:click="deleteTodo({{ $todo->id }})"
                    class="text-red-500 hover:text-red-700 dark:text-red-400 dark:hover:text-red-600"
                >
                    Delete
                </button>
            </li>
        @endforeach
    </ul>
</div>
```

This Volt component combines the component's logic and template in a single file. Let's break down the key parts:

- We use `state()` to define reactive properties like `newTodoTitle`.
- The `computed()` function is used to create a dynamic property for fetching todos.
- The `action()` defines methods that can be triggered from the template.
- The template section uses Livewire directives like `wire:submit` and `wire:click` to interact with the component's logic.

This approach allows for a more self-contained component definition, making it easier to understand and maintain your Livewire components rather than having the logic and template in separate files.

## Integrating the TODO List into the Dashboard

Now that we have the Volt component ready, let's integrate our TODO list into the main dashboard.

Open `resources/views/dashboard.blade.php` and replace its content with:

```html
<x-app-layout>
  <x-slot name="header">
    <h2 class="text-gray-800 dark:text-gray-200 text-xl font-semibold leading-tight">
      {{ __('Dashboard') }}
    </h2>
  </x-slot>

  <div class="py-12">
    <div class="mx-auto max-w-7xl lg:px-8 sm:px-6">
      <div class="dark:bg-gray-800 overflow-hidden bg-white shadow-sm sm:rounded-lg">
        <div class="text-gray-900 dark:text-gray-100 p-6">
          <livewire:todolist />
        </div>
      </div>
    </div>
  </div>
</x-app-layout>
```

This integrates our `TodoList` component into the Breeze dashboard by using the `livewire:todolist` directive. When you visit the dashboard, you should see the TODO list component displayed on the page.

As we have used the `auth()->user()` method in the `TodoList` component to fetch the user's todos, each user will see their own list of todos when they visit the dashboard, however we have not yet implemented any authorization to ensure that users can only manage their own todos. We'll cover this in the next section.

## Adding Authorization

To ensure users can only manage their own todos, let's implement some basic authorization.

In addition to the authentication provided by Laravel Breeze, Laravel provides an authorization system that allows you to define policies for your models. These policies define the rules for accessing and managing resources, such as todos in our case.

Create a new policy for the `Todo` model using the following command:

```bash
php artisan make:policy TodoPolicy --model=Todo
```

Open `app/Policies/TodoPolicy.php` and update it to define the authorization rules for updating and deleting todo items:

```php
<?php

namespace App\Policies;

use App\Models\Todo;
use App\Models\User;
use Illuminate\Auth\Access\Response;

class TodoPolicy
{
    public function update(User $user, Todo $todo): bool
    {
        return $user->id === $todo->user_id;
    }

    public function delete(User $user, Todo $todo): bool
    {
        return $user->id === $todo->user_id;
    }
}
```

Here, we define two methods: `update` and `delete`. These methods check if the authenticated user is the owner of the todo by comparing the user's ID with the todo's `user_id`. If the user is the owner, the method returns `true`, allowing the user to update or delete the todo. Otherwise, it returns `false` and denies access.

Now, update the `TodoList` component to use these policies when toggling completion status or deleting todos:

```php
$toggleComplete = action(function (Todo $todo) {
    if (auth()->user()->cannot('update', $todo)) {
        return;
    }
    $todo->update(['is_completed' => !$todo->is_completed]);
});

$deleteTodo = action(function (Todo $todo) {
    if (auth()->user()->cannot('delete', $todo)) {
        return;
    }
    $todo->delete();
});
```

In these updated actions:

- We use `auth()->user()->cannot('update', $todo)` and `auth()->user()->cannot('delete', $todo)` to check if the current user is authorized to perform the respective actions based on the `TodoPolicy` rules.
- If the user is not authorized, the function returns early without performing the action preventing unauthorized access.
- If the user is authorized, the action proceeds as before allowing the user to toggle completion status or delete the todo.

This way you can make sure that users can only toggle completion status or delete todos that they own, as defined in the `TodoPolicy`.

## Writing Tests for Your TODO Application with Pest

Testing is an important part of the development process. There are a few different ways to write tests in Laravel, including PHPUnit and Pest. Choosing the right testing framework depends on your preference and the requirements of your project.

In this section, we'll cover writing tests using Pest, a more expressive and minimalistic testing framework for PHP.

Pest provides a [Livewire plugin](https://pestphp.com/docs/plugins#livewire), which allows you to write tests for Livewire components in a more readable and concise way. To install the Pest plugin for Livewire, run the following command:

```bash
composer require pestphp/pest-plugin-livewire --dev
```

### Setting Up the Test Environment

For this example, we will use an in-memory SQLite database for testing. This ensures that tests run quickly and do not affect your production database.

However, to learn more about testing in Laravel along with Neon branding, check out the [Testing Laravel Applications with Neon's Database Branching](/guides/laravel-test-on-branch). This guide will help you set up a separate database branch for testing, allowing you to test your application with real data rather than an in-memory database.

To get started, ensure your `.env.testing` file is configured to use an in-memory SQLite database for testing:

```env
APP_KEY=base64:kf_your_app_key_here
DB_CONNECTION=sqlite
DB_DATABASE=:memory:
```

This will allow us to use the `RefreshDatabase` trait to reset the database before each test, ensuring a clean slate for testing.

> It is important to note that the `RefreshDatabase` trait will clear the database before each test, so make sure to use a separate database for testing to avoid data loss as the database will be reset for each test meaning that any data that you have in the database will be lost.

### Creating and Using a `TodoFactory`

Factories in Laravel generate sample data for models, useful for testing and database seeding with realistic data thanks to the Faker library.

Let's create a factory for our `Todo` model which will generate random todo items for testing:

```bash
php artisan make:factory TodoFactory --model=Todo
```

Update `database/factories/TodoFactory.php` to add the necessary fields and states for generating todo items:

```php
<?php

namespace Database\Factories;

use App\Models\Todo;
use App\Models\User;
use Illuminate\Database\Eloquent\Factories\Factory;

class TodoFactory extends Factory
{
    protected $model = Todo::class;

    public function definition()
    {
        return [
            'user_id' => User::factory(),
            'title' => $this->faker->sentence(4),
            'description' => $this->faker->paragraph(),
            'is_completed' => $this->faker->boolean(20),
            'created_at' => $this->faker->dateTimeBetween('-1 month', 'now'),
            'updated_at' => $this->faker->dateTimeBetween('-1 month', 'now'),
        ];
    }

    public function completed()
    {
        return $this->state(['is_completed' => true]);
    }

    public function incomplete()
    {
        return $this->state(['is_completed' => false]);
    }
}
```

This factory generates random todo items with titles, descriptions, and completion status. We've also defined two states: `completed` and `incomplete` to create todos with specific completion statuses.

### Creating Feature Tests

Let's create a feature test file for our TODO list functionality using the following command:

```bash
php artisan make:test TodoListTest
```

This command creates a new test file at `tests/Feature/TodoListTest.php`. Open this file and replace its contents with the following:

```php
<?php

use App\Models\User;
use App\Models\Todo;
use function Pest\Laravel\get;
use function Pest\Laravel\{actingAs};
use function Pest\Livewire\livewire;

uses(\Illuminate\Foundation\Testing\RefreshDatabase::class);

test('user can view todos', function () {
    $user = User::factory()->create();

    $todos = Todo::factory()->count(3)->create([
        'user_id' => $user->id,
    ]);

    actingAs($user);

    livewire('todolist')
        ->assertSee($todos[0]->title)
        ->assertSee($todos[1]->title)
        ->assertSee($todos[2]->title);
});

test('user can delete a todo', function () {
    $user = User::factory()->create();

    $todo = Todo::factory()->create([
        'user_id' => $user->id,
    ]);

    actingAs($user);

    livewire('todolist')
        ->call('deleteTodo', $todo->id);

    expect(Todo::find($todo->id))->toBeNull();
});

test('user can not delete a todo that does not belong to them', function () {
    $user = User::factory()->create();
    $otherUser = User::factory()->create();

    $todo = Todo::factory()->create([
        'user_id' => $otherUser->id,
    ]);

    actingAs($user);

    livewire('todolist')
        ->call('deleteTodo', $todo->id);

    expect(Todo::find($todo->id))->not->toBeNull();
});
```

These tests cover the following scenarios:

- A user can view their todos when visiting the dashboard.
- A user can delete a todo that belongs to them.
- A user cannot delete a todo that belongs to another user.

The `actingAs()` function is used to authenticate the user before interacting with the Livewire component. This ensures that the user is authorized to perform the actions.

The `livewire()` function is used to interact with the Livewire component and make assertions based on the component's state.

### Running the Tests

Again, before you run the tests, note that the `RefreshDatabase` trait will clear the database before each test, so make sure to use a separate database for testing to avoid data loss like an in-memory SQLite database or a Neon database branch.

You can run these tests using the following command:

```bash
php artisan test
```

This will execute the tests and provide feedback on the results. Writing tests helps ensure that your application behaves as expected and catches bugs early in the development process.

## Conclusion

In this tutorial, we've built a simple yet functional TODO application using Laravel, Livewire, and Volt. We've covered:

1. Setting up a new Laravel project with Breeze, Livewire, and Volt
1. Creating a `Todo` model and migration
1. Implementing a Volt component for managing todos
1. Integrating the TODO list into the dashboard
1. Adding basic authorization to ensure users can only manage their own todos
1. Writing tests for the TODO application using Pest

This application provides a solid foundation for a TODO list, showing the power and simplicity of Laravel, Livewire, and Volt. From here, you could expand the functionality by adding features such as:

- Due dates for todos
- Categorization or tagging of todos
- Sorting and filtering options
- Sharing todos with other users
- Assigning todos to specific users

## Additional Resources

- [Laravel Documentation](https://laravel.com/docs)
- [Livewire Documentation](https://laravel-livewire.com/docs)
- [Laravel Breeze Documentation](https://laravel.com/docs/breeze)
- [Tailwind CSS Documentation](https://tailwindcss.com/docs)
- [Neon Documentation](/docs)


# Reverting a failed deployment and schema migration in Laravel

---
title: Reverting a failed deployment and schema migration in Laravel
subtitle: Learn how to revert a failed deployment and schema migration in Laravel using built-in tools like `migrate:rollback` and Neon's backup and restore capabilities.
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-05-26T00:00:00.000Z'
updatedOn: '2024-05-26T00:00:00.000Z'
---

Deploying new features and updates is an essential part of maintaining a modern web application. However, not every deployment goes as planned.

When something goes wrong, especially with schema migrations, the consequences can range from data inconsistencies to extended application downtime.

In this guide, you'll learn how to revert a failed deployment and schema migration in Laravel using built-in tools like `migrate:rollback` and the backup and restore capabilities of Neon. We'll cover practical steps and best practices for deployment and recovery, helping you prevent future deployment issues.

## Rolling back migrations in Laravel

Laravel provides a built-in [Artisan migrate command](https://laravel.com/docs/11.x/migrations#rolling-back-migrations) for reverting schema changes when a migration doesn't go as expected.

Here's how to use it:

### Using `php artisan migrate:rollback`

1. **Revert the last batch of migrations:**

   ```bash
   php artisan migrate:rollback
   ```

   This command will revert the last batch of migrations applied to your database.

2. **Revert a specific number of steps:**
   If you need more control, use the `--step` option:

   ```bash
   php artisan migrate:rollback --step=1
   ```

   This command will only roll back the most recent migration file.

3. **Rolling back to a specific point in time:**
   For more granular control over the rollback process, use the `--date` option with a date string.
   ```bash
   php artisan migrate:rollback --date="2024-05-01 12:00:00"
   ```
   This command will roll back all migrations that were executed after the specified date.

### Troubleshooting rollback issues

If you encounter issues while rolling back migrations, follow these troubleshooting steps:

- **Failed rollback command:** Check the Laravel logs for error messages. For instance, if you see an error about missing tables or columns:
  - Revisit the migration files and ensure they are consistent.
  - Adjust or fix the migrations as needed.

- **Ensure idempotency:**
  Make sure all migrations can be run and rolled back without side effects. Here's an example of an idempotent migration:

  ```php
  // database/migrations/2024_05_10_000000_add_status_to_users.php
  use Illuminate\Database\Migrations\Migration;
  use Illuminate\Database\Schema\Blueprint;
  use Illuminate\Support\Facades\Schema;

  class AddStatusToUsers extends Migration
  {
      public function up()
      {
          if (!Schema::hasColumn('users', 'status')) {
              Schema::table('users', function (Blueprint $table) {
                  $table->string('status')->default('active');
              });
          }
      }

      public function down()
      {
          if (Schema::hasColumn('users', 'status')) {
              Schema::table('users', function (Blueprint $table) {
                  $table->dropColumn('status');
              });
          }
      }
  }
  ```

### Database verification after rollback

After rolling back migrations, verify the database schema and data to ensure the rollback was successful.

- **Check migration status:**

  ```bash
  php artisan migrate:status
  ```

  This command lists all migrations, showing which ones have been applied.

- **Inspect database directly:**
  Use your database management tool to directly inspect the schema and data.

> **Note:** `migrate:rollback` can lead to data loss if not used carefully. Ensure you have a backup strategy in place.

## Restoring your data using Neon

If rolling back migrations doesn't solve the issue, [Neon's backup and restore](/docs/manage/backups) capabilities can quickly restore your database to a previous state.

### Key benefits of using Neon for restoration

1. **Point-in-Time Restoration:** Restore to a specific moment before the failed deployment.
2. **Restore from another Branch:** Use Neon's branching feature to restore from a stable branch.

### Restoration steps

To restore your database using Neon, you can either use the Neon dashboard or the Neon CLI or API. Follow the steps outlined in [Neon's Branch Restore Guide](/docs/guides/branch-restore#how-to-use-branch-restore):

After restoring the database, align your codebase with the restored data to ensure consistency.

For detailed steps, refer to the [Neon Branch Restore Guide](/docs/guides/branch-restore#how-to-use-branch-restore).

## Best practices for deployment and recovery

No deployment process is foolproof, but following best practices can help you recover quickly and prevent future issues.

### Use a staging environment

Replicate your production environment for testing before deploying features to production.

1. **Develop and test locally:**
   Run migrations and tests against your development database.

2. **Deploy changes and migrations to staging:**
   Ensure your staging environment closely resembles production.

3. **Perform thorough testing:**
   Use automated and manual testing to validate changes.

4. **Promote changes to production after verification:**
   Deploy to production only after all tests pass.

### Break down database changes

Smaller, manageable migrations make rollbacks simpler.

- **Reduce extensive issues:**
  Break changes into smaller, logical batches.

- **Example workflow:**
  1. Create multiple smaller migrations instead of one large one.

  ```bash
  php artisan make:migration add_status_to_users
  php artisan make:migration add_type_to_users
  ```

  2. Test each migration in isolation.

  ```bash
  php artisan migrate
  ```

### Implement a robust backup strategy

1. **Daily Full Backups:** Schedule daily full backups.
2. **Incremental Backups:** Use frequent snapshots throughout the day.
3. **Retention Policies:** Keep backups long enough for compliance and audits.

It is important to test your backup and restore process regularly to ensure it works as expected.

## Preventing future deployment issues

Even with a solid recovery plan, preventing deployment issues in the first place is the best approach. There are several strategies to minimize deployment problems:

### Automate and validate deployments

Implement a CI/CD pipeline to streamline the deployment process and add safeguards.

**Example CI/CD pipeline:**

1. **Build Stage:** Install dependencies and compile assets.

   ```yaml
   - name: Install Dependencies
     run: composer install
   - name: Compile Assets
     run: npm run build
   ```

2. **Test stage:** Run tests and validate coding standards.

   ```yaml
   - name: Run Unit Tests
     run: php artisan test
   - name: Run Coding Standards Check
     run: php artisan lint
   ```

3. **Deployment Stage:** Deploy to staging, run health checks, and promote to production if all tests pass.

### Use Neon's branching feature

Create isolated environments for testing and staging using [Neon's branching feature](/docs/introduction/branching).

1. **Create a branch:**
   Create a branch from your production database in the Neon dashboard.

2. **Deploy code to staging:**
   Point your staging environment to the new branch.

3. **Test migrations and features thoroughly:**
   Ensure migrations work correctly and features function as expected.

4. **Merge to production:**
   Deploy your changes to production after successful testing.

For a detailed guide on using Neon's branching feature with Laravel for testing and staging, refer to the [Testing Laravel Applications with Neon's Database Branching](/guides/laravel-test-on-branch).

### Set up monitoring and alerts

Proactively monitor your application and database. That way, you can catch issues early and respond quickly.

Some monitoring tools to consider can include tools like New Relic, Sentry, or Datadog.

On the database monitoring side, you can use [Neon's built-in monitoring capabilities](/docs/introduction/monitoring-page) to track performance metrics and receive alerts for potential issues.

## Conclusion

By using Laravel's built-in `migrate:rollback` command and Neon's backup and restore capabilities, you can revert a failed deployment quickly and safely. Follow best practices like testing in staging environments, breaking down database changes, and automating deployments to minimize future issues and maintain a smooth deployment process.

- [Laravel Migrations Documentation](https://laravel.com/docs/11.x/migrations)
- [Neon documentation](/docs)


# Creating a Multi-Tenant Application with Laravel and Neon

---
title: Creating a Multi-Tenant Application with Laravel and Neon
subtitle: Learn how to build a scalable multi-tenant application using Laravel and Neon's powerful database features
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-06-30T00:00:00.000Z'
updatedOn: '2024-06-30T00:00:00.000Z'
---

Multi-tenancy is a software architecture where a single instance of an application serves multiple tenants or clients.

Each tenant's data is isolated and remains invisible to other tenants. This approach is commonly used in Software as a Service (SaaS) applications. In this tutorial, we'll build the foundation for a multi-tenant SaaS application using Laravel and Neon.

By the end of this tutorial, you'll have a fully functional multi-tenant SaaS application where tenants can manage their own books, users, and settings, all while maintaining data isolation between tenants.

## Prerequisites

Before we start, make sure you have the following:

- PHP 8.1 or higher installed on your system
- [Composer](https://getcomposer.org/) for managing PHP dependencies
- [Node.js](https://nodejs.org/) and npm for managing front-end assets
- A [Neon](https://console.neon.tech/signup) account for database hosting
- Basic knowledge of Laravel and Livewire

## Setting up the Project

Let's start by creating a new Laravel project and setting up the necessary components.

### Creating a New Laravel Project

Open your terminal and run the following command to create a new Laravel project:

```bash
composer create-project laravel/laravel laravel-multi-tenant-saas
cd laravel-multi-tenant-saas
```

### Installing Required Packages

For our multi-tenant SaaS application, we'll use the following package:

- `stancl/tenancy`: A flexible multi-tenancy package for Laravel
- Laravel Breeze: A minimal authentication starter kit for Laravel

Start by installing the `stancl/tenancy` package:

```bash
composer require stancl/tenancy
```

After installing the package, let's set up the tenancy:

```bash
php artisan tenancy:install
```

Register the `TenancyServiceProvider` in the `bootstrap/providers.php` file:

```php
return [
    // ...
    App\Providers\TenancyServiceProvider::class,
],
```

Let's install Laravel Breeze with the Blade views:

```bash
composer require laravel/breeze --dev
php artisan breeze:install blade
```

Next, install the required NPM packages:

```bash
npm install
npm run dev
```

### Setting up the Database

Update your `.env` file with your Neon database credentials:

```env
DB_CONNECTION=pgsql
DB_HOST=your-neon-hostname.neon.tech
DB_PORT=5432
DB_DATABASE=your_database_name
DB_USERNAME=your_username
DB_PASSWORD=your_password
```

After updating the `.env` file, run the database migrations:

```bash
php artisan migrate
```

## Implementing Multi-Tenancy

Now that we have our basic setup, let's implement multi-tenancy in our application.

### Creating the Tenant Model

Create a `Tenant` model:

```bash
php artisan make:model Tenant
```

Update the `app/Models/Tenant.php` file:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Stancl\Tenancy\Database\Models\Tenant as BaseTenant;
use Stancl\Tenancy\Contracts\TenantWithDatabase;
use Stancl\Tenancy\Database\Concerns\HasDatabase;
use Stancl\Tenancy\Database\Concerns\HasDomains;

class Tenant extends BaseTenant implements TenantWithDatabase
{
    use HasFactory, HasDatabase, HasDomains;

}
```

This model extends the base `Tenant` model provided by the tenancy package and implements the `TenantWithDatabase` interface. We've also defined the fillable attributes and custom columns for our tenant.

The `HasDatabase` and `HasDomains` traits provided by the tenancy package allow us to manage tenant-specific databases and domains. This essentially means that each tenant will have its own database and domain providing data isolation between tenants.

To learn more about the tenancy package event system and how to customize the tenant model, refer to the [stancl/tenancy documentation](https://tenancyforlaravel.com/docs/v3/event-system).

### Configuring Tenancy

Update the `config/tenancy.php` file to use our custom `Tenant` model:

```php
'tenant_model' => \App\Models\Tenant::class,
```

Also, update the central domains configuration:

```php
'central_domains' => [
    'laravel-multi-tenant-saas.test',
    'localhost',
    'example.com',
],
```

Replace the default central domains with your own domain names.

This is an important part as this is how the tenancy package will determine which domain belongs to which tenant and load the tenant-specific data accordingly.

Feel free to review the other configuration options in the `config/tenancy.php` file to customize the tenancy behavior based on your requirements.

### Creating Tenant Migrations

The tenancy package has built-in event listeners that automatically run tenant-specific migrations when a tenant is created. For this we need to make sure that all of the tenant-specific migrations are in the `database/migrations/tenant` directory.

As each tenant will have its own database, the migrations in the tenant directory will be used to create tenant-specific tables in the tenant's database.

Start by copying the default User migration to the `database/migrations/tenant` directory:

```bash
cp database/migrations/0001_01_01_000000_create_users_table.php database/migrations/tenant
```

This will be the base migration for tenant-specific tables.

### Implementing Tenant Routes

The tenancy package provides middleware to handle tenant-specific routes. This allows you to define routes that are accessible only to tenants and not to central domains.

Start by creating a new file `routes/tenant.php` for tenant-specific routes with the following content:

```php
<?php

use Illuminate\Support\Facades\Route;
use Stancl\Tenancy\Middleware\InitializeTenancyByDomain;
use Stancl\Tenancy\Middleware\PreventAccessFromCentralDomains;

/*
|--------------------------------------------------------------------------
| Tenant Routes
|--------------------------------------------------------------------------
|
| Here you can register the tenant routes for your application.
| These routes are loaded by the TenantRouteServiceProvider.
|
| Feel free to customize them however you want. Good luck!
|
*/

Route::middleware([
    'web',
    InitializeTenancyByDomain::class,
    PreventAccessFromCentralDomains::class,
])->group(function () {
    Route::get('/', function () {
        return 'This is your multi-tenant application. The id of the current tenant is ' . tenant('id');
    });

    // Here you can add more tenant-specific routes
});
```

These routes will be loaded by the `TenantRouteServiceProvider` and will be accessible only to tenants. The `InitializeTenancyByDomain` middleware will set the current tenant based on the domain, and the `PreventAccessFromCentralDomains` middleware will prevent access from central domains.

For more information on how to customize the tenancy routes, refer to the [stancl/tenancy documentation](https://tenancyforlaravel.com/docs/v3/routes).

### Implementing Tenant Creation

Create a controller for tenant registration, this would usually be done by the admin users of the application:

```bash
php artisan make:controller TenantController
```

Update the `app/Http/Controllers/TenantController.php` controller and implement the tenant registration process:

```php
<?php

namespace App\Http\Controllers;

use App\Models\Tenant;
use Illuminate\Http\Request;

class TenantController extends Controller
{
    public function showRegistrationForm()
    {
        return view('tenant.register');
    }

    public function register(Request $request)
    {
        $request->validate([
            'domain' => 'required|string|max:255|unique:domains,domain',
        ]);

        $tenant = Tenant::create();
        $tenant->domains()->create(['domain' => $request->domain]);

        return redirect()->route('tenant.registered', $request->domain);
    }

    public function registered($domain)
    {
        return view('tenant.registered', compact('domain'));
    }
}
```

This controller handles tenant registration, creates a new tenant in the database, and sets up the tenant's domain. The `TenancyServiceProvider` will automatically map the tenancy events to the listener, which will create the tenant's database and run the tenant-specific migrations inside the `database/migrations/tenant` directory for the new tenant.

In a nutshell, the controller has three methods:

- `showRegistrationForm()`: Displays the tenant registration form
- `register()`: Registers a new tenant, which creates a new tenant record and domain
- `registered()`: Displays a success message after registration

This controller will be used to manage tenant registration in our application. Allowing new tenants to register and create their own subdomain and database for their account.

Add routes for tenant registration in `routes/web.php`:

```php
use App\Http\Controllers\TenantController;

Route::get('/register', [TenantController::class, 'showRegistrationForm'])->name('tenant.register');
Route::post('/register', [TenantController::class, 'register']);
Route::get('/registered/{domain}', [TenantController::class, 'registered'])->name('tenant.registered');
```

Create the corresponding views for tenant registration starting by creating the `resources/views/tenant/register.blade.php` file:

```html
<x-guest-layout>
  <form method="POST" action="{{ route('tenant.register') }}">
    @csrf
    <div class="mt-4">
      <x-input-label for="domain" :value="__('Subdomain')" />
      <div class="flex">
        <x-text-input
          id="domain"
          class="mt-1 block w-full"
          type="text"
          name="domain"
          :value="old('domain')"
          required
        />
        <span class="text-gray-600 ml-2 mt-1">.example.com</span>
      </div>
    </div>

    <div class="mt-4 flex items-center justify-end">
      <x-primary-button class="ml-4"> {{ __('Register Tenant') }} </x-primary-button>
    </div>
  </form>
</x-guest-layout>
```

Then create the `resources/views/tenant/registered.blade.php` file to display the success message after registration:

```html
<x-guest-layout>
  <div class="text-gray-600 mb-4 text-sm">
    {{ __('Your tenant has been registered successfully!') }}
  </div>

  <div class="mt-4 flex items-center justify-between">
    <div>
      Your tenant URL:
      <a
        href="https://{{ $domain }}.example.com"
        class="text-gray-600 hover:text-gray-900 text-sm underline"
        target="_blank"
        >https://{{ $domain }}.example.com</a
      >
    </div>
  </div>
</x-guest-layout>
```

This completes the tenant registration process. Tenants can now register and create their own subdomain and database for their account. In a real-world scenario, you would protect the registration routes with authentication middleware to ensure that only authorized admin users can create new tenants.

### Verifying Tenant Registration

To verify that the registration process works, visit `http://laravel-multi-tenant-saas.test/register` and register a new tenant. After registration, you should see the success message with the tenant's domain.

Next go to your Neon dashboard and verify that the new tenant's database has been created:

```sql
SELECT * FROM tenants;
```

You should see the newly created tenant in the `tenants` table. You can also check the `domains` table to verify that the tenant's domain has been added:

```sql
SELECT * FROM domains;
```

And to verify that you actually have a separate database for the new tenant, use the `\l` command in the `psql` console to list all databases or the following SQL query:

```sql
SELECT datname FROM pg_database WHERE datistemplate = false;
```

The tenant's database should be listed in the results and it should be named `tenant{tenant_id}`.

> The tenancy package allows you to configure the database naming convention for tenants. By default, the database name is `tenant{tenant_id}` where `{tenant_id}` is the ID of the tenant. You can also configure the package to use separate schemas instead of separate databases for tenants.

With that done, you've successfully implemented tenant registration in your multi-tenant SaaS application. Next let's implement the tenant onboarding process.

### Implementing Tenant Onboarding

Now that you can register new tenants, let's create an onboarding process.

Each tenant will need to create an account to access their dashboard. The domain will be used to identify the tenant, so we'll use the domain as the tenant's subdomain, e.g., `tenant1.example.com`.

Create a new controller for tenant onboarding:

```bash
php artisan make:controller Tenant/OnboardingController
```

Update the `app/Http/Controllers/Tenant/OnboardingController.php` to handle the onboarding process:

```php
<?php

namespace App\Http\Controllers\Tenant;

use App\Http\Controllers\Controller;
use App\Models\User;
use Illuminate\Http\Request;
use Illuminate\Support\Facades\Hash;

class OnboardingController extends Controller
{
    public function show()
    {
        if (User::count() > 0) {
            return redirect()->route('tenant.dashboard');
        }

        return view('tenant.onboarding');
    }

    public function store(Request $request)
    {
        $validated = $request->validate([
            'name' => 'required|string|max:255',
            'email' => 'required|string|email|max:255|unique:users',
            'password' => 'required|string|min:8|confirmed',
        ]);

        $user = User::create([
            'name' => $validated['name'],
            'email' => $validated['email'],
            'password' => Hash::make($validated['password']),
        ]);

        auth()->login($user);

        return redirect()->route('tenant.dashboard')->with('success', 'Welcome to your new account!');
    }
}
```

Add routes for the onboarding process in `routes/tenant.php` inside the `Route::middleware` group for tenant routes:

```php
use App\Http\Controllers\Tenant\OnboardingController;

Route::middleware([
    'web',
    InitializeTenancyByDomain::class,
    PreventAccessFromCentralDomains::class,
])->group(function () {
    // Existing routes
    // ...

    Route::get('/onboarding', [OnboardingController::class, 'show'])->name('tenant.onboarding');
    Route::post('/onboarding', [OnboardingController::class, 'store'])->name('tenant.onboarding.store');

});
```

Create the onboarding view in `resources/views/tenant/onboarding.blade.php`:

```html
<x-guest-layout>
  <form method="POST" action="{{ route('tenant.onboarding.store') }}">
    @csrf

    <div>
      <x-input-label for="name" :value="__('Name')" />
      <x-text-input
        id="name"
        class="mt-1 block w-full"
        type="text"
        name="name"
        :value="old('name')"
        required
        autofocus
        autocomplete="name"
      />
    </div>

    <div class="mt-4">
      <x-input-label for="email" :value="__('Email')" />
      <x-text-input
        id="email"
        class="mt-1 block w-full"
        type="email"
        name="email"
        :value="old('email')"
        required
        autocomplete="username"
      />
    </div>

    <div class="mt-4">
      <x-input-label for="password" :value="__('Password')" />
      <x-text-input
        id="password"
        class="mt-1 block w-full"
        type="password"
        name="password"
        required
        autocomplete="new-password"
      />
    </div>

    <div class="mt-4">
      <x-input-label for="password_confirmation" :value="__('Confirm Password')" />
      <x-text-input
        id="password_confirmation"
        class="mt-1 block w-full"
        type="password"
        name="password_confirmation"
        required
        autocomplete="new-password"
      />
    </div>

    <div class="mt-4 flex items-center justify-end">
      <x-primary-button class="ml-4"> {{ __('Complete Setup') }} </x-primary-button>
    </div>
  </form>
</x-guest-layout>
```

For simplicity, we're extending the Breeze guest layout for the onboarding form. But you can customize the layout to match your application's design and even have different layouts for the onboarding process based on each tenant's requirements.

To test the onboarding process, visit `http://tenant1.example.com/onboarding` and complete the onboarding form. After submitting the form, you should be redirected to the tenant dashboard which we'll implement next.

### Implementing Tenant Dashboard

Create a new controller for the tenant dashboard:

```bash
php artisan make:controller Tenant/DashboardController
```

Update the `app/Http/Controllers/Tenant/DashboardController.php` to display the tenant dashboard:

```php
<?php

namespace App\Http\Controllers\Tenant;

use App\Http\Controllers\Controller;
use Illuminate\Http\Request;

class DashboardController extends Controller
{
    public function index()
    {
        return view('tenant.dashboard');
    }
}
```

Create the dashboard view in `resources/views/tenant/dashboard.blade.php`:

```html
<x-app-layout>
  <x-slot name="header">
    <h2 class="text-gray-800 text-xl font-semibold leading-tight">{{ __('Dashboard') }}</h2>
  </x-slot>

  <div class="py-12">
    <div class="mx-auto max-w-7xl lg:px-8 sm:px-6">
      <div class="overflow-hidden bg-white shadow-sm sm:rounded-lg">
        <div class="text-gray-900 p-6">{{ __("You're logged in!") }}</div>
      </div>
    </div>
  </div>
</x-app-layout>
```

Add a route for the tenant dashboard in `routes/tenant.php` inside the `Route::middleware` group for tenant routes:

```php
use App\Http\Controllers\Tenant\DashboardController;

Route::middleware([
    'web',
    InitializeTenancyByDomain::class,
    PreventAccessFromCentralDomains::class,
])->group(function () {
    // Existing routes
    // ...
    Route::get('/dashboard', [DashboardController::class, 'index'])->name('tenant.dashboard');

});
```

To test the tenant dashboard, visit `http://tenant1.example.com/dashboard` after completing the onboarding process. You should see the dashboard view with a welcome message.

You can also check the `users` table in the tenant's database to verify that the user account created during onboarding has been added:

```sql
SELECT * FROM users;
```

This will show you the user account created during the onboarding process for that specific tenant in the tenant's database rather than the central database.

## Conclusion

In this tutorial, we've built a simple multi-tenant application using Laravel and Neon. We've covered:

1. Setting up the project and implementing multi-tenancy
2. Creating a tenant registration process
3. Implementing tenant onboarding
4. Adding a tenant dashboard for individual tenants

This implementation provides a foundation for building more complex SaaS applications with Laravel and Neon. You can further expand on this system by:

- Adding more features to the tenant dashboard
- Implementing billing and subscription management
- Enhancing security with two-factor authentication
- Adding more tenant-specific customizations

Using the `stancl/tenancy` package along with Neon, each tenant will have its own database. Thanks to Neon's autoscaling feature, you can easily scale your application as you onboard more tenants.

There are other packages and tools available to help you build multi-tenant applications with Laravel. You can explore these options based on your requirements and choose the one that best fits your needs. Some of the popular packages include:

- [spatie/laravel-multitenancy](https://spatie.be/docs/laravel-multitenancy/v3/introduction)
- [tenancy/tenancy](https://github.com/tenancy/tenancy)

## Additional Resources

- [Laravel Documentation](https://laravel.com/docs)
- [stancl/tenancy Documentation](https://tenancyforlaravel.com/)
- [Neon Documentation](/docs/)


# An Overview of Laravel and Postgres on Neon

---
title: An Overview of Laravel and Postgres on Neon
subtitle: Learn how to integrate Laravel with Postgres on Neon, leveraging Laravel's Eloquent ORM and migrations for efficient database management.
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-05-25T00:00:00.000Z'
updatedOn: '2024-05-25T00:00:00.000Z'
---

When combining the robust features of [Laravel](https://laravel.com/), a highly expressive PHP framework, with the efficiency and scalability of Postgres on Neon, developers gain a powerful toolset for web development.

Laravel's native support for Postgres ensures a smooth integration process. When working with Neon Postgres, the transition is nearly seamless, thanks to Laravel's database agnostic [migrations](https://laravel.com/docs/11.x/migrations) and [Eloquent ORM](https://laravel.com/docs/11.x/eloquent), which effortlessly maps application objects to database tables.

## Create a Neon project

If you do not have one already, create a Neon project. Save your connection details including your password. They are required when defining connection settings.

1. Navigate to the [Projects](https://console.neon.tech/app/projects) page in the Neon Console.
2. Click **New Project**.
3. Specify your project settings and click **Create Project**.

## Setting Up Your Environment for Laravel and Neon Postgres

Start by installing Laravel. For installation instructions, refer to the [Laravel documentation](https://laravel.com/docs/11.x/installation).

To get Laravel working with Neon Postgres, you'll need to configure your environment settings.

This process involves updating the `.env` file in your Laravel project to include the details for your Neon Postgres database connection.

Here's what you need to update in the `.env` file:

```env
DB_CONNECTION=pgsql
DB_HOST=your-neon-hostname.neon.tech
DB_PORT=5432
DB_DATABASE=<your-database-name>
DB_USERNAME=<your-username>
DB_PASSWORD=<your-password>
```

- `DB_CONNECTION`: This tells Laravel that you're using a PostgreSQL database.
- `DB_HOST`: Here, you'll put the address of your Neon database.
- `DB_PORT`: This is the port number for PostgreSQL, which is usually 5432.
- `DB_DATABASE`: The name of your database on Neon.
- `DB_USERNAME` and `DB_PASSWORD`: Your login credentials for the Neon database.

With these settings, Laravel can connect to your Neon Postgres database, allowing your application to interact with it.

## Using Eloquent and Migrations in Laravel

Laravel's migration system and Eloquent ORM are powerful tools that simplify database management and interaction.

When you use Eloquent with Neon Postgres, it allows you to handle database operations without writing any SQL queries directly, thanks to Laravel's expressive syntax. Along with the Laravel migration system, you can easily manage your database schema and perform operations like creating tables, defining relationships, and querying data.

### Database Migrations and Schema Management

Laravel's migration system is an essential tool for database schema management. Migrations ensure that your database structure is properly version-controlled.

To create a new migration in Laravel, you use the `make:migration` Artisan command:

```bash
php artisan make:migration create_books_table --create=books
```

This will create a new migration file in the `database/migrations` directory. In this file, you define the schema for the `books` table:

```php
Schema::create('books', function (Blueprint $table) {
    $table->id();
    $table->string('title');
    $table->unsignedBigInteger('author_id');
    $table->year('publication_year');
    $table->timestamps();
});
```

Once you've defined the schema, you can run the migration to create the table in your Neon Postgres database:

```bash
php artisan migrate
```

This will execute the migration and create the `books` table in your database and keep track of the migration history.

### Defining Models in Laravel

First, you define a model that represents a table in your database. Each model corresponds to a table and allows you to interact with the table's records. For example, to create a `Book` model, you would create a `Book.php` file in the `app/Models` directory or run the following Artisan command:

```bash
php artisan make:model Book
```

The command will create a `Book.php` file in the `app/Models` directory.

In the model file, you can define details about the table and its columns:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class Book extends Model
{
    protected $fillable = ['title', 'author', 'publication_year'];
}
```

The `fillable` array in the model protects your application from mass-assignment vulnerabilities by specifying which attributes should be assignable.

Additionally, in the model, you can define relationships with other models, set up mutators and accessors, and perform various database operations.

### Creating and Saving Records

With the model set up, you can create and save new records to your database using Eloquent.

In your controller or wherever you need to create a new record, you can instantiate the model, set its properties, and save it:

```php
$book = new Book;
$book->title = 'Sample Book';
$book->author_id = 1;
$book->publication_year = 2021;
$book->save();
```

This creates a new instance of the `Book` model, sets its properties (`title`, `author_id`, `publication_year`), and then saves the new record to the `books` table in your Neon Postgres database.

Using Eloquent, you can manage your database records with simple, expressive syntax, making your code cleaner and more maintainable.

## Using Queries Efficiently

When building an application, it's important to write efficient queries. The same is true when working with Laravel and Postgres.

Laravel's Eloquent ORM has features like relationships and eager loading that help you write better queries.

For example, if you're fetching authors and their books, instead of making a separate database query for each author's books (which can slow things down), you can use eager loading.

Let's say you have an `Author` model with a `books` relationship:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class Author extends Model
{
    use HasFactory;

    protected $fillable = ['name', 'email', 'biography'];

    public function books()
    {
        return $this->hasMany(Book::class);
    }
}
```

With this relationship defined, you can use eager loading to fetch authors and their books in a more efficient way:

```php
$authors = Author::with('books')->get();
```

This code fetches all authors and their associated books, reducing the number of queries made to the database.

## Postgres specific features in Laravel

Leveraging Postgres's advanced features within your Laravel application can significantly boost performance, enhance data integrity, and provide flexible data storage options.

### Utilizing Indexes

Indexes are a crucial aspect of database optimization, especially when dealing with large datasets. They work by creating a data structure that allows the database engine to quickly locate and retrieve the data without scanning the entire table.

For instance, if your application includes a feature that allows users to search for books by their titles, querying a large database without an index can be time-consuming. By indexing the `title` column, you significantly improve query performance:

```php
Schema::table('books', function (Blueprint $table) {
    $table->index('title');
});
```

After adding the index, queries that search for books by title, like the one below, become much more efficient, reducing the response time for your users:

```php
$books = Book::where('title', 'like', '%search-term%')->paginate(10);
```

### Ensuring Data Integrity with Foreign Key Constraints

Foreign key constraints are vital for maintaining referential integrity between tables. They ensure that relationships between tables remain consistent, preventing orphaned records and ensuring data accuracy.

In the context of a book application, where each book is linked to an author, defining a foreign key constraint ensures that every book in your database is tied to an existing author. Here's how you can define such a constraint in a Laravel migration:

```php
Schema::table('books', function (Blueprint $table) {
    $table->foreign('author_id')->references('id')->on('authors')->onDelete('cascade');
});
```

By using the `foreign()` method, you can ensure that when an author is deleted, all their books are also removed from the database, preventing orphaned records and maintaining the integrity of your data.

### Leveraging JSONB for Flexible Data Storage

Postgres's JSONB data type offers a powerful way to store and query JSON data, providing flexibility for your application's data storage needs. Unlike the standard JSON data type, JSONB stores data in a decomposed binary format, allowing for efficient querying.

Imagine you want to store various metadata about each book, such as tags, reviews, or custom attributes. The JSONB data type allows you to store this information in a structured, queryable format:

```php
Schema::table('books', function (Blueprint $table) {
    $table->jsonb('metadata')->nullable();
});
```

With this `metadata` column, you can easily store and retrieve structured data related to each book, making your application more flexible and adaptable to changing requirements.

By integrating these Postgres features into your Laravel application, you can enhance its performance, maintain data integrity, and provide a scalable solution for managing complex data structures.

## Testing and Neon Postgres Branches

When integrating Neon Postgres with your Laravel application, leveraging database branches for testing is a robust strategy to ensure the reliability and consistency of your tests.

Neon Postgres Branches allow you to create isolated database environments, similar to branching in version control systems like Git. By using a separate database branch for testing, you ensure that your test executions are isolated from your production data, maintaining data integrity and consistency.

Usually, when running tests in Laravel, you would use a separate database for testing to avoid affecting your production data. In most cases, developers use an in-memory SQLite database for testing. However, Neon Postgres branches offer a more solid solution for testing your Laravel application.

1. Neon Postgres lets you create branches of your database. This means you can have a dedicated branch just for testing purposes, where you can freely run tests, apply migrations, and modify data without affecting your production database.

2. With a testing branch, you can execute your entire suite of tests in an environment that mirrors production without the risk of corrupting your actual production data. This is particularly useful for integration tests that interact with the database.

3. Configuring your Laravel application to use a separate database branch for testing is straightforward. You adjust your testing environment configuration to point to the testing branch of your Neon Postgres database, ensuring that when Laravel runs tests, it uses this isolated database instance.

## Conclusion

Combining Laravel with Postgres on Neon offers a powerful and efficient environment for developing web applications. Laravel's seamless integration with Postgres allows developers to take advantage of the full power of both the framework and the database, providing a flexible, scalable, and developer-friendly platform.

The ability to use database branches for testing with Neon Postgres brings an additional layer of robustness to your development process, allowing for isolated testing environments that mirror your production setup without risking data integrity.

Laravel's expressive syntax combined with Neon Postgres's powerful features allow developers to build complex, data-driven applications efficiently and effectively.


# Implementing Queue Workers and Job Processing in Laravel with Neon Postgres

---
title: Implementing Queue Workers and Job Processing in Laravel with Neon Postgres
subtitle: Learn how to implement efficient background processing in Laravel using queue workers and Neon Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-07-14T00:00:00.000Z'
updatedOn: '2024-07-14T00:00:00.000Z'
---

Laravel provides a powerful and flexible system for handling background processing through queues and scheduling. This allows you to improve your application's performance by offloading time-consuming tasks and automating recurring processes. In this comprehensive guide, we'll explore how to implement queue workers, job processing, and scheduled tasks in Laravel using Postgres as the queue driver.

By the end of this tutorial, you'll know how to build a system for background processing and task automation, using the power of Laravel queues and the scheduler with Neon Postgres.

## Prerequisites

Before we begin, ensure you have the following:

- PHP 8.1 or higher installed on your system
- [Composer](https://getcomposer.org/) for managing PHP dependencies
- A [Neon](https://console.neon.tech/signup) account for Postgres database hosting
- Basic knowledge of Laravel and database operations

## Setting up the Project

Let's start by creating a new Laravel project and setting up the necessary components. We'll use Composer to create a new Laravel project and configure it to use Postgres as the queue driver.

### Creating a New Laravel Project

Open your terminal and run the following command to create a new Laravel project:

```bash
composer create-project laravel/laravel laravel-queue-demo
cd laravel-queue-demo
```

### Setting up the Database

Update your `.env` file with your Neon Postgres database credentials:

```env
DB_CONNECTION=pgsql
DB_HOST=your-neon-hostname.neon.tech
DB_PORT=5432
DB_DATABASE=your_database_name
DB_USERNAME=your_username
DB_PASSWORD=your_password
```

Run the migrations:

```bash
php artisan migrate
```

This will create the necessary tables in your Neon Postgres database.

## Implementing Laravel Queues with Postgres

Out of the box, Laravel provides a unified API for working with queues, allowing you to push jobs onto the queue and process them in the background. We'll configure Laravel to use Postgres as the queue driver and create a sample job to demonstrate the queue processing.

### Configuring the Queue Connection

To configure Laravel to use Postgres as the queue driver, update the `QUEUE_CONNECTION` in the `.env` file:

```env
QUEUE_CONNECTION=database
```

This tells Laravel to use the database driver for the queue system. Some of the other available drivers are `sync`, `redis`, `beanstalkd`, `sqs`, and `null`.

### Creating the Jobs Table

As we're using the database driver for queues, we need to create a table to store the jobs.

If you don't already have a `jobs` table in your database, Laravel provides an Artisan command to generate the migration for the jobs table:

```bash
php artisan queue:table
php artisan migrate
```

The `jobs` table will be created in your Postgres database. It has the following columns:

- `id`: The unique identifier for the job.
- `queue`: The name of the queue the job belongs to.
- `payload`: The serialized job payload.
- `attempts`: The number of times the job has been attempted.
- `reserved_at`: The timestamp when the job was reserved by a worker.
- `available_at`: The timestamp when the job is available to be processed.
- `created_at`: The timestamp when the job was created.

## Creating and Dispatching Jobs

Now that we've set up the queue system, let's create a sample job and dispatch it to the queue.

We'll create a job called `GenerateDatabaseReport` that simulates generating a complex report from your database. When the job is dispatched, it will log a message indicating that the report has been processed.

Such jobs can be used to perform time-consuming tasks like sending emails, processing images, or interacting with external APIs without blocking the main application.

### Creating a Job

Let's start by creating a job called `GenerateDatabaseReport` using the following Artisan command:

```bash
php artisan make:job GenerateDatabaseReport
```

This will create a file at `app/Jobs/GenerateDatabaseReport.php`. Update the job class with the following content to simulate processing a report:

```php
<?php

namespace App\Jobs;

use Illuminate\Bus\Queueable;
use Illuminate\Contracts\Queue\ShouldQueue;
use Illuminate\Foundation\Bus\Dispatchable;
use Illuminate\Queue\InteractsWithQueue;
use Illuminate\Queue\SerializesModels;
use Illuminate\Support\Facades\Log;

class GenerateDatabaseReport implements ShouldQueue
{
    use Dispatchable, InteractsWithQueue, Queueable, SerializesModels;

    protected $reportId;

    public function __construct($reportId)
    {
        $this->reportId = $reportId;
    }

    public function handle()
    {
        sleep(5);
        Log::info("Processed report: {$this->reportId}");
    }
}
```

Let's break down the key components of this job class:

1. `implements ShouldQueue`: This interface tells Laravel that this job should be pushed onto the queue instead of running synchronously.

2. Use statements:
   - `Dispatchable`: Allows the job to be dispatched to the queue.
   - `InteractsWithQueue`: Provides methods for interacting with the queue.
   - `Queueable`: Allows the job to be pushed onto queues.
   - `SerializesModels`: Automatically serializes and deserializes Eloquent models in the job.

3. `protected $reportId`: This property stores the ID of the report to be processed.

4. `__construct($reportId)`: The constructor accepts a report ID and assigns it to the `$reportId` property.

5. `handle()` method: This is where the main logic of the job is implemented. In this example:
   - `sleep(5)` simulates a time-consuming process.
   - `Log::info(...)` logs a message indicating that the report has been processed.

When this job is dispatched, Laravel will serialize it and store it in the database. When a queue worker picks up the job, it will deserialize it and call the `handle()` method.

The `sleep(5)` call is just for demonstration purposes. In a real-world scenario, you'd replace this with actual report processing logic, such as fetching data from the database, generating a report, and storing it in a file or sending it via email.

### Dispatching the Job

For testing purposes, let's dispatch the `GenerateDatabaseReport` job when a specific route is accessed. This will simulate pushing the job onto the queue for processing, but in a real-world scenario, you'd dispatch jobs from your application logic based on specific events or triggers (e.g., user registration, order completion) or using the Laravel scheduler for recurring tasks like sending daily reports.

Add a route in `routes/web.php`:

```php
use App\Jobs\GenerateDatabaseReport;

Route::get('/dispatch-job', function () {
    GenerateDatabaseReport::dispatch(1);
    return 'Job dispatched!';
});
```

This route will dispatch the `GenerateDatabaseReport` job with the report ID `1` when accessed. You can test this by visiting `/dispatch-job` in your browser or using a tool like Postman or `curl`, which will trigger the job processing in the background, returning a response immediately instead of waiting for the job to complete.

As we are using the `database` queue driver, the job will be stored in the `jobs` table in your Neon Postgres database.

If you were to check the `jobs` table in your database, you would see an entry for the dispatched job with the serialized payload and other metadata:

```sql
SELECT * FROM jobs;
```

This will show you the job entry in the `jobs` table, which includes the serialized payload, queue name, and other metadata.

## Running Queue Workers

Now that we've dispatched a job to the queue, we need to run a queue worker to process the job. Queue workers listen for new jobs on the queue and execute them in the background.

Start a queue worker using the following Artisan command:

```bash
php artisan queue:work
```

This will start a queue worker that listens for new jobs on the default queue. You can specify the queue name using the `--queue` option if you have multiple queues. Having multiple queues allows you to prioritize jobs based on their importance or processing requirements.

You will see the following output:

```php
   INFO  Processing jobs from the [default] queue.

  2024-07-13 16:29:51 App\Jobs\GenerateDatabaseReport ........... RUNNING
  2024-07-13 16:29:58 App\Jobs\GenerateDatabaseReport ........... 6s DONE
```

The job will be picked up by the queue worker, processed, and the log message will be written to the log file. You can check the log file to verify that the job was processed successfully. It should take approximately 5 seconds to process the job due to the `sleep(5)` call in the job logic but in a real-world scenario, the processing time will depend on the actual logic implemented in the job.

If you were to check the `jobs` table in your database after the job has been processed, you would see that the job has been removed from the table, indicating that it has been successfully processed.

```sql
SELECT * FROM jobs;
```

This will show you that the job has been removed from the `jobs` table after processing.

## Handling Job Failures and Retries

Laravel provides mechanisms for handling failed jobs and configuring job retries. Let's explore how to configure failed job storage, handle failed jobs, and set up job retries.

### Configuring Failed Job Storage

If you don't already have a table for failed jobs in your database, you can use Laravel's Artisan commands to create one. Run the following commands:

```bash
php artisan queue:failed-table
php artisan migrate
```

This will create a `failed_jobs` table in your Postgres database to store information about failed jobs for debugging and analysis. The failed jobs will be stored in this table when a job fails to process and later retried or manually processed as needed.

### Handling Failed Jobs

Add a `failed` method to your job class to handle failed jobs:

```php
public function failed(\Throwable $exception)
{
    Log::error("Failed to process report {$this->reportId}: {$exception->getMessage()}");
}
```

This method will be called when a job fails to process. You can log the error message or perform additional actions based on the failure, such as sending an email notification or updating a status in the database.

### Configuring Job Retries

In some cases, you may want to retry a job if it fails to process. Laravel allows you to configure the number of retries and the timeout for a job.

Add retry and timeout configurations to your job class, for example:

```php
public $tries = 3;
public $timeout = 120; // 2 minutes
```

This configuration will retry the job up to 3 times if it fails and set a timeout of 2 minutes for each job execution. You can adjust these values based on your application's requirements.

To simulate a job failure, you can throw an exception in the `handle` method:

```php
public function handle()
{
    throw new \Exception("Failed to process report: {$this->reportId}");
}
```

Visit the `/dispatch-job` route to dispatch the job with the exception thrown and run the queue worker to process the job.

```bash
php artisan queue:work
```

When the job is dispatched and processed, it will fail due to the exception thrown in the `handle` method. The job will be retried based on the configuration you've set.

After the job fails to process, you can check the `failed_jobs` table in your database to see the failed job entry:

```sql
SELECT * FROM failed_jobs;
```

In the `failed_jobs` table, you'll see the failed job entry with information about the job, the exception message, and the number of attempts made.

You can also manually retry a failed job using the `queue:retry` Artisan command:

```bash
php artisan queue:retry job-id
```

Replace `job-id` with the ID of the failed job you want to retry. This will requeue the job for processing.

## Implementing Scheduled Jobs

Laravel's scheduler allows you to expressively define your command schedule within your Laravel application itself. In Laravel 11, this is done using the `routes/console.php` file, simplifying the process and keeping all routing-related code in one place.

### Configuring the Scheduler

Let's add a scheduled task that dispatches our `GenerateDatabaseReport` job every hour:

```php
<?php

use Illuminate\Support\Facades\Schedule;
use App\Jobs\GenerateDatabaseReport;

Schedule::job(new GenerateDatabaseReport(1))->hourly();
```

This will dispatch the `GenerateDatabaseReport` job every hour. You can define more complex schedules using the `Schedule` facade, such as daily, weekly, or custom schedules based on your application's requirements.

### Running the Scheduler

To run the scheduler, you still need to add the following Cron entry to your server:

```shell
* * * * * cd /path-to-your-project && php artisan schedule:run >> /dev/null 2>&1
```

This Cron will call the Laravel command scheduler every minute. Laravel then evaluates your scheduled tasks and runs the tasks that are due.

This approach provides a way to automate recurring tasks in your application, such as sending daily reports, cleaning up temporary files, or updating data from external sources, all without having to manage multiple Cron jobs.

## Additional Queue Processing Techniques

Laravel offers several handy techniques for working with queues, allowing you to build more complex and efficient job processing systems. Let's explore some of these techniques in detail, focusing on how they work with Postgres as our queue driver.

### Job Chaining

Job chaining is a powerful feature in Laravel that allows you to specify a sequence of jobs that should be run in order. This is particularly useful when you have a series of related tasks that need to be executed sequentially.

Here's how job chaining works:

```php
use App\Jobs\GenerateDatabaseReport;
use App\Jobs\VerifyDatabaseReport;
use App\Jobs\GeneratedDatabaseReport;

GenerateDatabaseReport::dispatch(1)
    ->chain([
        new VerifyDatabaseReport(1),
        new GeneratedDatabaseReport(1)
    ]);
```

In this example:

1. The `GenerateDatabaseReport` job is dispatched first.
2. Once `GenerateDatabaseReport` completes successfully, `VerifyDatabaseReport` is automatically dispatched.
3. After `VerifyDatabaseReport` finishes, `GeneratedDatabaseReport` is dispatched.

If any job in the chain fails, the subsequent jobs won't be executed. This ensures that your entire process maintains integrity.

You can also add delays between chained jobs:

```php
GenerateDatabaseReport::dispatch(1)
    ->chain([
        new VerifyDatabaseReport(1),
        (new GeneratedDatabaseReport(1))->delay(now()->addMinutes(10))
    ]);
```

This will delay the `GeneratedDatabaseReport` job by 10 minutes after `VerifyDatabaseReport` completes.

### Job Batching

Job batching allows you to group related jobs together, monitor their execution as a single unit, and perform actions when the entire batch completes. This is incredibly useful for processing large datasets or performing complex, multi-step operations.

To use job batching with Postgres, you first need to create a batches table:

```bash
php artisan queue:batches-table
php artisan migrate
```

Here's an example of job batching in Laravel:

```php
use Illuminate\Support\Facades\Bus;
use App\Jobs\GenerateDatabaseReport;

$batch = Bus::batch([
    new GenerateDatabaseReport(1),
    new GenerateDatabaseReport(2),
    new GenerateDatabaseReport(3),
])->then(function (Batch $batch) {
    // All jobs completed successfully...
    Log::info('All reports processed successfully');
})->catch(function (Batch $batch, Throwable $e) {
    // First batch job failure detected...
    Log::error('Batch job failed: ' . $e->getMessage());
})->finally(function (Batch $batch) {
    // The batch has finished executing...
    Log::info('Batch processing completed');
})->dispatch();
```

Key points about job batching:

- The `then()` callback is executed if all jobs in the batch complete successfully.
- The `catch()` callback is executed if any job in the batch fails.
- The `finally()` callback is always executed when the batch finishes, regardless of success or failure.

You can also add jobs to an existing batch:

```php
$batch->add(new GenerateDatabaseReport(4));
```

And you can check the progress of a batch using the `progress()` method:

```php
$batchId = $batch->id;

// Later...
$batch = Bus::findBatch($batchId);
$progress = $batch->progress(); // Returns a percentage
```

### Rate Limiting

Rate limiting is very helpful for preventing your application from overwhelming external services or your own database. When using Postgres as your queue driver, you can implement rate limiting at the application level. Here's an example of how you might do this:

```php
use Illuminate\Support\Facades\DB;
use Carbon\Carbon;

class GenerateDatabaseReport implements ShouldQueue
{
    public function handle()
    {
        $key = 'process-report';
        $limit = 10; // Number of jobs
        $duration = 60; // Time period in seconds

        $count = DB::table('jobs')
            ->where('queue', $key)
            ->where('created_at', '>=', Carbon::now()->subSeconds($duration))
            ->count();

        if ($count < $limit) {
            // Job logic...
            Log::info('Processing report...');
        } else {
            // Rate limit exceeded, release the job back to the queue
            $this->release(10); // Release back to queue after 10 seconds
        }
    }
}
```

In this example:

- We're using the `jobs` table in Postgres to track our rate limit.
- We allow 10 jobs to be processed every 60 seconds.
- If a job can be processed within this limit, the job logic is executed.
- If the rate limit has been exceeded, the job is released back to the queue with a 10-second delay.

You can also use different keys for different types of jobs:

```php
$key = "process-report:{$this->reportId}";
```

This allows you to have separate rate limits for different reports or different types of jobs.

## Monitoring and Managing Queues

Laravel provides several Artisan commands for monitoring and managing your queues:

- `queue:work`: Process new jobs as they are pushed onto the queue
- `queue:listen`: Similar to `queue:work`, but will reload the worker after each job
- `queue:retry`: Retry a failed job
- `queue:failed`: List all of the failed jobs
- `queue:flush`: Delete all of the failed jobs

## Implementing Supervisor for Queue Workers

Running queue workers using the `queue:work` command works well for development environments, but it's not suitable for production environments.

For production environments, use [Supervisor](http://supervisord.org/) to ensure your queue workers are always running.

### Installing and Configuring Supervisor

Install Supervisor:

```bash
sudo apt-get install supervisor
```

Create a new Supervisor configuration file:

```bash
sudo nano /etc/supervisor/conf.d/laravel-worker.conf
```

Add the following content:

```ini
[program:laravel-worker]
process_name=%(program_name)s_%(process_num)02d
command=php /path/to/your/project/artisan queue:work database --sleep=3 --tries=3 --max-time=3600
autostart=true
autorestart=true
user=www-data
numprocs=8
redirect_stderr=true
stdout_logfile=/path/to/your/project/worker.log
stopwaitsecs=3600
```

Start Supervisor:

```bash
sudo supervisorctl reread
sudo supervisorctl update
sudo supervisorctl start laravel-worker:*
```

## Conclusion

In this guide, we've explored how to implement queue workers, job processing, and scheduled tasks in Laravel using Postgres as the queue driver. We've covered creating and dispatching jobs, running queue workers, handling job failures and retries, implementing scheduled jobs, and setting up Supervisor for production environments.

## Additional Resources

- [Laravel Queues Documentation](https://laravel.com/docs/11.x/queues)
- [Laravel Task Scheduling](https://laravel.com/docs/11.x/scheduling)
- [Supervisor Documentation](http://supervisord.org/)
- [Neon Documentation](/docs)


# A Deep Dive into Laravel's Routes, Middleware, and Validation: Optimizing Database Interactions

---
title: "A Deep Dive into Laravel's Routes, Middleware, and Validation: Optimizing Database Interactions"
subtitle: Explore Laravel's core features to build efficient and secure web applications with optimized database interactions using Neon Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-07-14T00:00:00.000Z'
updatedOn: '2024-07-14T00:00:00.000Z'
---

Laravel, a popular PHP framework, provides a wide range of tools for building web applications. Among its core features are routing, middleware, and validation, which work together to create secure, efficient, and well-structured applications. In this guide, we'll explore these concepts, with a particular focus on how they interact with and optimize database operations.

By the end of this tutorial, you'll have a good understanding of how to structure your Laravel application's request lifecycle, from the initial route hit to the final database query, all while ensuring proper validation and middleware checks.

## Prerequisites

Before we begin, ensure you have the following:

- PHP 8.1 or higher installed on your system
- [Composer](https://getcomposer.org/) for managing PHP dependencies
- A [Neon](https://console.neon.tech/signup) account for Postgres database hosting
- Basic knowledge of Laravel and database operations

## Setting up the Project

Let's start by creating a new Laravel project and setting up the necessary components.

### Creating a New Laravel Project

Open your terminal and run the following command to create a new Laravel project:

```bash
composer create-project laravel/laravel laravel-routes-middleware-validation
cd laravel-routes-middleware-validation
```

### Setting up the Database

Update your `.env` file with your Neon Postgres database credentials:

```env
DB_CONNECTION=pgsql
DB_HOST=your-neon-hostname.neon.tech
DB_PORT=5432
DB_DATABASE=your_database_name
DB_USERNAME=your_username
DB_PASSWORD=your_password
```

### Understanding Laravel Routing

Routing in Laravel is a fundamental concept that defines how your application responds to incoming HTTP requests. It's the entry point for all requests to your application, determining which code should be executed based on the URL and HTTP method.

#### Basic Routing

Let's start with a basic route that interacts with the database. We'll create a route to fetch and display a list of users.

Open `routes/web.php` and add the following route:

```php
use App\Models\User;
use Illuminate\Support\Facades\Route;

Route::get('/users', function () {
    $users = User::all();
    return view('users.index', ['users' => $users]);
});
```

This route does the following:

1. It responds to GET requests to the `/users` URL.
2. It uses a closure function to define the route's behavior.
3. Inside the closure, it fetches all users from the database using the `User` model.
4. It returns a view named `users.index`, passing the fetched users to the view.

While this approach works for simple routes, it's generally not recommended for larger applications. As your application grows, putting logic directly in route closures can lead to cluttered and hard-to-maintain code.

#### Introducing Controllers

In practice, it's better to use controllers to handle the logic for your routes. Controllers group related request handling logic into a single class. Let's create a controller for our user-related routes:

```bash
php artisan make:controller UserController
```

This command creates a new `UserController` in `app/Http/Controllers/UserController.php`. Now, let's modify our route to use this controller:

```php
use App\Http\Controllers\UserController;

Route::get('/users', [UserController::class, 'index']);
```

In `UserController.php` is where you define your logic, like fetching users from the database:

```php
namespace App\Http\Controllers;

use App\Models\User;

class UserController extends Controller
{
    public function index()
    {
        $users = User::all();
        return view('users.index', ['users' => $users]);
    }
}
```

This approach separates our route definition from its logic, making our code more organized and easier to maintain.

#### Route Parameters

Route parameters allow you to capture parts of the URI as variables. They're particularly helpful for creating dynamic routes. Let's create a route to display a specific user's details:

```php
Route::get('/users/{id}', [UserController::class, 'show']);
```

In `UserController.php`, add the `show` method to fetch and display a specific user:

```php
public function show($id)
{
    $user = User::findOrFail($id);
    return view('users.show', ['user' => $user]);
}
```

This route and method do the following:

1. The `{id}` in the route definition is a route parameter.
2. Laravel passes this parameter to the `show` method.
3. We use `findOrFail` to fetch the user by ID.
4. If the user is not found, Laravel automatically returns a 404 response.
5. If found, we return a view with the user's details.

#### Route Model Binding

Laravel offers an even more elegant way to handle route parameters with Eloquent models. It's called implicit route model binding:

```php
Route::get('/users/{user}', [UserController::class, 'show']);
```

And in the controller we can type-hint the `User` model:

```php
public function show(User $user)
{
    return view('users.show', ['user' => $user]);
}
```

With this approach:

1. Laravel automatically resolves `{user}` to an instance of the `User` model.
2. If no matching model is found, it automatically returns a 404 response.
3. This reduces boilerplate code and uses Laravel's model binding feature.

#### Route Groups

Route groups allow you to share route attributes across multiple routes. This is particularly useful for applying middleware, prefixes, or namespaces to a set of routes.

```php
Route::middleware(['auth'])->group(function () {
    Route::get('/dashboard', [DashboardController::class, 'index']);
    Route::get('/profile', [ProfileController::class, 'show']);
});
```

This group does the following:

1. It applies the `auth` middleware to all routes within the group.
2. The `dashboard` and `profile` routes are now protected and only accessible to authenticated users.
3. It keeps our routes DRY (Don't Repeat Yourself) by applying shared attributes in one place.

You can also nest route groups for more complex structures:

```php
Route::prefix('admin')->middleware(['auth', 'admin'])->group(function () {
    Route::get('/users', [AdminUserController::class, 'index']);
    Route::get('/posts', [AdminPostController::class, 'index']);
});
```

This creates a group of admin routes that:

1. All start with `/admin`
2. Require authentication and admin privileges
3. Are handled by admin-specific controllers

## Implementing Middleware

Middleware acts as a powerful mechanism for filtering HTTP requests hitting your application. It's essential for implementing features like authentication, CORS handling, and request/response modifications. In Laravel 11, the way middleware is handled has been streamlined for better performance and easier configuration.

By using middleware, you can:

1. Perform actions before the request reaches your application
2. Perform actions after the application generates a response
3. Modify the request or response as needed

### Creating Custom Middleware

Let's create a custom middleware to check if a user has admin privileges. You can use the following Artisan command:

```bash
php artisan make:middleware CheckAdminStatus
```

This creates a new file `app/Http/Middleware/CheckAdminStatus.php`. Let's update it with our logic to check for admin status:

```php
<?php

namespace App\Http\Middleware;

use Closure;
use Illuminate\Http\Request;
use Symfony\Component\HttpFoundation\Response;

class CheckAdminStatus
{
    public function handle(Request $request, Closure $next): Response
    {
        if (!$request->user() || !$request->user()->is_admin) {
            return redirect('/')->with('error', 'You do not have admin access.');
        }

        return $next($request);
    }
}
```

This middleware:

1. Checks if there's an authenticated user and if they have admin status
2. If not, it redirects to the home page with an error message
3. If the user is an admin, it allows the request to proceed

### Registering Middleware

In Laravel 11, middleware registration has been simplified. You no longer need to register middleware in the `Kernel.php` file. Instead, you can register middleware directly in your `bootstrap/app.php` file:

```php
$app->routeMiddleware([
    'auth' => App\Http\Middleware\Authenticate::class,
    'admin' => App\Http\Middleware\CheckAdminStatus::class,
]);
```

This registers the `CheckAdminStatus` middleware with the key `admin`, allowing you to apply it to specific routes.

### Applying Middleware to Routes

Now you can apply this middleware to routes that require admin access:

```php
use App\Http\Controllers\AdminController;

Route::middleware(['auth', 'admin'])->group(function () {
    Route::get('/admin/dashboard', [AdminController::class, 'dashboard']);
    Route::get('/admin/users', [AdminController::class, 'users']);
});
```

This route group:

1. Applies both the `auth` and `admin` middleware
2. Groups together routes that should only be accessible to authenticated admin users
3. Uses controller methods to handle the requests, keeping the route file clean

### Middleware Parameters

Laravel allows you to pass parameters to your middleware. This can be useful when you need to customize middleware behavior based on the route or request context.

Let's modify our `CheckAdminStatus` middleware to accept a required permission level:

```php
public function handle(Request $request, Closure $next, int $requiredLevel): Response
{
    if (!$request->user() || $request->user()->admin_level < $requiredLevel) {
        return redirect('/')->with('error', 'You do not have sufficient privileges.');
    }

    return $next($request);
}
```

You can then use this middleware with parameters in your routes:

```php
Route::get('/admin/users', [AdminController::class, 'users'])
    ->middleware('admin:2'); // Requires admin level 2 or higher
```

As a good practice, each middleware should have a single responsibility.

## Implementing Validation

Validation is an important aspect of any web application. It allows you to check that incoming data meets specific criteria before processing. Laravel provides a validation system that integrates easily with your routes, controllers, and database operations.

### Basic Validation

Let's start with a basic example of validating user input when creating a new user:

```php
use App\Models\User;
use Illuminate\Http\Request;
use Illuminate\Support\Facades\Hash;
use Illuminate\Validation\Rules\Password;

Route::post('/users', function (Request $request) {
    $validated = $request->validate([
        'name' => 'required|string|max:255',
        'email' => 'required|string|email|max:255|unique:users',
        'password' => ['required', 'confirmed', Password::min(8)],
    ]);

    $user = User::create([
        'name' => $validated['name'],
        'email' => $validated['email'],
        'password' => Hash::make($validated['password']),
    ]);

    return redirect()->route('users.show', ['id' => $user->id])
        ->with('success', 'User created successfully');
});
```

This example demonstrates several key points:

1. The `validate` method automatically returns a 422 response with validation errors if validation fails.
2. Validated data is returned if validation passes, allowing you to safely use it.
3. The `unique:users` rule checks the database to ensure the email isn't already in use.

### Validation Error Handling

By default, Laravel automatically redirects the user back to the previous page with the validation errors and old input if validation fails. You can access these in your views:

```php
@if ($errors->any())
    <div class="alert alert-danger">
        <ul>
            @foreach ($errors->all() as $error)
                <li>{{ $error }}</li>
            @endforeach
        </ul>
    </div>
@endif

<form method="POST" action="/users">
    @csrf
    <input type="text" name="name" value="{{ old('name') }}">
    <!-- Other form fields -->
</form>
```

This code snippet displays validation errors and repopulates your form fields with old user input.

### Custom Error Messages

You can customize validation error messages by passing an array of messages as the second argument to the `validate` method:

```php
$validated = $request->validate([
    'name' => 'required|string|max:255',
    'email' => 'required|string|email|max:255|unique:users',
], [
    'name.required' => 'A name is required',
    'email.unique' => 'This email is already registered',
]);
```

This allows you to provide more user-friendly error messages.

### Form Request Validation

For more complex validation scenarios, Laravel provides Form Request classes. These are particularly useful when you have validation logic that you want to reuse across multiple controllers or routes.

Let's create a form request for updating user profiles:

```bash
php artisan make:request UpdateUserProfileRequest
```

Now, let's update `app/Http/Requests/UpdateUserProfileRequest.php`:

```php
<?php

namespace App\Http\Requests;

use Illuminate\Foundation\Http\FormRequest;
use Illuminate\Validation\Rule;

class UpdateUserProfileRequest extends FormRequest
{
    public function authorize()
    {
        return $this->user() !== null;
    }

    public function rules()
    {
        return [
            'name' => 'required|string|max:255',
            'email' => [
                'required',
                'string',
                'email',
                'max:255',
                Rule::unique('users')->ignore($this->user()->id),
            ],
            'bio' => 'nullable|string|max:1000',
            'avatar' => 'nullable|image|max:1024',
        ];
    }

    public function messages()
    {
        return [
            'email.unique' => 'This email is already in use by another account.',
            'avatar.max' => 'The avatar must not be larger than 1MB.',
        ];
    }
}
```

Now we can use this Form Request in our controller:

```php
use App\Http\Requests\UpdateUserProfileRequest;

class ProfileController extends Controller
{
    public function update(UpdateUserProfileRequest $request)
    {
        $user = $request->user();
        $user->update($request->validated());

        if ($request->hasFile('avatar')) {
            $user->avatar = $request->file('avatar')->store('avatars', 'public');
            $user->save();
        }

        return redirect()->route('profile')
            ->with('success', 'Profile updated successfully');
    }
}
```

This approach offers several benefits:

1. Validation logic is encapsulated and reusable across multiple routes or controllers.
2. The controller stays clean and focused on its primary responsibility of handling requests and responses.
3. The `authorize` method allows for permission checks before validation.

### Custom Validation Rules

Laravel allows you to create custom validation rules. This is useful when you have specific validation requirements that aren't covered by Laravel's built-in rules and when you want to reuse these rules across your application.

Let's create a rule to ensure a string contains no spaces:

```bash
php artisan make:rule NoSpaces
```

Update the `app/Rules/NoSpaces.php` file to add the validation logic:

```php
<?php

namespace App\Rules;

use Closure;
use Illuminate\Contracts\Validation\ValidationRule;

class NoSpaces implements ValidationRule
{
    public function validate(string $attribute, mixed $value, Closure $fail): void
    {
        if (str_contains($value, ' ')) {
            $fail('The :attribute must not contain spaces.');
        }
    }
}
```

All that we validate is that the string doesn't contain any spaces using the `str_contains` function.

You can now use this rule in your validations:

```php
use App\Rules\NoSpaces;

$request->validate([
    'username' => ['required', 'string', new NoSpaces],
]);
```

This custom rule ensures that the `username` field doesn't contain any spaces before it's stored in the database, but you can use it for any other validation logic you need.

## Optimizing Database Interactions

Efficient database interactions are very important when building high-performance Laravel applications. As your application scales, optimizing these interactions becomes increasingly important. Let's explore various techniques to improve database performance.

### Understanding the N+1 Query Problem and Eager Loading

The N+1 query problem is a common performance issue in ORM systems. It occurs when you fetch a list of records and then make additional queries for each record to retrieve related data.

#### Example of N+1 Problem:

```php
$posts = Post::all();
foreach ($posts as $post) {
    echo $post->author->name; // This causes an additional query for each post
}
```

This code results in 1 query to fetch all posts, plus N queries (where N is the number of posts) to fetch each post's author. This can lead to a large number of queries and slow performance.

#### Solving with Eager Loading:

Eager loading solves this by loading all related data in a single query:

```php
Route::get('/posts', function () {
    $posts = Post::with('author')->paginate(20);
    return view('posts.index', ['posts' => $posts]);
});
```

This loads all posts and their authors in just 2 queries, regardless of the number of posts. The `with('author')` method specifies the relationship to eager load and prevents the N+1 problem.

#### Advanced Eager Loading:

You can eager load multiple relationships and even nest them:

```php
$posts = Post::with(['author', 'comments.user'])->get();
```

This loads posts, their authors, comments on each post, and the user who made each comment.

### Query Optimization Techniques

#### Indexing

Indexes are crucial for query performance. They allow the database to find data without scanning the entire table.

You can learn more about indexing in the [Neon documentation](/docs/postgres/indexes/).

In Laravel migrations, you can add indexes like this:

```php
Schema::table('users', function (Blueprint $table) {
    $table->index('email');
    $table->index(['last_name', 'first_name']);
});
```

Consider indexing:

- Foreign keys
- Columns used in WHERE clauses
- Columns used for sorting (ORDER BY)

Remember, while indexes speed up reads, they can slow down writes, so use them judiciously.

#### Chunking Results

When working with large datasets, use chunking to process records in batches:

```php
User::chunk(100, function ($users) {
    foreach ($users as $user) {
        // Process user
    }
});
```

This prevents loading all records into memory at once, reducing memory usage and improving performance. This can be especially useful for tasks like sending emails to all users or processing large datasets.

### Caching Strategies

Caching can significantly reduce database load for frequently accessed, rarely changing data. Cache data that's expensive to compute or retrieve from the database and doesn't change frequently.

#### Basic Caching:

Using the Laravel cache facade, you can cache data like this:

```php
use Illuminate\Support\Facades\Cache;

Route::get('/stats', function () {
    $stats = Cache::remember('site_stats', 3600, function () {
        return [
            'user_count' => User::count(),
            'post_count' => Post::count(),
            'comment_count' => Comment::count(),
        ];
    });

    return view('stats', ['stats' => $stats]);
});
```

This caches the stats for an hour (3600 seconds). The data is recalculated and cached if it's not found in the cache. The cache is stored in the default cache store as configured in your `.env` file.

After the data is cached, subsequent requests will retrieve the data from the cache instead of querying the database again.

#### Model Caching:

For individual models, you can cache queries:

```php
$user = Cache::remember('user:' . $id, 3600, function () use ($id) {
    return User::find($id);
});
```

### Query Builder Optimization

When using Laravel's query builder, there are several techniques to optimize your queries:

#### Select Specific Columns:

Instead of selecting all columns, specify only the ones you need. This is particularly useful when fetching large datasets from tables with many columns, not all of which are required.

Let's say you only need the `id`, `name`, and `email` columns from the `users` table:

```php
$users = DB::table('users')->select('id', 'name', 'email')->get();
```

This reduces the amount of data transferred from the database. To verify the generated SQL query, you can use the `toSql` method:

```php
$sql = DB::table('users')->select('id', 'name', 'email')->toSql();
dd($sql);
```

This will output the generated SQL query for debugging purposes.

#### Use Proper Data Types:

This is not specific to Laravel but is important for query performance. You should always make sure that you're using appropriate data types in your migrations. For example, use `tinyInteger` for boolean fields instead of `integer`.

#### Avoid Using `orWhere` Excessively:

Excessive use of `orWhere` can lead to slow queries. Consider using `whereIn` instead:

```php
// Instead of:
$users = User::where('status', 'active')
             ->orWhere('status', 'pending')
             ->get();

// Use:
$users = User::whereIn('status', ['active', 'pending'])->get();
```

### Eloquent Performance Tips

Besides the above techniques, there are some additional tips to optimize Eloquent queries:

#### Use Lazy Collections for Large Datasets:

When working with large datasets, use lazy collections to conserve memory:

```php
User::cursor()->each(function ($user) {
    // Process user
});
```

This loads users one at a time from the database instead of loading all at once, reducing memory usage.

#### Leverage Raw Queries for Complex Operations:

For very complex queries, sometimes a raw query can be more efficient. You can use Laravel's `DB` facade to run raw SQL queries:

```php
$users = DB::select('SELECT * FROM users WHERE id > ? AND email = ?', [1, 'example@example.com']);
```

You can abstract raw queries into a repository or service class to keep your controllers clean.

## Conclusion

In this guide, we've explored Laravel's routing system, middleware, and validation, with a focus on optimizing database interactions.

Always consider the performance implications of your routes and database queries, especially as your application scales. Use middleware to keep your routes clean and secure, and implement thorough validation to ensure data integrity.

By following these practices and continually refining your approach, you'll be well-equipped to build Laravel applications that are both powerful and performant.

## Additional Resources

- [Laravel Routing Documentation](https://laravel.com/docs/11.x/routing)
- [Laravel Middleware Documentation](https://laravel.com/docs/11.x/middleware)
- [Laravel Validation Documentation](https://laravel.com/docs/11.x/validation)
- [Laravel Query Builder Documentation](https://laravel.com/docs/11.x/queries)
- [Neon Documentation](/docs)


# Implementing Soft Deletes in Laravel and Postgres

---
title: Implementing Soft Deletes in Laravel and Postgres
subtitle: Learn how to implement and optimize soft deletes in Laravel for improved data management and integrity.
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-07-20T00:00:00.000Z'
updatedOn: '2024-07-20T00:00:00.000Z'
---

Laravel is a PHP framework that offers a lot of features to simplify database operations. One such feature is soft deletes, which allows you to "delete" records without actually removing them from your database.

This approach is particularly useful when you need to maintain data integrity, implement data recovery features, or comply with data retention policies.

In this guide, we'll explore Laravel's soft delete functionality, covering everything to get you started, from setting up soft deletes to performance considerations.

## Prerequisites

Before we dive in, ensure you have:

- PHP 8.1 or higher installed
- Laravel 10.x or 11.x set up
- A [Neon](https://neon.tech) account for Postgres database hosting
- Basic understanding of Laravel and Eloquent ORM

## Understanding Soft Deletes

When enabling soft deletes, you essentially add a `deleted_at` timestamp to your database records. When a record is "deleted", Laravel sets this timestamp instead of removing the record entirely. This allows you to:

1. Recover accidentally deleted data.
2. Maintain referential integrity.
3. Implement data archiving strategies.
4. Comply with data retention policies.

Let's explore how to implement soft deletes in Laravel along with Neon Postgres.

## Setting up the Project

Before we go further into implementing soft deletes, let's set up a new Laravel project. If you already have a Laravel project, you can skip this step.

### Creating a New Laravel Project

Open your terminal and run the following command to create a new Laravel project:

```bash
composer create-project laravel/laravel soft-deletes
cd soft-deletes
```

This will create a new Laravel project in a directory named `soft-deletes`. Navigate to the project directory to continue with the setup.

## Implementing Soft Deletes

Implementing soft deletes in Laravel involves two main steps: preparing the database and updating the model.

For this guide, we'll use a `posts` table as an example. You can apply the same steps to any other table in your application.

### Step 1: Creating the Model and Migration

If you don't already have a model for the table you want to apply soft deletes to, you'll need to create one. Let's start by creating a `Post` model along with a migration file. Laravel provides an Artisan command that can do both in one go:

```bash
php artisan make:model Post -m
```

This command creates two files:

1. `app/Models/Post.php`: The `Post` model file.
2. `database/migrations/xxxx_xx_xx_xxxxxx_create_posts_table.php`: A migration file to create the `posts` table.

The `-m` flag tells Artisan to create a migration file along with the model.

### Step 2: Updating the Migration

Now, let's update the migration file to include the `deleted_at` column required for soft deletes. Open the newly created migration file in the `database/migrations` directory and update the `up` method:

```php
public function up(): void
{
    Schema::create('posts', function (Blueprint $table) {
        $table->id();
        $table->string('title');
        $table->text('content');
        $table->timestamps();
        $table->softDeletes();
    });
}
```

The `softDeletes()` method adds a nullable `deleted_at` timestamp column to your table which Laravel uses for soft deletes.

### Step 3: Running the Migration

With our migration file prepared, we can now run it to create the `posts` table in our database:

```bash
php artisan migrate
```

This command executes all pending migrations, creating the `posts` table with the `deleted_at` column.

If you were to connect to your database, you'd see a new `posts` table with the following columns:

```sql
SELECT * FROM posts;

+----+-------+---------+------------+------------+------------+
| id | title | content | created_at | updated_at | deleted_at |
+----+-------+---------+------------+------------+------------+
```

### Step 4: Updating the Model

Finally, we need to update our `Post` model to use the `SoftDeletes` trait. Open `app/Models/Post.php` and update it as follows:

```php {6,10}
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Model;
use Illuminate\Database\Eloquent\SoftDeletes;

class Post extends Model
{
    use SoftDeletes;

    protected $fillable = ['title', 'content'];
}
```

By adding the `use SoftDeletes;` line, we're telling Laravel that this model should use soft delete functionality when deleting records.

With these steps completed, your `Post` model is now set up to use soft deletes. When you call `$post->delete()`, Laravel will set the `deleted_at` timestamp instead of actually removing the record from the database.

### Adding Soft Deletes to an Existing Table

If you're adding soft deletes to an existing table, you'll need to create a separate migration to add the `deleted_at` column. You can do this with the command:

```bash
php artisan make:migration add_soft_deletes_to_posts_table --table=posts
```

This command creates a new migration file where you can add the `deleted_at` column to the `posts` table:

```php
public function up(): void
{
    Schema::table('posts', function (Blueprint $table) {
        $table->softDeletes();
    });
}
```

After creating the migration, run `php artisan migrate` to apply the changes to your database.

## Using Soft Deletes

Now that we've set up soft deletes in our Laravel application, let's explore how to use them in practice. We'll cover basic operations like deleting, restoring, and permanently deleting records, as well as querying with soft deletes.

### Basic Operations

#### Deleting a Record

To soft-delete a record, you can use the `delete()` method just as you would for a regular delete operation:

```php
$post = Post::find(1);
$post->delete();
```

When this code runs, several things happen behind the scenes:

1. Laravel checks if the `SoftDeletes` trait is used in the `Post` model.
2. Instead of running a SQL `DELETE` query, it performs an `UPDATE` query.
3. The `deleted_at` column is set to the current timestamp.
4. The model's `deleted_at` attribute is updated in memory.

This approach allows you to maintain the record in the database while marking it as deleted. It's beneficial when you need to keep records for auditing purposes or when you want to implement a "trash" feature in your application.

If you did not have soft deletes enabled, the `$post->delete()` method would generate the following SQL query:

```sql
DELETE FROM posts WHERE id = {id};
```

However, with soft deletes enabled, the query looks like this:

```sql
UPDATE posts SET deleted_at = '2024-05-26 12:00:00' WHERE id = {id};
```

This way, the record is not removed from the database but is instead marked as "deleted".

If you were to now try to retrieve the post with `Post::find(1)`, it would not return the record because it has been "soft deleted". Under the hood, Laravel automatically adds a `WHERE deleted_at IS NULL` clause to your queries to exclude soft-deleted records, e.g.:

```sql
SELECT * FROM posts WHERE id = 1 AND deleted_at IS NULL;
```

So, you won't see the soft-deleted record in your query results unless you explicitly ask for it.

#### Restoring a Soft-Deleted Record

To bring back a soft-deleted record, you use the `restore()` method on the model:

```php
$post = Post::withTrashed()->find(1);
$post->restore();
```

Here's what happens when you run this code:

1. `withTrashed()` tells Laravel to include soft-deleted records in the query.
2. `find(1)` retrieves the post, even if it's soft-deleted.
3. `restore()` sets the `deleted_at` column back to `NULL`.

This process effectively "undeletes" the record, making it visible in normal queries again.

#### Permanently Deleting a Record

If you need to remove a record from the database permanently, use `forceDelete()`:

```php
$post = Post::withTrashed()->find(1);
$post->forceDelete();
```

This method:

1. Bypasses the soft delete mechanism.
2. Executes a SQL `DELETE` query to permanently remove the record.
3. Removes any associated files or resources if you've set up your model to handle this.

Use `forceDelete()` with caution, as it permanently removes data and can't be undone, unless you have a backup strategy in place.

### Querying with Soft Deletes

Soft deletes affect how you query your database. Laravel provides methods to control whether soft-deleted records are included in query results or not.

#### Retrieving Only Non-Deleted Records

By default, Laravel excludes soft-deleted records from query results:

```php
$activePosts = Post::all(); // Only returns non-deleted posts
```

As we mentioned earlier, Laravel automatically adds a where clause to your query:

```sql
SELECT * FROM posts WHERE deleted_at IS NULL
```

This ensures that your queries don't return "deleted" records unless you explicitly ask for them.

#### Including Soft-Deleted Records

To include soft-deleted records in your query, use `withTrashed()`:

```php
$allPosts = Post::withTrashed()->get();
```

This method removes the `WHERE deleted_at IS NULL` clause from the query, allowing you to retrieve all records, regardless of their deleted status. The generated SQL query looks like this:

```sql
SELECT * FROM posts;
```

So using `withTrashed()` is useful when you need to access soft-deleted records for auditing or recovery purposes.

#### Retrieving Only Soft-Deleted Records

In some cases, you may need to retrieve only soft-deleted records. Laravel provides the `onlyTrashed()` method for this purpose:

```php
$deletedPosts = Post::onlyTrashed()->get();
```

This method adds a `WHERE deleted_at IS NOT NULL` clause to your query, returning only the "deleted" records.

### Using the `DB` Facade

While Eloquent provides a high-level API for working with soft deletes, sometimes you might need to use raw SQL queries or the Query Builder. The `DB` facade in Laravel allows you to work with soft deletes at a lower level, giving you more control over your database operations.

Here are some examples with explanations:

```php
use Illuminate\Support\Facades\DB;

// Soft delete a record
DB::table('posts')->where('id', 1)->update(['deleted_at' => now()]);
```

This query manually sets the `deleted_at` column to the current timestamp, effectively soft-deleting the record. Unlike Eloquent's `delete()` method, this doesn't trigger any model events.

```php
// Restore a soft-deleted record
DB::table('posts')->where('id', 1)->update(['deleted_at' => null]);
```

Here, we're restoring a soft-deleted record by setting its `deleted_at` column back to null. This makes the record visible to normal queries again.

```php
// Query including soft-deleted records
$allPosts = DB::table('posts')->get();
```

This query retrieves all records, including soft-deleted ones. The `DB` facade doesn't automatically exclude soft-deleted records like Eloquent does.

```php
// Query only non-deleted records
$activePosts = DB::table('posts')->whereNull('deleted_at')->get();
```

To exclude soft-deleted records, we explicitly add a `whereNull('deleted_at')` clause. This mimics Eloquent's default behavior.

```php
// Query only soft-deleted records
$deletedPosts = DB::table('posts')->whereNotNull('deleted_at')->get();
```

This query retrieves only soft-deleted records by checking for non-null `deleted_at` values.

```php
// Permanently delete a soft-deleted record
DB::table('posts')->where('id', 1)->delete();
```

This operation permanently removes the record from the database, regardless of its soft-deleted status. Be cautious with this as it's irreversible.

The `DB` facade bypasses Eloquent's model events and global scopes, so you'll need to handle any related logic manually if needed.

## General Best Practices

When working with soft deletes in Laravel, there are several best practices to consider for optimal performance and data integrity. Here are some recommendations:

### 1. Regular Cleanup of Old Soft-Deleted Records

One of the main downsides of soft deletes is that records remain in your database even after they're "deleted". This can lead to unnecessary data bloat over time.

To prevent your database from growing too large, consider implementing a cleanup routine:

```php
Post::onlyTrashed()
    ->where('deleted_at', '<', now()->subYears(2))
    ->forceDelete();
```

This code permanently removes records that have been soft-deleted for more than two years. Here's why this is important:

- Soft-deleted records still occupy space in your database. Regular cleanup prevents unnecessary database growth.
- Fewer records generally mean faster queries, even when using `withTrashed()`.
- Some data protection regulations require data to be permanently deleted after a certain period.

You can schedule this command to run regularly using Laravel's task scheduler so that old soft-deleted records are cleaned up automatically.

### 2. Use Soft Deletes Carefully

While soft deletes are useful, they're not always necessary for every model. You should consider the following factors when deciding whether to use soft deletes:

- Use soft deletes for important data that might need to be restored.
- If a model has many important relationships, soft deletes can help maintain data integrity.
- For tables with a very high volume of records, consider the potential performance impact of soft deletes.
- For data privacy or compliance reasons, permanent deletion might be more appropriate.

### 3. Implement Access Controls

If your application allows users to access soft-deleted records, ensure that unauthorized users can't access them:

```php
public function show(Post $post)
{
    if ($post->trashed()) {
        abort(404);
    }
    return view('posts.show', compact('post'));
}
```

This prevents unauthorized access to soft-deleted records, which could contain sensitive or outdated information. If you need to allow certain users to access soft-deleted records, implement appropriate access controls based on user roles or permissions.

### 4. Be Cautious with Indexing

Regarding indexing the `deleted_at` column, there's debate in the community. Some argue against it because:

- Most queries filter for non-deleted records (`WHERE deleted_at IS NULL`), which may not benefit from an index on `deleted_at`.
- An index on `deleted_at` could potentially slow down write operations.

Instead, consider your specific use case:

- If you frequently query for soft-deleted records or restore them, an index might be beneficial.
- If your primary operations are on non-deleted records, you might not need an index on `deleted_at`.

Always measure the performance impact in your specific scenario before deciding on indexing strategy.

For more information about indexes in general, refer to Neon's documentation on [indexes](/docs/postgres/indexes).

## Testing Soft Deletes

As with anything, testing is important, that way you can make sure your soft delete implementation works correctly. Here's an example test case:

Laravel provides several tools and assertions specifically for testing soft deletes. Let's go over some common tests you might want to include in your test suite.

### Testing Soft Delete Functionality

Let's start with a test to ensure a post is correctly soft deleted:

```php
public function it_soft_deletes_a_post()
{
    $post = Post::factory()->create();

    $post->delete();

    $this->assertSoftDeleted($post);
    $this->assertDatabaseHas('posts', ['id' => $post->id]);
    $this->assertDatabaseMissing('posts', [
        'id' => $post->id,
        'deleted_at' => null
    ]);
}
```

This test:

1. Creates a post using a factory.
2. Soft deletes the post.
3. Asserts that the post is soft deleted using Laravel's `assertSoftDeleted` method.
4. Checks that the post still exists in the database.
5. Verifies that there's no record with a null `deleted_at` for this post.

### Testing Restore Functionality

Next, let's test the restore functionality:

```php
public function it_restores_a_soft_deleted_post()
{
    $post = Post::factory()->create();
    $post->delete();

    $post->restore();

    $this->assertDatabaseHas('posts', [
        'id' => $post->id,
        'deleted_at' => null
    ]);
    $this->assertNotSoftDeleted($post);
}
```

This test:

1. Creates and soft deletes a post.
2. Restores the post.
3. Checks that the post exists in the database with a null `deleted_at`.
4. Uses Laravel's `assertNotSoftDeleted` to confirm the post is no longer soft deleted.

### Testing Query Scopes

It's also important to test that your queries are correctly scoping soft deleted records:

```php
public function it_excludes_soft_deleted_posts_from_regular_queries()
{
    $activePost = Post::factory()->create();
    $deletedPost = Post::factory()->create();
    $deletedPost->delete();

    $posts = Post::all();

    $this->assertTrue($posts->contains($activePost));
    $this->assertFalse($posts->contains($deletedPost));
}

public function it_includes_soft_deleted_posts_when_using_with_trashed()
{
    $activePost = Post::factory()->create();
    $deletedPost = Post::factory()->create();
    $deletedPost->delete();

    $posts = Post::withTrashed()->get();

    $this->assertTrue($posts->contains($activePost));
    $this->assertTrue($posts->contains($deletedPost));
}
```

These two tests ensure that:

1. Regular queries exclude soft deleted records and only return active posts.
2. Queries using `withTrashed()` include soft deleted records.

### Testing Force Delete

Finally, let's test the force delete functionality:

```php
public function it_permanently_deletes_a_post()
{
    $post = Post::factory()->create();

    $post->forceDelete();

    $this->assertDatabaseMissing('posts', ['id' => $post->id]);
    $this->assertDatabaseCount('posts', 0);
}
```

This test verifies that force deleting a post removes it entirely from the database.

## Conclusion

Laravel's soft delete feature provides a way to manage data deletion without losing valuable information. By using soft deletes, you can improve your application's data integrity and provide features like data recovery or undo functionality to your users.

Consider the performance implications of soft deletes, especially when working with large datasets. Utilize Neon Postgres's capabilities, such as [indexing](/docs/postgres/indexes) and [table partitioning](/docs/postgres/ddl-partitioning), to maintain high performance as your application scales.

When implementing soft deletes, always think about the lifecycle of your data. Plan on implementing policies for permanent deletion of old soft-deleted records to manage database growth optimally and comply with data retention regulations.

## Additional Resources

- [Laravel Documentation on Soft Deletes](https://laravel.com/docs/eloquent#soft-deleting)
- [Neon Postgres Documentation](/docs)
- [Laravel Eloquent Performance Tips](https://laravel.com/docs/eloquent-relationships#eager-loading)


# Testing Laravel Applications with Neon's Database Branching

---
title: Testing Laravel Applications with Neon's Database Branching
subtitle: Leveraging Realistic Production Data for Robust Testing with Laravel and Neon Branching
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-05-26T00:00:00.000Z'
updatedOn: '2024-05-26T00:00:00.000Z'
---

[Laravel](https://laravel.com) is a popular PHP framework widely used for building web applications. It includes powerful tools for automated testing, with [PEST](https://pestphp.com/) being a preferred option due to its simplicity and effectiveness.

Testing with realistic data is crucial as it helps ensure that your application performs well under real-world conditions. Neon's database branching feature offers a unique solution by allowing you to test with actual production data without affecting your live database, thus maintaining data integrity and security.

## 1. Understanding Laravel Testing Approaches

In Laravel, developers commonly use a local SQLite database for testing. This method is favored because it allows for starting with a clean state for each test run by applying all database migrations and seeders. This setup is ideal for parallel testing, ensuring that tests run quickly and do not interfere with each other.

However, testing with SQLite can differ significantly from a production environment that might use a different database system, such as MySQL or PostgreSQL. These differences can affect the application's behavior and lead to unexpected issues in production. Therefore, testing with real data can provide a more accurate assessment of how the application will perform in its live environment. Using production data, though, presents risks and challenges related to security and data management.

## 2. Neon Branching

Neon offers a database [branching feature](/docs/introduction/branching) that allows you to create isolated branches of your database for development, testing, and more.

A branch in Neon is a copy-on-write clone of your data that can be made from the current database state or any past state. This means you can have an exact copy of your production data at a specific point in time to use for testing.

Some key benefits of Neon branching include:

- **Isolation:** Branches are completely isolated from the original database and other branches, ensuring that the operations performed on one do not affect others.
- **Efficiency:** Branching is quick and does not burden the parent database, as it uses a copy-on-write mechanism. This means the original database's performance remains unaffected, even when multiple branches are in use.
- **Flexibility:** You can modify or delete branches without impacting the original data. Changes to a branch are independent and only record the differences from the point of branch creation.
- **Simplicity:** Every Neon project starts with a production branch, and new branches can be created from this root or from any other branch. This structure simplifies managing different versions of your database.

Neon's branching is particularly useful in continuous integration and delivery pipelines, enhancing developer productivity by reducing the setup time needed for test environments.

This feature allows you to test with realistic data scenarios without the overhead of maintaining multiple separate databases.

## 3. Setting Up Your Testing Environment

Setting up a robust testing environment with Neon and Laravel involves several steps, from configuring your Neon account to setting up Laravel for testing with PEST. This section will guide you through the prerequisites, installation, configuration, and initial test creation to ensure your application is ready for effective testing.

### Prerequisites

Before you begin, ensure you have the following:

- **Neon Account:** You need an account with Neon to access their database services. You can sign up at [Neon's official website](https://neon.tech).
- **API Keys:** Generate API keys from the Neon console. These keys will allow your Laravel application to interact with the Neon database programmatically.
- **Local Development Environment:** Laravel requires a PHP environment (PHP 7.4 or later) and [Composer](https://getcomposer.org/) for managing dependencies.
- **Laravel Installation:** A Laravel project set up on your local machine. For installation instructions, refer to the [Laravel documentation](https://laravel.com/docs/11.x/installation).

### Installation and Configuration

To set up your testing environment with Neon and Laravel, follow these steps:

1. **Configure Database Connection:**
   - After creating your Neon account and a new database branch, obtain the connection details from the Neon dashboard.
   - Open your Laravel project and update the `.env` file with the Neon database connection parameters:

     ```env
     DB_CONNECTION=pgsql
     DB_HOST=your-neon-hostname.neon.tech
     DB_PORT=5432
     DB_DATABASE=<your-database-name>
     DB_USERNAME=<your-username>
     DB_PASSWORD=<your-password>
     ```

2. **Install PEST PHP:**
   - PEST is a testing framework for PHP that works seamlessly with Laravel. Install PEST via Composer with the following command:

     ```
     composer require pestphp/pest --dev
     composer require pestphp/pest-plugin-laravel --dev
     ```

#### Creating a Migration and Seeder

1. **Generate Migration and Model:**
   - Run the following command to create a new migration file for a `questions` table and its associated model:

   ```
   php artisan make:model Question -m
   ```

   - Open the generated migration file in the `database/migrations` directory and add fields to the `questions` table schema:

     ```php
     Schema::create('questions', function (Blueprint $table) {
        $table->id();
        $table->string('title');
        $table->text('description');
        $table->timestamps();
     });
     ```

2. **Create Seeder:**
   - Generate a seeder to populate the `questions` table:

     ```
     php artisan make:seeder QuestionsTableSeeder
     ```

   - Open the `database/seeders/QuestionsTableSeeder.php` and in the `run` method, add code to create sample questions:

     ```php
     public function run()
     {
        $questions = [
            ['What is Laravel?', 'A PHP framework for web artisans.'],
            ['What is MVC?', 'A design pattern called Model-View-Controller.'],
            ['What is PHP?', 'A popular general-purpose scripting language.'],
            ['How do databases work?', 'Databases store data in an organized manner.'],
            ['What is OOP?', 'Object-Oriented Programming is a programming paradigm.'],
            ['What is a variable in programming?', 'A variable is used to store information.'],
            ['What is an API?', 'Application Programming Interface, a way for systems to interact.'],
            ['What are webhooks?', 'Webhooks allow applications to send automated messages or information.'],
            ['What is JSON?', 'JSON is a format for storing and transporting data.'],
            ['What is a function in programming?', 'A function is a block of code designed to perform a particular task.']
        ];

        foreach ($questions as $q) {
            Question::create([
                'title' => $q[0],
                'description' => $q[1]
            ]);
        }
     }
     ```

   - Register the seeder in `DatabaseSeeder.php`:

     ```php
     $this->call(QuestionsTableSeeder::class);
     ```

3. **Run Migrations and Seeders:**
   - Migrate the database to create the `questions` table:

     ```
     php artisan migrate
     ```

   - Seed the database with test data:

     ```
     php artisan db:seed
     ```

#### Creating a Questions Controller

1. **Generate the Controller:**
   A controller is a PHP class that handles HTTP requests. You can create a controller to manage questions data in your Laravel application.
   - Use Artisan to create a new controller named `QuestionController`:

     ```
     php artisan make:controller QuestionController
     ```

2. **Add a Method to Retrieve Questions:**
   - Open the newly created `QuestionController` in the `app/Http/Controllers` directory.
   - Add a method to fetch and return all questions:

     ```php
     public function index() {
         $questions = \App\Models\Question::all();
         return response()->json($questions);
     }
     ```

3. **Update Routes:**
   - Open the `routes/web.php` file and add a route to handle GET requests for questions:

     ```php
     Route::get('/questions', [\App\Http\Controllers\QuestionController::class, 'index']);
     ```

This setup provides a simple API endpoint to retrieve all questions from the database.

To, verify the setup, you can run the Laravel development server:

```
php artisan serve
```

Access the `/questions` endpoint in your browser or a tool like Postman to see the JSON response with the seeded questions.

#### Writing a PEST Test for the `QuestionController`

1. **Create the Test File:**
   - PEST allows you to write tests in a very expressive way. You can create a test file specifically for the `QuestionController`:

     ```
     php artisan pest:test QuestionTest
     ```

2. **Write the Test:**
   Usually, you would write a test that uses the `RefreshDatabase` trait to migrate the database and then seed it with test data before each test. But in this case, we will use the Neon branch to test with real data instead.
   - Open the generated test file in `tests/Feature` and add a test to check the `/questions` endpoint:

     ```php
     it('can retrieve questions from the database', function () {
        $response = $this->get('/questions');
        $response->assertStatus(200)
                ->assertJsonStructure([
                    '*' => ['id', 'title', 'description', 'created_at', 'updated_at']
                ]);
     });
     ```

   - This test does the following:
     - It sends a GET request to the `/questions` endpoint.
     - It asserts that the HTTP status is `200` and checks the JSON structure to match the expected fields for questions.

### Running the Tests

Run the updated tests to ensure your controller behaves correctly:

```
./vendor/bin/pest
```

PEST will execute the test and provide feedback on the test results.

## 4. Using Neon Branching with Laravel

You should never run tests against your production database, as it can lead to data corruption and security risks. Especially if you are using `RefreshDatabase` or `DatabaseTransactions` traits, which can delete or modify data during testing. This is where Neon branching comes in handy.

Neon's branching feature enables you to create isolated database environments, which is ideal for testing changes without impacting the production database.

This can be particularly useful when testing complex features or changes that require realistic data scenarios. Especially when there are schema changes or data migrations involved, Neon branching provides a safe and efficient way to validate your application's behavior on a copy of your production data.

### Creating a Neon Branch

1. **Log In to Neon Dashboard:**
   - Access your Neon dashboard by logging in at [Neon's official website](https://neon.tech).

2. **Select Your Database:**
   - Navigate to the database project that you are using for your production environment.

3. **Create a New Branch:**
   - Click on "Branches" in the sidebar menu.
   - Click on "Create Branch."
   - Name your new branch (e.g., "testing-branch") and specify if it should be created from the current state of the database or from a specific point in time. This creates a copy-on-write clone of your database.
   - Wait for the branch to be fully provisioned, which usually takes just a few seconds.

### Integrating Neon Branching with Laravel Testing

Go back to your Laravel project and integrate the Neon branch into your testing setup:

1. **Update Environment Configuration:**
   - Once your branch is created, obtain the connection details (hostname, database name, username, and password) from the Neon dashboard.
   - Create a new environment file in your Laravel project, such as `.env.testing`, and configure it to use the Neon testing branch. This ensures that your testing environment uses its database configuration.

     ```env
     DB_CONNECTION=pgsql
     DB_HOST=your-neon-testing-hostname.neon.tech
     DB_PORT=5432
     DB_DATABASE=<your-testing-database-name>
     DB_USERNAME=<your-testing-username>
     DB_PASSWORD=<your-testing-password>
     ```

1. **Update PHPUnit Configuration:**
   - Ensure that PHPUnit (used by PEST for running tests) is configured to use the `.env.testing` file. Update your `phpunit.xml` file to specify the environment file:

     ```xml
     <php>
         <env name="APP_ENV" value="testing"/>
         <env name="DB_CONNECTION" value="pgsql"/>
     </php>
     ```

1. **Run Tests:**
   - With the testing branch configured, you can write tests that interact with the database as if it were production data, without the risk of affecting real user data. Use PEST to run your tests:

     ```bash
     ./vendor/bin/pest
     ```

   - Examine the output from PEST to ensure your application behaves as expected against the testing branch. This approach allows you to test changes in a controlled environment that mirrors your production setup.

## 5. Managing Neon Branches with `neonctl` CLI

Automated testing is an essential aspect of software development, ensuring that new code contributions don't break existing functionality. Neon's database branching feature enables you to create isolated environments for testing changes without affecting your production database.

With the `neonctl` CLI tool, managing these branches becomes straightforward and seamless.

### Installing `neonctl`

Before you can start using `neonctl`, you need to install it on your local machine. Follow the installation instructions provided in the [Neon CLI documentation](/docs/reference/cli-install) to set up `neonctl` on your system.

### Using `neonctl` to Manage Branches

Once `neonctl` is installed, you can use it to interact with your Neon database branches. Here are the basic commands for managing branches:

#### 1. [Creating a Branch](/docs/reference/cli-branches#create)

To create a new branch, use the `neonctl branches create` command:

```bash
neonctl branches create --project-id PROJECT_ID --parent PARENT_BRANCH_ID --name BRANCH_NAME
```

Replace `PROJECT_ID`, `PARENT_BRANCH_ID`, and `BRANCH_NAME` with the appropriate values for your Neon project. This command will create a new branch based on the specified parent branch.

#### 2. [Listing Branches](/docs/reference/cli-branches#list)

To list all branches in your Neon project, use the `neonctl branches list` command:

```bash
neonctl branches list --project-id PROJECT_ID
```

Replace `PROJECT_ID` with your Neon project ID. This command will display a list of all branches along with their IDs, names, and other relevant information.

#### 3. [Obtaining Connection String](/docs/reference/cli-connection-string)

Once you've created a branch, you'll need to obtain the connection string to configure your Laravel application. Use the `neonctl connection-string` command:

```bash
neonctl connection-string BRANCH_ID
```

Replace `BRANCH_ID` with the ID of the branch you want to connect to. This command will output the connection string that you can use to configure your Laravel `.env` file.

#### 4. [Deleting a Branch](/docs/reference/cli-branches#delete)

After you've finished testing with a branch, you can delete it using the `neonctl branches delete` command:

```bash
neonctl branches delete BRANCH_ID
```

Replace `BRANCH_ID` with the ID of the branch you want to delete. This command will remove the branch from your Neon project, ensuring that resources are not left unused.

### Integrating Neon Branches with Laravel Testing

Once you've created a Neon branch using `neonctl`, you can integrate it into your Laravel testing workflow:

1. **Obtain Connection Details:** Use `neonctl connection-string` to get the connection details for the branch.
2. **Update `.env.testing` File:** Update your Laravel `.env.testing` file with the connection details obtained from `neonctl`.
3. **Run Tests:** Execute your Laravel tests as usual, ensuring that they interact with the Neon branch database.
4. **Clean Up:** After testing is complete, use `neonctl branches delete` to delete the branch and clean up resources.

## Conclusion

Testing Laravel applications with Neon's database branching offers a solution for ensuring the reliability and performance of your codebase.

By using realistic production data in a controlled testing environment, developers can confidently validate their changes without risking the integrity of live databases.

Neon's branching feature provides isolation, efficiency, flexibility, and simplicity, making it a valuable tool for streamlining the testing process.

## Additional Resources

- [Laravel Documentation](https://laravel.com/docs) - Official documentation for the Laravel PHP framework, covering installation, configuration, and usage guides.
- [Neon Documentation](/docs) - Comprehensive documentation for Neon's database services, including guides, tutorials, and API references.
- [GitHub Actions Tutorials](https://docs.github.com/en/actions/learn-github-actions) - Learn how to automate your workflow with GitHub Actions, including tutorials on setting up continuous integration for Laravel applications.
- [Neon Branching GitHub Actions Guide](/docs/guides/branching-github-actions) - Step-by-step guide on integrating Neon database branching with GitHub Actions for automated testing workflows.


# Building a Multi-Step Form with Laravel Volt, Folio, and Neon Postgres

---
title: Building a Multi-Step Form with Laravel Volt, Folio, and Neon Postgres
subtitle: Learn how to create a multi-step form with Laravel Volt, Folio, and Neon Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-10-19T00:00:00.000Z'
updatedOn: '2024-10-19T00:00:00.000Z'
---

In this guide, we'll walk through the process of building a multi-step form using Laravel [Volt](https://livewire.laravel.com/docs/volt), [Folio](https://laravel.com/docs/11.x/folio), and Neon Postgres.

Laravel Volt provides reactivity for dynamic form interactions, Folio offers file-based routing for a clean project structure, and Neon Postgres serves as our scalable database solution.

Our example app will be a job application form with multiple steps, including personal information, education, and work experience.

## Prerequisites

Before we begin, make sure you have:

- PHP 8.1 or higher installed
- Composer for managing PHP dependencies
- A [Neon](https://console.neon.tech/signup) account for Postgres hosting
- Basic familiarity with Laravel and Postgres

## Setting up the Project

Let's start by creating a new Laravel project and setting up the necessary components.

1. Create a new Laravel project:

   ```bash
   composer create-project laravel/laravel job-application-form
   cd job-application-form
   ```

2. Install Laravel Folio for file-based routing:

   ```bash
   composer require laravel/folio
   ```

3. Install the Volt Livewire adapter for Laravel, this will also install the Livewire package:

   ```bash
   composer require livewire/volt
   ```

4. After installing Volt, you can install the Volt service provider:

   ```bash
   php artisan volt:install
   ```

## Configuring the Database Connection

Update your `.env` file with your Neon Postgres credentials:

```env
DB_CONNECTION=pgsql
DB_HOST=your-neon-hostname.neon.tech
DB_PORT=5432
DB_DATABASE=your_database_name
DB_USERNAME=your_username
DB_PASSWORD=your_password
```

Replace `your-neon-hostname.neon.tech`, `your_database_name`, `your_username`, and `your_password` with your Neon Postgres connection details.

## Database Design

Let's create the database migrations for our job application form. We'll use separate tables for each section and leverage Postgres JSON columns for flexible data storage for additional information.

First, let's create the migration for the applicants table using the following `artisan` command:

```bash
php artisan make:migration create_applicants_table
```

Note that the `create_applicants_table` migration name follows the Laravel convention of `create_{table_name}_table`, where `{table_name}` is the name of the table you're creating. That way, Laravel can automatically determine the table name from the migration name, and also it will be easier to identify the purpose of the migration file by its name for other developers.

This command generates a new migration file in the `database/migrations` directory. Open the newly created file and update its content as follows:

```php
<?php

use Illuminate\Database\Migrations\Migration;
use Illuminate\Database\Schema\Blueprint;
use Illuminate\Support\Facades\Schema;

return new class extends Migration
{
    public function up()
    {
        Schema::create('applicants', function (Blueprint $table) {
            $table->id();
            $table->string('first_name');
            $table->string('last_name');
            $table->string('email')->unique();
            $table->jsonb('additional_info')->nullable();
            $table->timestamps();
        });
    }

    public function down()
    {
        Schema::dropIfExists('applicants');
    }
};
```

This migration creates the `applicants` table with fields for `first_name`, `last_name`, and `email`. The `email` field is set as unique to prevent duplicate applications. We've also included a `jsonb` column called `additional_info` for storing any extra data that doesn't fit into the predefined columns. This flexibility is one of the advantages of using Postgres with Laravel.

Next, let's create the migration for the educations table:

```bash
php artisan make:migration create_educations_table
```

Update the newly created migration file with the following content:

```php
<?php

use Illuminate\Database\Migrations\Migration;
use Illuminate\Database\Schema\Blueprint;
use Illuminate\Support\Facades\Schema;

return new class extends Migration
{
    public function up()
    {
        Schema::create('educations', function (Blueprint $table) {
            $table->id();
            $table->foreignId('applicant_id')->constrained()->onDelete('cascade');
            $table->string('institution');
            $table->string('degree');
            $table->date('start_date');
            $table->date('end_date')->nullable();
            $table->jsonb('additional_info')->nullable();
            $table->timestamps();
        });
    }

    public function down()
    {
        Schema::dropIfExists('educations');
    }
};
```

This migration creates the `educations` table. It includes a foreign key `applicant_id` that references the `id` column in the `applicants` table. The `onDelete('cascade')` ensures that if an applicant is deleted, their education records are also removed. We've included fields for the institution, degree, and start/end dates. Again, we have an `additional_info` jsonb column for flexibility.

Finally, let's create the migration for the work experiences table:

```bash
php artisan make:migration create_work_experiences_table
```

Update this migration file with the following content:

```php
<?php

use Illuminate\Database\Migrations\Migration;
use Illuminate\Database\Schema\Blueprint;
use Illuminate\Support\Facades\Schema;

return new class extends Migration
{
    public function up()
    {
        Schema::create('work_experiences', function (Blueprint $table) {
            $table->id();
            $table->foreignId('applicant_id')->constrained()->onDelete('cascade');
            $table->string('company');
            $table->string('position');
            $table->date('start_date');
            $table->date('end_date')->nullable();
            $table->text('responsibilities');
            $table->jsonb('additional_info')->nullable();
            $table->timestamps();
        });
    }

    public function down()
    {
        Schema::dropIfExists('work_experiences');
    }
};
```

This migration creates the `work_experiences` table. Similar to the `educations` table, it has a foreign key relationship with the `applicants` table. It includes fields for the company, position, start/end dates, and responsibilities. The `responsibilities` field is of type `text` to allow for longer descriptions. We've also included an `additional_info` jsonb column here.

Now that we've created all our migrations, we can run them to create the tables in our database:

```bash
php artisan migrate
```

This command will execute all the migrations we've just created, setting up the database schema for our job application form.

One thing to note is that we've used the `jsonb` column type for storing additional information in each table. This allows us to store flexible data structures without needing to define a fixed schema. Postgres' JSONB data type is ideal for this use case.

For your Laravel migrations, you should not use the Neon Postgres Pooler. The Pooler is designed to manage connections for long-running processes, such as web servers, and is not necessary for short-lived processes like migrations.

## Creating Models

Next, let's create models for our `Applicant`, `Education`, and `WorkExperience` tables. Models in Laravel are used to interact with database tables and represent the data in your application in an object-oriented way.

Laravel provides an easy way to generate models using the `artisan` command. To create the `Applicant` model run:

```bash
php artisan make:model Applicant
```

This command creates a new file `app/Models/Applicant.php`. Open this file and update it with the following content:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class Applicant extends Model
{
    use HasFactory;

    protected $fillable = [
        'first_name',
        'last_name',
        'email',
        'additional_info'
    ];

    protected $casts = [
        'additional_info' => 'array',
    ];

    public function educations()
    {
        return $this->hasMany(Education::class);
    }

    public function workExperiences()
    {
        return $this->hasMany(WorkExperience::class);
    }
}
```

Now, create the `Education` model:

```bash
php artisan make:model Education
```

Update the newly created file at `app/Models/Education.php` with the following content:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class Education extends Model
{
    use HasFactory;

    public $table = 'educations';

    protected $fillable = [
        'applicant_id',
        'institution',
        'degree',
        'start_date',
        'end_date',
        'additional_info'
    ];

    protected $casts = [
        'start_date' => 'date',
        'end_date' => 'date',
        'additional_info' => 'array',
    ];

    public function applicant()
    {
        return $this->belongsTo(Applicant::class);
    }
}
```

Finally, create the `WorkExperience` model:

```bash
php artisan make:model WorkExperience
```

And update the `app/Models/WorkExperience.php` file with the following content:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class WorkExperience extends Model
{
    use HasFactory;

    protected $fillable = [
        'applicant_id',
        'company',
        'position',
        'start_date',
        'end_date',
        'responsibilities',
        'additional_info'
    ];

    protected $casts = [
        'start_date' => 'date',
        'end_date' => 'date',
        'additional_info' => 'array',
    ];

    public function applicant()
    {
        return $this->belongsTo(Applicant::class);
    }
}
```

Let's quickly note the most important parts in these model definitions:

- We've used the `$fillable` property to specify which attributes can be mass-assigned. This is a security feature to prevent unintended mass assignment vulnerabilities.
- We've defined relationships between models. An `Applicant` has many `Education` and `WorkExperience` records, while `Education` and `WorkExperience` belong to an `Applicant`.
- We've used the `$casts` property to automatically cast certain attributes to specific types. For example, we're casting the `additional_info` field to an array, which works well with Postgres' JSONB column type.
- The `start_date` and `end_date` fields are cast to date objects, which allows for easy date manipulation in PHP.

These models will allow us to easily interact with our database tables using Laravel's Eloquent ORM. They provide a convenient way to retrieve, create, update, and delete records, as well as define relationships between different tables.

## Creating a layout for the multi-step form

Before we create the form components, let's set up a layout for our multi-step form. We'll create a main layout file that includes the necessary CSS and JavaScript assets including the Livewire scripts.

Create a new Blade layout file at `resources/views/layouts/app.blade.php`:

```blade
<!DOCTYPE html>
<html lang="{{ str_replace('_', '-', app()->getLocale()) }}">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Job Application Form</title>
    <script src="https://cdn.tailwindcss.com"></script>
    @vite(['resources/css/app.css', 'resources/js/app.js'])
    @livewireStyles
</head>
<body>
    <div class="container mx-auto mt-8">
        @yield('content')
    </div>

    @livewireScripts
</body>
</html>
```

In this layout file:

- We've included the necessary meta tags for character encoding, viewport settings and the page title.
- We've used the `@vite` directive to include the CSS and JavaScript assets. This directive is provided by the Laravel Vite package, which integrates Laravel with the Vite build tool for modern frontend development.
- We've included the Livewire styles and scripts. Livewire is a full-stack framework for Laravel that allows you to build dynamic interfaces without writing JavaScript.

To compile the frontend assets, you'll need to run the following commands:

```bash
npm install
npm run build
```

## Implementing File-based Routing with Folio

Laravel Folio was introduced in 2023, and it offers a new approach to routing in Laravel applications.

It simplifies routing by allowing you to create routes simply by adding Blade templates to a specific directory. This file-based routing system makes your project structure cleaner and more intuitive.

It is not a replacement for Laravel's built-in routing system but rather a complementary feature that simplifies routing for certain types of applications.

First, let's set up the directory structure for our multi-step form. Create the following directory structure in your `resources/views/pages` folder:

```shell
resources/
└── views/
    └── pages/
        ├── index.blade.php
        └── apply/
            ├── index.blade.php
            ├── personal-info.blade.php
            ├── education.blade.php
            ├── work-experience.blade.php
            └── review.blade.php
            └── confirmation.blade.php
```

With Folio, each of these Blade files automatically becomes a route. For example:

- `pages/index.blade.php` will be accessible at the root URL `/`
- `pages/apply/personal-info.blade.php` will be accessible at `/apply/personal-info`

To create a Folio page, you can use the `php artisan folio:page` command. For example, to create a page for the personal information step:

```bash
php artisan folio:page apply/personal-info
```

The above will create a blade file for the in `resources/views/pages/apply/personal-info.blade.php`:

```blade
<div>
    <h2>Personal Information</h2>
    <!-- Your form content will go here -->
</div>
```

You can list all available Folio routes using the following Artisan command:

```bash
php artisan folio:list
```

You can create similar pages for the education, work experience, and review steps:

```bash
php artisan folio:page apply/education
php artisan folio:page apply/work-experience
php artisan folio:page apply/review
```

We will update these files with the form components later in the guide.

The main thing to remember here is that with Folio, you don't need to manually define routes in a separate routes file. The mere presence of a Blade file in the `pages` directory automatically creates a corresponding route.

## Building the Multi-Step Form with Volt

Volt is a powerful addition to Laravel Livewire that allows you to build reactive components without writing JavaScript. Unlike traditional Livewire components, Volt lets you define your component's state and validation rules directly in the view file, eliminating the need for a separate component class.

Let's create Volt components for each step of our multi-step form.

### Personal Information Form

First, create the personal information form component:

```bash
php artisan make:volt personal-info-form
```

That will create a file at `resources/views/livewire/personal-info-form.blade.php`. Update the file with the following content:

```blade
<?php

use function Livewire\Volt\state;
use function Livewire\Volt\rules;

state([
    'first_name' => '',
    'last_name' => '',
    'email' => '',
]);

rules([
    'first_name' => 'required|min:2',
    'last_name' => 'required|min:2',
    'email' => 'required|email|unique:applicants,email',
]);

$saveAndContinue = function () {
    $this->validate();

    $applicant = \App\Models\Applicant::create($this->only(['first_name', 'last_name', 'email']));
    session(['applicant_id' => $applicant->id]);

    return redirect()->route('apply.education');
};

?>

<div class="max-w-lg p-8 mx-auto bg-white rounded-lg shadow-md">
    <h2 class="mb-6 text-2xl font-semibold text-gray-800">Personal Information</h2>

    <form wire:submit.prevent="saveAndContinue">
        <!-- First Name -->
        <div class="mb-4">
            <label for="first_name" class="block text-sm font-medium text-gray-700">First Name</label>
            <input type="text" id="first_name" wire:model="first_name" class="block w-full px-3 py-2 mt-1 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-indigo-500 focus:border-indigo-500 sm:text-sm">
            @error('first_name')
                <span class="text-sm text-red-500">{{ $message }}</span>
            @enderror
        </div>

        <div class="mb-4">
            <label for="last_name" class="block text-sm font-medium text-gray-700">Last Name</label>
            <input type="text" id="last_name" wire:model="last_name" class="block w-full px-3 py-2 mt-1 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-indigo-500 focus:border-indigo-500 sm:text-sm">
            @error('last_name')
                <span class="text-sm text-red-500">{{ $message }}</span>
            @enderror
        </div>

        <div class="mb-6">
            <label for="email" class="block text-sm font-medium text-gray-700">Email</label>
            <input type="email" id="email" wire:model="email" class="block w-full px-3 py-2 mt-1 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-indigo-500 focus:border-indigo-500 sm:text-sm">
            @error('email')
                <span class="text-sm text-red-500">{{ $message }}</span>
            @enderror
        </div>

        <div>
            <button type="submit" class="w-full px-4 py-2 text-white transition duration-200 bg-indigo-600 rounded-md hover:bg-indigo-700">
                Next
            </button>
        </div>
    </form>
</div>
```

Quick explanation of the code above:

- We define the component's state using the `state` function, which initializes the form fields.
- The `rules` function sets up validation rules for each field.
- The `saveAndContinue` function handles form submission. It validates the form, creates a new `Applicant` record, stores the `applicant_id` in the session, and redirects to the next step.
- The form fields are bound to the component's state using `wire:model`.
- Validation errors are displayed using `@error`.

In the same way, you can create components for the education, work experience, and review steps.

### Education Form

Next, create the education form component:

```bash
php artisan make:volt education-form
```

Update `resources/views/livewire/education-form.blade.php`:

```blade
<?php

use function Livewire\Volt\state;
use function Livewire\Volt\rules;

state([
    'institution' => '',
    'degree' => '',
    'start_date' => '',
    'end_date' => '',
]);

rules([
    'institution' => 'required|min:2',
    'degree' => 'required|min:2',
    'start_date' => 'required|date',
    'end_date' => 'nullable|date|after:start_date',
]);

$saveAndContinue = function () {
    $this->validate();

    $applicantId = session('applicant_id');
    \App\Models\Education::create(array_merge($this->all(), ['applicant_id' => $applicantId]));

    return redirect()->route('apply.work-experience');
};

?>

<div class="max-w-lg p-8 mx-auto bg-white rounded-lg shadow-md">
    <h2 class="mb-6 text-2xl font-semibold text-gray-800">Education</h2>

    <form wire:submit.prevent="saveAndContinue">
        <div class="mb-4">
            <label for="institution" class="block text-sm font-medium text-gray-700">Institution</label>
            <input type="text" id="institution" wire:model="institution" class="block w-full px-3 py-2 mt-1 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-indigo-500 focus:border-indigo-500 sm:text-sm">
            @error('institution')
                <span class="text-sm text-red-500">{{ $message }}</span>
            @enderror
        </div>

        <div class="mb-4">
            <label for="degree" class="block text-sm font-medium text-gray-700">Degree</label>
            <input type="text" id="degree" wire:model="degree" class="block w-full px-3 py-2 mt-1 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-indigo-500 focus:border-indigo-500 sm:text-sm">
            @error('degree')
                <span class="text-sm text-red-500">{{ $message }}</span>
            @enderror
        </div>

        <div class="mb-4">
            <label for="start_date" class="block text-sm font-medium text-gray-700">Start Date</label>
            <input type="date" id="start_date" wire:model="start_date" class="block w-full px-3 py-2 mt-1 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-indigo-500 focus:border-indigo-500 sm:text-sm">
            @error('start_date')
                <span class="text-sm text-red-500">{{ $message }}</span>
            @enderror
        </div>

        <div class="mb-6">
            <label for="end_date" class="block text-sm font-medium text-gray-700">End Date</label>
            <input type="date" id="end_date" wire:model="end_date" class="block w-full px-3 py-2 mt-1 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-indigo-500 focus:border-indigo-500 sm:text-sm">
            @error('end_date')
                <span class="text-sm text-red-500">{{ $message }}</span>
            @enderror
        </div>

        <div>
            <button type="submit" class="w-full px-4 py-2 text-white transition duration-200 bg-indigo-600 rounded-md hover:bg-indigo-700">
                Next
            </button>
        </div>
    </form>
</div>
```

### Work Experience Form

Next, let's create the work experience form component:

```bash
php artisan make:volt work-experience-form
```

Update `resources/views/livewire/work-experience-form.blade.php` similar to the previous components:

```blade
<?php

use function Livewire\Volt\state;
use function Livewire\Volt\rules;

state([
    'company' => '',
    'position' => '',
    'start_date' => '',
    'end_date' => '',
    'responsibilities' => '',
]);

rules([
    'company' => 'required|min:2',
    'position' => 'required|min:2',
    'start_date' => 'required|date',
    'end_date' => 'nullable|date|after:start_date',
    'responsibilities' => 'required|min:10',
]);

$saveAndContinue = function () {
    $this->validate();

    $applicantId = session('applicant_id');
    \App\Models\WorkExperience::create(array_merge($this->all(), ['applicant_id' => $applicantId]));

    return redirect()->route('apply.review');
};

?>

<div class="max-w-lg p-8 mx-auto bg-white rounded-lg shadow-md">
    <h2 class="mb-6 text-2xl font-semibold text-gray-800">Work Experience</h2>

    <form wire:submit.prevent="saveAndContinue">
        <div class="mb-4">
            <label for="company" class="block text-sm font-medium text-gray-700">Company</label>
            <input type="text" id="company" wire:model="company" class="block w-full px-3 py-2 mt-1 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-indigo-500 focus:border-indigo-500 sm:text-sm">
            @error('company')
                <span class="text-sm text-red-500">{{ $message }}</span>
            @enderror
        </div>

        <div class="mb-4">
            <label for="position" class="block text-sm font-medium text-gray-700">Position</label>
            <input type="text" id="position" wire:model="position" class="block w-full px-3 py-2 mt-1 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-indigo-500 focus:border-indigo-500 sm:text-sm">
            @error('position')
                <span class="text-sm text-red-500">{{ $message }}</span>
            @enderror
        </div>

        <div class="mb-4">
            <label for="start_date" class="block text-sm font-medium text-gray-700">Start Date</label>
            <input type="date" id="start_date" wire:model="start_date" class="block w-full px-3 py-2 mt-1 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-indigo-500 focus:border-indigo-500 sm:text-sm">
            @error('start_date')
                <span class="text-sm text-red-500">{{ $message }}</span>
            @enderror
        </div>

        <div class="mb-4">
            <label for="end_date" class="block text-sm font-medium text-gray-700">End Date</label>
            <input type="date" id="end_date" wire:model="end_date" class="block w-full px-3 py-2 mt-1 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-indigo-500 focus:border-indigo-500 sm:text-sm">
            @error('end_date')
                <span class="text-sm text-red-500">{{ $message }}</span>
            @enderror
        </div>

        <div class="mb-6">
            <label for="responsibilities" class="block text-sm font-medium text-gray-700">Responsibilities</label>
            <textarea id="responsibilities" wire:model="responsibilities" class="block w-full px-3 py-2 mt-1 border border-gray-300 rounded-md shadow-sm focus:outline-none focus:ring-indigo-500 focus:border-indigo-500 sm:text-sm"></textarea>
            @error('responsibilities')
                <span class="text-sm text-red-500">{{ $message }}</span>
            @enderror
        </div>

        <div>
            <button type="submit" class="w-full px-4 py-2 text-white transition duration-200 bg-indigo-600 rounded-md hover:bg-indigo-700">
                Next
            </button>
        </div>
    </form>
</div>
```

### Review Form

Finally, create the review form component:

```bash
php artisan make:volt review-form
```

Update `resources/views/livewire/review-form.blade.php` as we did for the other components:

```blade
<?php

use function Livewire\Volt\state;
use function Livewire\Volt\{mount};
use App\Models\Applicant;

state(['applicant' => null]);

mount(function () {
    $applicantId = session('applicant_id');
    $this->applicant = Applicant::with(['educations', 'workExperiences'])->findOrFail($applicantId);
});

$submit = function () {
    session()->flash('message', 'Your application has been submitted successfully!');
    return redirect()->route('apply.confirmation');
};

?>

<div class="max-w-2xl p-8 mx-auto bg-white rounded-lg shadow-md">
    <h2 class="mb-6 text-2xl font-semibold text-gray-800">Review Your Application</h2>

    <div class="mb-6">
        <h3 class="mb-3 text-xl font-semibold text-gray-700">Personal Information</h3>
        <p><strong>Name:</strong> {{ $applicant->first_name }} {{ $applicant->last_name }}</p>
        <p><strong>Email:</strong> {{ $applicant->email }}</p>
    </div>

    <div class="mb-6">
        <h3 class="mb-3 text-xl font-semibold text-gray-700">Education</h3>
        @foreach($applicant->educations as $education)
            <div class="p-4 mb-4 rounded-lg shadow-sm bg-gray-50">
                <p><strong>Institution:</strong> {{ $education->institution }}</p>
                <p><strong>Degree:</strong> {{ $education->degree }}</p>
                <p><strong>Period:</strong> {{ $education->start_date }} - {{ $education->end_date ?: 'Present' }}</p>
            </div>
        @endforeach
    </div>

    <div class="mb-6">
        <h3 class="mb-3 text-xl font-semibold text-gray-700">Work Experience</h3>
        @foreach($applicant->workExperiences as $experience)
            <div class="p-4 mb-4 rounded-lg shadow-sm bg-gray-50">
                <p><strong>Company:</strong> {{ $experience->company }}</p>
                <p><strong>Position:</strong> {{ $experience->position }}</p>
                <p><strong>Period:</strong> {{ $experience->start_date }} - {{ $experience->end_date ?: 'Present' }}</p>
                <p><strong>Responsibilities:</strong> {{ $experience->responsibilities }}</p>
            </div>
        @endforeach
    </div>

    <div>
        <button wire:click="submit" class="w-full px-4 py-2 text-white transition duration-200 bg-indigo-600 rounded-md hover:bg-indigo-700">
            Submit Application
        </button>
    </div>
</div>
```

These Volt components handle the state management, validation, and submission logic for each step of the multi-step form. That way Volt simplifies the process of creating interactive components by allowing you to define both the logic and the template in a single file.

To use these components in your Folio pages and make the routes named, you can include them like this. Named routes allow you to easily reference routes by name throughout your application. We also need to extend a layout for each page to ensure a consistent structure.

First, in each file, you will define a named route using the `name` function and extend the layout.

- For the `resources/views/pages/apply/personal-info.blade.php` file:

  ```blade
  <?php
  use function Laravel\Folio\name;

  name('apply.personal-info');
  ?>

  @extends('layouts.app')

  @section('title', 'Personal Information')

  @section('content')
  <div>
      <livewire:personal-info-form />
  </div>
  @endsection
  ```

We need to do the same for the other pages:

- For the `resources/views/pages/apply/education.blade.php` file:

  ```blade
  <?php
  use function Laravel\Folio\name;

  name('apply.education');
  ?>

  @extends('layouts.app')

  @section('title', 'Education')

  @section('content')
  <div>
      <livewire:education-form />
  </div>
  @endsection
  ```

- For the `resources/views/pages/apply/work-experience.blade.php` file:

  ```blade
  <?php
  use function Laravel\Folio\name;

  name('apply.work-experience');
  ?>

  @extends('layouts.app')

  @section('title', 'Work Experience')

  @section('content')
  <div>
      <livewire:work-experience-form />
  </div>
  @endsection
  ```

- And for the `resources/views/pages/apply/review.blade.php` file:

  ```blade
  <?php
  use function Laravel\Folio\name;

  name('apply.review');
  ?>

  @extends('layouts.app')

  @section('title', 'Review')

  @section('content')
  <div>
      <livewire:review-form />
  </div>
  @endsection
  ```

### Confirmation Page

Finally, create a confirmation page for the application submission:

```bash
php artisan folio:page apply/confirmation
```

Update the `resources/views/pages/apply/confirmation.blade.php` file:

```blade
<?php
use function Laravel\Folio\name;

name('apply.confirmation');
?>

@extends('layouts.app')

@section('title', 'Confirmation')

@section('content')
<div class="max-w-md p-8 mx-auto text-center bg-white rounded-lg shadow-md">
    <h2 class="mb-4 text-2xl font-semibold text-green-600">Application Submitted</h2>

    <p class="mb-6 text-gray-700">{{ session('message') }}</p>

    <a href="/" class="inline-block px-4 py-2 text-white transition duration-200 bg-indigo-600 rounded-md hover:bg-indigo-700">
        Return to Homepage
    </a>
</div>
@endsection
```

This page displays a success message after the application is submitted and provides a link to return to the homepage.

## Testing the Multi-Step Form

To manually verify that everything works as expected, follow these steps:

1. If you haven't already, start the Laravel development server:

   ```
   php artisan serve
   ```

1. Open your browser and navigate to `http://localhost:8000/apply/personal-info`.

1. Fill out the personal information form and submit it. You should be redirected to the education form.

1. Fill out the education form and submit it. You should be redirected to the work experience form.

1. Fill out the work experience form and submit it. You should be redirected to the review page.

1. On the review page, verify that all the information you entered is displayed correctly.

1. Submit the application and verify that you see a success message.

1. To check if the data was persisted correctly:
   - Open a database client (like pgAdmin for Postgres) and connect to your Neon database.
   - Check the `applicants`, `educations`, and `work_experiences` tables. You should see your submitted data.
   - Verify that the `applicant_id` in the `educations` and `work_experiences` tables matches the `id` in the `applicants` table for your submission.

1. Try refreshing the page or closing and reopening your browser, then navigate back to `http://localhost:8000/apply/review`. You should still see your submitted data, demonstrating that the data persists across sessions.

## Testing

Besides manual testing, you can also write automated tests to make sure your multi-step form works correctly. Laravel provides a testing suite that allows you to write unit, feature, and browser tests.

Create feature tests for your multi-step form to ensure each step works correctly. Here's an example for the personal info step:

```php
<?php

namespace Tests\Feature;

use Tests\TestCase;
use App\Models\Applicant;
use Livewire\Volt\Volt;
use Illuminate\Foundation\Testing\RefreshDatabase;

class PersonalInfoTest extends TestCase
{
    use RefreshDatabase;

    public function test_can_submit_personal_info()
    {
        Volt::test('personal-info-form')
            ->set('first_name', 'John')
            ->set('last_name', 'Doe')
            ->set('email', 'john@example.com')
            ->call('saveAndContinue')
            ->assertRedirect('/apply/education');

        $this->assertDatabaseHas('applicants', [
            'first_name' => 'John',
            'last_name' => 'Doe',
            'email' => 'john@example.com',
        ]);

        $this->assertNotNull(session('applicant_id'));
    }
}
```

This test checks if:

1. The form can be submitted with valid data.
1. The data is correctly stored in the database.
1. The `applicant_id` is stored in the session.
1. The user is redirected to the next step after submission.

You can create similar tests for the education and work experience steps.

To learn more about testing in Laravel, check out the [Testing Laravel Applications with Neon's Database Branching](/guides/laravel-test-on-branch) guide.

## Conclusion

In this guide, we've built a multi-step form using Laravel Volt, Folio, and Neon Postgres. We've covered form validation, data storage, and routing, demonstrating how these tools can be used together to create a dynamic and interactive form.

To further improve this project, consider adding features like:

- File uploads for resumes
- Email notifications to applicants
- An admin interface to review applications

One thing to keep in mind is always to validate and sanitize user inputs, optimize your database queries, and thoroughly test your application before deploying to production.

## Additional Resources

- [Laravel Documentation](https://laravel.com/docs)
- [Neon Documentation](/docs)
- [Neon Branching GitHub Actions Guide](/docs/guides/branching-github-actions)

<NeedHelp />


# Building a Real-Time Task Board with Laravel, Neon, and WebSockets

---
title: Building a Real-Time Task Board with Laravel, Neon, and WebSockets
subtitle: Learn how to create a collaborative task management system using Laravel, Neon Postgres, and WebSockets
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-08-17T00:00:00.000Z'
updatedOn: '2024-08-17T00:00:00.000Z'
---

Real-time features can significantly improve user experience in web applications. They allow users to see updates immediately without refreshing the page. In this guide, we'll demonstrate how to add real-time functionality to a Laravel application using Neon Postgres and WebSockets.

We'll build a collaborative task board where team members can create, update, and move tasks in real-time. By the end of this guide, you'll understand how to set up WebSockets in Laravel, store and retrieve data using Neon Postgres, and broadcast updates to connected clients instantly.

## Prerequisites

Before we begin, make sure you have the following:

- PHP 8.1 or higher installed on your system
- Composer for managing PHP dependencies
- A [Neon](https://console.neon.tech/signup) account and project

## Setting up the Laravel project

To get started we will need to create a new Laravel project and configuring it with Neon Postgres.

1. Create a new Laravel project:

   ```bash
   composer create-project laravel/laravel realtime-taskboard
   cd realtime-taskboard
   ```

   This will create a new Laravel project in a directory named `realtime-taskboard`. And using the `cd` command, we'll navigate to the project directory.

2. Configure the Neon database connection. Open your `.env` file and update the database settings:

   ```env
   DB_CONNECTION=pgsql
   DB_HOST=your-neon-hostname.neon.tech
   DB_PORT=5432
   DB_DATABASE=your_database_name
   DB_USERNAME=your_username
   DB_PASSWORD=your_password
   ```

   Replace the placeholders with your Neon database details which you can find in the Neon console.

3. Laravel provides a few starter kits for authentication. We'll use Laravel Breeze for this project to set up authentication:

   ```bash
   composer require laravel/breeze --dev
   php artisan breeze:install blade
   php artisan migrate
   npm install
   npm run dev
   ```

   This will install Laravel Breeze, set up authentication views using Blade, run the migrations to create the necessary tables. The `npm install` and `npm run dev` commands install the frontend dependencies and compile the assets.

Now that we've set up the Laravel project and connected it to Neon Postgres, let's create the task board.

## Creating the Task model and migration

Laravel uses models to interact with the database and migrations to create database tables. The task board will consist of tasks that users can create, update, and move between different statuses (e.g., 'To Do', 'In Progress', 'Done').

Let's create a model and migration for our tasks table.

1. Generate the model and migration:

   ```bash
   php artisan make:model Task -m
   ```

   This command will create a `Task` model and a migration file for the `tasks` table.

2. Update the migration file in `database/migrations` to create the `tasks` table with the necessary columns:

   ```php
   public function up()
   {
       Schema::create('tasks', function (Blueprint $table) {
           $table->id();
           $table->foreignId('user_id')->constrained()->onDelete('cascade');
           $table->string('title');
           $table->text('description')->nullable();
           $table->enum('status', ['todo', 'in_progress', 'done'])->default('todo');
           $table->timestamps();
       });
   }
   ```

   This migration creates a `tasks` table with columns for the task `title`, `description`, `status`, and the user who created the task.

3. Run the migration:

   ```bash
   php artisan migrate
   ```

   This will create the `tasks` table in your Neon Postgres database with the specified columns and constraints in the migration file.

4. Update the `Task` model in `app/Models/Task.php`:

   ```php
   <?php

   namespace App\Models;

   use Illuminate\Database\Eloquent\Factories\HasFactory;
   use Illuminate\Database\Eloquent\Model;

   class Task extends Model
   {
       use HasFactory;

       protected $fillable = ['title', 'description', 'status'];

       public function user()
       {
           return $this->belongsTo(User::class);
       }
   }
   ```

   This model defines the relationship between tasks and users. Each task belongs to a user thanks to the `user()` method defined in the model. The `fillable` property specifies which attributes can be mass-assigned. Laravel then makes it easy to create, update, and retrieve tasks using the `Task` model.

5. One more thing that we will have to do is to update the `User` model in `app/Models/User.php` to define the relationship between users and tasks:

   ```php
   public function tasks()
   {
       return $this->hasMany(Task::class);
   }
   ```

   This defines the relationship between users and tasks. Each user can have multiple tasks and can be retrieved using the `tasks()` method on the `User` model.

## Setting up WebSockets

Laravel provides built-in support for broadcasting events using WebSockets. We'll use WebSockets to broadcast task creation and updates in real-time to connected clients using Pusher.

Instead of using Pusher, there are other options like [Laravel WebSockets](https://beyondco.de/docs/laravel-websockets) by Beyond Code, which is a self-hosted WebSockets server for Laravel applications. However, for this guide, we'll use Pusher, as it takes care of the WebSockets infrastructure for us.

With Laravel 11 to install broadcasting, you can run the following command:

```bash
php artisan install:broadcasting
```

Follow the prompts to set up broadcasting and when asked for `reverb`, select "No" as we are going to use Pusher instead.

To set up Pusher, you need to do the following:

1. Sign up for a free account at [Pusher](https://pusher.com/).

2. After creating an account and a new app, update your `.env` file with your Pusher credentials:

   ```env
   PUSHER_APP_ID="your-pusher-app-id"
   PUSHER_APP_KEY="your-pusher-key"
   PUSHER_APP_SECRET="your-pusher-secret"
   PUSHER_HOST=
   PUSHER_PORT=443
   PUSHER_SCHEME="https"
   PUSHER_APP_CLUSTER="mt1"

   VITE_APP_NAME="${APP_NAME}"
   VITE_PUSHER_APP_KEY="${PUSHER_APP_KEY}"
   VITE_PUSHER_HOST="${PUSHER_HOST}"
   VITE_PUSHER_PORT="${PUSHER_PORT}"
   VITE_PUSHER_SCHEME="${PUSHER_SCHEME}"
   VITE_PUSHER_APP_CLUSTER="${PUSHER_APP_CLUSTER}"

   BROADCAST_DRIVER=pusher
   ```

   The `PUSHER_APP_ID`, `PUSHER_APP_KEY`, and `PUSHER_APP_SECRET` values can be found in your Pusher dashboard. The `PUSHER_APP_CLUSTER` value depends on the region where your app is hosted. For example, `mt1` is for the `mt1` region.

   The `VITE_` variables are used for client-side configuration in our JavaScript code using Vite. In our case, we'll use Vite to manage our frontend assets along with Laravel Echo and Pusher.

3. Install Laravel Echo and Pusher JS:

   ```bash
   npm install --save-dev laravel-echo pusher-js
   ```

   Laravel Echo is a JavaScript library that makes it easy to work with WebSockets and listen for events. Pusher JS is the JavaScript client library for Pusher that Laravel Echo uses to communicate with the Pusher service where our events are broadcasted to.

4. Open `resources/js/echo.js` and update it with your Pusher credentials:

   ```javascript
   import Echo from 'laravel-echo';

   import Pusher from 'pusher-js';
   window.Pusher = Pusher;

   window.Echo = new Echo({
     broadcaster: 'pusher',
     key: import.meta.env.VITE_PUSHER_APP_KEY,
     cluster: import.meta.env.VITE_PUSHER_APP_CLUSTER,
     forceTLS: true,
   });
   ```

   Make sure that you still have the `npm run dev` command running in the background to compile the assets and make the changes available in the browser.

## Creating the task board interface

Now that we've set up the database, models, and WebSockets, let's create the task board interface where users can view, create, and update tasks.

Create a new blade file at `resources/views/taskboard.blade.php` where we'll build the task board interface:

```html
<x-app-layout>
  <x-slot name="header">
    <h2 class="text-gray-800 text-xl font-semibold leading-tight">{{ __('Task Board') }}</h2>
  </x-slot>

  <div class="py-12">
    <div class="mx-auto max-w-7xl lg:px-8 sm:px-6">
      <div class="overflow-hidden bg-white shadow-sm sm:rounded-lg">
        <div class="border-gray-200 border-b bg-white p-6">
          <div class="grid grid-cols-1 gap-6 md:grid-cols-3">
            <div id="todo" class="bg-gray-100 rounded-lg p-4 shadow">
              <h3 class="text-blue-600 mb-4 text-lg font-semibold">To Do</h3>
              <div class="task-column space-y-4">
                <!-- Tasks will be inserted here -->
              </div>
            </div>
            <div id="in_progress" class="bg-gray-100 rounded-lg p-4 shadow">
              <h3 class="text-yellow-600 mb-4 text-lg font-semibold">In Progress</h3>
              <div class="task-column space-y-4">
                <!-- Tasks will be inserted here -->
              </div>
            </div>
            <div id="done" class="bg-gray-100 rounded-lg p-4 shadow">
              <h3 class="text-green-600 mb-4 text-lg font-semibold">Done</h3>
              <div class="task-column space-y-4">
                <!-- Tasks will be inserted here -->
              </div>
            </div>
          </div>

          <div class="mt-8">
            <h3 class="mb-4 text-lg font-bold">Add a New Task</h3>
            <form id="task-form" class="space-y-4">
              @csrf
              <div>
                <label for="title" class="text-gray-700 block text-sm font-medium"
                  >Task Title</label
                >
                <input
                  type="text"
                  name="title"
                  id="title"
                  placeholder="Enter task title"
                  class="border-gray-300 focus:ring-blue-500 focus:border-blue-500 mt-1 block w-full rounded-md border p-2 shadow-sm"
                  required
                />
              </div>
              <div>
                <label for="description" class="text-gray-700 block text-sm font-medium"
                  >Task Description</label
                >
                <textarea
                  name="description"
                  id="description"
                  placeholder="Enter task description"
                  class="border-gray-300 focus:ring-blue-500 focus:border-blue-500 mt-1 block w-full rounded-md border p-2 shadow-sm"
                ></textarea>
              </div>
              <input type="hidden" name="status" value="todo" />
              <div>
                <button
                  type="submit"
                  class="bg-blue-500 hover:bg-blue-600 focus:ring-blue-500 w-full rounded-md px-4 py-2 text-white shadow focus:outline-none focus:ring-2 focus:ring-offset-2"
                >
                  Add Task
                </button>
              </div>
            </form>
          </div>
        </div>
      </div>
    </div>
  </div>

  <script>
    document.addEventListener('DOMContentLoaded', function () {
      const taskForm = document.getElementById('task-form');
      const columns = ['todo', 'in_progress', 'done'];

      function addTaskToColumn(task) {
        const column = document.querySelector(`#${task.status} .task-column`);
        const taskElement = document.createElement('div');
        taskElement.id = `task-${task.id}`;
        taskElement.className = 'bg-white p-4 rounded-lg shadow';
        taskElement.innerHTML = `
                    <h4 class="font-semibold text-lg mb-2">${task.title}</h4>
                    <p class="text-gray-600 mb-4">${task.description || 'No description'}</p>
                    <select class="task-status w-full p-2 border border-gray-300 rounded-md" data-task-id="${task.id}">
                        ${columns.map((status) => `<option value="${status}" ${status === task.status ? 'selected' : ''}>${status.replace('_', ' ')}</option>`).join('')}
                    </select>
                `;
        column.appendChild(taskElement);
      }

      function loadTasks() {
        axios
          .get('/tasks')
          .then((response) => {
            response.data.forEach(addTaskToColumn);
          })
          .catch((error) => {
            console.error('Error loading tasks:', error);
          });
      }

      taskForm.addEventListener('submit', function (e) {
        e.preventDefault();
        const formData = new FormData(this);
        axios
          .post('/tasks', Object.fromEntries(formData))
          .then((response) => {
            this.reset();
            addTaskToColumn(response.data);
          })
          .catch((error) => {
            console.error('Error creating task:', error);
          });
      });

      document.addEventListener('change', function (e) {
        if (e.target.classList.contains('task-status')) {
          const taskId = e.target.dataset.taskId;
          const newStatus = e.target.value;
          axios
            .put(`/tasks/${taskId}`, { status: newStatus })
            .then((response) => {
              const taskElement = document.getElementById(`task-${taskId}`);
              if (taskElement) {
                taskElement.remove();
              }
              addTaskToColumn(response.data);
            })
            .catch((error) => {
              console.error('Error updating task:', error);
            });
        }
      });

      Echo.channel('taskboard')
        .listen('TaskCreated', (e) => {
          addTaskToColumn(e.task);
        })
        .listen('TaskUpdated', (e) => {
          const taskElement = document.getElementById(`task-${e.task.id}`);
          if (taskElement) {
            taskElement.remove();
          }
          addTaskToColumn(e.task);
        });

      loadTasks();
    });
  </script>
</x-app-layout>
```

The majority of the code is HTML and JavaScript that creates the task board interface and handles task creation and updates. The real-time functionality of our task board is powered by WebSockets, implemented using Laravel Echo and Pusher. Here's a breakdown of how it works:

1. **Channel Setup**: We create a channel named 'taskboard' for our real-time communications:

   ```javascript
   Echo.channel('taskboard');
   ```

   This channel will be used for broadcasting and listening to task-related events.

2. **Event Listening**: We set up listeners for two types of events:

   ```javascript
   .listen('TaskCreated', (e) => {
       addTaskToColumn(e.task);
   })
   .listen('TaskUpdated', (e) => {
       const taskElement = document.getElementById(`task-${e.task.id}`);
       if (taskElement) {
           taskElement.remove();
       }
       addTaskToColumn(e.task);
   });
   ```

   - `TaskCreated`: When a new task is created, we add it to the appropriate column.
   - `TaskUpdated`: When a task is updated, we remove the old task element and add the updated one.

   For more information on how events are broadcasted and listened to, check out the [Getting Started with Laravel Events and Listeners](/guides/laravel-events-and-listeners) guide.

3. **Server-Side Broadcasting**: For the above to work, in our Laravel controllers, we will broadcast these events after creating or updating a task:

   ```php
   broadcast(new TaskCreated($task))->toOthers();
   broadcast(new TaskUpdated($task))->toOthers();
   ```

   The `toOthers()` method ensures that the event is not sent back to the user who initiated the action.

4. **Real-Time Updates**: When these events are received, the task board updates instantly for all connected users, providing a collaborative, real-time experience.

This WebSockets implementation allows for immediate synchronization across all clients without the need for polling or page refreshes, creating a smooth and responsive user experience.

## Handling tasks

As mentioned earlier, we'll use Laravel controllers to handle task creation and updates and broadcast events to connected clients.

1. Create a controller for tasks:

   ```bash
   php artisan make:controller TaskController
   ```

2. Update `app/Http/Controllers/TaskController.php`:

   ```php
   <?php

   namespace App\Http\Controllers;

   use App\Models\Task;
   use App\Events\TaskCreated;
   use App\Events\TaskUpdated;
   use Illuminate\Http\Request;

   class TaskController extends Controller
   {
       public function index()
       {
           return Task::all();
       }

       public function store(Request $request)
       {
           $validatedData = $request->validate([
               'title' => 'required|string|max:255',
               'description' => 'nullable|string',
           ]);

           $task = $request->user()->tasks()->create($validatedData);
           broadcast(new TaskCreated($task))->toOthers();
           return $task;
       }

       public function update(Request $request, Task $task)
       {
           $validatedData = $request->validate([
               'status' => 'required|in:todo,in_progress,done',
           ]);

           $task->update($validatedData);
           broadcast(new TaskUpdated($task))->toOthers();
           return $task;
       }
   }
   ```

   These methods handle task creation, retrieval, and updates. When a task is created or updated, the corresponding event is broadcast to all connected clients using the `TaskCreated` or `TaskUpdated` event and the `broadcast` method.

3. Create events for task creation and updates:

   ```bash
   php artisan make:event TaskCreated
   php artisan make:event TaskUpdated
   ```

   These commands will create two event classes in the `app/Events` directory. We'll update these classes to broadcast the task data to the 'taskboard' channel.

4. Update `app/Events/TaskCreated.php`:

   ```php
   <?php

   namespace App\Events;

   use App\Models\Task;
   use Illuminate\Broadcasting\Channel;
   use Illuminate\Broadcasting\InteractsWithSockets;
   use Illuminate\Contracts\Broadcasting\ShouldBroadcast;
   use Illuminate\Foundation\Events\Dispatchable;
   use Illuminate\Queue\SerializesModels;

   class TaskCreated implements ShouldBroadcast
   {
       use Dispatchable, InteractsWithSockets, SerializesModels;

       public $task;

       public function __construct(Task $task)
       {
           $this->task = $task;
       }

       public function broadcastOn()
       {
           return new Channel('taskboard');
       }
   }
   ```

   This class defines the `TaskCreated` event, which broadcasts the newly created task to the 'taskboard' channel, allowing all connected clients to receive the update.

   If you need to broadcast the event to a specific user, you can use the `private` channel instead of the `public` channel.

5. Update `app/Events/TaskUpdated.php` similarly, just change the class name to `TaskUpdated`:

   ```php
   <?php

   namespace App\Events;

   use App\Models\Task;
   use Illuminate\Broadcasting\Channel;
   use Illuminate\Broadcasting\InteractsWithSockets;
   use Illuminate\Contracts\Broadcasting\ShouldBroadcast;
   use Illuminate\Foundation\Events\Dispatchable;
   use Illuminate\Queue\SerializesModels;

   class TaskUpdated implements ShouldBroadcast
   {
       use Dispatchable, InteractsWithSockets, SerializesModels;

       public $task;

       public function __construct(Task $task)
       {
           $this->task = $task;
       }

       public function broadcastOn()
       {
           return new Channel('taskboard');
       }
   }
   ```

6. Update `routes/web.php` to add routes for tasks:

   ```php
   use App\Http\Controllers\TaskController;

   Route::middleware(['auth'])->group(function () {
       Route::get('/taskboard', function () {
           return view('taskboard');
       })->name('taskboard');

       Route::get('/tasks', [TaskController::class, 'index']);
       Route::post('/tasks', [TaskController::class, 'store']);
       Route::put('/tasks/{task}', [TaskController::class, 'update']);
   });
   ```

   These routes allow users to view the task board, retrieve tasks, create new tasks, and update existing tasks.

   For more information on Laravel routing, check out the [Laravel's Routes, Middleware, and Validation guide](/guides/laravel-routes-middleware-validation).

## Testing the real-time task board

Now that everything is set up, let's test our real-time task board.

1. Start your Laravel development server:

   ```bash
   php artisan serve
   ```

   This will start the Laravel development server on `http://localhost:8000`. If you already have a server running, you can skip this step.

2. In another terminal, start the Laravel queue worker to process the broadcast events:

   ```bash
   php artisan queue:work
   ```

   This will ensure that the broadcast events are processed and sent to connected clients.

   To learn more about Laravel queues, check out the [Implementing Queue Workers and Job Processing in Laravel with Neon Postgres](/guides/laravel-queue-workers-job-processing) guide.

3. Open two different browsers and visit `http://localhost:8000/taskboard`.

4. Log in with two different user accounts.

5. Start creating and moving tasks in one browser. You should see the tasks appear and move in real-time in the other browser.

6. If you were to visit your Pusher dashboard, you should see the events being broadcasted.

## How it works

Here's how the whole process of the real-time updates work:

1. When a user creates or updates a task, it's sent to the server.

2. The server saves the task in the Neon Postgres database.

3. After saving the task, the server broadcasts a `TaskCreated` or `TaskUpdated` event using Pusher.

4. Pusher sends this event to all connected users except the sender using the 'taskboard' channel.

5. The JavaScript code listening for these events receives the new or updated task and adds or moves it on the task board.

This process happens very quickly, giving the appearance of real-time updates.

## Optimizing for larger applications

As your task board grows, you might need to optimize it for better performance:

1. **Pagination**: Instead of loading all tasks at once, implement pagination to load tasks in smaller batches. Currently, we are loading all tasks using `Task::all()`, which can be inefficient for large datasets.

2. **Caching**: Use Laravel's caching features to cache frequently accessed data, reducing database queries.

3. **Database Indexing**: Add indexes to frequently queried columns in your Neon Postgres database to speed up queries. For more information, check out the Neon Postgres documentation on [Indexes](/docs/postgres/indexes).

4. **Queue Workers**: Use multiple queue workers to process broadcast events concurrently, especially in high-traffic applications. Also, consider using Laravel Horizon for monitoring and managing your queue workers.

5. **Private Channels**: If you need to broadcast events to specific users or groups, use private channels to ensure data privacy. This is useful for applications with user-specific data or private conversations between users where data should not be shared with others.

## Conclusion

In this guide, we've built a simple real-time collaborative task board using Laravel, Neon Postgres, and WebSockets. This example shows how you can create interactive, real-time web applications that update instantly across multiple users using Laravel's broadcasting feature.

## Additional Resources

- [Laravel Broadcasting Documentation](https://laravel.com/docs/broadcasting)
- [Pusher Documentation](https://pusher.com/docs)
- [Neon Documentation](/docs)

<NeedHelp />


# Building a Todo CLI App with Laravel Zero and Neon Postgres

---
title: Building a Todo CLI App with Laravel Zero and Neon Postgres
subtitle: Learn how to create a command-line interface (CLI) application using Laravel Zero and Neon Postgres for efficient task management.
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-07-01T00:00:00.000Z'
updatedOn: '2024-07-01T00:00:00.000Z'
---

[Laravel Zero](https://laravel-zero.com/) is a micro-framework that provides a starting point for your console application.

Combined with Neon's serverless Postgres database, you can create powerful CLI tools with persistent storage.

In this guide, we'll build a Todo CLI app that allows users to manage their tasks efficiently from the command line.

By the end of this tutorial, you'll have a fully functional Todo CLI app where users can add, list, update, and delete tasks. We'll also implement task prioritization and due dates to enhance the app's functionality.

## Prerequisites

Before we start, make sure you have the following:

- PHP 8.1 or higher installed on your system
- [Composer](https://getcomposer.org/) for managing PHP dependencies
- A [Neon](https://console.neon.tech/signup) account for database hosting
- Basic knowledge of PHP and Laravel

## Setting up the Project

Let's start by creating a new Laravel Zero project and setting up the necessary components.

### Creating a New Laravel Zero Project

Open your terminal and run the following command to create a new Laravel Zero project:

```bash
composer create-project laravel-zero/laravel-zero todo-cli
cd todo-cli
```

This command creates a new Laravel Zero project in a directory named `todo-cli` and installs all the necessary dependencies.

Once the project is created, you can run the following command to rename the default `app` namespace to `todo`:

```bash
php application app:rename todo
```

To test the application, run:

```bash
php todo inspire
```

This command should display an inspirational quote from Laravel as a confirmation that the application is set up correctly.

### Installing Required Add-Ons

Out of the box Laravel Zero provides a basic structure for CLI applications. Laravel Zero offers a variety of add-ons to extend the functionality of your CLI app which includes database support, testing, logging, file system, scheduler, and more.

To install the database package, run:

```bash
php todo app:install database
```

> The `todo` command is the name of the executable file we defined in the previous step using the `app:rename` command.

Additionally, we can install the `fakerphp/faker` package to generate sample data for testing later on:

```bash
composer require fakerphp/faker --dev
```

### Configuring the Database

The `config/database.php` file will include the database configuration for your application looking like this:

```php
'connections' => [
    // ... other connections ...

    'neon' => [
        'driver' => 'pgsql',
        'url' => env('DATABASE_URL'),
        'host' => env('DB_HOST', '127.0.0.1'),
        'port' => env('DB_PORT', '5432'),
        'database' => env('DB_DATABASE', 'forge'),
        'username' => env('DB_USERNAME', 'forge'),
        'password' => env('DB_PASSWORD', ''),
        'charset' => 'utf8',
        'prefix' => '',
        'prefix_indexes' => true,
        'schema' => 'public',
        'sslmode' => 'prefer',
    ],
],
```

Rather than hardcoding the database credentials in the configuration file, we can use environment variables to store sensitive information securely. Create a `.env` file in the root of your project and add your Neon database credentials:

```env
DB_CONNECTION=neon
DATABASE_URL=postgres://your-username:your-password@your-neon-hostname/your-database
DB_HOST=your-neon-hostname
DB_PORT=5432
DB_DATABASE=your-database
DB_USERNAME=your-username
DB_PASSWORD=your-password
```

Make sure to replace the placeholders with your actual Neon database details.

## Creating the Todo App Structure

Now that we have our basic setup, let's create the structure for our Todo CLI app, including models, migrations, and commands.

### Creating the Task Model and Migration

Just like a standard Laravel application, we'll create a `Task` model to represent the tasks in our Todo app.

```bash
php todo make:model Task -m
```

Update the migration file in `database/migrations` to define the structure of our tasks table:

```php
public function up()
{
    Schema::create('tasks', function (Blueprint $table) {
        $table->id();
        $table->string('title');
        $table->text('description')->nullable();
        $table->enum('status', ['pending', 'in_progress', 'completed'])->default('pending');
        $table->enum('priority', ['low', 'medium', 'high'])->default('medium');
        $table->date('due_date')->nullable();
        $table->timestamps();
    });
}
```

This migration creates a `tasks` table with columns for the task title, description, status, priority, and due date.

Now, update the `app/Task.php` model to define the fillable attributes:

```php
<?php

namespace App;

use Illuminate\Database\Eloquent\Model;

class Task extends Model
{
    protected $fillable = ['title', 'description', 'status', 'priority', 'due_date'];

    protected $casts = [
        'due_date' => 'date',
    ];
}
```

Run the migration to create the tasks table in your Neon database:

```bash
php todo migrate
```

This command will create the `tasks` table in your Neon database.

### Creating Commands

Laravel Zero uses commands to define the CLI functionality. Let's create commands for adding, listing, updating, and deleting tasks.

#### Add Task Command

Create a new command to add tasks:

```bash
php todo make:command AddTaskCommand
```

Update the `app/Commands/AddTaskCommand.php` the file to define the command signature and functionality:

```php
<?php

namespace App\Commands;

use App\Task;
use Carbon\Carbon;
use LaravelZero\Framework\Commands\Command;

class AddTaskCommand extends Command
{
    protected $signature = 'task:add {title} {--description=} {--priority=medium} {--due-date=}';
    protected $description = 'Add a new task';

    public function handle()
    {
        $task = Task::create([
            'title' => $this->argument('title'),
            'description' => $this->option('description'),
            'priority' => $this->option('priority'),
            'due_date' => $this->option('due-date') ? Carbon::parse($this->option('due-date')) : null,
        ]);

        $this->info("Task added successfully! ID: {$task->id}");
    }
}
```

This command allows users to add a new task with a title, description, priority, and due date.

Rundown of the command signature:

- `{title}`: The title of the task (required argument). Required arguments are passed without the `--` prefix.
- `--description`: The description of the task (optional option)
- `--priority`: The priority of the task (optional option with a default value of `medium`)
- `--due-date`: The due date of the task (optional option)

The `handle` method creates a new task record in the database with the provided details and displays a success message with the ID of the newly created task.

To verify that the command works, run:

```bash
php todo task:add "Complete Laravel Zero guide" --description="Write a guide on creating a Todo CLI app" --priority=high --due-date=2024-07-15
```

You should see a success message with the ID of the newly created task.

#### List Tasks Command

Next, let's create a command to list tasks so we can view all tasks or filter them by status or priority:

```bash
php todo make:command ListTasksCommand
```

Update the `app/Commands/ListTasksCommand.php` file:

```php
<?php

namespace App\Commands;

use App\Task;
use LaravelZero\Framework\Commands\Command;

class ListTasksCommand extends Command
{
    protected $signature = 'task:list {--status=} {--priority=}';
    protected $description = 'List all tasks';

    public function handle()
    {
        $query = Task::query();

        if ($this->option('status')) {
            $query->where('status', $this->option('status'));
        }

        if ($this->option('priority')) {
            $query->where('priority', $this->option('priority'));
        }

        $tasks = $query->get();

        $headers = ['ID', 'Title', 'Status', 'Priority', 'Due Date'];
        $rows = $tasks->map(function ($task) {
            return [
                $task->id,
                $task->title,
                $task->status,
                $task->priority,
                $task->due_date ? $task->due_date->format('Y-m-d') : 'N/A',
            ];
        });

        $this->table($headers, $rows);
    }
}
```

Rundown of the command signature:

- `--status`: Filter tasks by status (optional option)
- `--priority`: Filter tasks by priority (optional option)

The `handle` method retrieves tasks from the database based on the provided filters (if any) and displays them in a table format.

Now you can list all tasks or filter them by status or priority:

```bash
php todo task:list
```

The result will be displayed in a table format with columns for ID, Title, Status, Priority, and Due Date:

```sql
+----+-----------------------------+---------+----------+------------+
| ID | Title                       | Status  | Priority | Due Date   |
+----+-----------------------------+---------+----------+------------+
| 1  | Complete Laravel Zero guide | pending | high     | 2024-07-15 |
+----+-----------------------------+---------+----------+------------+
```

To filter tasks by status or priority, you can use the `--status` and `--priority` options:

```bash
php todo task:list --status=pending
```

#### Update Task Command

Now that we have the ability to add and list tasks, let's create a command to update tasks:

```bash
php todo make:command UpdateTaskCommand
```

Update the `app/Commands/UpdateTaskCommand.php` file and define the command signature and functionality:

```php
<?php

namespace App\Commands;

use App\Task;
use Carbon\Carbon;
use LaravelZero\Framework\Commands\Command;

class UpdateTaskCommand extends Command
{
    protected $signature = 'task:update {id} {--title=} {--description=} {--status=} {--priority=} {--due-date=}';
    protected $description = 'Update an existing task';

    public function handle()
    {
        $task = Task::find($this->argument('id'));

        if (!$task) {
            $this->error("Task not found!");
            return;
        }

        $updates = [];

        if ($this->option('title')) {
            $updates['title'] = $this->option('title');
        }

        if ($this->option('description')) {
            $updates['description'] = $this->option('description');
        }

        if ($this->option('status')) {
            $updates['status'] = $this->option('status');
        }

        if ($this->option('priority')) {
            $updates['priority'] = $this->option('priority');
        }

        if ($this->option('due-date')) {
            $updates['due_date'] = Carbon::parse($this->option('due-date'));
        }

        $task->update($updates);

        $this->info("Task updated successfully!");
    }
}
```

Rundown of the command signature:

- `{id}`: The ID of the task to update (required argument)
- `--title`: The new title of the task (optional option)
- `--description`: The new description of the task (optional option)
- `--status`: The new status of the task (optional option)
- `--priority`: The new priority of the task (optional option)
- `--due-date`: The new due date of the task (optional option)

The `handle` method retrieves the task by ID, checks if it exists, and updates the task with the provided details. It displays a success message if the task is updated successfully.

To update a task, run:

```bash
php todo task:update 1 --status=in_progress
```

You will get a `Task updated successfully!` message if the task is updated successfully.

To verify the update, list the tasks again:

```bash
php todo task:list
```

Now you should see the updated status of the task:

```sql
+----+-----------------------------+-------------+----------+------------+
| ID | Title                       | Status      | Priority | Due Date   |
+----+-----------------------------+-------------+----------+------------+
| 1  | Complete Laravel Zero guide | in_progress | high     | 2024-07-15 |
+----+-----------------------------+-------------+----------+------------+
```

Try to also update other fields like the title, description, priority, or due date.

#### Delete Task Command

Finally to complete the basic CRUD operations, let's create a command to delete tasks:

```bash
php todo make:command DeleteTaskCommand
```

Update the `app/Commands/DeleteTaskCommand.php` file:

```php
<?php

namespace App\Commands;

use App\Task;
use LaravelZero\Framework\Commands\Command;

class DeleteTaskCommand extends Command
{
    protected $signature = 'task:delete {id}';
    protected $description = 'Delete a task';

    public function handle()
    {
        $task = Task::find($this->argument('id'));

        if (!$task) {
            $this->error("Task not found!");
            return;
        }

        $task->delete();

        $this->info("Task deleted successfully!");
    }
}
```

Here the `id` argument is required to identify the task to be deleted.

The `handle` method retrieves the task by ID, checks if it exists, deletes the task, and displays a success message if the task is deleted successfully.

To delete a task, run:

```bash
php todo task:delete 1
```

You should see a `Task deleted successfully!` message if the task is deleted successfully.

You can also try deleting a task that doesn't exist to see the error message:

```bash
php todo task:delete 2
```

This will display a `Task not found!` error message.

## Implementing Additional Features

To enhance our Todo CLI app, let's implement some additional features like task prioritization and due date reminders.

### Task Prioritization

We've already included a priority field in our tasks table. Let's update the `ListTasksCommand` to sort tasks by priority:

Update the `handle` method in `app/Commands/ListTasksCommand.php`:

```php {14-19}
public function handle()
{
    $query = Task::query();

    if ($this->option('status')) {
        $query->where('status', $this->option('status'));
    }

    if ($this->option('priority')) {
        $query->where('priority', $this->option('priority'));
    }

    $tasks = $query->get(); // [!code --]
    $tasks = $query->orderByRaw("CASE
        WHEN priority = 'high' THEN 1
        WHEN priority = 'medium' THEN 2
        WHEN priority = 'low' THEN 3
        ELSE 4
    END")->get();

    // ... rest of the method
}
```

This modification sorts the tasks by priority, with high priority tasks appearing first.

### Due Date Reminders

Let's create a new command to show tasks that are due soon:

```bash
php todo make:command DueTasksCommand
```

Update the `app/Commands/DueTasksCommand.php` to include the due date reminder functionality:

```php
<?php

namespace App\Commands;

use App\Task;
use Carbon\Carbon;
use LaravelZero\Framework\Commands\Command;

class DueTasksCommand extends Command
{
    protected $signature = 'task:due {days=7}';
    protected $description = 'Show tasks due within the specified number of days';

    public function handle()
    {
        $days = (int) $this->argument('days');
        $dueDate = Carbon::now()->addDays($days);

        $tasks = Task::where('status', '!=', 'completed')
            ->whereDate('due_date', '<=', $dueDate)
            ->orderBy('due_date')
            ->get();

        if ($tasks->isEmpty()) {
            $this->info("No tasks due within the next {$days} days.");
            return;
        }

        $headers = ['ID', 'Title', 'Priority', 'Due Date', 'Days Left'];
        $rows = $tasks->map(function ($task) {
            $daysLeft = Carbon::now()->diffInDays($task->due_date, false);
            return [
                $task->id,
                $task->title,
                $task->priority,
                $task->due_date->format('Y-m-d'),
                $daysLeft,
            ];
        });

        $this->table($headers, $rows);
    }
}
```

This command shows tasks that are due within a specified number of days (default is 7) and aren't completed yet.

The `handle` method calculates the due date based on the provided number of days, retrieves tasks that are due within that period, and displays them in a table format with columns for ID, Title, Priority, Due Date, and Days Left.

To show tasks due within the next 14 days, run:

```bash
php todo task:due 14
```

## Testing the Todo CLI App

Now that we have implemented our Todo CLI app, let's try the complete workflow to ensure everything works as expected.

1. Add a task:

```bash
php todo task:add "Complete Laravel Zero guide" --description="Write a guide on creating a Todo CLI app" --priority=high --due-date=2024-07-15
```

2. List all tasks:

```bash
php todo task:list
```

3. Update a task:

```bash
php todo task:update 1 --status=in_progress
```

4. Show due tasks:

```bash
php todo task:due 14
```

5. Delete a task:

```bash
php todo task:delete 1
```

## Building the Application

Rather than having to run the `php todo` command each time, Laravel Zero allows you to build your application into a single executable file. To do this, run:

```bash
php todo app:build
```

This command will generate a standalone executable in the `builds` directory, which you can distribute and run on other systems without requiring PHP or Composer to be installed.

You will be prompted to choose the version of the application you want to build. Select the version that best suits your needs.

To run the built application, use the following command:

```bash
./builds/todo
```

To handle your database environment variables, you can create a `.env` file in the same directory as the built executable or set the environment variables directly in your system.

## Conclusion

In this tutorial, we've built a fully functional Todo CLI app using Laravel Zero and Neon Postgres. We've implemented features such as adding, listing, updating, and deleting tasks, as well as task prioritization and due date reminders.

This implementation provides a solid foundation for a CLI-based task management system, but there are always ways to improve and expand its functionality:

- Implement task categories or tags
- Add support for recurring tasks
- Implement data export and import functionality
- Add user authentication for multi-user support
- Implement task dependencies (subtasks)

By combining the power of Laravel Zero and the scalability of Neon Postgres, you can quickly create efficient and powerful CLI applications that meet your specific needs.

## Additional Resources

- [Laravel Zero Documentation](https://laravel-zero.com/docs/introduction)
- [Illuminate Database Documentation](https://laravel.com/docs/database)
- [Carbon Documentation](https://carbon.nesbot.com/docs/)
- [Neon Documentation](/docs)


# Using LlamaIndex with Postgres to Build your own Reverse Image Search Engine

---
title: Using LlamaIndex with Postgres to Build your own Reverse Image Search Engine
subtitle: A step-by-step guide to build your own Reverse Image Search engine in an Astro application with LlamaIndex and Postgres
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-06-11T00:00:00.000Z'
updatedOn: '2024-06-11T00:00:00.000Z'
---

Have you ever searched for an image using an... image? [Google Images](https://images.google.com/) is a widely used example of such reverse image search engine. Do you wonder how it's able to show highly similar images in the search results? Well, in this guide you will learn how to create such an image engine on your own. You will learn how to create a system that's able to index images into a collection, and return images that are highly similar to the uploaded one.

## Prerequisites

To follow along this guide, you will need the following:

- [Node.js 18](https://nodejs.org/en) or later
- A [Neon](https://console.neon.tech/signup) account
- A [Vercel](https://vercel.com) account

## Steps

- [Provisioning a Serverless Postgres](#provisioning-a-serverless-postgres)
- [Create a new Astro application](#create-a-new-astro-application)
- [Enabling Server Side Rendering in Astro with Vercel](#enabling-server-side-rendering-in-astro-with-vercel)
- [Setting up a Postgres Database Connection](#setting-up-a-postgres-database-connection)
  - [Initialize Postgres Vector Store in LlamaIndex](#initialize-postgres-vector-store-in-llamaindex)
- [Build the Image Indexing API Endpoint](#build-the-image-indexing-api-endpoint)
- [Build the Reverse Image Search API Endpoint](#build-the-reverse-image-search-api-endpoint)
- [Build Reverse Image Search User Interface](#build-reverse-image-search-user-interface)
- [Deploy to Vercel](#deploy-to-vercel)

## Provisioning a Serverless Postgres

Using Serverless Postgres database helps you scale down to zero. With Neon, you only have to pay for what you use.

To get started, go to the [Neon console](https://console.neon.tech/app/projects) and enter the name of your choice as the project name.

You will then be presented with a dialog that provides a connecting string of your database. You can enable the **Connection pooling** toggle for a pooled connection string.

![](/guides/images/llamaindex-postgres-search-images/create-database.png)

All Neon connection strings have the following format:

```bash
postgres://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>?sslmode=require&channel_binding=require
```

- `user` is the database user.
- `password` is the database user’s password.
- `endpoint_hostname` is the host with neon.tech as the [TLD](https://www.cloudflare.com/en-gb/learning/dns/top-level-domain/).
- `port` is the Neon port number. The default port number is 5432.
- `dbname` is the name of the database. “neondb” is the default database created with each Neon project.
- `?sslmode=require&channel_binding=require` optional query parameters that enforce the [SSL](https://www.cloudflare.com/en-gb/learning/ssl/what-is-ssl/) mode and channel binding while connecting to the Postgres instance for better security.

Save this connecting string somewhere safe to be used as the `POSTGRES_URL` further in the guide. Proceed further in this guide to create a Astro application.

## Create a new Astro application

Let’s get started by creating a new Astro project. Open your terminal and run the following command:

```bash
npm create astro@latest my-app
```

`npm create astro` is the recommended way to scaffold an Astro project quickly.

When prompted, choose:

- `Empty` when prompted on how to start the new project.
- `Yes` when prompted if plan to write Typescript.
- `Strict` when prompted how strict Typescript should be.
- `Yes` when prompted to install dependencies.
- `Yes` when prompted to initialize a git repository.

Once that’s done, you can move into the project directory and start the app:

```bash
cd my-app
npm run dev
```

The app should be running on [localhost:4321](http://localhost:4321/). Let's close the development server for now.

Next, execute the command in your terminal window below to install the necessary libraries and packages for building the application:

```bash
npm install dotenv llamaindex@0.3.4 uuid
```

The above command installs the following packages:

- [dotenv](https://npmjs.com/package/dotenv): A library for handling environment variables.
- [llamaindex](https://npmjs.com/package/llamaindex): A data framework for creating LLM applications.
- [uuid](https://npmjs.com/package/uuid): A library to generate RFC-compliant UUIDs in JavaScript.

Then, make the following additions in your `tsconfig.json` file to make relative imports within the project easier:

```json
{
  "extends": "astro/tsconfigs/base",
  "compilerOptions": {
    // [!code ++]
    "baseUrl": ".", // [!code ++]
    "paths": {
      // [!code ++]
      "@/*": ["src/*"] // [!code ++]
    } // [!code ++]
  } // [!code ++]
}
```

To complete the initial set up, let's move on to enabling server-side rendering in the Astro application.

## Enabling Server Side Rendering in Astro with Vercel

To index and search from images, you are going to require server-side operations to be executed. Enable server side rendering in your Astro project by executing the following command:

```bash
npx astro add vercel
```

When prompted, choose:

- `Yes` when prompted to install the Vercel dependencies.
- `Yes` when prompted to make changes to Astro configuration file.

The command above installed the following dependency:

- `@astrojs/vercel`: The adapter that allows you to server-side render your Astro application on Vercel.

With this, your Astro application is all set to run in the development mode and deploy to Vercel without changes. Now, let' configure connection to Postgres to index and query from a given set of images.

## Setting up a Postgres Database Connection

First, create an `.env` file in the root directory of your project with the following environment variable to initiate the setup of a database connection:

```bash
# Neon Postgres Pooled Connection URL

POSTGRES_URL="postgres://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>?sslmode=require&channel_binding=require&channel_binding=require"
```

The file, `.env` should be kept secret and not included in Git history. Ensure that `.env` is added to the `.gitignore` file in your project.

Now, let's move on to using Postgres as the vector store to power your image search application with reverse image search capabilities.

### Initialize Postgres Vector Store in LlamaIndex

To index and query images (via their vector embeddings), you will use the Postgres-compatible `PGVectorStore` class by llamaindex. It enables you to write minimal code by extracting image features and creating vector embeddings under the hood. Inside `src` directory, create a `neon.ts` file with the following code:

```tsx
// File: src/neon.ts

import 'dotenv/config';
import { PGVectorStore } from 'llamaindex';

// Create and export a new instance of PGVectorStore
// This instance represents the vector store using PostgreSQL as the backend
export default new PGVectorStore({
  connectionString: process.env.POSTGRES_URL,
  dimensions: 512,
});
```

The code above begins with importing the `dotenv/config`, loading all the environment variables into the scope. Further, it exports an instance of `PGVectorStore` initialized using the Postgres Pooled Connection URL obtained earlier.

Now, let's move on to writing an API endpoint in the Astro application with which you can index the given set of image URLs in your Postgres database.

## Build the Image Indexing API Endpoint

LlamaIndex in combination with ClipEmbedding, internally retrieves the remote images, extracts their features using the CLIP model, and generates their vector embeddings. This process involves analyzing various visual and semantic aspects of the image to create a numerical representation that captures its essence. By handling feature extraction and embedding generation, llamaindex enables you to focus on building the rest of your application.

To index images via an API endpoint, create a file `src/pages/api/upsert.ts` with the following code:

```tsx
// File: src/pages/api/upsert.ts

import { v4 as uuidv4 } from 'uuid';
import imageVectorStore from '@/neon';
import type { APIContext } from 'astro';
import { ClipEmbedding, ImageDocument, Settings, VectorStoreIndex } from 'llamaindex';

// Set the embedding model to Clip for image embeddings
Settings.embedModel = new ClipEmbedding();

export async function POST({ request }: APIContext) {
  // Parse the JSON body of the request to get the list of image URLs
  const { images = [] }: { images: string[] } = await request.json();
  // Convert image URLs into ImageDocument objects
  const documents = images.map(
    (imageURL: string) =>
      new ImageDocument({
        // Generate a unique ID for each image document
        id_: uuidv4(),
        // Convert imageURL to a URL object
        image: new URL(imageURL),
        // Attach metadata with the image URL
        metadata: { url: imageURL },
      })
  );
  // Index the ImageDocument objects in the vector store
  await VectorStoreIndex.fromDocuments(documents, { imageVectorStore });
}
```

The code above begins with importing modules including `uuid`, `llamaindex`, and `imageVectorStore` (an alias for the instance of the `PGVectorStore` instance created earlier). Further, the code **sets the embedding model to Clip for image embeddings**. This takes care of extracting features from images, and creating their vector embeddings.

In the `POST` function, it handles incoming requests, expecting a JSON body with an array of image URLs. It converts each URL into an `ImageDocument` object, generating a unique ID for each, and attaching metadata containing the original URL.

The `ImageDocument` objects are then indexed in the vector store using `VectorStoreIndex.fromDocuments()`, which takes the documents array and options object as parameters. **The `imageVectorStore` is specified as the target store for indexing**. The entire process allows for efficient storage (and retrieval) of image embeddings in your application.

With indexing complete, let's move on to building the reverse image search API endpoint.

## Build the Reverse Image Search API Endpoint

First, let's walk through the process of reverse image search. A user would upload an image and then you'd need to find similar images. Greater the similarity, higher the priority in the image search results. The common ground for computing similarities between images is a numerical representation of their visual and semantic features. The computation of these features is done via CLIP model (by OpenAI) internally in the LlamaIndex library. LlamaIndex, upon querying would return a set of images along with the similarity score, i.e. how closely are the two images related. This allows you to efficiently handles this similarity search of images based on their embeddings.

To reverse image search via an API endpoint, create a file `src/pages/api/query.ts` with the following code:

```tsx
// File: src/pages/api/query.ts

import type { APIContext } from 'astro';

export async function POST({ request }: APIContext) {
  // Parse the form data from the request to get the file
  const data = await request.formData();
  const file = data.get('file') as File;
  // If no file is provided, return a 400 Bad Request response
  if (!file) return new Response(null, { status: 400 });
  // Read the file contents into a buffer
  const fileBuffer = await file.arrayBuffer();
  // Create a Blob from the buffer with the correct MIME type
  const fileBlob = new Blob([fileBuffer], { type: file.type });
  // ...
}
```

The code above implements a `POST` function, expecting a form data request with a file attached. It retrieves and validates the presence of file in the request.

Further, it reads its contents into a buffer using `arrayBuffer()`, and creates a Blob with the correct MIME type. This allows the API endpoint to accept images directly in the request, saving compute (on the server-side) for fetching remote image URL to search on.

Now with the following code you get to the final set of operations, i.e. creating vector embeddings of the image and returning highly similar images.

```tsx
// File: src/pages/api/query.ts

// ...

import neonStore from '@/neon';
import { ClipEmbedding, VectorStoreQueryMode } from 'llamaindex';

export async function POST({ request }: APIContext) {
  // ...
  // Get the image embedding using ClipEmbedding
  const image_embedding = await new ClipEmbedding().getImageEmbedding(fileBlob);
  // Query the Neon Postgres vector store for similar images
  const { similarities, nodes } = await neonStore.query({
    similarityTopK: 100,
    queryEmbedding: image_embedding,
    mode: VectorStoreQueryMode.DEFAULT,
  });
  // Initialize an array to store relevant image URLs
  const relevantImages: string[] = [];
  if (nodes) {
    similarities.forEach((similarity: number, index: number) => {
      // Check if similarity is greater than 90% (i.e., similarity threshold)
      if (100 - similarity > 90) {
        const document = nodes[index];
        relevantImages.push(document.metadata.url);
      }
    });
  }
  return new Response(JSON.stringify(relevantImages), {
    headers: { 'Content-Type': 'application/json' },
  });
}
```

The code above adds two new imports: `neonStore` (an alias for the `PGVectorStore` instance) and `ClipEmbedding` from `llamaindex`. **It initializes the embedding model as Clip for processing image embeddings**. It then utilizes the Clip embedding model to extract the image embedding. Further, it queries the Neon Postgres vector store for similar images using the extracted embedding. The query parameters include a similarity threshold and the image embedding.

The relevant images are filtered based on a similarity threshold of 90%, and their URLs are stored in an array. Finally, the endpoint returns a JSON response containing the URLs of the relevant images. This process enables efficient and high quality retrieval of similar images based on their embeddings, completing the reverse image search functionality within the application.

Now let's get to the final aspect of your Astro application, i.e. the user interface.

## Build Reverse Image Search User Interface

For the users to interact with your API endpoints, they require two things: an area on the webpage where they can upload the image they want to search similar ones for, and being able to see the search results.

Update the `index.astro` file in your `src/pages` directory with the following code to allow them to upload an image to search with.

```html
---
// File: src/pages/index.astro
---

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" type="image/svg+xml" href="/favicon.svg" />
    <meta name="viewport" content="width=device-width" />
    <meta name="generator" content="{Astro.generator}" />
    <title>Astro</title>
  </head>
  <body class="flex flex-col items-center">
    <form class="flex flex-col" id="fileUploadForm" enctype="multipart/form-data">
      <input
        class="rounded border px-4 py-3"
        type="file"
        id="fileInput"
        name="file"
        accept="image/*"
      />
      <button id="query" class="mt-3 max-w-max rounded bg-black px-4 py-1 text-white" type="submit">
        Query &rarr;
      </button>
    </form>
  </body>
</html>
```

The HTML above contains a form element with the id `fileUploadForm`, which allows users to upload image files. It consists of an input field of type file and a submit button labeled "Query". **The form is set to handle multipart/form-data encoding**.

To programtically render the search results, you would need to fetch the response from `/api/query` endpoint and then create `img` HTML elements on the webpage. Add the following JavaScript to your index route.

```html
---
// File: src/pages/index.astro
---

<html lang="en">
  <head>
    <!-- Head -->
  </head>
  <body class="flex flex-col items-center">
    <!-- Form -->
    <script>
      document.getElementById('fileUploadForm')?.addEventListener('submit', async function (event) {
        event.preventDefault()
        // remove the previous search results
        document.getElementById('searchResults')?.remove()
        // create a new form data object that contains the uploaded file
        const formData = new FormData()
        const fileInput = document.getElementById('fileInput') as HTMLInputElement
        if (!fileInput || !fileInput.files || fileInput.files.length === 0) return
        formData.append('file', fileInput.files[0])
        // query for similar images
        const queryCall = await fetch('/api/query', { method: 'POST', body: formData })
        const queryResp = await queryCall.json()
        // create the search results div
        const searchResultsDiv = document.createElement('div')
        searchResultsDiv.setAttribute('id', 'searchResults')
        // append all the image results to the search results div
        queryResp.forEach((eachImage: string) => {
          const img = document.createElement('img')
          img.setAttribute('class', 'size-100')
          img.setAttribute('src', eachImage)
          searchResultsDiv.append(img)
        })
        document.body.append(searchResultsDiv)
      })
    </script>
  </body>
</html>
```

The code above adds an event listener that is invoked in case the form is submitted. Upon submission, it extracts the uploaded file, creates a FormData object containing the file, and sends a POST request to `/api/query` with the file data.

The response from the query API is received and parsed as JSON. A new div element is dynamically created to hold the search results, and each image URL from the response is used to create an image element, which is then appended to the search results div.

The resulting search results, consisting of dynamically generated images are then visible to the users. This approach enables real-time querying and display of similar images based on the user upload(s).

With all that, your Astro application is ready to be deployed on Vercel with ease.

## Deploy to Vercel

The repository is now ready to deploy to Vercel. Use the following steps to deploy:

- Start by creating a GitHub repository containing your app's code.
- Then, navigate to the Vercel Dashboard and create a **New Project**.
- Link the new project to the GitHub repository you've just created.
- In **Settings**, update the **Environment Variables** to match those in your local `.env` file.
- Deploy.

## Summary

In this guide, you learned how to build a reverse image search engine in an Astro application using LlamaIndex and Serverless Postgres Database. During the process, you learned how to create vector embeddings of the images using ClipEmbeddings, to index and search from them using LlamaIndex Postgres vector store.

<NeedHelp />


# Local Development with Neon

---
title: Local Development with Neon
subtitle: Learn how to develop applications locally with Neon
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2024-11-05T00:00:00.000Z'
updatedOn: '2024-11-05T00:00:00.000Z'
---

Setting up your development environment should be simple and fast. With Neon's modern approach to PostgreSQL, you get exactly that. Here's how to create the perfect setup for your applications.

<Admonition type="note">
The setups described in this guide use the **Neon serverless driver** for connecting to a Postgres database hosted locally or on Neon over HTTP or WebSockets. To learn more, see [The Neon Serverless driver](/docs/serverless/serverless-driver).
</Admonition>

## Two ways to develop

When setting up a development environment with Neon, there are a couple of different approaches you can take:

1. **Database branching**
2. **Local PostgreSQL**

Let's explore both options to help you pick the right one.

## Database branching

Imagine creating a complete copy of your database as easily as creating a Git branch. That's [database branching](/docs/introduction/branching) with Neon – perfect for testing new features or updates without touching production data.

### Why use it?

- **Fast setup**: Create new environments in ~1 second
- **Zero configuration**: No local PostgreSQL installation required
- **True isolation**: Test changes without fear of breaking production
- **Cost-efficient**: Pay only for unique data and actual compute usage
- **Team-friendly**: Share database branches as easily as sharing Git branches
- **Autoscaling**: Resources scale to zero when you're not coding
- **Data reset**: Need a fresh start or a do-over? Reset your branch to match production in seconds

### Quickstart

1. Install the [**Neon CLI**](/docs/reference/neon-cli) by following the guide [here](/docs/reference/neon-cli#install).

2. **Connect your account**

   ```bash
   neonctl auth
   ```

3. **Create your branch**

   ```bash
   neonctl branches create --name dev/your-name

   # Get your connection details
   neonctl connection-string dev/your-name
   ```

   <Admonition type="note">
   You can also create branches through the Neon Console by navigating to your project and clicking the "Branches" tab. This provides a visual interface for branch management and configuration
   </Admonition>

4. **Set up your environment**

   ```bash
   # .env.development
   DATABASE_URL='postgresql://[user]:[password]@[endpoint]/[dbname]'
   ```

5. **Install dependencies**

   Dependencies include [Neon's serverless driver](/docs/serverless/serverless-driver) and a WebSockets library.

   <Admonition type="note">
   The Neon serverless driver supports connections over HTTP and WebSockets, depending on your requirements. This setup assumes that you could be using either. For the differences, refer to the [Neon's serverless driver docs](/docs/serverless/serverless-driver).
   </Admonition>

   <CodeTabs labels={["npm", "yarn", "pnpm"]}>

   ```bash
   npm install @neondatabase/serverless ws
   ```

   ```bash
   yarn add @neondatabase/serverless ws
   ```

   ```bash
   pnpm add @neondatabase/serverless ws
   ```

   </CodeTabs>

6. **Connect your app**

   ```javascript
   import { Pool, neon, neonConfig } from '@neondatabase/serverless';

   // Uncomment the following lines if you are on environments that do not support WebSocket, e.g, Node.js
   // import ws from 'ws';
   // neonConfig.webSocketConstructor = ws;

   export const pool = new Pool({ connectionString: process.env.DATABASE_URL });
   export const sql = neon(process.env.DATABASE_URL);
   ```

    <Admonition type="note">
      If you're using Drizzle or Prisma, replace your database connection string in your environment variables with your development branch's connection string.
    </Admonition>

### Tips and tricks

- **Stay organized**: Use prefixes like `dev/feature-auth` or `dev/alice`
- **Reset data**: Start fresh when needed:
  ```bash
  neon branches reset dev/your-name
  ```
- **Feature work**: Create dedicated branches:
  ```bash
  neon branches create --name dev/auth-system --parent main
  ```

## Local PostgreSQL

Sometimes you need to work offline or want full control over your database. Here's how to set up a local PostgreSQL instance that works perfectly with the Neon. This method uses:

- The [Neon Serverless driver](/docs/serverless/serverless-driver) to connect to your local database (same as the database branching setup described above)
- A Docker compose file that installs a local instance of PostgreSQL 17 and the Neon Proxy. The Neon Proxy lets you to connect to your local PostgreSQL database using the Neon serverless driver.

<Admonition type="note" title="kudos">
The Neon Proxy setup uses the [local-neon-http-proxy](https://github.com/TimoWilhelm/local-neon-http-proxy) Dockerfile, developed by [TimoWilhelm](https://github.com/TimoWilhelm).
</Admonition>

### Why use this method?

- **Full control**: Your own PostgreSQL instance
- **Offline work**: Code without internet dependency
- **Fast queries**: Zero network latency
- **Free development**: Use your local resources

### Docker Compose setup

Create a `docker-compose.yml` file with the following content:

```yaml
services:
  postgres:
    image: postgres:17
    command: '-d 1'
    volumes:
      - db_data:/var/lib/postgresql/data
    ports:
      - '5432:5432'
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=main
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U postgres']
      interval: 10s
      timeout: 5s
      retries: 5

  neon-proxy:
    image: ghcr.io/timowilhelm/local-neon-http-proxy:main
    environment:
      - PG_CONNECTION_STRING=postgres://postgres:postgres@postgres:5432/main
    ports:
      - '4444:4444'
    depends_on:
      postgres:
        condition: service_healthy

volumes:
  db_data:
```

Run the following command to start local PostgreSQL and the Neon Proxy, which helps you connect to your local database:

```bash
docker-compose up -d
```

<Admonition type="tip" title="Working offline?">
The [local-neon-http-proxy](https://github.com/TimoWilhelm/local-neon-http-proxy) Dockerfile setup uses [*.localtest.me](https://readme.localtest.me/) to enable testing with local URLs without adding entires to your host file. The `localtest.me` domain and all wildcard subdomains point to `127.0.0.1`.

However, this solution requires an internet connection. To work offline, you'll need to add an entry to your system's hosts file to map `db.localtest.me` to localhost:

```bash
127.0.0.1 db.localtest.me
```

For instructions on editing your hosts file on different operating systems, see [this guide](https://www.hostinger.in/tutorials/how-to-edit-hosts-file).

[dnsmask](https://help.ubuntu.com/community/Dnsmasq) is another option [suggested by a Neon user](https://github.com/neondatabase/website/issues/2690) for resolving domain names when there is no internet connection.
</Admonition>

### Connect your app

<Tabs labels={["Using neondatabase/serverless", "Using drizzle", "Using prisma"]}>

<TabItem>

1. Install Dependencies

   <CodeTabs labels={["npm", "yarn", "pnpm"]}>

   ```bash
   npm install @neondatabase/serverless ws
   ```

   ```bash
   yarn add @neondatabase/serverless ws
   ```

   ```bash
   pnpm add @neondatabase/serverless ws
   ```

   </CodeTabs>

2. **Configure the connection**

   ```typescript
   import { neon, neonConfig, Pool } from '@neondatabase/serverless';
   import ws from 'ws';

   let connectionString = process.env.DATABASE_URL;

   // Configuring Neon for local development
   if (process.env.NODE_ENV === 'development') {
     connectionString = 'postgres://postgres:postgres@db.localtest.me:5432/main';
     neonConfig.fetchEndpoint = (host) => {
       const [protocol, port] = host === 'db.localtest.me' ? ['http', 4444] : ['https', 443];
       return `${protocol}://${host}:${port}/sql`;
     };
     const connectionStringUrl = new URL(connectionString);
     neonConfig.useSecureWebSocket = connectionStringUrl.hostname !== 'db.localtest.me';
     neonConfig.wsProxy = (host) => (host === 'db.localtest.me' ? `${host}:4444/v2` : `${host}/v2`);
   }
   neonConfig.webSocketConstructor = ws;

   // Neon supports both HTTP and WebSocket clients. Choose the one that fits your needs:

   // HTTP Client (sql)
   // - Best for serverless functions and Lambda environments
   // - Ideal for stateless operations and quick queries
   // - Lower overhead for single queries
   // - Better for applications with sporadic database access
   export const sql = neon(connectionString);

   // WebSocket Client (pool)
   // - Best for long-running applications (like servers)
   // - Maintains a persistent connection
   // - More efficient for multiple sequential queries
   // - Better for high-frequency database operations
   export const pool = new Pool({ connectionString });
   ```

</TabItem>
<TabItem>

1. Install Dependencies

   <CodeTabs labels={["npm", "yarn", "pnpm"]}>

   ```bash
   npm install drizzle-orm @neondatabase/serverless ws
   ```

   ```bash
   yarn add drizzle-orm @neondatabase/serverless ws
   ```

   ```bash
   pnpm add drizzle-orm @neondatabase/serverless ws
   ```

   </CodeTabs>

2. **Configure the connection**

   ```typescript
   import { neon, neonConfig, Pool } from '@neondatabase/serverless';
   import { drizzle as drizzleWs } from 'drizzle-orm/neon-serverless';
   import { drizzle as drizzleHttp } from 'drizzle-orm/neon-http';
   import ws from 'ws';

   let connectionString = process.env.DATABASE_URL;

   // Configuring Neon for local development
   if (process.env.NODE_ENV === 'development') {
     connectionString = 'postgres://postgres:postgres@db.localtest.me:5432/main';
     neonConfig.fetchEndpoint = (host) => {
       const [protocol, port] = host === 'db.localtest.me' ? ['http', 4444] : ['https', 443];
       return `${protocol}://${host}:${port}/sql`;
     };
     const connectionStringUrl = new URL(connectionString);
     neonConfig.useSecureWebSocket = connectionStringUrl.hostname !== 'db.localtest.me';
     neonConfig.wsProxy = (host) => (host === 'db.localtest.me' ? `${host}:4444/v2` : `${host}/v2`);
   }
   neonConfig.webSocketConstructor = ws;

   const sql = neon(connectionString);
   const pool = new Pool({ connectionString });

   // Drizzle supports both HTTP and WebSocket clients. Choose the one that fits your needs:

   // HTTP Client:
   // - Best for serverless functions and Lambda environments
   // - Ideal for stateless operations and quick queries
   // - Lower overhead for single queries
   // - Better for applications with sporadic database access
   export const drizzleClientHttp = drizzleHttp({ client: sql });

   // WebSocket Client:
   // - Best for long-running applications (like servers)
   // - Maintains a persistent connection
   // - More efficient for multiple sequential queries
   // - Better for high-frequency database operations
   export const drizzleClientWs = drizzleWs({ client: pool });
   ```

3. **Migration setup**

   To ensure your Drizzle migrations run smoothly and without errors in your development environment, you can install the `postgres` package as a development dependency.

   <CodeTabs labels={["npm", "yarn", "pnpm"]}>

   ```bash
   npm i -D postgres
   ```

   ```bash
   yarn add -D postgres
   ```

   ```bash
   pnpm add -D postgres
   ```

   </CodeTabs>

</TabItem>

<TabItem>

Note that Driver Adapters are still in preview for Prisma. Please refer to the [Prisma documentation](https://www.prisma.io/docs/orm/overview/databases/neon) for the latest information.

1. Install Dependencies

   <CodeTabs labels={["npm", "yarn", "pnpm"]}>

   ```bash
   npm install @prisma/client @prisma/adapter-neon @neondatabase/serverless ws
   ```

   ```bash
   yarn add @prisma/client @prisma/adapter-neon @neondatabase/serverless ws
   ```

   ```bash
   pnpm add @prisma/client @prisma/adapter-neon @neondatabase/serverless ws
   ```

   </CodeTabs>

2. **Enable the Preview Flag**

   To use the Neon serverless driver with Prisma, enable the preview flag in your `schema.prisma` file.

   ```prisma
     generator client {
       provider        = "prisma-client-js"
       previewFeatures = ["driverAdapters"]
     }
   ```

3. **Configure the connection**

   ```typescript
   import { neonConfig } from '@neondatabase/serverless';
   import { PrismaNeon, PrismaNeonHTTP } from '@prisma/adapter-neon';
   import { PrismaClient } from '@prisma/client';
   import ws from 'ws';

   let connectionString =
     process.env.DATABASE_URL || 'postgres://postgres:postgres@db.localtest.me:5432/main';

   // Configuring Neon for local development
   if (process.env.NODE_ENV === 'development') {
     neonConfig.fetchEndpoint = (host) => {
       const [protocol, port] = host === 'db.localtest.me' ? ['http', 4444] : ['https', 443];
       return `${protocol}://${host}:${port}/sql`;
     };
     const connectionStringUrl = new URL(connectionString);
     neonConfig.useSecureWebSocket = connectionStringUrl.hostname !== 'db.localtest.me';
     neonConfig.wsProxy = (host) => (host === 'db.localtest.me' ? `${host}:4444/v2` : `${host}/v2`);
   }
   neonConfig.webSocketConstructor = ws;

   // Prisma supports both HTTP and WebSocket clients. Choose the one that fits your needs:

   // HTTP Client:
   // - Ideal for stateless operations and quick queries
   // - Lower overhead for single queries
   const adapterHttp = new PrismaNeonHTTP(connectionString!, {});
   export const prismaClientHttp = new PrismaClient({ adapter: adapterHttp });

   // WebSocket Client:
   // - Best for long-running applications (like servers)
   // - Maintains a persistent connection
   // - More efficient for multiple sequential queries
   // - Better for high-frequency database operations
   const adapterWs = new PrismaNeon({ connectionString });
   export const prismaClientWs = new PrismaClient({ adapter: adapterWs });
   ```

</TabItem>

</Tabs>

## Which development approach should you use?

Before choosing between cloud-hosted or local development, it's important to understand the benefits of each approach.

Cloud-hosted branches offer several compelling advantages:

### Cost-efficient development

- **Minimal storage costs**: Branches are extremely cost-effective as you only pay for unique data changes
- **Smart compute usage**: Development happens on small computes (0.25 vCPU) that scale to zero by default
- **Free Plan benefits**: Even the Free Plan includes 5 compute hours on dev branches
  - This translates to 20 hours of development time on a 0.25 vCPU compute
  - One compute hour at 1 vCPU equals four hours at 0.25 vCPU

### Developer-friendly features

- **Instant deployment**: Branches are created in seconds, just like Git branches
- **Branch reset**: Easily refresh your development data from the parent branch
- **Zero maintenance**: No need to manage local PostgreSQL installations

| Feature             | Database Branching                           | Local PostgreSQL                     |
| ------------------- | -------------------------------------------- | ------------------------------------ |
| Setup Time          | ✅ Instant (~1 second)                       | ⏱️ Requires initial configuration    |
| Configuration       | ✅ Zero configuration needed                 | 🔧 Requires local setup              |
| Team Collaboration  | ✅ Easy branch sharing and management        | 🤝 Requires additional setup         |
| Cost Management     | ✅ Pay only for unique data and compute time | 💻 Local resources only              |
| Resource Scaling    | ✅ Scale to zero when not in use             | ❌ Always consuming resources        |
| Offline Development | ❌ Requires internet connection              | ✅ Works offline                     |
| Network Latency     | 🌐 Depends on connection                     | ✅ Zero latency                      |
| Production Parity   | ✅ Identical to production                   | 🔄 Requires additional configuration |

## When to use each approach

### Choose database branching when:

- You want instant development environments
- You need efficient resource utilization
- You're working with a team

**Perfect for:**

- Most development workflows
- Team environments
- Rapid prototyping
- Feature development
- Testing database changes

### Consider local PostgreSQL when:

- Offline development is crucial
- You need zero network latency
- You require complete database control
- You have specific local testing requirements

## Best practices for cloud-hosted development with Neon branching

### Environment tips

- Keep development and production database branches separate
- Always Use clear branch naming
- Never commit credentials to a version control system

### Resource tips

- Use scale to zero for development branches
- Clean up unused branches
- Reset branches to match production when needed

### Security tips

- Use separate development credentials
- Rotate credentials regularly
- Keep production credentials isolated

## Start building

You're now ready to create a powerful development environment with Neon. Choose the approach that fits your team best and start building.

<NeedHelp/>


# Migrating from FaunaDB to Neon Postgres

---
title: Migrating from FaunaDB to Neon Postgres
subtitle: 'Learn how to migrate your data and applications from FaunaDB to Neon Postgres'
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-03-23T00:00:00.000Z'
updatedOn: '2025-03-23T00:00:00.000Z'
---

Neon, like Fauna, offers a **serverless architecture**—but it’s built on **Postgres**. That means you get the scalability of serverless along with the reliability and familiarity of a proven SQL database.

This guide is designed to help FaunaDB users understand how to transition to Neon Postgres.

<Admonition type="note">
Migrating from FaunaDB to Neon Postgres involves schema translation, data migration, and query conversion. This guide provides a structured approach to help you navigate the migration process effectively.
</Admonition>

## FaunaDB vs. Neon (Postgres)

Before diving into the migration process, it's important to understand the fundamental differences between FaunaDB and Neon (Postgres). While both are databases, they operate with distinct paradigms:

| Feature            | FaunaDB                                        | Neon (Postgres)                                                     |
| ------------------ | ---------------------------------------------- | ------------------------------------------------------------------- |
| **Database type**  | Multi-model (document-relational)              | Relational (SQL)                                                    |
| **Data model**     | JSON documents in collections, flexible schema | Tables with rows and columns, rigid schema                          |
| **Query language** | FQL (Fauna Query Language), functional         | SQL (Structured Query Language), declarative                        |
| **Schema**         | Implicit, schemaless/schema-optional, evolving | Explicit, schema-first, requires migrations                         |
| **Transactions**   | ACID, stateless, HTTPS requests                | ACID, stateful/Stateless, persistent TCP/HTTP/Websocket connections |
| **Server model**   | Serverless (managed), cloud-native             | Serverless (managed), cloud-native                                  |

## Migration steps

The migration process from FaunaDB to Neon Postgres involves several key steps, each essential for a successful transition. These steps include exporting your data and schema from FaunaDB, translating your schema to Postgres DDL, importing your data into Neon, and converting your queries from FQL to SQL. Let's break down these steps in detail:

### Step 1: Exporting data from FaunaDB

If you are on a paid FaunaDB plan, you can utilize the database's export functionality to save data in JSON format directly to an Amazon S3 bucket. [Fauna CLI](https://docs.fauna.com/fauna/current/build/cli/v4/) can be used to [export data to S3](https://docs.fauna.com/fauna/current/manage/exports/)

```bash
fauna export create s3 \
  --database <database_name> \
  --collection <collection_name> \
  --bucket <s3_bucket_name> \
  --path <s3_bucket_path> \
  --format simple
```

For smaller datasets, you can export data directly to your local filesystem using FQL. The following script demonstrates exporting data from FaunaDB collections as JSON files. For example, here's a Node.js script that exports data from specific collections (e.g., `Product`, `Category`) to JSON files (e.g., `Product.json`, `Category.json`):

```javascript
import { Client, fql, FaunaError } from 'fauna';
import fs from 'fs';

// Route queries to a specific database
// using the authentication secret in
// the `FAUNA_SECRET` environment variable.
const client = new Client();

// Specify the collections to export.
// You can retrieve a list of user-defined collections
// using a `Collection.all()` query.
const collectionsToExport = ['Product', 'Category'];

// Loop through the collections.
for (const collectionName of collectionsToExport) {
  try {
    // Compose a query using an FQL template string.
    // The query returns a Set containint all documents
    // in the collection.
    const query = fql`
      let collection = Collection(${collectionName})
      collection.all()`;

    // Run the query.
    const pages = client.paginate(query);

    // Iterate through the resulting document Set.
    const documents = [];
    for await (const page of pages.flatten()) {
      documents.push(page);
    }

    // Convert the 'documents' array to a JSON string.
    const jsonData = JSON.stringify(documents, null, 2);

    // Write the JSON string to a file named `<collectionName>.json`.
    fs.writeFileSync(`${collectionName}.json`, jsonData, 'utf-8');

    console.log(`${collectionName} collection data written to ${collectionName}.json`);
  } catch (error) {
    if (error instanceof FaunaError) {
      console.error(`Error exporting ${collectionName}:`, error);
    } else {
      console.error(`An unexpected error occurred for ${collectionName}:`, error);
    }
  }
}

client.close();
```

You can learn more about exporting data from FaunaDB in the [official documentation](https://docs.fauna.com/fauna/current/migrate/?lang=javascript).

For example, here's a sample JSON file for the exported `Product` collection:

```json
[
  {
    "data": [
      {
        "id": "426122714129891392",
        "coll": "Product",
        "ts": "2025-03-22T10:58:58.290Z",
        "name": "cups",
        "description": "Translucent 9 Oz, 100 ct",
        "price": 698,
        "stock": 100,
        "category": {
          "id": "426122714117308480",
          "coll": "Category"
        }
      },
      .... other records
  }
]
```

### Step 2: Exporting FaunaDB schema (FSL schema pull)

In addition to your data, you'll need to export your FaunaDB schema, defined in Fauna Schema Language (FSL).

**Using `fauna schema pull` command:**

The `fauna schema pull` command in the Fauna CLI is used to export your database schema to a local directory.

**Fauna CLI command:**

```bash
fauna schema pull \
  --database <your_faunadb_database_name> \
  --dir <local_schema_directory> \
  --active
```

For example, here's a sample FSL schema file `collections.fsl` exported from FaunaDB:

```fsl
collection Product {
  name: String
  description: String
  // Use an Integer to represent cents.
  // This avoids floating-point precision issues.
  price: Int
  category: Ref<Category>
  stock: Int

  // Use a unique constraint to ensure no two products have the same name.
  unique [.name]
  check stockIsValid (product => product.stock >= 0)
  check priceIsValid (product => product.price > 0)

  index byCategory {
    terms [.category]
  }

  index sortedByCategory {
    values [.category]
  }

  index byName {
    terms [.name]
  }

  index sortedByPriceLowToHigh {
    values [.price, .name, .description, .stock]
  }
}
```

### Step 3: Schema translation - FSL to Postgres DDL

<Admonition type="info" title="Manual schema translation 🥺">
Unfortunately, there isn't a fully automated FSL to SQL DDL converter as these are fundamentally different database paradigms. You'll need to manually translate your FaunaDB schema to Postgres DDL. This process involves mapping FaunaDB collections, fields, indexes, and constraints to Postgres tables, columns, indexes, and constraints.
</Admonition>

Begin by thoroughly examining the exported Fauna Schema Language (FSL) files. This step is crucial for gaining a comprehensive understanding of your FaunaDB schema structure. Pay close attention to the definitions of collections, their associated fields, indexes, and constraints.

For instance, the Product collection, as shown in the above example `collections.fsl` file, includes fields like `name`, `description`, `price`, `category`, and `stock`. The schema also specifies unique and check constraints for data integrity, along with indexes to optimize query performance.

Once you have a clear grasp of your exported FSL schema, the next step involves translating it into Postgres Data Definition Language (DDL). This translation process is necessary to create equivalent tables and indexes within your Postgres database. By accurately converting your FaunaDB schema into DDL, you ensure a smooth transition and maintain the structural integrity of your data during migration.

If you need a refresher on Postgres, you can refer to Neon's [PostgreSQL Tutorial](/postgresql/tutorial).

**Key translation considerations:**

- **Collections to tables:** Each FaunaDB collection in your FSL schema could become a Neon Postgres table.
- **Field definitions to columns:** FaunaDB field definitions will guide your Neon Postgres column definitions. Pay attention to data types like `String`, `Number`, `Time`, `Ref`, and optionality (`?` for nullable).
- **Unique constraints:** Translate FaunaDB `unique` constraints in FSL to `UNIQUE` constraints in your Postgres `CREATE TABLE` statements.
- **Indexes:** Translate FaunaDB `index` definitions in FSL to `CREATE INDEX` statements in Postgres. Consider the `terms` and `values` of FaunaDB indexes to create effective Postgres indexes.
- **Computed fields/functions:** FaunaDB's more advanced schema features like `compute`, functions will require careful consideration for translation. Computed fields might translate to Postgres views or computed columns. UDFs will likely need to be rewritten as stored procedures or application logic.

#### Example FSL to Postgres DDL translation

Let's consider the `Category` collection from the FSL schema and translate it to a `categories` table in Neon Postgres. Here's the FSL schema for the `Category` collection:

```fsl
collection Category {
  name: String
  description: String
  compute products: Set<Product> = (category => Product.byCategory(category))

  unique [.name]

  index byName {
    terms [.name]
  }
}
```

**Neon Postgres DDL (Translated):**

Here's how you can translate the `Category` collection to a `categories` table with the necessary constraints and indexes:

```sql
CREATE TABLE categories (
  id SERIAL PRIMARY KEY,
  name VARCHAR(255) NOT NULL,
  description TEXT,
  -- Constraints
  CONSTRAINT unique_category_name UNIQUE (name)
);

-- Indexes (create indexes after data migration if possible for speeding up data import)
CREATE INDEX idx_categories_name ON categories(name);
```

Now let's consider the `Product` collection from the FSL schema and translate it to a `products` table in Neon Postgres. Here's the FSL schema for the `Product` collection:

```fsl
collection Product {
  name: String
  description: String
  price: Int
  category: Ref<Category>
  stock: Int

  unique [.name]
  check stockIsValid (product => product.stock >= 0)
  check priceIsValid (product => product.price > 0)

  index byCategory {
    terms [.category]
  }

  index sortedByCategory {
    values [.category]
  }

  index byName {
    terms [.name]
  }

  index sortedByPriceLowToHigh {
    values [.price, .name, .description, .stock]
  }
}
```

**Neon Postgres DDL (Translated):**

Now that you have a `categories` table created in Neon Postgres, here's how you can translate the `Product` collection to a `products` table with the necessary constraints, references and indexes:

```sql
CREATE TABLE products (
  id SERIAL PRIMARY KEY,
  name VARCHAR(255) NOT NULL,
  description TEXT,
  price INT NOT NULL,
  category_id INT NOT NULL,
  stock INT NOT NULL,
  -- Constraints
  CONSTRAINT unique_product_name UNIQUE (name),
  CONSTRAINT stock_valid CHECK (stock >= 0),
  CONSTRAINT price_valid CHECK (price > 0),
  -- Foreign key
  CONSTRAINT fk_category
    FOREIGN KEY (category_id)
    REFERENCES categories(id)
    ON DELETE RESTRICT
    ON UPDATE CASCADE
);

-- Indexes (create indexes after data migration if possible for speeding up data import)
CREATE INDEX idx_products_category ON products(category_id);
CREATE INDEX idx_products_price_asc ON products(price) INCLUDE (name, description, stock);
```

Here we are adding a foreign key constraint `fk_category` to ensure that the `category_id` in the `products` table references the `id` column in the `categories` table. This constraint enforces referential integrity between the two tables.

<Admonition type="tip" title="Don't want to use Raw SQL?">
If you prefer a more programmatic approach to schema translation, you can use any Postgres library or ORM (object-relational mapping) tool in your chosen programming language. These tools can help automate the schema creation process and provide a more structured way to define your Postgres schema. Learn more on our [language guides](/docs/get-started/languages) and [ORM guides](/docs/get-started/orms) section.
</Admonition>

### Step 4: Data import to Neon Postgres

With your Neon Postgres database ready and your data exported from FaunaDB, the next step is to import this data into your newly created tables.

For this guide, we'll demonstrate importing data from the `product.json` file (exported from FaunaDB) into the `products` table in Neon Postgres.

This example Node.js script reads the `Product.json` file, parses the JSON data, and then generates and executes `INSERT` statements to populate your `products` table in Neon Postgres.

You can get `NEON_CONNECTION_STRING` from your Neon dashboard. Learn more about [Connecting Neon to your stack](/docs/get-started/connect-neon)

```javascript
import pg from 'pg';
import fs from 'fs';

const { Client } = pg;

async function importProducts() {
  const neonConnectionString = process.env.NEON_CONNECTION_STRING;
  const client = new Client({ connectionString: neonConnectionString });

  try {
    await client.connect();
    const rawData = fs.readFileSync('Product.json');
    const productData = JSON.parse(rawData);

    // Start transaction
    await client.query('BEGIN');

    // Build a bulk insert query for a batch of records
    const insertValues = [];
    const placeholders = [];

    productData[0].data.forEach((product, index) => {
      const { name, description, price, stock, category } = product;
      const categoryId = Number(category.id);

      const offset = index * 5;
      placeholders.push(
        `($${offset + 1}, $${offset + 2}, $${offset + 3}, $${offset + 4}, $${offset + 5})`
      );
      insertValues.push(name, description, price, stock, categoryId);
    });

    const insertQuery = `
          INSERT INTO products (name, description, price, stock, category_id)
          VALUES ${placeholders.join(', ')}
        `;

    await client.query(insertQuery, insertValues);
    await client.query('COMMIT');
    console.log('Products imported successfully!');
  } catch (error) {
    console.error('Error during product import:', error);
    await client.query('ROLLBACK');
  } finally {
    await client.end();
  }
}

importProducts();
```

You can adapt this script to import data from other collections by adjusting the file paths, table names, and data transformations as needed.

<Admonition type="tip" title="Importing multiple collections with references">

When importing data that spans multiple collections with relationships (for instance, `Product` collection documents referencing `Category` collection documents), it is **essential to import data in the correct order** to maintain data integrity.

Specifically, you must **import the data for the _referenced_ collection (e.g., `Category`) _before_ importing the data for the _referencing_ collection (e.g., `Product`)**.

Keep the following considerations in mind when importing data with relationships:

- **Establish referenced data first:** Postgres, being a relational database, relies on foreign key constraints to enforce relationships between tables. When you import data into the `Product` table that is intended to reference entries in the `Category` table, those `Category` entries must already exist in Postgres.

- **ID handling depends on your strategy:** While FaunaDB uses its own distributed document ID system, Postgres ID generation is more flexible. **Whether you need to transform IDs depends on your chosen ID strategy in Postgres:**
  - **Scenario 1: Using Postgres-Generated IDs:** If you are using Postgres's default ID generation mechanisms (like `SERIAL`, `UUID`, or `IDENTITY` columns), then **Postgres will automatically generate _new_ IDs** for the rows in your tables. In this scenario, you _will_ need to manage ID transformation for relationships.

  - **Scenario 2: Retaining FaunaDB IDs:** If you are explicitly setting IDs during import to retain FaunaDB IDs in Postgres, you must ensure that the IDs are correctly mapped and managed. You may choose this approach if you:
    - Want to retain FaunaDB IDs for compatibility and speed up the migration process.
    - Have a strategy to manage ID collisions and ensure uniqueness at the application level.

- **Managing IDs for Relationships (Regardless of ID retention):** Even if you _do_ successfully retain FaunaDB IDs in Postgres (Scenario 2), you still need to be mindful of how relationships are established. If you are using foreign keys in Postgres (the recommended approach for relational data), you must ensure that the IDs used in your referencing tables (e.g., `product.category_id`) **correctly match the IDs in the referenced table (e.g., `categories.id`)**. This will be be valid if you are mapping the JSON data to Postgres tables without any transformation.

- **Strategies for ID management (If Not Retaining FaunaDB IDs):** If you are using Postgres-generated IDs, you will need a strategy to: - **Option 1: Pre-map IDs:** Before importing `Product` data, you might need to process your JSON data to replace the FaunaDB `Category` document IDs with the **newly generated Postgres IDs** of the corresponding categories. This involves creating a mapping between the FaunaDB IDs and the Postgres-generated IDs for the `Category` table and replacing the `category.id` references in your `Product.json` data dump with the corresponding Postgres IDs. - **Option 2: Lookup-based Insertion:** During the import of `Product` data, instead of directly inserting IDs, you might perform a **lookup** in the already imported `Category` table based on a unique identifier (like category name) from your JSON data to retrieve the correct Postgres `category_id` to use as a foreign key. You can use the [example below](#inserting-a-new-document) as a reference.

</Admonition>

### Step 5: Query conversion - FQL to SQL

<Admonition type="tip" title="Gradual migration with Flags">
We recommend using a flag-based approach to gradually migrate your application from FaunaDB to Neon Postgres. This approach involves running your application with both FaunaDB and Neon Postgres connections simultaneously, using a feature flag to switch between the two databases. This strategy allows you to test and validate your application's behavior on Neon Postgres without disrupting your production environment. Once you see that your application is functioning correctly with Neon Postgres, you can fully transition away from FaunaDB.
</Admonition>

This is a critical step in the migration process, as it involves converting your application's FaunaDB queries (written in Fauna Query Language - FQL) to equivalent SQL queries.

Here are some key translation patterns to consider when converting Fauna's `FQL` to Postgres `SQL`:

#### Retrieving all documents from a collection

For example, to retrieve all documents from the `Product` collection using FQL:

```fql
let collection = Collection("Product")
collection.all()
```

Assuming you have ported 'Product' collection to 'products' table in Neon Postgres, the equivalent SQL query would be:

```sql
SELECT * FROM products;
```

#### Filtering data - Simple WHERE clause

For example, to filter products by name in FQL:

```fql
Product.where(.name == 'avocados');
```

The equivalent SQL query would be:

```sql
SELECT * FROM products WHERE name = 'avocados';
```

#### Filtering data - Nested field WHERE clause (Joins equivalent)

For example, to filter products by category name in FQL:

```fql
Product.where(.category?.name == "party")
```

The equivalent SQL query would involve a join between the `products` and `categories` tables:

```sql
SELECT p.*
FROM products p
JOIN categories c ON p.category_id = c.id
WHERE c.name = 'party';
```

#### Ordering data

For example, to order products by price in ascending order:

```fql
Product.all().order(.price)
```

The equivalent SQL query would be:

```sql
SELECT * FROM products ORDER BY price ASC;
```

#### Counting documents

For example, to count all documents in the `Product` collection in FQL:

```fql
Product.all().count();
```

The equivalent SQL query would be:

```sql
SELECT COUNT(*) FROM products;
```

#### Aggregations

For example, to calculate the total stock count of all products in FQL:

```fql
let stockCounts = Product.all().map(doc => doc.stock )
stockCounts.aggregate(0, (a, b) => a + b)
```

The equivalent SQL query would be:

```sql
SELECT SUM(stock) FROM products;
```

#### Filtering data - `AND` and `OR` conditions

For example, to filter products that are both priced above $10 AND have less than 50 units in stock in FQL:

```fql
Product.where(.price > 10 && .stock < 50)
```

The equivalent SQL query to achieve the same filtering would be:

```sql
SELECT * FROM products WHERE price > 10 AND stock < 50;
```

For example, to filter products that are either in the "party" category OR priced below 20 in FQL:

```fql
Product.where(.category?.name == "party" || .price < 20)
```

The equivalent SQL query, involving a join to access the category name, would be:

```sql
SELECT p.*
FROM products p
JOIN categories c ON p.category_id = c.id
WHERE c.name = 'party' OR p.price < 20;
```

#### Updating a document

For example, to update the description of a product named "avocados" in FQL:

```fql
let productRef = Product.where(.name == 'avocados').first()
Product.byId(productRef!.id)!.update({ description: "Fresh, ripe avocados from California" })
```

The equivalent SQL query to update the same product description would be:

```sql
UPDATE products
SET description = 'Fresh, ripe avocados from California'
WHERE name = 'avocados'
LIMIT 1;
```

#### Deleting a document

For example, to delete a product named "pizza" in FQL:

```fql
let productRef = Product.where(.name == 'pizza').first()
Product.byId(productRef!.id)!.delete()
```

The equivalent SQL query to delete the same product would be:

```sql
DELETE FROM products
WHERE name = 'pizza'
LIMIT 1;
```

#### Inserting a new document

For example, to insert a new product "Organic Strawberries" in FQL, linking it to the "produce" category:

```fql
Product.create({
  name: "Organic Strawberries",
  price: 699,
  stock: 150,
  description: "Fresh strawberries",
  category: Category.where(.name == "produce").first()
})
```

The equivalent SQL query to insert the same product and link it to the "produce" category would be:

```sql
INSERT INTO products (name, price, stock, description, category_id)
VALUES ('Organic Strawberries', 699, 150, 'Fresh strawberries', (SELECT id FROM categories WHERE name = 'produce'))
RETURNING *;
```

**Actionable steps for query conversion:**

1.  **Review application queries:** Identify the key queries in your application that interact with FaunaDB.
2.  **Translate FQL to SQL (focus on key queries):** Translate these key FQL queries into equivalent SQL queries, focusing on the patterns shown in the examples above.
3.  **Test SQL queries:** Test your translated SQL queries against your Neon Postgres database to ensure they function correctly, return the expected data, and are performant. You might need to use [`EXPLAIN ANALYZE`](/postgresql/postgresql-tutorial/postgresql-explain) in Postgres to analyze query performance and optimize indexes if needed.

<Admonition type="note" title="Recommendation for complex queries">
Given the potential volume of unstructured data insertion and retrieval queries in your application, which can be challenging to implement within a short timeframe, we recommend prioritizing the queries that are most critical to your application's core functionality and performance. For handling deeply nested unstructured data, consider using the [JSONB datatype in Postgres](/postgresql/postgresql-tutorial/postgresql-json)
</Admonition>

## Resources

- [The Future of Fauna](https://fauna.com/blog/the-future-of-fauna)
- [Migrate off Fauna](https://docs.fauna.com/fauna/current/migrate/?lang=javascript)
- Modernizing from PostgreSQL to Serverless with Fauna: [Part 1](https://fauna.com/blog/modernizing-from-postgresql-to-serverless-with-fauna-part-1) [Part 2](https://fauna.com/blog/modernizing-from-postgresql-to-serverless-with-fauna-part-2) [Part 3](https://fauna.com/blog/modernizing-from-postgresql-to-serverless-with-fauna-part-3)

<NeedHelp />


# Migrating from Tembo.io to Neon Postgres

---
title: Migrating from Tembo.io to Neon Postgres
subtitle: 'Learn how to migrate your data and applications from Tembo.io to Neon Postgres'
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-05-08T00:00:00.000Z'
updatedOn: '2025-05-08T00:00:00.000Z'
---

[Tembo.io](https://legacy.tembo.io/cloud) recently announced that it's sunsetting its managed Postgres service. If you've decided to migrate your serviced from Tembio.io to Neon, follow the steps in this guide.

<Admonition type="warning" title="Tembo Shutdown Timeline">
Please be aware of Tembo's [official shutdown timeline](https://tembo-io.notion.site/Tembo-Cloud-Migration-Guide-1de7c9367d6a80349570e7469ba7f17b)

| Date          | Action                           |
| ------------- | -------------------------------- |
| May 5, 2025   | Instance creation disabled       |
| May 30, 2025  | Free instance migration deadline |
| June 27, 2025 | Paid instance migration deadline |

Plan your migration accordingly to avoid any disruption to your services.
</Admonition>

## Tembo—Neon feature comparison

While both Tembo and Neon provide managed Postgres, Neon's architecture offers some advantages. Here’s a quick comparison of key features:

| Feature                   | Tembo                                | Neon Postgres                                                                    |
| ------------------------- | ------------------------------------ | -------------------------------------------------------------------------------- |
| **Compute**               | Manual scaling                       | Autoscaling, scale-to-zero                                                       |
| **Branching**             | NA                                   | Instant data branching for dev, test, and CI/CD workflows ("branch per feature") |
| **Storage**               | Manual scaling of storage            | Auto-scaling storage                                                             |
| **Point-in-Time Restore** | Standard backup/restore capabilities | Instant PITR to any point within your history retention window                   |

## Migration options overview

There are several ways to migrate your Tembo Postgres database to Neon. The best option depends on your database size, acceptable downtime, and technical comfort.

1.  Neon Import Data Assistant (easiest, for smaller databases \<10GB)
2.  `pg_dump` and `pg_restore`
3.  Logical replication (Near-zero downtime, for live production databases)

## Pre-migration preparation (common steps)

Before you begin any migration method, complete these essential preparation steps:

1.  **Assess your Tembo database:**
    - **Database size:** Determine the total size of your database. This will help you choose the right migration method.
    - **Postgres extensions:** Identify all custom Postgres extensions used in your Tembo instance. Run the following query on your Tembo database:
      ```sql
      SELECT e.extname AS "Name", e.extversion AS "Version", n.nspname AS "Schema", c.description AS "Description"
      FROM pg_catalog.pg_extension e
      LEFT JOIN pg_catalog.pg_namespace n ON n.oid = e.extnamespace
      LEFT JOIN pg_catalog.pg_description c ON c.objoid = e.oid AND c.classoid = 'pg_catalog.pg_extension'::pg_catalog.regclass
      ORDER BY "Name";
      ```
      This provides a list of your installed extensions, their versions, and descriptions. Compare this list to Neon's supported extensions. Neon supports many common extensions, and the full list is available here: [Neon supported extensions list](/docs/extensions/pg-extensions). For any unsupported extensions, consider finding alternatives or modifying your application.

2.  **Set up your Neon project:** If you don't have one, [create a Neon account and project](/docs/get-started/signing-up#sign-up).

## Option 1: Using Neon's Import Data Assistant

Neon's Import Data Assistant automates moving your existing database to Neon. It creates a new branch in your Neon project for your imported data.

Before you start with the assistant, You'll need:

- **Tembo connection string:** You'll need a direct connection string to your Tembo database in the format:
  `postgresql://username:password@host:port/database?sslmode=require&channel_binding=require`
- **Admin privileges:** Ensure the user in the connection string has `SUPERUSER` or sufficient privileges (`CREATE`, `SELECT`, `INSERT`, `REPLICATION`) on the source Tembo database.
- **Database size:** Your Tembo database must be **smaller than 10GB**.
- **Region:** The feature is currently supported only for Neon projects in AWS regions.

### Steps to import using the assistant:

1.  **Launch the assistant:**
    - **From the Projects page:** Click "Import database" to create a new project and import data.
      ![Import Data Assistant from Projects page](/docs/import/import_data_assistant_project.png)
    - **From within an existing project:** Use the Getting Started widget on a project dashboard.
      ![Import Data Assistant from Quick Start widget](/docs/import/import_data_assistant_quickstart_widget.png)
2.  **Check compatibility:** Enter your Tembo database connection string. Neon will verify:
    - Database size (within 10GB limit).
    - Postgres version compatibility (Neon supports Postgres 14-17).
    - Extension compatibility.
    - Region availability.
3.  **Import your data:** Once checks pass, Neon will:
    - Create a new branch for your imported data.
    - Copy your data automatically using `pg_dump` and `pg_restore`.
    - Verify the import.

    <Admonition type="note">
    During import, your source Tembo database remains untouched; Neon only reads from it.
    </Admonition>

4.  **Access your imported data:**
    - Navigate to the **Branches** page of your Neon project. Your newly imported database branch will be listed there, typically named with a timestamp (e.g., `import-2025-xx-xx`).
      ![Branches page showing imported branch](/docs/import/import_data_assistant_branch.png)
    - Click on the three dots next to the branch name and select **Set as default** to make it your default branch.
    - **Optional cleanup:**
      - Delete the old branches (`production` and `development`) if they are no longer needed.
      - Rename the new branch to `production` for clarity and consistency.

## Option 2: `pg_dump` and `pg_restore`

This is the traditional method for Postgres migrations and offers full control. It involves taking a full dump of your Tembo database and restoring it to Neon.

### Prerequisites:

- `psql`, `pg_dump`, and `pg_restore` client utilities installed locally. Use versions compatible with your Tembo Postgres version and Neon (Postgres 14-17). It's generally recommended to use the latest client versions.
- Connection string or parameters for your source Tembo database.
- Connection string for your target Neon database: You can find the connection string by clicking the **Connect** button on your Project Dashboard. It will look something like this:
  ```bash
  postgresql://[user]:[password]@[neon_hostname]/[dbname]
  ```

### Export data from Tembo using `pg_dump`

Use the following command to create a dump of your Tembo database. Use a direct, unpooled connection.

```bash
pg_dump -Fc -v -d "postgresql://user:pass@tembo_host:port/source_db" -f your_tembo_dump.dump
```

> Replace the connection string with your actual Tembo database connection string.

The command options used are:

- `-Fc`: Custom format (compressed, suitable for `pg_restore`).
- `-v`: Verbose mode.
- `-d`: Source database connection string or name.
- `-f`: Output file name.

### Restore data to Neon using `pg_restore`

- The role performing the `pg_restore` operation in Neon becomes the owner of restored objects by default.
- Roles created in the Neon Console are members of `neon_superuser`. This role can create objects but is not a full PostgreSQL `SUPERUSER` and cannot run `ALTER OWNER` for objects it doesn't own.
- If your Tembo database uses multiple roles for object ownership, your dump file will contain `ALTER OWNER` commands. These may cause non-fatal errors during restore to Neon.
- To avoid ownership errors, you can use the `--no-owner` option with `pg_restore`. All objects will then be owned by the Neon role executing the restore.

Run the following command to restore the dump to your Neon database:

```bash
pg_restore -v --no-owner -d "postgresql://neon_user:neon_pass@neon_host:port/target_db" your_tembo_dump.dump
```

> Replace the connection string with your actual Neon database connection string.

The command options used are:

- `-v`: Verbose mode.
- `--no-owner`: Ignores original ownership, objects owned by `neon_user`.
- `-d`: Target Neon database connection string.

For more detailed usage, refer to [Migrate data from Postgres with pg_dump and pg_restore](/docs/import/migrate-from-postgres).

## Option 3: Logical replication

Logical replication allows for near-zero downtime migration by continuously streaming data changes from your Tembo database (publisher) to your Neon database (subscriber).

### Prepare Tembo (source publisher)

- **Enable logical replication:** Refer to Tembo's documentation for enabling [logical replication](https://legacy.tembo.io/docs/getting-started/postgres_guides/postgres-wal-configuration/)

- **Create publication:** Define a publication on Tembo for the tables you want to replicate.

  ```sql
  CREATE PUBLICATION neon_migration_pub FOR TABLE table1, table2;
  ```

- Allow Connections from Neon to Tembo (IP Allow List):

  If you are having IP allow list restrictions on your Tembo database, you need to allow connections from Neon to Tembo. This is necessary for the logical replication process to work correctly.
  1.  **Obtain Neon NAT Gateway IP Addresses:**
      Refer to Neon's [NAT Gateway IP addresses](/docs/introduction/regions#nat-gateway-ip-addresses) to find the list of IP addresses for your Neon project's region. You will need to add these specific IP addresses to your Tembo project's allow list.

  2.  **Configure IP Allow List in Tembo.io:**
      - Log in to your Tembo.io dashboard.
      - Navigate to **Settings > Network Settings**.
      - Locate the **IP Allow List** section.
      - For each Neon NAT Gateway IP address obtained in the previous step, click **Add New**.
      - Enter each Neon IP address in the provided field.
        ![Tembo IP Allow List](/docs/guides/tembo-ip-allowlist.webp)
      - After adding all necessary Neon IP addresses, click **Save Changes** to apply the new network restrictions.

### Prepare Neon (target subscriber)

**Create schema:** Copy the schema from Tembo to Neon. You can use `pg_dump` to export the schema and `psql` to import it into Neon.

- Dumping schema from Tembo:
  ```bash
  pg_dump --schema-only \
      --no-privileges \
      "postgresql://user:pass@tembo_host:port/source_db" \
      > schema.sql
  ```
- Restoring schema to Neon:

  ```bash
  psql \
      "postgresql://neon_user:neon_pass@neon_host:port/target_db" \
      < schema.sql
  ```

  > Replace the connection strings with your actual Tembo and Neon database connection strings.

### Create subscription on Neon

Connect to your Neon database and create a subscription.

    ```sql
    CREATE SUBSCRIPTION tembo_to_neon_sub
    CONNECTION 'postgresql://app:PASSWORD@hostname.data-1.use1.tembo.io/app'
    PUBLICATION neon_migration_pub;
    ```

    > Replace connection string with your Tembo database connection string.

- Initial data synchronization will begin. This can take time for large databases.
- Data changes on Tembo will be replicated to Neon.

### Monitor replication

To confirm your Neon database is synchronized with Tembo, monitor the Write-Ahead Log (WAL).

**On Tembo (Publisher):**
You can check the current WAL log sequence number (LSN) using:

```sql
SELECT pg_current_wal_lsn();
```

**On Neon (Subscriber):**
The subscriber is up-to-date when its `received_lsn` (last log sequence number received) and `latest_end_lsn` (last log sequence number applied) are identical. Check this using:

```sql
SELECT subname, received_lsn, latest_end_lsn
FROM pg_catalog.pg_stat_subscription
WHERE subname = 'tembo_to_neon_sub';
```

If `received_lsn` and `latest_end_lsn` are the same for your subscription, Neon has processed all the data it has received from Tembo. For complete synchronization, this `latest_end_lsn` on Neon should also align with the current LSN on the Tembo publisher.

### Perform the cutover (switch applications)

Once Neon is fully synchronized and replication lag is minimal:

- Briefly stop application writes to the Tembo database (maintenance mode).
- Wait for any final changes to replicate to Neon.
- Update your application's connection string to point to the Neon database.
- Resume application traffic, now directed at Neon.
- Thoroughly test your application.

## Post-migration (common steps)

1.  **Verify data:**
    - Run checksums or row counts on key tables in both Tembo and Neon to ensure data integrity.
    - Perform functional testing of your application against Neon.

2.  **Update application connection strings:** Ensure all parts of your application and any related services are now using the Neon database connection string.

3.  **Cleanup for logical replication:**
    If you used logical replication, you can drop the subscription from Neon once you're satisfied with the migration.
    ```sql
    DROP SUBSCRIPTION tembo_to_neon_sub;
    ```

## Resources

- [Tembo Cloud Migration Guide](https://tembo-io.notion.site/Tembo-Cloud-Migration-Guide-1de7c9367d6a80349570e7469ba7f17b)
- Neon Docs:
  - [Import Data Assistant](/docs/import/import-data-assistant)
  - [Migrate data from Postgres with `pg_dump` and `pg_restore`](/docs/import/migrate-from-postgres)
  - [Replicate data from Postgres to Neon (Logical replication)](/docs/guides/logical-replication-postgres-to-neon)
  - [Connect to Neon](/docs/connect/connect-from-any-app)
  - [Supported Postgres extensions in Neon](/docs/extensions/pg-extensions)
- PostgreSQL Documentation:
  - [`pg_dump`](https://www.postgresql.org/docs/current/app-pgdump.html)
  - [`pg_restore`](https://www.postgresql.org/docs/current/app-pgrestore.html)
  - [Logical Replication](https://www.postgresql.org/docs/current/logical-replication.html)

<NeedHelp />


# Build an AI-powered knowledge base chatbot using n8n and Neon Postgres

---
title: Build an AI-powered knowledge base chatbot using n8n and Neon Postgres
subtitle: A step-by-step guide to creating an AI-powered knowledge base chatbot using n8n, Google Drive, and Neon Postgres
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-05-27T00:00:00.000Z'
updatedOn: '2025-05-27T00:00:00.000Z'
---

This guide demonstrates how to build a powerful **AI-powered internal knowledge base chatbot** using **n8n** and **Neon**. n8n is a low-code platform that allows you to connect various applications and services, enabling you to automate complex processes through a visual workflow editor. In this guide, we'll use n8n to orchestrate the integration between **Google Drive**, **Neon Postgres**, and **Google Gemini** to create a chatbot that can answer questions based on your documents stored in Google Drive. Neon will be used as a vector store to index and retrieve document chunks, while Google Drive will serve as the source of your documents.

It will be built using a **Retrieval-Augmented Generation (RAG)** approach, which combines the power of large language models (LLMs) with your own documents. This allows the chatbot to access and utilize your documents as a knowledge base, enabling it to answer questions accurately and contextually.

## Prerequisites

Before you begin, ensure you have the following:

- **n8n Instance:** A running n8n instance. This can be a [self-hosted](https://docs.n8n.io/hosting/) version or an [n8n cloud account](https://app.n8n.cloud).
- **Google Account:** A Google account with access to Google Drive.
- **Neon Account and Project:** A Neon account and a project are needed to create a Postgres database. You can sign up for a free account at [pg.new](https://pg.new).
- **Google Cloud Platform (GCP) Account:** A [GCP account](https://cloud.google.com/free) is needed to enable the Google Drive API and to manage OAuth credentials. Free tier accounts are sufficient for this guide.
- **Google Gemini API key:** You'll need an API key for Google Gemini to generate embeddings and power the chat model. You can obtain this from [Google AI Studio](https://aistudio.google.com/apikey).
  ![Google Gemini API Key](/docs/guides/gemini-api-key.png)

## Architecture overview

The solution consists of two main n8n workflows:

1.  **Indexing workflow:** This workflow is triggered when a new file is added to a specified Google Drive folder. It downloads the file, splits it into manageable chunks, generates vector embeddings for each chunk using Google Gemini, and then stores these chunks and their embeddings in a Neon Postgres database (acting as a PGVector store).
2.  **Chat workflow:** This workflow provides a chat interface. When a user sends a message (a question), the AI Agent retrieves relevant document chunks from the Neon Postgres vector store based on the query's semantic similarity. These retrieved chunks are then passed as context to a Google Gemini chat model, which generates a response.

## Workflow 1: Indexing Google Drive documents into Neon Postgres

This workflow automates the process of ingesting and preparing your Google Drive documents for the chatbot.

![Workflow 1 Overview](/docs/guides/n8n/n8n-workflow-1-overview.png)

### Step 1: Setting up the Google Drive trigger

1.  **Create a new workflow:** In your n8n dashboard, create a new workflow.
2.  **Add Google Drive trigger:** Click the `+` button to add the first step. Search for and select "Google Drive".
3.  **Configure trigger:** In the Google Drive node parameters, select "On changes involving a specific folder" under "Triggers".
4.  **Connect Google Drive account (OAuth2):**
    - Under "Credential to connect with", click "Select Credential" and then "Create new credential".
    - This will open a dialog for "Google Drive account (Google Drive OAuth2 API)". Note the "OAuth Redirect URL" provided by n8n (you will need this in the next steps).

    ![Configuring Google Drive Trigger node](/docs/guides/n8n/n8n-add-google-drive-folder-node.gif)

5.  **Create Google Drive OAuth credentials:**
    - Open the [Google Cloud console](https://console.cloud.google.com/) and navigate to "APIs & Services" -> "OAuth consent screen".
    - If not configured, click "Get started".
    - **App Information:** Provide an "App name" (e.g., `personal-n8n`), select "User support email", and enter your email address.
    - **Audience:** Select "External". Click "Next".
    - **Contact Information:** Confirm your email. Click "Next".
    - **Finish:** Review and click "Create".
    - Now, navigate to "APIs & Services" -> "Clients". Click "Create OAuth client" and select "OAuth client ID".
    - Select "Web application" as the "Application type".
    - Under "Authorized redirect URIs", paste the "OAuth Redirect URL" which you copied from n8n (e.g., `https://your-n8n-instance.com/rest/oauth2-credential/callback`). Click "Create".
    - You will be redirected to a page showing your OAuth 2.0 Client IDs. Select the one you just created. Copy the "Client ID" and "Client secret" values from this page, you will need them in n8n.
    - Navigate to "Audience" Page. Click "+ Add users" and add the Google account email you intend to use for authorizing n8n.

    ![Configuring Google Drive OAuth credentials in GCP](/docs/guides/n8n/n8n-configure-gdrive-oauth.gif)

6.  **Enable Google Drive API in GCP:**
    - In the GCP console, search for "Google Drive API".
    - Select "Google Drive API" and click "Enable" if it's not already enabled.

    ![Enabling Google Drive API in GCP](/docs/guides/n8n/n8n-enable-gdrive-api.gif)

7.  **Finalize n8n Credential:**
    - Back in n8n, paste the "Client ID" and "Client secret" you copied from GCP into the respective fields in the "Google Drive account (OAuth2)" credential dialog.
    - After entering the Client ID and Secret, click "Sign in with Google".
    - Authenticate with the Google account you added as a test user. Grant the necessary permissions.
    - You should see "Account connected". Click "Save" and close the dialog.

    ![Finalizing Google Drive OAuth credential in n8n](/docs/guides/n8n/n8n-finalize-gdrive-oauth.gif)

8.  **Configure folder and watch settings:**
    - Back in the Google Drive Trigger node, set the "Folder" to the specific folder you want to monitor for new files. You can enter the folder name or select from the list.
    - Select "File Created" under "Watch For". This will trigger the workflow when a new file is added to the specified folder.
9.  **Test the Trigger:**
    - Upload a sample PDF document to your specified Google Drive folder.
    - In n8n, click "Fetch Test Event" on the Google Drive Trigger node. It should detect the new file and show its details in the output on the right side of the editor. This confirms that the trigger is working correctly.

    ![Testing the Google Drive Trigger in n8n](/docs/guides/n8n/n8n-test-gdrive-trigger.gif)

### Step 2: Downloading the File content

1.  **Add Google Drive Node (Action):** Click the `+` after the trigger node. Search for and select "Google Drive".
2.  Select "Download File" as the operation.
3.  Under **Credential to connect with** select your previously created Google Drive account.
4.  Under **Resource:** select "File".
5.  Under **Operation:** select "Download".
6.  Select **File > By ID**. Drag the "File ID" from the Google Drive Trigger node to this field. This ensures the node downloads the file that triggered the workflow. Check the image below for reference.
7.  Select **Options > Add option > File Name.** Similarly, drag the "File Name" from the Google Drive Trigger node to this field. This will help in identifying the file later.
    ![Configuring Google Drive Download node](/docs/guides/n8n/n8n-google-drive-download-node.gif)

### Step 3: Setting up the Neon Postgres PGVector store Node

This node will store the document chunks and their embeddings in Neon Postgres using the [`pgvector` extension](/docs/extensions/pgvector).

1.  **Add Postgres PGVector Store Node:** Click the `+` after the Google Drive download node. Search for "Postgres PGVector Store" and add it.
2.  Under **Actions** select "Add documents to vector store".
3.  A dialog will appear prompting you to either select an existing credential or create a new one. Click "Create new credential".

    <Admonition type="tip">
    You can get your Neon database connection details from the Neon console. Learn more: [Connect from any application](/docs/connect/connect-from-any-app)
    </Admonition>

4.  Fill in your Neon database details:
    - **Host:** Your Neon host
    - **Database:** Your Neon database name
    - **User:** Your Neon database user
    - **Password:** Your Neon database password
    - **SSL:** Set to "Require".
5.  Click "Save".
6.  Configure the Postgres PGVector Store node parameters:
    - **Operation Mode:** Select "Insert Documents".
    - **Table Name:** Enter a name for your table (e.g., `n8n_vectors`). The table will be created automatically by n8n.
    - **Embedding Batch Size:** (e.g., 200, default)

![Configuring Postgres PGVector Store node](/docs/guides/n8n/n8n-pgvector-store-node.gif)

### Step 4: Chunking and processing the documents

The PGVector Store node has inputs for "Document" and "Embeddings". We will add nodes to handle the document loading and text splitting before generating embeddings.

1. Click on the "Document" input anchor of the Postgres PGVector Store node.
2. Search for and select "Default Data Loader" with the following parameters:
   - **Type of Data:** Select "Binary".
   - **Mode:** Select "Load All Input Data".
   - **Data Format:** Select "Automatically Detect by Mime Type".
   - **Options > Metadata > Add property:**
     - **Name:** `file_name`
     - **Value (Expression):** Drag the "File Name" from the Google Drive Download node to this field. This will help in identifying the file later.
3. **Add Recursive character text splitter Node:**
   - Click on the "Text Splitter" input anchor of the Default Data Loader node.
   - Search for and select "Recursive Character Text Splitter".
   - Set the **Chunk Size** to `1000` (default, or adjust as needed).
   - Set the **Chunk Overlap** to `100` (or adjust as needed, depending on how much context you want to retain between chunks).

![Configuring Data Loader and Text Splitter nodes](/docs/guides/n8n/n8n-data-loader-text-splitter-nodes.gif)

### Step 5: Generating Embeddings

1. Click on the "Embeddings" input anchor of the Postgres PGVector Store node.
2. Search for and select "Embeddings Google Gemini".
3. **Configure Gemini credentials:**
   - **Credential to connect with:** Click "Create new credential". A dialog will open prompting you to enter your Google Gemini API key.
   - Paste your Google Gemini API Key obtained from Google AI Studio in the [Prerequisites](#prerequisites) section.
   - Click "Save". It should show "Connection tested successfully".
4. Choose an embedding model. In this guide, `models/text-embedding-004` is used. You can select a different embedding model based on your requirements.

<Admonition type="important" title="Model Dimensionality">
Each embedding model produces vectors of a specific dimensionality. Ensure the model selected here is consistent with the model you'll use for retrieval in the chat workflow in the next section. For example, if you use `models/text-embedding-004` here, you should also use the same model in the chat workflow for retrieval.
</Admonition>

![Configuring Embeddings Google Gemini node](/docs/guides/n8n/n8n-embeddings-gemini-node.gif)

### Step 6: Testing the Indexing workflow

Click on the play icon (▶️) on the "Postgres PGVector Store" node to execute the workflow. This assumes you have already ran the Google Drive trigger and download file nodes successfully. You can also click the "Test workflow" button at the bottom of the n8n editor to run the entire workflow.

![Testing the indexing workflow in n8n](/docs/guides/n8n/n8n-test-indexing-workflow.gif)

The workflow should execute successfully, which will split the document into chunks, generate embeddings for each chunk, and store them in Neon Postgres database.

### Step 7: Verifying the embeddings in Neon (optional)

1. Log in to your Neon console.
2. Navigate to your database and then to the "Tables" section.
3. You should see the `n8n_vectors` (or your chosen name) table created, populated with document chunks and their embedding vectors.

![Verifying data in Neon Console](/docs/guides/n8n/n8n-verify-data-in-neon-console.png)

<Admonition type="important" title="Save your workflow">
Make sure to save your workflow by clicking the "Save" button in the top right corner of the n8n editor. This ensures that all your configurations are stored and can be reused later.
</Admonition>

## Workflow 2: Chat Trigger with AI Agent and Retrieval

This workflow will provide the user interface to interact with your AI knowledge base.

![Workflow 2 Overview](/docs/guides/n8n/n8n-workflow-2-overview.png)

### Step 1: Setting up the Chat Trigger and AI Agent

1. You can create a new n8n workflow or continue in the same Workflow 1. In this guide, we use the same workflow.
2. **Add Chat Trigger:** Click the `+` button. Search for "Chat Trigger" and add it.
3. **Add AI Agent Node:** Click the `+` after the Chat Trigger. Search for "AI Agent" and add it.

![Configuring Chat Trigger and AI Agent nodes](/docs/guides/n8n/n8n-chat-trigger-ai-agent-nodes.gif)

### Step 2: Configuring the Chat model

1. Click on the "Chat Model" input anchor of the AI Agent node.
2. Search for and select "Google Gemini Chat Model".
3. Select your existing "Gemini" API account for the credential.
4. Choose a chat model that suits your needs. In this guide, we use `models/gemini-2.5-flash-preview-05-20`, which is a good balance of performance and cost for most use cases.

![Configuring Google Gemini Chat Model node](/docs/guides/n8n/n8n-gemini-chat-model-node.gif)

### Step 3: Configuring the Tool (Vector Store Retriever)

The AI Agent will use this tool to retrieve information from your Neon vector store.

1. Click on the "Tool" input anchor of the AI Agent node.
2. Search for and select "Postgres PGVector Store".
3. Select your previously created "Neon" credential for the Postgres PGVector Store.
4. Select "Retrieve Documents (As Tool for AI Agent)" as the operation mode.
5. **Name the Tool:** Give it a descriptive name, e.g., `internal_knowledge_base`.
6. **Description:** Provide a description for the AI Agent, e.g., `Docs for Internal Knowledge Base`.
7. **Table Name:** Enter the same table name used in Workflow 1 (e.g., `n8n_vectors` or whatever you named it).
8. **Limit:** Set the maximum number of document chunks to retrieve (e.g., `4`) for the AI Agent to use as context.
9. **Include Metadata:** Toggle this ON to include metadata in the retrieved documents. This can be useful for providing filename or other context in the chat responses.

![Configuring Postgres PGVector Store as Tool for Retrieval](/docs/guides/n8n/n8n-pgvector-store-tool-node-for-retrieval.gif)

### Step 4: Connecting Embeddings for Retrieval

The retrieval process also needs to generate embeddings for the user's query to find similar document chunks from the vector store. This is done using the same embedding model used during indexing.

1. Click on the "Embedding" input anchor of the "Postgres PGVector Store" (Tool) node.
2. Search for and select "Embeddings Google Gemini".
3. Select your "Gemini" API account for the credential which you created in Workflow 1.
4. Select the same embedding model used in Workflow 1 (e.g., `models/text-embedding-004`).

After all these configurations, your Workflow should look like this:

![Workflow 2 after configuration of all nodes](/docs/guides/n8n/n8n-workflow-2-after-configuration.png)

### Step 5: Testing the Chatbot

1.  You can click "Open chat" next to the "Test workflow" button at the bottom of the n8n editor. This will open a chat interface where you can interact with your AI knowledge base chatbot.
2.  **Ask a Question:** Type a question related to the content of the documents you indexed into Google Drive. For example, here we indexed a PDF document about "Neon RLS" and we ask it "What is Neon RLS, explain to me as if I am a 5".

    ![Asking a question in the chat interface](/docs/guides/n8n/n8n-ask-chatbot-question.png)

3.  **Verify Response and Logs:**
    - The chatbot should provide an answer based on the retrieved documents. The answer will be generated by the Google Gemini chat model, using the context provided by the retrieved document chunks.
    - You can inspect the "Latest Logs from AI Agent node" in n8n to see the input, the tool being called (Postgres PGVector Store), and the output from the Gemini Chat Model for debugging or verification purposes.

![Testing the chat workflow in n8n](/docs/guides/n8n/n8n-test-chat-workflow.gif)

### Step 6: Activating the Chat workflow

1. Once you are satisfied with the testing, save your workflow and then toggle the "Active" switch in the n8n navbar to activate the workflow. This will allow it to run automatically when triggered by the Chat Trigger node or Google Drive Trigger node.

   ![Saving and activating the chat workflow in n8n](/docs/guides/n8n/n8n-activate-chat-workflow.png)

2. To get the "Chat URL", click on the "Chat Trigger" node and copy the URL provided in the "Chat URL" field. This URL can be shared with users to access the chatbot interface without needing to log in to n8n.

   ![Getting the Chat URL in n8n](/docs/guides/n8n/n8n-get-chat-url.png)

3. This will allow users to ask questions and receive answers based on the indexed documents in your Google Drive. At any point you can add more documents to the Google Drive folder you specified in the Google Drive Trigger node, and the indexing workflow will automatically process them, updating the knowledge base (vector store on Neon) and making them available for the retrieval in the chat workflow.

4. For example here we add a new PDF document about "Neon Auth" to the Google Drive folder. The indexing workflow will automatically pick it up, process it, and update the knowledge base.

   ![Adding a new document to Google Drive](/docs/guides/n8n/n8n-add-new-document-to-gdrive.png)

   We then ask the chatbot about "Neon Auth" and it provides an answer based on the newly indexed document.

   ![Asking the chatbot about the newly indexed document](/docs/guides/n8n/n8n-ask-chatbot-about-new-document.png)

5. You can also embed the chatbot on a webpage using the "Embedded Chat" option in the Chat Trigger node if desired.

## How it works: A Brief recap

1.  **Indexing:** New files in a designated Google Drive folder trigger an n8n workflow. The files are downloaded, broken into smaller text chunks, and each chunk is converted into a numerical representation (embedding) by Google Gemini. These embeddings, along with the text chunks and metadata (like the filename), are stored in your Neon Postgres database, which uses the pgvector extension to handle these vector embeddings.
2.  **Retrieval & Generation (RAG):** When you ask a question in the chat interface, your query is also converted into an embedding. The n8n AI Agent uses this query embedding to search the Neon Postgres vector store for the text chunks whose embeddings are most similar to your query's embedding. These relevant chunks are retrieved and provided as context to the Google Gemini chat model. The chat model then generates a human-like answer based on your question and the provided context from your documents.

This entire process ensures that the chatbot's answers are grounded in the information contained within your Google Drive documents.

## Debugging tips

You can debug individual n8n nodes by clicking on the node and checking the **Output** tab on the right side of the editor. This displays the data flowing through each node, helping you pinpoint workflow issues.

For instance, the image below shows the output of a **AI Agent** node after a question is asked:

    ![Debugging Chat Trigger Node Output](/docs/guides/n8n/n8n-chat-trigger-node-output.png)

Here, you can see the input message, the AI Agent's response, and the tool (**Postgres PGVector Store**) used to retrieve relevant document chunks. The document chunks retrieved from the Neon Postgres vector store are also visible. This insight helps you understand how the chatbot generates its responses based on the indexed documents.

## Resources

- [n8n Documentation](https://docs.n8n.io/)
- [Neon Documentation](/docs)
- [`pgvector` extension documentation](/docs/extensions/pgvector)
- [Build a RAG chatbot with Astro, Postgres, and LlamaIndex](/guides/chatbot-astro-postgres-llamaindex)
- [RAG Chatbot (OpenAI + LangChain)](/templates/rag-chatbot-langchain)

<NeedHelp/>


# Getting started with Neon Auth and Next.js

---
title: Getting started with Neon Auth and Next.js
subtitle: Build a Next.js todo app using Neon Auth and Drizzle ORM
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-08-11T00:00:00.000Z'
updatedOn: '2025-08-11T00:00:00.000Z'
---

[Neon Auth](/docs/neon-auth/overview) integrates user authentication directly with your Neon Postgres database, solving a common development challenge: keeping user data synchronized between systems. Instead of building and maintaining custom sync logic or webhook handlers, Neon Auth automatically populates and updates a `neon_auth.users_sync` table in your database in real-time. This allows you to treat user profiles as regular database rows, ready for immediate use in SQL joins and application logic.

This guide will walk you through building a simple todo application using Next.js, Neon Auth, and Drizzle ORM. You'll learn how to:

- Set up a Next.js project and enable Neon Auth.
- Integrate Neon Auth to add sign-up, sign-in, and sign-out functionality.
- Use Drizzle ORM to interact with the `neon_auth.users_sync` table.
- Create protected server actions using Neon Auth.

## Prerequisites

Before you begin, ensure you have the following:

- **Node.js:** Version `18` or later installed on your machine. You can download it from [nodejs.org](https://nodejs.org/).
- **Neon account:** A free Neon account. If you don't have one, sign up at [Neon](https://console.neon.tech/signup).

<Steps>

## Set up the Next.js project

To get started, create a new Next.js project. Open your terminal and run the following command:

```bash
npx create-next-app@latest neon-auth-todo --typescript --tailwind --use-npm --eslint --app --no-src-dir --import-alias "@/*" --no-turbopack
cd neon-auth-todo
```

This command sets up a new Next.js project with TypeScript, Tailwind CSS, and ESLint configured.

Open the project in your favorite code editor (e.g., VSCode, Cursor, Windsurf).

## Create a Neon project and enable Neon Auth

You'll need to create a Neon project and enable Neon Auth.

1.  **Create a Neon project:** Navigate to [pg.new](https://pg.new) to create a new Neon project. Give your project a name, such as `neon-auth-todo`.

2.  **Enable Neon Auth:**
    - In your project's dashboard, go to the **Auth** page from the sidebar.
    - Click **Enable Neon Auth**. This will provision the necessary infrastructure for authentication and user management.

    ![Neon Console - Enable Neon Auth button](/docs/guides/enable-neon-auth.png)

3.  **Get environment variables:**
    - After enabling Neon Auth, navigate to the **Configuration** tab on the Auth page.
    - Select **Next.js** as your framework.
    - You will see the required environment variables. Copy the entire block, which includes your Neon Auth keys and the database connection string.

    ![Neon Console - Neon Auth configuration keys for Next.js](/docs/guides/neon-auth-example-config-keys.png)

## Integrate Neon Auth into your app

Now, you will integrate Neon Auth into your Next.js application.

1.  **Run the Neon Auth setup command:**
    In your project's root directory, run the following command to initialize the Neon Auth setup:

    ```bash
    npx @stackframe/init-stack@latest --no-browser
    ```

    > Enter "Y" when prompted to proceed with the installation.

    You should see output similar to this:

    ```
    npx @stackframe/init-stack@latest --no-browser
    Need to install the following packages:
    @stackframe/init-stack@2.8.28
    Ok to proceed? (y) y


          ██████
      ██████████████
    ████████████████████
    ████████████████████                WELCOME TO
    █████████████████        ╔═╗╔╦╗╔═╗╔═╗╦╔═  ┌─┐┬ ┬┌┬┐┬ ┬
    █████████████            ╚═╗ ║ ╠═╣║  ╠╩╗  ├─┤│ │ │ ├─┤
    █████████████   ████     ╚═╝ ╩ ╩ ╩╚═╝╩ ╩  ┴ ┴└─┘ ┴ ┴ ┴
       █████████████████
          ██████      ██
    ████            ████
      █████    █████
          ██████


    ? Found a Next.js project at /home/user/neon-auth-todo/neon-auth-todo — ready to install Stack Auth? Yes

    Installing dependencies...

    npm warn ERESOLVE overriding peer dependency

    added 194 packages, and audited 531 packages in 60s

    166 packages are looking for funding
      run `npm fund` for details

    2 low severity vulnerabilities

    To address all issues, run:
      npm audit fix

    Run `npm audit` for details.
    √ Command npm install @stackframe/stack succeeded

    Writing files...

    √ Done writing files

    Installation succeeded!

    Commands executed:
      npm install @stackframe/stack

    Files written:
      app/layout.tsx
      .env.local
      stack.tsx
      app/handler/[...stack]/page.tsx
      app/loading.tsx

    ===============================================

    Successfully installed Stack! 🚀🚀🚀
    ```

2.  **Configure environment variables:**
    Paste the environment variables you copied from the Neon Auth configuration into the `.env.local` file.

    ```env
    # Neon Auth environment variables for Next.js
    NEXT_PUBLIC_STACK_PROJECT_ID=YOUR_NEON_AUTH_PROJECT_ID
    NEXT_PUBLIC_STACK_PUBLISHABLE_CLIENT_KEY=YOUR_NEON_AUTH_PUBLISHABLE_KEY
    STACK_SECRET_SERVER_KEY=YOUR_NEON_AUTH_SECRET_KEY

    # Your Neon connection string
    DATABASE_URL=YOUR_NEON_CONNECTION_STRING
    ```

## Set up Drizzle ORM

For database interactions, you will use Drizzle ORM.

1.  **Install Drizzle ORM:**

    ```bash
    npm install drizzle-orm @neondatabase/serverless
    npm install -D drizzle-kit dotenv
    ```

2.  **Create Drizzle config:**
    Create a file named `drizzle.config.ts` in your project root and add the following configuration:

    ```typescript
    import { defineConfig } from 'drizzle-kit';
    import { config } from 'dotenv';

    config({ path: './.env.local' });

    export default defineConfig({
      dialect: 'postgresql',
      schema: './app/db/schema.ts',
      out: './drizzle',
      dbCredentials: {
        url: process.env.DATABASE_URL!,
      },
    });
    ```

    This config tells Drizzle Kit where to find your database schema and where to output migration files.

## Define the application schema

Drizzle ORM provides a built-in helper function to work with Neon Auth's `users_sync` table. Instead of manually defining the schema or pulling it from the database, you can use the `usersSync` helper from `drizzle-orm/neon`.

The most important part of this schema is creating a direct link between a todo and the user who owns it. You will achieve this by establishing a foreign key relationship from your `todos` table to the `users_sync` table.

This schema defines the `todos` table with the following columns:

- **`id`**: A unique, auto-incrementing identifier for each todo.
- **`ownerId`**: A text column that stores the user's ID. This column is configured with a foreign key that `references` the `id` in the `neon_auth.users_sync` table, ensuring data integrity.
- **`task`**: The text content of the todo item.
- **`isComplete`**: A boolean flag to track the todo's status.
- **`insertedAt`**: A timestamp automatically set when a todo is created.

### Create the schema file

Create a `db` directory inside the `app` folder, then add a file named `schema.ts` within it:

```plaintext
app/
  db/
    schema.ts
```

Add the following code to `app/db/schema.ts`:

```typescript
import { pgTable, text, timestamp, bigint, boolean } from 'drizzle-orm/pg-core';
import { usersSync } from 'drizzle-orm/neon';

export const todos = pgTable('todos', {
  id: bigint('id', { mode: 'bigint' }).primaryKey().generatedByDefaultAsIdentity(),
  ownerId: text('owner_id')
    .notNull()
    .references(() => usersSync.id),
  task: text('task').notNull(),
  isComplete: boolean('is_complete').notNull().default(false),
  insertedAt: timestamp('inserted_at', { withTimezone: true }).defaultNow().notNull(),
});
```

The `usersSync` helper from `drizzle-orm/neon` automatically provides the correct schema definition for the `neon_auth.users_sync` table, eliminating the need for manual schema introspection.

### Generate and apply migrations

Now, generate the SQL migration file to create the `todos` table.

```bash
npx drizzle-kit generate
```

This creates a new SQL file in the `drizzle` directory. Apply this migration to your Neon database by running:

```bash
npx drizzle-kit migrate
```

Your `todos` table now exists in your Neon database. You can verify this in the **Tables** section of your Neon project console.

![Neon Auth todos table](/docs/guides/neon-auth-todos-table.png)

## Create the database client

Create a file at `app/db/index.ts` to instantiate the Drizzle client.

```typescript
import { neon } from '@neondatabase/serverless';
import { drizzle } from 'drizzle-orm/neon-http';

const sql = neon(process.env.DATABASE_URL!);
export const db = drizzle(sql);
```

## Build the application UI

You will create a simple user interface for your todo app using React components.

1.  **Create the Header Component:**
    This component will display sign-in/sign-up links or user information and a sign-out button. Create `app/header.tsx`:

    ```tsx
    'use client';

    import Link from 'next/link';
    import { useStackApp, useUser } from '@stackframe/stack';

    export function Header() {
      const user = useUser();
      const app = useStackApp();

      return (
        <header className="dark:bg-gray-900 fixed left-0 top-0 z-50 w-full bg-white shadow-md">
          <div className="mx-auto flex max-w-4xl items-center justify-between px-6 py-3">
            <div className="text-gray-800 dark:text-gray-100 text-xl font-bold tracking-tight">
              My Todo App
            </div>
            <nav>
              {user ? (
                <div className="flex items-center gap-4">
                  <span className="text-gray-600 dark:text-gray-300">
                    Hello{' '}
                    <span className="dark:text-gray-100 font-medium">{user.primaryEmail}</span>
                  </span>
                  <Link
                    href={app.urls.signOut}
                    className="text-red-500 dark:text-red-400 text-sm hover:underline"
                  >
                    Sign Out
                  </Link>
                </div>
              ) : (
                <div className="flex items-center gap-4">
                  <Link
                    href={app.urls.signIn}
                    className="text-blue-600 dark:text-blue-400 text-sm hover:underline"
                  >
                    Sign In
                  </Link>
                  <span className="text-gray-400 dark:text-gray-500">|</span>
                  <Link
                    href={app.urls.signUp}
                    className="text-green-600 dark:text-green-400 text-sm hover:underline"
                  >
                    Sign Up
                  </Link>
                </div>
              )}
            </nav>
          </div>
        </header>
      );
    }
    ```

    The `useUser()` hook provides the current user's state, while `useStackApp()` provides access to utility URLs like `signIn` and `signOut`.

    <Admonition type="info" title="Neon Auth Hooks">
      The Neon Auth SDK for Next.js offers a comprehensive set of hooks to manage authentication and user data throughout your application. It provides distinct tools tailored for different rendering environments, such as the `useUser` hook for Client Components and the `stackServerApp` object for server-side logic.

    To explore the full API, including hooks for more advanced features like handling teams and permissions, refer to the [Neon Auth: Next.js SDK Overview](/docs/neon-auth/sdk/nextjs/overview).
    </Admonition>

2.  **Create the Todo components:**

    For all CRUD operations, you'll use server actions to securely handle form submissions and update the database directly from your Next.js components. The implementation details of these server actions will be covered later in the guide.

    Create a new file `app/todos.tsx` to define the form for adding todos and the list to display them.

    ```tsx
    import { addTodo, toggleTodo, deleteTodo } from '@/app/actions/todoActions';
    import { stackServerApp } from '@/stack';
    import { revalidatePath } from 'next/cache';

    type Todo = {
      id: bigint;
      task: string;
      isComplete: boolean;
    };

    export async function TodoForm() {
      const user = await stackServerApp.getUser();

      if (!user) {
        return (
          <p className="text-gray-500 mt-4 text-center">Please log in to manage your todos.</p>
        );
      }

      return (
        <form
          action={async (formData) => {
            'use server';
            await addTodo(formData.get('task') as string);
            revalidatePath('/');
          }}
          className="flex gap-2"
        >
          <input
            type="text"
            name="task"
            placeholder="New todo"
            className="flex-1 rounded-md border px-2 py-1"
            required
          />
          <button
            type="submit"
            className="bg-blue-500 hover:bg-blue-600 rounded-md px-3 py-1 text-white"
          >
            Add
          </button>
        </form>
      );
    }

    export function TodoList({ todos }: { todos: Todo[] }) {
      if (todos.length === 0) {
        return <p className="text-gray-500 mt-8 text-center">No todos yet. Add one above!</p>;
      }

      return (
        <ul className="mt-4 space-y-2">
          {todos.map((todo) => (
            <li
              key={todo.id.toString()}
              className="flex items-center justify-between border-b py-2"
            >
              <span className={todo.isComplete ? 'text-gray-400 line-through' : ''}>
                {todo.task}
              </span>
              <div className="flex gap-2">
                <form
                  action={async () => {
                    'use server';
                    await toggleTodo(todo.id, !todo.isComplete);
                    revalidatePath('/');
                  }}
                >
                  <button type="submit" className="text-green-500 hover:text-green-700 text-sm">
                    {todo.isComplete ? 'Undo' : 'Done'}
                  </button>
                </form>
                <form
                  action={async () => {
                    'use server';
                    await deleteTodo(todo.id);
                    revalidatePath('/');
                  }}
                >
                  <button type="submit" className="text-red-500 hover:text-red-700 text-sm">
                    Delete
                  </button>
                </form>
              </div>
            </li>
          ))}
        </ul>
      );
    }
    ```

    The above code defines two main components for managing todos: `TodoForm` and `TodoList`.
    - `TodoForm` is a form for adding new todos. It checks if the user is logged in and, if so, allows them to submit a new todo item. On submission, it calls the `addTodo` server action and refreshes the page to show the updated list.

    - `TodoList` displays the current user's todos. Each todo item has buttons to mark it as complete/incomplete or delete it. These actions are handled by the `toggleTodo` and `deleteTodo` server actions, respectively. The UI updates automatically after each action, and if there are no todos, a message prompts the user to add one.

3.  **Create the main page:**
    Replace the content of `app/page.tsx` with the following code. This will be the main page that displays the todo list and the form to add new todos.

    ```tsx
    import { getTodos } from '@/app/actions/todoActions';
    import { stackServerApp } from '@/stack';
    import { Header } from './header';
    import { TodoForm, TodoList } from './todos';

    export default async function HomePage() {
      const todos = await getTodos();

      return (
        <main className="mx-auto max-w-lg p-6 pt-24">
          <Header />
          <h1 className="mb-4 text-2xl font-bold">My Todos</h1>
          <TodoForm />
          <TodoList todos={todos} />
        </main>
      );
    }
    ```

## Implement server actions

To manage todos, you need to create server actions that will handle the database operations. These actions will be responsible for adding, retrieving, updating, and deleting todos.

Create a new file `app/actions/todoActions.ts`:

```typescript
'use server';

import { db } from '@/app/db';
import { todos } from '@/app/db/schema';
import { stackServerApp } from '@/stack';
import { eq, desc, and } from 'drizzle-orm';

export async function addTodo(task: string) {
  const user = await stackServerApp.getUser();
  if (!user) throw new Error('Not authenticated');

  await db.insert(todos).values({
    task,
    ownerId: user.id,
  });
}

export async function getTodos() {
  const user = await stackServerApp.getUser();
  if (!user) return [];

  return db.select().from(todos).where(eq(todos.ownerId, user.id)).orderBy(desc(todos.insertedAt));
}

export async function toggleTodo(id: bigint, isComplete: boolean) {
  const user = await stackServerApp.getUser();
  if (!user) throw new Error('Not authenticated');

  await db
    .update(todos)
    .set({ isComplete })
    .where(and(eq(todos.id, id), eq(todos.ownerId, user.id)));
}

export async function deleteTodo(id: bigint) {
  const user = await stackServerApp.getUser();
  if (!user) throw new Error('Not authenticated');

  await db.delete(todos).where(and(eq(todos.id, id), eq(todos.ownerId, user.id)));
}
```

In each action, `stackServerApp.getUser()` retrieves the currently logged-in user. If no user is found, the action either fails or returns an empty state. This ensures that all database operations are securely tied to the authenticated user's ID.

## Run and test the application

You are now ready to run your application.

1.  **Start the development server:**

    ```bash
    npm run dev
    ```

2.  **Test the app:**
    - Open your browser to `http://localhost:3000`.
    - You will see the header with "Sign In" and "Sign Up" links.
    - Click **Sign Up** to create a new account. You'll be redirected to the signup page.
      ![Neon Auth todo app signup page](/docs/guides/neon-auth-todo-app-signup.png)

      > Sign up using one of the available OAuth providers (e.g., Google, GitHub) or with your email address.

    - After signing up, you'll be redirected back to the app, now logged in.
    - Add, complete, and delete a few todos to test the full functionality.

![Neon Auth Todo App Demo](/docs/guides/neon-auth-todo-app-demo.png)

</Steps>

## Using Neon Auth in production

Before deploying your application to a live environment, you must complete the following security configurations. These steps are crucial to ensure your application is secure and provides a trusted experience for your users.

### Configure production OAuth credentials

The default OAuth providers (e.g., Google, GitHub) use shared, demo credentials. These are strictly for development and testing purposes. **Do not use them in production.**

For a live application, you must create and configure your own OAuth credentials for each provider. This ensures your application is secure and displays your own branding on the provider's consent screen, creating a trusted experience for your users.

> **For detailed instructions, see: [Neon Auth: Production OAuth setup](/docs/neon-auth/best-practices#production-oauth-setup)**

### Restrict redirect domains

To prevent malicious actors from hijacking your authentication flows, you must explicitly whitelist the domains your application will use for authentication redirects (e.g., your main website, admin panels).

When a user signs in, Neon Auth will only redirect them to a domain on this approved list. Any attempts to redirect to an unlisted domain will be blocked, protecting your users from phishing attacks and other security threats.

> **For detailed steps, see: [Neon Auth best practices: Restricting redirect domains](/docs/neon-auth/best-practices#restricting-redirect-domains)**

### Set up a custom email server

By default, Neon Auth sends transactional emails (like email verification and password resets) from a shared server using the `noreply@stackframe.co` address. For a production application, this can appear unprofessional and may cause emails to be filtered as spam.

To ensure a trusted user experience and improve email deliverability, you should configure Neon Auth to send emails from your own domain using a custom SMTP server.

> **For instructions, see: [Neon Auth best practices: Email server setup](/docs/neon-auth/best-practices#email-server)**

### Claim your project for Advanced configuration

Neon Auth is powered by [Stack Auth](https://stack-auth.com/), providing a managed authentication experience directly within the Neon Console. While most features can be used out of the box, you may need more advanced control for certain production use cases.

For advanced configurations or to add OAuth providers beyond the defaults (Github and Google), you can claim your project. Claiming moves the project's management from Neon to your direct control within the Stack Auth dashboard.

You should consider claiming your project if you need to:

- **Add new OAuth providers** (e.g., Spotify, Discord, Apple etc) and manage their unique client IDs/secrets.
- **Enable production mode** to enforce stricter security settings required for a live application.
- **Manage multiple environments** (e.g., development, staging, production) directly within the Stack Auth interface.

> **For more information, see: [Claiming a Neon Auth project](/docs/neon-auth/claim-project)**

## Advanced features

You've now built a basic application with Neon Auth. This is just the beginning. Neon Auth also provides more advanced capabilities for complex applications:

- **[Teams and organizations](/docs/neon-auth/concepts/orgs-and-teams):** Group users into teams to manage access and permissions for B2B applications or collaborative projects.
- **[App/User RBAC permissions](/docs/neon-auth/concepts/permissions):** Implement fine-grained Role-Based Access Control (RBAC) with both team-specific and global (project-level) permissions.
- **[Custom user data](/docs/neon-auth/concepts/custom-user-data):** Store additional information on user objects using different metadata fields (`clientMetadata`, `serverMetadata`, `clientReadOnlyMetadata`) to control data visibility and mutability between the client and server.

## Summary

Congratulations! You've successfully built a full-stack, secure todo application with Next.js and Neon Auth. You learned how to seamlessly integrate authentication, leverage the automatic user data sync with `neon_auth.users_sync`, and protect server-side logic using a unified auth and database solution.

Neon Auth handles the complexity of user management and data synchronization, allowing you to focus on building your application's core features.

## Resources

- [Neon Auth Overview](/docs/neon-auth/overview)
- [How Neon Auth works](/docs/neon-auth/how-it-works)
- [Neon Auth Best Practices & FAQ](/docs/neon-auth/best-practices)
- [Neon Auth: Next.js SDK Overview](/docs/neon-auth/sdk/nextjs/overview)
- [Neon Auth Components](/docs/neon-auth/components/components)

<NeedHelp/>


# Get started with Neon Serverless Postgres on Azure

---
title: Get started with Neon Serverless Postgres on Azure
subtitle: A step-by-step guide to deploying Neon's serverless Postgres via the Azure Marketplace
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2024-12-12T00:00:00.000Z'
updatedOn: '2024-12-12T00:00:00.000Z'
---

Neon's serverless Postgres is now available as a native integration within the Azure ecosystem, allowing organizations to provision Neon organizations directly from the Azure portal. This integration combines the power of Neon's serverless architecture with Azure's robust cloud infrastructure, offering features like Single Sign-On (SSO), unified billing, and resource management. This guide walks you through the process of setting up and managing Neon Serverless Postgres via the Azure Marketplace.

## Prerequisites

- An [Azure account](https://azure.microsoft.com/free/) with an active subscription

## Key benefits of Neon on Azure

The Azure Native integration delivers significant advantages that enhance both developer productivity and business operations:

- **Azure Portal Integration**: Create and manage Neon organizations directly through the Azure portal, eliminating the need to switch between platforms
- **Enhanced Development Experience**: Leverage familiar tools, including the Azure CLI and popular SDKs (Node.js, Python, .NET, Java, Go) for consistent workflow management
- **Single Sign-On**: Access Neon using your Microsoft credentials, improving security and simplifying authentication
- **Unified Billing**: Track and manage Neon expenses alongside other Azure services in one consolidated bill
- **Cost Optimization**: Optimize your cloud spend by applying Neon usage toward your [Microsoft Azure Consumption Commitment](https://learn.microsoft.com/en-us/marketplace/azure-consumption-commitment-benefit).

The Neon integration is available through the [Azure Marketplace](/docs/introduction/billing-azure-marketplace).

## Creating your Neon organization

To get started with Neon on Azure, you'll need to create a Neon organization within your Azure environment. This process is streamlined through the Azure portal, allowing you to manage your Neon resources alongside other Azure services.

### Access Neon through Azure Marketplace

1. Navigate to the [Azure Portal](https://portal.azure.com).
2. Search for **Neon Serverless Postgres** in the search bar.
3. Select the offering from the Marketplace results.
   ![Search for Neon Serverless Postgres on Azure](/docs/guides/search-for-neon-on-azure.png)

## Choosing the right plan

You will be prompted to select a plan based on your organization's requirements:
While starting with the Free Plan is an excellent way to explore Neon's capabilities, understanding the full range of plans helps you make informed decisions as your needs evolve.

### Free plan: Perfect for getting started

The Free Plan provides an excellent entry point for developers and small projects. With 10 projects, 0.5 GB-month storage, and 190 compute hours, you can build and test applications while experiencing Neon's core features like [database branching](/docs/introduction/branching), [read replicas](/docs/introduction/read-replicas) and [Postgres extensions](/docs/extensions/pg-extensions)

### Scale plan: Growing with your application

As your application grows, the Scale Plan offers substantial increases in resources and features for growing organizations, including:

- 50 GB-month storage + 250 GB-month archive storage
- 750 compute hours
- 1,000 projects
- Autoscaling up to 8 vCPUs
- Point-in-time restore (PITR) up to 14 days
- IP Allow Rules for enhanced security
- [Datadog integration](/docs/guides/datadog) for advanced monitoring

### Business plan: Enterprise-ready features

For organizations requiring enterprise-grade features, the Business Plan provides everything on the Scale plan plus more resources and features:

- 500 GB storage + 2500 GB-month archive storage
- 1000 compute hours
- 5,000 projects
- Autoscaling up to 16 vCPUs
- Larger fixed compute sizes up to 56 vCPU
- Point-in-time restore (PITR) up to 30 days
- Enhanced compliance with SOC 2 and ISO certifications
- Guaranteed 99.95% SLA for mission-critical applications
- Priority support for quick issue resolution
- Private Link support for secure connectivity

<Admonition type="note" title="Note">
For custom enterprise requirements beyond the Business Plan, [you can reach out to Neon's sales team](/contact-sales).
</Admonition>

## Configuring your resource

For this guide, we'll use the Free Tier, but you can easily upgrade later as your needs grow. We will proceed to the resource configuration stage by clicking on **Subscribe** with the Free Plan selected.

![Subscribe to Neon on Azure](/docs/guides/subscribe-to-neon-on-azure.png)

After selecting your plan, you'll need to configure your Neon resource within Azure. Begin by selecting your Azure subscription and creating or choosing an existing resource group. The resource group helps you organize and manage related Azure resources together. Next, provide a meaningful resource name that helps you identify this Neon deployment within your Azure environment.

![Neon Resource Configuration on Azure](/docs/guides/neon-resource-configuration-on-azure.png)

You can create a Neon resource in the `East US 2`, `Germany West Central`, and `West US 3` regions, but keep in mind that this is the region for the Azure resource, not your Neon database — you'll select a region for your actual database in a later step. Finally, create a distinctive organization name that reflects your company or project. This organization name will be visible in both Azure and the Neon Console.

Click **Review + Create** to proceed to the final stage of the deployment process. The Azure portal will validate your configuration settings, ensuring that all required fields are correctly filled. Once the validation is successful, click **Create** to deploy your Neon resource. Azure will begin provisioning your resource, which typically takes a few minutes. You should see a notification indicating that your Neon resource has been successfully deployed.

![Neon Resource Deployment Complete on Azure](/docs/guides/neon-resource-deployment-complete-on-azure.png)

## Managing your Neon resource

### Accessing the Neon Console

After successful deployment, accessing your Neon Console is straightforward through Azure's SSO integration. Navigate to your newly created Neon resource in the Azure portal, where you'll find the **SSO link** under **Portal URL**.

![Neon SSO Link in Azure Portal](/docs/guides/neon-sso-link-in-azure-portal.png)

This link provides access to the Neon Console using your Azure credentials, eliminating the need for separate authentication processes.

### Creating your first project

Upon accessing the Neon Console, you'll be directed to the project creation view. Here, you can create your first Neon project.

![Neon Azure Create Project](/docs/guides/neon-azure-create-project.png)

Neon organizes your databases into projects, each serving as an independent unit for database management and development. You can customize your project with the following details:

- **Project name:** Typically named for your application or user.
- **Database name:** Specify whatever database name makes sense. The default name is `neondb`. The name will form part of your database connection string.
- **Region:** When selecting a region, consider the geographical location of your applications and users. Choose the region that provides the lowest latency for your primary user base.
- **Advanced options:** Neon uses branches for data, similar to git branches. The default branch is named `main`. If you want, you can specify a different name.
- **Autoscaling range**: Set your project's autoscaling range. Compute resources will scale up and down within the specified range to meet demand.

### Exploring your project and database

Once your project is created, a default branch and Postgres database are automatically generated within it. You will be redirected to the project dashboard, which displays details of your project. This dashboard is your central hub for monitoring resources, setting up connections, and managing your database.

![Neon Project Dashboard](/docs/guides/neon-project-dashboard.png)

Here are some key features you'll find:

- **Connection string:** The connection string provides the necessary details to connect your applications to the database.
- **Branching:** You can create new database branches from the default one. Learn more about [database branching here](/docs/introduction/branching).
- **Monitoring**: The dashboard allows you to monitor resource usage, including compute, storage, and connections count.

### Connecting your application

To connect your application to the Neon database, copy the connection URL from the project dashboard and use it in your application stack. This URL contains the necessary credentials and connection details to establish a connection to your Neon database.

For more about connecting your application to Neon, see [Connect to Neon](/docs/connect/connect-intro). Neon also provides the following resources to help get you up and running:

- [Neon framework guides](/docs/get-started/frameworks)
- [Neon language guides](/docs/get-started/languages)
- [Connection examples repo](https://github.com/neondatabase/examples)
- [Application templates repo](/templates)

## Platform-Specific Management: Azure Portal vs. Neon Console

### Azure Portal: Your Administrative hub

The Azure portal provides a unified management experience for your Neon resources, serving as the primary entry point within the Azure ecosystem. It allows you to create and manage Neon organizations, which can then be accessed through the dedicated Neon Console. The Azure portal also centralizes your Neon financial management, providing cost views, allowing application of your [Microsoft Azure Consumption Commitment](https://learn.microsoft.com/en-us/marketplace/azure-consumption-commitment-benefit), and enabling unified billing reports.

Additionally, accessing Neon via the Azure portal enables SSO, which simplifies access to the Neon Console, and lets you manage Neon organization access via Azure RBAC.

In essence, the Azure portal provides a centralized platform for managing the administrative and financial aspects of your Neon integration within Azure. On the other hand, the management of databases, individual projects, and their advanced features is managed in the Neon Console, which we'll discuss next.

### Neon Console: Your Database management and development hub

The Neon Console is your dedicated environment for the day-to-day management of your Postgres databases and development workflows, providing a specialized set of tools beyond what's available in the Azure portal. This is where you create and manage individual database projects, configure autoscaling to adapt to your workload, and access database monitoring metrics to ensure optimal performance.

In the Neon Console, you can create read replicas to scale your read operations without impacting your primary database. You can also leverage Neon's branching capabilities, enabling you to quickly create instant copies of your database for isolated development and testing purposes. This can be instrumental when implementing database changes or developing new features.

If needed, the Neon Console facilitates instant database restoration via it's point in time restore feature. Additionally, the console provides performance and usage monitoring, allowing you to track your usage, monitor database connections, and more. The Neon Console also allows for various integrations with popular developer tools and services, enhancing the overall workflow, which is essential to ensure that your database functions are streamlined and integrated into your development ecosystem.

In short, the Neon Console provides the essential capabilities needed for database administration, development, and optimization of your Postgres databases, giving you granular control over your data and your development workflow.

## Support

Neon offers a range of support options to help you get the most out of your Neon on Azure deployment. The following table outlines the support channels available at each plan level:

| Support channels                                                                                | Free Plan | Scale Plan | Business Plan | Enterprise |
| :---------------------------------------------------------------------------------------------- | :-------: | :--------: | :-----------: | :--------: |
| [Neon Discord Server](/docs/introduction/support#neon-discord-server) (not an official channel) |  &check;  |  &check;   |    &check;    |  &check;   |
| [Neon AI Chat](/docs/introduction/support#neon-ai-chat) (not an official channel)               |  &check;  |  &check;   |    &check;    |  &check;   |
| [Support tickets](/docs/introduction/support#support-tickets)                                   |     -     |  &check;   |    &check;    |  &check;   |
| [Prioritized support tickets](/docs/introduction/support#prioritized-support-tickets)           |     -     |     -      |    &check;    |  &check;   |
| [Video chat](/docs/introduction/support#video-chat)                                             |     -     |     -      |      \*       |     \*     |
| [SLAs](/docs/introduction/support#slas)                                                         |     -     |     -      |    &check;    |  &check;   |

<div style={{margin: '-30px 0 30px 0'}}>
<small><sup>*</sup>Video chats may be scheduled on a case-by-case basis. See [Video chat](/docs/introduction/support#video-chat).</small>
</div>

## Additional Resources

- [Neon Documentation](/docs)
- [Microsoft Azure Documentation](https://docs.microsoft.com/en-us/azure/)
- [Azure Marketplace](https://azuremarketplace.microsoft.com/en-us/)
- [Microsoft Azure Consumption Commitment Benefit](https://learn.microsoft.com/en-us/marketplace/azure-consumption-commitment-benefit)

<NeedHelp />


# Automated Database Branching with GitHub Actions

---
title: Automated Database Branching with GitHub Actions
subtitle: Learn how to automate database branching for your application using Neon and GitHub Actions
enableTableOfContents: true
author: dhanush-reddy
createdAt: '2024-11-29T00:00:00.000Z'
---

Database changes can be one of the trickiest parts of application development. When multiple developers work on features that require database modifications, they often face challenges like conflicting schema changes, difficulty in testing migrations, and the risk of breaking the production database.

Database branching solves these problems by allowing developers to create isolated database environments for each feature branch, just like they do with code.
This guide demonstrates how to implement automated database branching using Neon and GitHub Actions, where each pull request gets its own database branch, complete with the necessary schema changes. You'll build a Next.js Todo application that showcases this workflow, which automates several critical database operations, including:

- Creating a new database branch when a pull request is opened
- Automatically applying schema migrations to the new branch
- Showing schema diffs directly in your pull request
- Syncing schema changes to production when the PR is merged

By the end of this guide, you'll have a system where database changes are as seamless as code changes, with each feature safely isolated in its own environment until it's ready for production. This approach not only makes database changes safer but also gives developers the confidence to experiment with schema changes without fear of breaking the production environment.

## Prerequisites

- A [Neon account](https://console.neon.tech)
- A [GitHub account](https://github.com/)
- Node.js installed on your machine
- Basic familiarity with Next.js and TypeScript

## Setting Up Your Neon Database

1. Create a new Neon project from the [Neon Console](https://console.neon.tech). For instructions, see [Create a project](/docs/manage/projects#create-a-project).
2. Note your connection string from the connection details page.

   Your connection string will look similar to this:

   ```shell
   postgres://[user]:[password]@[neon_hostname]/[dbname]?sslmode=require&channel_binding=require
   ```

## Set up the project

1. Create a new Next.js project with TypeScript:

   ```bash shouldWrap
   npx create-next-app@14 todo-app --typescript --tailwind --use-npm --eslint --app --no-src-dir --import-alias "@/*"
   cd todo-app
   ```

2. Install the required dependencies:

   ```bash
   npm install drizzle-orm @neondatabase/serverless dotenv
   npm install -D drizzle-kit
   ```

## Configure the database schema

This guide demonstrates database schema definition using Drizzle ORM. The underlying principles can be easily adapted to your preferred ORM, such as Prisma, TypeORM, or Sequelize.

1. Create `app/db/schema.ts`:
   The following code defines the database schema for a simple Todo application:

   ```typescript
   import { integer, text, boolean, pgTable } from 'drizzle-orm/pg-core';

   export const todo = pgTable('todo', {
     id: integer('id').primaryKey(),
     text: text('text').notNull(),
     done: boolean('done').default(false).notNull(),
   });
   ```

2. Create `drizzle.config.ts` in your project root:

   ```typescript
   import { config } from 'dotenv';
   import { defineConfig } from 'drizzle-kit';

   config({ path: '.env' });

   export default defineConfig({
     schema: './app/db/schema.ts',
     out: './migrations',
     dialect: 'postgresql',
     dbCredentials: {
       url: process.env.DATABASE_URL!,
     },
   });
   ```

3. Add database scripts to your `package.json`:

   ```json
   {
    ...
     "scripts": {
        ...
       "db:generate": "drizzle-kit generate",
       "db:migrate": "drizzle-kit migrate"
     }
   }
   ```

4. Create a `.env` file in your project root:

   ```bash shouldWrap
   DATABASE_URL=postgres://[user]:[password]@[neon_hostname]/[dbname]?sslmode=require&channel_binding=require
   ```

5. Push your code to a GitHub repository.

## Set up the Neon GitHub integration

The [Neon GitHub integration](/docs/guides/neon-github-integration) connects your Neon project to your application repository and automatically sets a `NEON_API_KEY` secret and `NEON_PROJECT_ID` variable for you. These variables will support the GitHub Actions workflow we'll create in a later step.

1. In the Neon Console, navigate to the **Integrations** page in your Neon project.
2. Locate the **GitHub** card and click **Add**.
   ![GitHub App card](/docs/guides/github_card.png)
3. On the **GitHub** drawer, click **Install GitHub App**.
4. If you have more than one GitHub account, select the account where you want to install the GitHub app.
5. Select the GitHub repository to connect to your Neon project, and click **Connect**.

   The final page of the GitHub integration setup provides a sample GitHub Actions workflow. With this workflow as a example, we'll create a custom GitHub Actions workflow in the next steps.

## Create the GitHub Actions workflow

Create `.github/workflows/neon_workflow.yaml` file and add the following code:

```yaml
name: Create/Delete Branch for Pull Request

on:
  pull_request:
    types:
      - opened
      - reopened
      - synchronize
      - closed

# Ensures only the latest commit runs, preventing race conditions in concurrent PR updates
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}

jobs:
  setup:
    name: Setup
    outputs:
      branch: ${{ steps.branch_name.outputs.current_branch }}
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write
    steps:
      - name: Get branch name
        id: branch_name
        uses: tj-actions/branch-names@v8

  create_neon_branch:
    name: Create Neon Branch
    needs: setup
    if: |
      github.event_name == 'pull_request' && (
      github.event.action == 'synchronize'
      || github.event.action == 'opened'
      || github.event.action == 'reopened')
    runs-on: ubuntu-latest
    steps:
      - name: Create Neon Branch
        id: create_neon_branch
        uses: neondatabase/create-branch-action@v5
        with:
          project_id: ${{ vars.NEON_PROJECT_ID }}
          branch_name: preview/pr-${{ github.event.number }}-${{ needs.setup.outputs.branch }}
          api_key: ${{ secrets.NEON_API_KEY }}

      - name: Checkout
        uses: actions/checkout@v4

      - name: Run Migrations on Preview Branch
        run: npm install && npm run db:generate && npm run db:migrate
        env:
          DATABASE_URL: '${{ steps.create_neon_branch.outputs.db_url }}'

      - name: Post Schema Diff Comment to PR
        uses: neondatabase/schema-diff-action@v1
        with:
          project_id: ${{ vars.NEON_PROJECT_ID }}
          compare_branch: preview/pr-${{ github.event.number }}-${{ needs.setup.outputs.branch }}
          api_key: ${{ secrets.NEON_API_KEY }}

  delete_neon_branch:
    name: Delete Neon Branch and Apply Migrations on Production Database
    needs: setup
    if: |
      github.event_name == 'pull_request' &&
      github.event.action == 'closed'
    runs-on: ubuntu-latest
    steps:
      - name: Delete Neon Branch
        uses: neondatabase/delete-branch-action@v3
        with:
          project_id: ${{ vars.NEON_PROJECT_ID }}
          branch: preview/pr-${{ github.event.number }}-${{ needs.setup.outputs.branch }}
          api_key: ${{ secrets.NEON_API_KEY }}

      - name: Checkout
        if: github.event.pull_request.merged == true
        uses: actions/checkout@v4

      - name: Apply migrations to production
        if: github.event.pull_request.merged == true
        run: |
          npm install
          npm run db:generate
          npm run db:migrate
        env:
          DATABASE_URL: '${{ secrets.DATABASE_URL }}'
```

<Admonition type="note" title="Note">
To set up GitHub Actions correctly:

1. **Enable Workflow Permissions**:
   Go to your repository's GitHub Actions settings, navigate to **Actions** > **General**, and set **Workflow permissions** to **Read and write permissions**.

2. **Add Database Connection String**:
   Add a `DATABASE_URL` secret to your repository under **Settings** > **Secrets and variables** > **Actions**, using the connection string for your production database that you noted earlier. While you're here, you should see the `NEON_API_KEY` secret and `NEON_PROJECT_ID` variable that have already been set by the Neon GitHub integration.

</Admonition>

<Admonition type="tip">
The step outputs from the `create_neon_branch` action will only be available within the same job (`create_neon_branch`). Therefore, write all test code, migrations, and related steps in that job itself. The outputs are marked as secrets. If you need separate jobs, refer to [GitHub's documentation on workflow commands](https://docs.github.com/en/actions/reference/workflows-and-actions/workflow-commands#workflow) for patterns on how to handle this.
</Admonition>

It's important to understand the roles of your GitHub secrets. The `NEON_API_KEY` (created by the integration) is used to manage your Neon project, like creating and deleting branches. The `DATABASE_URL` secret you just created points exclusively to your primary production database. The workflow uses this only after a PR is successfully merged to apply migrations, ensuring a safe separation from the ephemeral preview databases used during testing.

## Understanding the workflow

The GitHub Actions workflow automates database branching and schema management for pull requests. Here's a breakdown of the workflow:

### Create Branch Job

This job runs when a pull request is opened, reopened, or synchronized:

1. **Branch Creation**:
   - Uses Neon's [`create-branch-action`](https://github.com/marketplace/actions/neon-create-branch-github-action) to create a new database branch
   - Names the branch using the pattern `preview/pr-{number}-{branch_name}`
   - Inherits the schema and data from the parent branch

2. **Migration Handling**:
   - Installs project dependencies
   - Generates migration files using Drizzle
   - Applies migrations to the newly created branch
   - Uses the branch-specific `DATABASE_URL` for migration operations

3. **Schema Diff Generation**:
   - Uses Neon's [`schema-diff-action`](https://github.com/marketplace/actions/neon-schema-diff-github-action)
   - Compares the schema of the new branch with the parent branch
   - Automatically posts the differences as a comment on the pull request
   - Helps reviewers understand database changes at a glance

### Delete Branch Job

This job executes when a pull request is closed (either merged or rejected):

1. **Production Migration**:
   - If the PR is merged, applies migrations to the production database
   - Uses the main `DATABASE_URL` stored in repository secrets
   - Ensures production database stays in sync with merged changes

2. **Cleanup**:
   - Removes the preview branch using Neon's [`delete-branch-action`](https://github.com/marketplace/actions/neon-database-delete-branch)

## Flow Summary

Here's how the entire process works from start to finish:

1. Developer creates a new feature branch and makes schema changes
2. When they open a pull request:
   - A new database branch is automatically created
   - Schema migrations are generated and applied
   - A schema diff comment is posted on the PR
3. During PR review:
   - Reviewers can see exactly what database changes are being made
   - The isolated database branch prevents conflicts with other features
   - Additional commits trigger automatic migration updates
4. When the PR is approved and merged:
   - Migrations are automatically applied to the production database
   - The preview branch is deleted
   - The schema changes are now live in production
5. If the PR is closed without merging:
   - The preview branch is automatically deleted
   - No changes are made to the production database

This automated workflow ensures that:

1. Every feature gets its own isolated database environment
2. Schema changes are automatically tracked and documented in the pull request
3. Migrations are consistently applied across environments
4. Production database stays in sync with merged code
5. Database resources are efficiently managed
6. The risk of manual migration errors is minimized

## Test the workflow

To test the workflow, perform the following steps:

1. Create a new feature branch:

   ```bash
   git checkout -b feature/add-todo-created-at
   ```

2. Modify the schema in `db/schema/todos.ts`:

   ```typescript
   export const todo = pgTable('todo', {
     id: integer('id').primaryKey(),
     text: text('text').notNull(),
     done: boolean('done').default(false).notNull(),
     created_at: timestamp('created_at').notNull().defaultNow(),
   });
   ```

3. Commit and push your changes:

   ```bash
   git add .
   git commit -m "feat: add created_at field to todo"
   git push origin feature/add-todo-created-at
   ```

4. Open a pull request on GitHub

The workflow will:

- Create a new database branch for your PR
- Apply the schema migration
- Post a schema diff comment on the PR
  ![Schema Diff Comment](/docs/guides/github_schema_diff_example_comment.png)
- After merging, apply the changes to production

## Source code

You can find the complete source code for this example on GitHub.

<DetailIconCards>
<a href="https://github.com/neondatabase-labs/neon-github-actions-integration" description="Get started with automated database branching using Neon and GitHub Actions" icon="github">Get started with automated database branching</a>
</DetailIconCards>

## Resources

- [Neon GitHub Integration Documentation](/docs/guides/neon-github-integration)
- [Database Branching Workflows](/branching)
- [GitHub Actions Documentation](https://docs.github.com/en/actions)

<NeedHelp/>


# How to set up Neon Local with Docker Compose and JavaScript Postgres clients

---
author: paul-scanlon
enableTableOfContents: true
createdAt: '2025-04-30T00:00:00.000Z'
updatedOn: '2025-04-30T00:00:00.000Z'
title: How to set up Neon Local with Docker Compose and JavaScript Postgres clients
subtitle: A practical guide to Neon Local with JavaScript and Docker Compose for local and production setups
---

ICYMI we recently launched [Neon Local](/blog/make-yourself-at-home-with-neon-local).

## What is Neon Local?

Neon Local is a proxy service that creates a local interface to your Neon cloud database. By default, it automatically creates a new database branch when your container starts and deletes it when the container stops. Your app connects to a local Postgres endpoint while Neon Local handles routing and authentication to the correct project and branch. _This means you don’t have to update connection strings across branches._

Our [docs](/docs/local/neon-local) cover how to use Neon Local with both our [serverless driver](/docs/serverless/serverless-driver) and [pg](https://github.com/brianc/node-postgres), but one area that might cause some confusion is how to switch between Neon Local in **development** and your Neon cloud database in **production**.

In this guide, I’ll show you how to set up your project to work in both development and production environments.

## 1. Example application

To demonstrate, I’ve built a simple React + Vite + Express app. It has one route (`/`) that runs `SELECT version()` and returns the result.

You can find the project here:

- [neon-local-example-application](https://github.com/neondatabase-labs/neon-local-example-react-express-application)

## 2. Setup

### Environment variables

You’ll need to configure the following environment variables. They are also listed in the example application's repository [README](https://github.com/neondatabase-labs/neon-local-example-react-express-application).

| Variable          | Description                    | Example / Value               |
| ----------------- | ------------------------------ | ----------------------------- |
| `DATABASE_URL`    | Connection string for database | _(Set appropriately)_         |
| `NODE_ENV`        | Node environment mode          | `production` or `development` |
| `NEON_API_KEY`    | Neon API authentication key    | `napi_6ngd23amjggx7...`       |
| `NEON_PROJECT_ID` | Neon project identifier        | `rosty-king-89...`            |
| `PORT`            | Server port                    | `8080`                        |

If you need help finding up any of these variables, refer to the following resources:

- [Connection examples](/docs/manage/projects#project-settings)
- [Creating API keys](/docs/manage/api-keys#creating-api-keys)
- [Project settings](/docs/manage/projects#project-settings)

### Docker for Mac

This guide uses Docker for Mac. If you don’t have it installed yet, follow this guide:

- [Install Docker Desktop on Mac](https://docs.docker.com/desktop/setup/install/mac-install/)

![Docker for Mac](/guides/images/neon-local-docker-compose-javascript/neon-local-docker-for-mac.jpg)

## 3. Running the app

If you've cloned the repo, and followed the install instructions, run the app with one of the following commands:

- **development**:  
  `docker compose --profile dev --env-file .env.dev up --watch`
- **production**:  
  `docker compose --profile prod --env-file .env.prod up --build`

Once the app is running, go to [http://localhost:8080/](http://localhost:8080/) in your browser.

## 4. Ephemeral branches

If you started the app in **development** mode, go to the Neon console, and you’ll see a new branch has been created using the `main`, or `production` branch as a base.

<video autoPlay playsInline muted loop controls width="800" height="600">
  <source type="video/mp4" src="/videos/guides/neon-local-docker-compose-javascript/docker-compose-up-watch.mp4"/>
</video>

If you started the app in **production** mode, the app will connect to the database defined by the `DATABASE_URL`, and no new branch will be created.

In the next section, we’ll look at the Docker configuration and how the app determines whether to connect to Neon Local or the cloud instance defined by `DATABASE_URL`.

## 5. Configuration

### Docker Compose

Here’s the `docker-compose.yml` setup, which defines two services. The first, `app`, starts the Express server, responsible for data fetching and server-side rendering of the React app. The second, `db`, configures the Neon Local Docker image.

Additionally, the `app` service defines two profiles, `dev` and `prod`. The `db` service also defines a `dev` profile, ensuring that the `db` service is only used when the app is run in **development** mode.

#### How Docker Profiles Work

Docker profiles allow you to group services within your `docker-compose.yml` file, enabling you to choose which services to start depending on the environment or use case.

- The `app` service has both `dev` and `prod` profiles. This means the `app` can be run in either development or production mode.
- The `db` service is only included in the `dev` profile, meaning it will only run when the app is started in development mode.

```yaml
services:
  app:
    build: .
    ports:
      - '${PORT}:${PORT}'
    volumes:
      - .:/app
      - /app/node_modules
    environment:
      - PORT=${PORT}
      - NODE_ENV=${NODE_ENV}
      - DATABASE_URL=${DATABASE_URL}
      - NEON_API_KEY=${NEON_API_KEY}
      - NEON_PROJECT_ID=${NEON_PROJECT_ID}
    profiles:
      - dev
      - prod

  db:
    image: neondatabase/neon_local:latest
    ports:
      - '5432:5432'
    environment:
      NEON_API_KEY: ${NEON_API_KEY}
      NEON_PROJECT_ID: ${NEON_PROJECT_ID}
      DRIVER: serverless
    profiles:
      - dev
```

You can view the `src` of this file in the repository: [docker-compose.yml](https://github.com/neondatabase-labs/neon-local-example-react-express-application/blob/main/docker-compose.yml)

## 6. Connecting to the database

### Serverless Driver

The database connection is established using our [serverless driver](/docs/serverless/serverless-driver). It uses a ternary operation to determine whether to connect to Neon Local or the `DATABASE_URL`, depending on the mode the application is running in.

```javascript
import 'dotenv/config';

import { neon, neonConfig } from '@neondatabase/serverless';

if (process.env.NODE_ENV !== 'prod') {
  neonConfig.fetchEndpoint = 'http://db:5432/sql';
}

const connectionString =
  process.env.NODE_ENV === 'prod' ? process.env.DATABASE_URL : 'postgres://neon:npg@db:5432/neondb';

export const sql = neon(connectionString);
```

You can view the `src` of this file in the repository: [src/db.js](https://github.com/neondatabase-labs/neon-local-example-react-express-application/blob/main/src/db.js):

### node-postgres

Alternatively, if you prefer to use `pg`, here's how the connection is configured. Note that you'll need to add `?sslmode=no-verify` to the end of the Neon Local connection string.

```javascript
import 'dotenv/config';

import pg from 'pg';
const { Pool } = pg;

const connectionString =
  process.env.NODE_ENV === 'prod'
    ? process.env.DATABASE_URL
    : 'postgres://neon:npg@db:5432/neondb?sslmode=no-verify';

export const pool = new Pool({ connectionString });
```

Additionally, you'll also need to change the `DRIVER` to `postgres` in your `docker-compose.yml` file:

```diff
environment:
  NEON_API_KEY: ${NEON_API_KEY}
  NEON_PROJECT_ID: ${NEON_PROJECT_ID}
  DRIVER: serverless // [!code --]
  DRIVER: postgres // [!code ++]

```

## Wrapping up

And that’s it. By default, Neon Local handles creating and deleting a branch whenever you start or stop the container. If you want more control, such as setting a parent branch or disabling branch deletion, check out the [configuration options in the docs](/docs/local/neon-local).

Neon Local simplifies the management of temporary database environments, making it easier to work with isolated instances for testing or short-term use. While it’s **not** a fully "local" database, it streamlines the workflow, especially for CI/CD pipelines where short-lived environments are needed to run tests but don’t need to stick around.

Neon Local is still in its early stages, with several improvements on the way. But for now, it could be exactly what you need to streamline your workflows. Give it a try today and [share your feedback with us](https://github.com/neondatabase-labs/neon_local).


# Getting started with Neon Local and Neon Local Connect

---
title: Getting started with Neon Local and Neon Local Connect
subtitle: Learn how to set up and use Neon Local and Neon Local Connect for seamless local development with Neon
author: 'dhanush-reddy'
enableTableOfContents: true
createdAt: '2025-08-17T00:00:00.000Z'
updatedOn: '2025-08-17T00:00:00.000Z'
---

One of Neon's most powerful features is database branching, the ability to instantly create isolated, copy-on-write clones of your database for any task. Just as you create a Git branch for every new feature or bug fix, you can create a parallel database branch. This eliminates environment drift, prevents developers from overwriting each other's work on shared staging databases, and ensures every development environment is a perfect, isolated replica of production.

But how do you bring this cloud-native power into your local development workflow seamlessly? Constantly switching connection strings for each branch is tedious and error-prone. This is the problem that **Neon Local** and the **Neon Local Connect** VS Code extension solve. They act as a smart local proxy to your Neon database in the cloud. Your application connects to a single, static `localhost` address, while the tools handle all the complexity of routing and branch management behind the scenes.

This guide will walk you through setting up and using both Neon Local and Neon Local Connect to create a powerful, modern development workflow. You'll learn how to:

- Install and configure the Neon Local Connect extension in VS Code.
- Utilize the built-in Schema Viewer, SQL Editor, and Table Editor.
- Connect your local application to a Neon database using a single, static `localhost` connection string.
- Manage database branches (create, switch, reset) directly from your IDE.
- Use Neon Local with Docker Compose for CI/CD or non-VS Code environments.

<Admonition type="note" title="Neon Local vs. a Local Postgres Instance">
This guide focuses on **Neon Local**, a **local proxy** for your **cloud-hosted Neon Postgres database**. It enables you to use Neon's powerful branching features with a convenient `localhost` connection, allowing you to seamlessly switch between branches, create new branches, and manage them directly from your IDE.

This is different from [Local Development with Neon](/guides/local-development-with-neon) guide, which shows you how to run a completely separate, **local instance of Postgres** for fully offline development.

For most modern workflows that leverage Neon's features, the proxy-based approach in this guide is recommended.
</Admonition>

## Prerequisites

Before you begin, ensure you have the following:

- **Neon account:** A free Neon account. If you don't have one, sign up at [Neon](https://console.neon.tech/signup).
- **VS Code:** Or any compatible editor based on VS Code, such as Cursor or Windsurf.
- **Docker:** Docker Desktop must be installed and running on your machine. You can download it from [docker.com](https://www.docker.com/products/docker-desktop/).
- **Node.js:** Version `18` or later to run the example application.

## Neon Local Connect

The easiest way to get started is with the Neon Local Connect VS Code extension. It manages the underlying Docker container for you, providing a full control panel for your Neon database within your editor.

### Install the extension

First, install the extension from the Visual Studio Marketplace or OpenVSX.

1.  Open your editor and navigate to the **Extensions** view (`Ctrl+Shift+X` or `Cmd+Shift+X`).
2.  Search for "Neon Local Connect".
3.  Click **Install**.
    ![Neon Local Connect Extension](/docs/local/extension-in-vs-code.png)

You can also install it directly from the marketplace:

<DetailIconCards>

<a href="https://marketplace.visualstudio.com/items?itemName=databricks.neon-local-connect" description="For VS Code & compatible editors" icon="vscode">VS Code Marketplace</a>

<a href="https://open-vsx.org/extension/databricks/neon-local-connect" description="For Cursor, Windsurf etc." icon="download">OpenVSX Marketplace</a>

</DetailIconCards>

### Connect to your Neon account

Once installed, a new Neon icon will appear in your Activity Bar.

1.  Click the Neon icon to open the Neon Local Connect panel.
2.  Click **Sign in with Neon**. This will open a browser window to authenticate your Neon account using OAuth.
    ![Sign in with your Neon account](/docs/local/sign-in.png)
3.  Authorize the application to connect to your Neon account.
    ![Neon OAuth authorization in browser](/docs/local/authorize.png)

### Connect to a Database branch

After authenticating, the extension fetches your Neon projects and branches.

1.  Select your **Organization** and **Project**.
2.  Choose the **Branch** you want to work on (e.g., `development`). You can connect to an **existing branch** or an **ephemeral branch** that is created on connection and destroyed on disconnection.
    <Admonition type="note" title="Using ephemeral branches">
    For using ephemeral branches, you need to authenticate using a Neon API key. This is recommended to ensure that temporary branches are automatically cleaned up when your session ends. To get started, see [Creating API keys](/docs/manage/api-keys). In the Neon Local Connect panel, click **Import API Key** to add your key.
    </Admonition>
3.  Click **Connect**.

The extension will now start a Neon Local Docker container in the background and establish a proxy connection to your selected branch. Once connected, the panel will display a static, local connection string.

The connection string will always be the same, regardless of which branch you connect to:
`postgres://neon:npg@localhost:5432/<database_name>`

### Extension features

Neon Local Connect turns your IDE into a powerful database management tool, eliminating the need to switch contexts.

#### Database schema view

Once connected, a **Database Schema** view appears in the sidebar. This tree view lets you explore your entire database structure: databases, schemas, tables, columns, and relationships (PKs, FKs). Right-click any table for quick actions like **Query Table**, **View Table Data**, **Truncate**, or **Drop**.

![Database Schema View](/docs/local/database_schema_view.png)

#### Built-in SQL editor

Execute queries directly in your IDE. Right-click a table and select "Query Table" to open a pre-filled `SELECT *` query, or open a blank editor from the command palette.

- **View results** in a filterable, sortable table.
- **Export data** to CSV/JSON.
- **See performance stats** and detailed error messages.

  ![SQL Editor in your IDE](/docs/local/sql_editor_view.png)

#### Table data management

For quick edits, right-click a table and select "View Table Data" to open a spreadsheet-like interface.

- **Edit rows** by clicking the pen (edit) icon next to any row (requires a primary key).
- **Insert and delete rows** with dedicated buttons.
- **Paginate** through large datasets.
- Changes are applied to your database immediately.

  ![Table Data Editor](/docs/local/table_data_view.png)

#### Branch management from the panel

The Neon Local Connect panel also provides easy branch management:

- **Create a new branch:** Click the "Branch" dropdown, select "Create new branch...", give it a name, and choose a parent.
- **Switch branches:** Simply select a different branch from the dropdown. Your `localhost` connection will now point to the new branch no code changes needed.
- **Reset a branch:** Revert a branch to its parent's state to discard changes and get a clean slate. To reset a branch, right click the branch in **Database schema view** and select "Reset from Parent Branch".
  ![Reset Branch](/docs/local/reset.png)

### Connect your application

Add the static connection string from the extension panel to your project's `.env.local` file.

```ini
DATABASE_URL="postgres://neon:npg@localhost:5432/<database_name>"
```

> Replace `<database_name>` with your actual database name

Your app now connects to `localhost:5432`, and Neon Local securely routes traffic to the active cloud branch.

Follow the [Typical development workflow](#typical-development-workflow) section for understanding how to leverage Neon Local effectively.

## Neon Local

For non-VS Code users or CI/CD integration, you can use Neon Local directly. This gives you the same power, controlled in a programmatic way.

Neon Local is a Docker-based proxy that connects to your Neon database, allowing you to run a local instance of your cloud database. It provides a static connection string (`localhost:5432`) that routes to the active branch, making it easy to switch branches without changing your code.

### Docker compose configuration

Here is a `docker-compose.yml` that defines your `app` and the `db` (Neon Local) service.

```yaml
services:
  app:
    build: .
    ports:
      - '${PORT}:${PORT}'
    environment:
      - DATABASE_URL="postgres://neon:npg@db:5432/${DB_NAME}?sslmode=no-verify"
    depends_on:
      - db

  db:
    image: neondatabase/neon_local:latest
    ports:
      - '5432:5432'
    environment:
      - NEON_API_KEY=${NEON_API_KEY}
      - NEON_PROJECT_ID=${NEON_PROJECT_ID}
      # Choose one of the following:
      - BRANCH_ID=${BRANCH_ID} # For existing branches
      # - PARENT_BRANCH_ID=${PARENT_BRANCH_ID} # For ephemeral branches
```

#### Key environment variables

| Variable           | Description                                                                               | Required | Default                  |
| ------------------ | ----------------------------------------------------------------------------------------- | -------- | ------------------------ |
| `NEON_API_KEY`     | Your Neon API key.                                                                        | Yes      | N/A                      |
| `NEON_PROJECT_ID`  | Your Neon project ID.                                                                     | Yes      | N/A                      |
| `BRANCH_ID`        | Connects to a specific existing branch. Mutually exclusive with `PARENT_BRANCH_ID`.       | No       | N/A                      |
| `PARENT_BRANCH_ID` | Creates an ephemeral branch from a parent. If omitted, uses the project's default branch. | No       | Project's default branch |
| `DELETE_BRANCH`    | Set to `false` to prevent branches from being deleted when the container stops.           | No       | `true`                   |

If you need to use the `docker run` command instead of Docker Compose, you can checkout [Neon Local Docs](/docs/local/neon-local) for more information.

### Advanced configuration

#### Persistent branches per Git Branch

For a workflow where a database branch's lifecycle matches a Git branch, you can configure Neon Local to persist its state.

Add `volumes` to your `db` service in `docker-compose.yml`:

```yaml
db:
  image: neondatabase/neon_local:latest
  ports:
    - '5432:5432'
  environment:
    NEON_API_KEY: ${NEON_API_KEY}
    NEON_PROJECT_ID: ${NEON_PROJECT_ID}
    DELETE_BRANCH: false
  volumes:
    - ./.neon_local/:/tmp/.neon_local
    - ./.git/HEAD:/tmp/.git/HEAD:ro,consistent
```

This configuration uses your current Git branch name to manage a persistent database branch.

<Admonition type="note">
This will create a `.neon_local` directory in your project to store metadata. Be sure to add `.neon_local/` to your `.gitignore` to avoid committing database information.
</Admonition>

<Admonition type="note" title="Git integration using Docker on Mac">
If using Docker Desktop for Mac, ensure that your VM settings use **gRPC FUSE** instead of **VirtioFS**. There is currently a known bug with VirtioFS that prevents proper branch detection and live updates inside containers.
  ![Docker Desktop are set to gRPC FUSE](/docs/local/neon-local-docker-settings.jpg)
</Admonition>

## Typical development workflow

Neon's branching is designed to integrate seamlessly with modern, Git-based development workflows. By pairing each Git branch with a corresponding database branch, you can ensure complete isolation, prevent conflicts, and maintain a clean production database.

Here’s a practical look at how to use Neon Local in your daily tasks.

#### The scenario: Starting a new task

You've just been assigned a ticket to build a new user profile page. The first step is always to create a new Git branch to isolate your code changes.

```bash
git checkout main
git pull
git checkout -b feature/new-user-profile
```

Now that your code is isolated, you need to isolate your database. You have two primary options depending on the scope of your task.

#### Option 1: Long-lived feature

**When to use it:** This is the standard approach for most feature work, bug fixes that require review, or any task that will span multiple sessions or involve collaboration. You create a persistent database branch that mirrors the lifecycle of your Git branch.

<Tabs labels={["Using Neon Local Connect", "Using Neon Local (CLI)"]}>
<TabItem>

With the VS Code extension, creating a persistent branch is trivial:

1.  In the Neon Local Connect panel, click the **Branch** dropdown menu.
2.  Select **Create new branch...**.
3.  Enter a name for your branch. It's good practice to match your Git branch name, like `feature/new-user-profile`.
4.  Choose a parent branch to copy data and schema from (e.g., `production` or `development`).
5.  The extension will instantly create the branch and connect you to it. Your `localhost` connection now points to this new, isolated environment.

</TabItem>
<TabItem>

When using the CLI, you create the branch in the Neon Console and then configure Neon Local to connect to it.

1.  Navigate to your project in the **[Neon Console](https://console.neon.tech/)**.
2.  Go to the **Branches** tab and click **New Branch**.
3.  Name the branch (`feature/new-user-profile`) and select a parent.
4.  Once created, copy the **Branch ID** from the branch details.
5.  In your `docker-compose.yml`, ensure the `db` service is configured to use this specific `BRANCH_ID`.

    ```yaml
    services:
      db:
        # ... other settings
        environment:
          - NEON_API_KEY=${NEON_API_KEY}
          - NEON_PROJECT_ID=${NEON_PROJECT_ID}
          - BRANCH_ID=<your_copied_branch_id> # Connect to the specific branch
    ```

6.  Run `docker compose up` to start the proxy connected to your new feature branch.

</TabItem>
</Tabs>

#### Option 2: Quick experiment or test

**When to use it:** Perfect for quick bug fixes, running a single test suite, or experimenting with a schema change that you might throw away. An ephemeral branch is created on-the-fly and automatically deleted when you're done.

<Tabs labels={["Using Neon Local Connect", "Using Neon Local (CLI)"]}>
<TabItem>

1.  In the Neon Local Connect panel, click the **Connection Type** dropdown menu.
2.  Instead of selecting "Connect to Neon Branch", choose **Connect to ephemeral Neon branch** option.
3.  Under **Branch**, select a parent branch (e.g., `production`) to base the ephemeral branch on.
4.  Click **Connect**.

The extension creates a temporary branch for your session. When you click **Disconnect**, the branch and all its changes are automatically deleted from your Neon project, leaving no trace.

<Admonition type="note">
For using ephemeral branches, you need to authenticate using a Neon API key. To get started, see [Creating API keys](/docs/manage/api-keys).
</Admonition>

</TabItem>
<TabItem>

With the CLI, you create an ephemeral branch by specifying a `PARENT_BRANCH_ID` instead of a `BRANCH_ID`.

1.  In the **[Neon Console](https://console.neon.tech/)**, find the **Branch ID** of the branch you want to use as a parent (e.g., your `production` or `development` branch).
2.  In your `docker-compose.yml`, configure the `db` service to use this parent ID.

    ```yaml
    services:
      db:
        # ... other settings
        environment:
          - NEON_API_KEY=${NEON_API_KEY}
          - NEON_PROJECT_ID=${NEON_PROJECT_ID}
          - PARENT_BRANCH_ID=<your_parent_branch_id> # Create ephemeral branch from this parent
    ```

3.  Run `docker compose up`. Neon Local will create a new, temporary branch from this parent.
4.  When you're finished, run `docker compose down`. The ephemeral branch will be automatically deleted from your Neon project.

</TabItem>
</Tabs>

## Connecting your application conditionally

Your application code needs to seamlessly switch between connecting to Neon Local for development and your live Neon database for production. The standard way to manage this is by using the `NODE_ENV` environment variable.

The core logic is straightforward: when `process.env.NODE_ENV` is set to `'development'`, your application should use the static `localhost` connection string provided by Neon Local. For any other environment (such as `'production'` on platforms like Vercel, AWS, or other cloud providers), your app should use the actual Neon database URL, typically stored in your deployment environment's configuration or secrets on your cloud provider.

The implementation details vary slightly depending on the database driver or ORM you are using.

<Tabs labels={["@neondatabase/serverless", "Drizzle ORM", "Prisma", "Other drivers"]}>
<TabItem>

The Neon serverless driver is designed to communicate with a Neon database over HTTP/WebSocket. To redirect this traffic to your local Neon Local proxy, you must override its default behavior in your development environment.

This is done by reconfiguring `neonConfig` to point to `localhost`.

1. Install Dependencies

   <CodeTabs labels={["npm", "yarn", "pnpm"]}>

   ```bash
   npm install @neondatabase/serverless ws
   ```

   ```bash
   yarn add @neondatabase/serverless ws
   ```

   ```bash
   pnpm add @neondatabase/serverless ws
   ```

   </CodeTabs>

2. **Configure the connection**

   ```typescript
   import { neon, neonConfig, Pool } from '@neondatabase/serverless';
   import ws from 'ws';

   let connectionString =
     process.env.DATABASE_URL || 'postgres://neon:npg@localhost:5432/<database_name>';

   if (process.env.NODE_ENV === 'development') {
     // Point the serverless driver to the local proxy
     neonConfig.fetchEndpoint = 'http://localhost:5432/sql';
     neonConfig.poolQueryViaFetch = true;
   }

   // Use the WebSocket constructor for Node.js
   neonConfig.webSocketConstructor = ws;

   // Neon supports both HTTP and WebSocket clients. Choose the one that fits your needs:

   // HTTP Client (sql)
   // - Best for serverless functions and Lambda environments
   // - Ideal for stateless operations and quick queries
   // - Lower overhead for single queries
   // - Better for applications with sporadic database access
   export const sql = neon(connectionString);

   // WebSocket Client (pool)
   // - Best for long-running applications (like servers)
   // - Maintains a persistent connection
   // - More efficient for multiple sequential queries
   // - Better for high-frequency database operations
   export const pool = new Pool({ connectionString });
   ```

</TabItem>
<TabItem>

> If you’re using `drizzle-orm` with the standard Postgres wire protocol (not the Neon serverless adapter), refer to the **Other drivers** section.

Using Drizzle with Neon’s serverless adapters requires a similar setup to the one used for the Neon serverless driver directly: configure `neonConfig` to point to your local Neon Local proxy.

1. Install Dependencies

   <CodeTabs labels={["npm", "yarn", "pnpm"]}>

   ```bash
   npm install drizzle-orm @neondatabase/serverless ws
   ```

   ```bash
   yarn add drizzle-orm @neondatabase/serverless ws
   ```

   ```bash
   pnpm add drizzle-orm @neondatabase/serverless ws
   ```

   </CodeTabs>

2. **Configure the connection**

   ```typescript
   import { neon, neonConfig, Pool } from '@neondatabase/serverless';
   import { drizzle as drizzleWs } from 'drizzle-orm/neon-serverless';
   import { drizzle as drizzleHttp } from 'drizzle-orm/neon-http';
   import ws from 'ws';

   let connectionString =
     process.env.DATABASE_URL || 'postgres://neon:npg@localhost:5432/<database_name>';

   if (process.env.NODE_ENV === 'development') {
     // Point the serverless driver to the local proxy
     neonConfig.fetchEndpoint = 'http://localhost:5432/sql';
     neonConfig.poolQueryViaFetch = true;
   }

   // Use the WebSocket constructor for Node.js
   neonConfig.webSocketConstructor = ws;

   const sql = neon(connectionString);
   const pool = new Pool({ connectionString });

   // Drizzle supports both HTTP and WebSocket clients. Choose the one that fits your needs:

   // HTTP Client:
   // - Best for serverless functions and Lambda environments
   // - Ideal for stateless operations and quick queries
   // - Lower overhead for single queries
   // - Better for applications with sporadic database access
   export const drizzleClientHttp = drizzleHttp({ client: sql });

   // WebSocket Client:
   // - Best for long-running applications (like servers)
   // - Maintains a persistent connection
   // - More efficient for multiple sequential queries
   // - Better for high-frequency database operations
   export const drizzleClientWs = drizzleWs({ client: pool });
   ```

</TabItem>
<TabItem>

> If you are using `prisma` with the standard Postgres wire protocol (not the Neon serverless adapter), refer to the **Other drivers** section.

Using Prisma with Neon’s serverless adapters requires a similar setup to the one used for the Neon serverless driver directly: configure `neonConfig` to point to your local Neon Local proxy.

1. Install Dependencies

   <CodeTabs labels={["npm", "yarn", "pnpm"]}>

   ```bash
   npm install @prisma/client @prisma/adapter-neon @neondatabase/serverless ws
   ```

   ```bash
   yarn add @prisma/client @prisma/adapter-neon @neondatabase/serverless ws
   ```

   ```bash
   pnpm add @prisma/client @prisma/adapter-neon @neondatabase/serverless ws
   ```

   </CodeTabs>

2. **Enable the Preview Flag**

   To use the Neon serverless driver with Prisma, enable the preview flag in your `schema.prisma` file.

   ```prisma
   // schema.prisma
   generator client {
     provider        = "prisma-client-js"
     previewFeatures = ["driverAdapters"]
   }
   ```

3. **Configure the connection**

   ```typescript
   import { neonConfig } from '@neondatabase/serverless';
   import { PrismaNeon, PrismaNeonHTTP } from '@prisma/adapter-neon';
   import { PrismaClient } from './generated/prisma/client.js';
   import ws from 'ws';

   const connectionString =
     process.env.DATABASE_URL || 'postgres://neon:npg@localhost:5432/<database_name>';

   if (process.env.NODE_ENV === 'development') {
     // Point the serverless driver to the local proxy
     neonConfig.fetchEndpoint = 'http://localhost:5432/sql';
     neonConfig.poolQueryViaFetch = true;
   }

   // Use the WebSocket constructor for Node.js
   neonConfig.webSocketConstructor = ws;

   // Prisma supports both HTTP and WebSocket clients. Choose the one that fits your needs:
   // HTTP Client:
   // - Ideal for stateless operations and quick queries
   // - Lower overhead for single queries
   const adapterHttp = new PrismaNeonHTTP(connectionString, {});
   export const prismaClientHttp = new PrismaClient({ adapter: adapterHttp });

   // WebSocket Client:
   // - Best for long-running applications (like servers)
   // - Maintains a persistent connection
   // - More efficient for multiple sequential queries
   // - Better for high-frequency database operations
   const adapterWs = new PrismaNeon({ connectionString });
   export const prismaClientWs = new PrismaClient({ adapter: adapterWs });
   ```

</TabItem>

<TabItem>

Standard PostgreSQL drivers like `node-postgres` (`pg`) do not require major changes because they communicate over the standard Postgres wire protocol. Neon Local exposes a standard Postgres endpoint on `localhost:5432`.

The only change needed is to switch the connection string and adjust the SSL setting. SSL is required for production connections to Neon but is not needed for the local proxy.

```typescript
import { Client, Pool } from 'pg';

const connectionString =
  process.env.DATABASE_URL || 'postgres://neon:npg@localhost:5432/<database_name>';
let sslConfig;

if (process.env.NODE_ENV === 'development') {
  sslConfig = { rejectUnauthorized: false };
}

const pool = new Pool({
  connectionString,
  ssl: sslConfig,
});

const client = new Client({
  connectionString,
  ssl: sslConfig,
});

export { pool, client };
```

For **Drizzle ORM** using the standard Postgres driver is straightforward. You simply configure the connection string and SSL settings based on your environment:

```typescript
import { drizzle } from 'drizzle-orm/node-postgres';

let connectionString =
  process.env.DATABASE_URL || 'postgres://neon:npg@localhost:5432/<database_name>';
let sslConfig;

if (process.env.NODE_ENV === 'development') {
  sslConfig = { rejectUnauthorized: false };
}

export const drizzleClient = drizzle({
  connection: {
    connectionString,
    ssl: sslConfig,
  },
});
```

For **Prisma** with the standard Postgres driver you again only need to set the database URL appropriately for your environment. Prisma will handle the rest:

```typescript
import { PrismaClient } from './generated/prisma/client.js';

const prismaClient = new PrismaClient({
  datasources: {
    db: {
      url: process.env.DATABASE_URL || 'postgres://neon:npg@localhost:5432/<database_name>',
    },
  },
});
```

- In any case, use the local Neon Local connection string for development and your production Neon connection string in deployed environments.
- Conditionally disabling SSL for local development ensures compatibility with the Neon Local proxy, while production connections remain secure.

</TabItem>
</Tabs>

<Admonition type="note" title="Other languages and drivers">
The `neonConfig` setup is **exclusive to the `@neondatabase/serverless` driver** and its wrappers (Drizzle, Prisma adapter) in Node.js environments.

For applications written in other languages (like Python, Go, Ruby, Java, etc.) that use standard PostgreSQL drivers, you can follow the same pattern as the `node-postgres (pg)` example:

1.  Read the database connection string from an environment variable.
2.  In your local development environment, set this variable to `postgres://neon:npg@localhost:5432/<database_name>`.
3.  In production, set it to your real Neon connection string.
4.  You may need to conditionally disable SSL for the local connection.

No other code modifications are necessary.
</Admonition>

## Summary

You've now learned how to integrate Neon local and Neon Local Connect into your workflow. With Neon Local Connect, you get a full-featured database IDE that makes managing branches as easy as managing Git branches. For more advanced or automated setups, Neon Local with Docker provides the same powerful capabilities for any environment, ensuring you always have clean, isolated, and production-like databases for development and testing.

## Resources

- [Neon Local Documentation](/docs/local/neon-local)
- [Neon Local Connect extension guide](/docs/local/neon-local-vscode)
- [Example application using Neon Local](https://github.com/neondatabase-labs/neon-local-example-react-express-application)

<NeedHelp/>


# How to Use Neon MCP Server with GitHub Copilot in VS Code

---
title: 'How to Use Neon MCP Server with GitHub Copilot in VS Code'
subtitle: 'Learn how to make GitHub Copilot your full-stack teammate'
author: boburmirzo
enableTableOfContents: true
createdAt: '2025-05-10T00:00:00.000Z'
updatedOn: '2025-05-10T00:00:00.000Z'
---

GitHub Copilot has changed how developers write code, but when combined with an [MCP](https://modelcontextprotocol.io/) (Model Copilot Protocol) server, it also connects your services. With MCP, Copilot can create database tables, understand your database schema and generate relevant code for your API, data models, or business logic.

In this guide, you'll learn how to use the [**Neon Serverless Postgres MCP server**](https://github.com/neondatabase-labs/mcp-server-neon) with **GitHub Copilot in VS Code** to build a sample REST API quickly. We'll walk through how to create an **Azure Function** that fetches data from a **Neon database**, all without writing a single line of code manually.

![GitHub Copilot with Neon MCP Server in VS Code demo](/docs/guides/github-copilot-neon-mcp-server/github-copilot-with-neon-mcp-server-demo.gif)

## From Code Generation to Database Management with GitHub Copilot

AI agents are no longer just helping write code—they’re creating and managing databases. When a chatbot logs a customer conversation, or a new region spins up in the Azure cloud, or a new user signs up, an AI agent can automatically create a database in seconds. No need to open a dashboard or call an API. This is the next evolution of software development: **infrastructure as intent**. With tools like database MCP servers, agents can now generate real backend services as easily as they generate code.

GitHub Copilot becomes your full-stack teammate. It can answer database-related questions, fetch your database connection string, update environment variables in your Azure Function, generate SQL queries to populate tables with mock data, and even help you create new databases or tables on the fly. All directly from your editor, with natural language prompts. Neon has a dedicated [MCP server](https://github.com/neondatabase-labs/mcp-server-neon) that makes it possible for Copilot to directly understand the structure of your Postgres database.

Let's get started with using the Neon MCP server and GitHub Copilot.

<Admonition type="important" title="Neon MCP Server Security Considerations">
The Neon MCP Server grants powerful database management capabilities through natural language requests. **Always review and authorize actions requested by the LLM before execution.** Ensure that only authorized users and applications have access to the Neon MCP Server.

The Neon MCP Server is intended for local development and IDE integrations only. **We do not recommend using the Neon MCP Server in production environments.** It can execute powerful operations that may lead to accidental or unauthorized changes.

For more information, see [MCP security guidance →](/docs/ai/neon-mcp-server#mcp-security-guidance).
</Admonition>

## What You'll Need

- Node.js (>= v18.0.0) and npm: Download from [nodejs.org](https://nodejs.org/).
- An Azure subscription ([create one](https://azure.microsoft.com/free/cognitive-services) for free)
- Install either the stable or Insiders release of VS Code:
  - [Stable release](https://code.visualstudio.com/download)
  - [Insiders release](https://code.visualstudio.com/insiders)
- Install the [GitHub Copilot](https://marketplace.visualstudio.com/items?itemName=GitHub.copilot), [GitHub Copilot for Azure](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azure-github-copilot), and [GitHub Copilot Chat](https://marketplace.visualstudio.com/items?itemName=GitHub.copilot-chat) extensions for VS Code
- [Azure Functions Core Tools](https://learn.microsoft.com/en-us/azure/azure-functions/functions-run-local?tabs=macos%2Cisolated-process%2Cnode-v4%2Cpython-v2%2Chttp-trigger%2Ccontainer-apps&pivots=programming-language-csharp)  (for local testing)

## Connect GitHub Copilot to the Neon MCP Server

### Create Neon Database

Visit the [Neon on Azure Marketplace](https://portal.azure.com/#view/Azure_Marketplace_Neon/NeonCreateResource.ReactView) page and follow the [Create a Neon resource](/docs/azure/azure-deploy#create-a-neon-resource) guide to deploy Neon on Azure for your subscription. Neon offers a [Free plan](/pricing) that provides more than enough resources to build a proof of concept or kick off a new startup project.

### Install the Neon MCP Server for VS Code

Neon MCP Server offers two options for connecting your VS Code MCP client to Neon. We will use the [Remote Hosted MCP Server option](https://github.com/neondatabase-labs/mcp-server-neon?tab=readme-ov-file#option-1-remote-hosted-mcp-server-preview). This is the simplest setup — no need to install anything locally or configure a Neon API key in your client.

In your project directory, create a new file named `.vscode/mcp.json` and add the following configuration:

```json
{
  "servers": {
    "Neon": {
      "url": "https://mcp.neon.tech/mcp",
      "type": "http"
    }
  },
  "inputs": []
}
```

<Admonition type="note">
By default, the Remote MCP Server connects to your personal Neon account. To connect to an organization's account, you must authenticate with an API key. For more information, see [API key-based authentication](/docs/ai/neon-mcp-server#api-key-based-authentication).
</Admonition>

Click on `Start` on the MCP server. A browser window will open with an OAuth prompt. Just follow the steps to give your VS Code MCP client access to your Neon account.

![Start Neon MCP Server in GitHub Copilot](/docs/guides/github-copilot-neon-mcp-server/github-copilot-start-neon-mcp-server.png)

## Generate an Azure Function REST API using GitHub Copilot

**Step 1:** Create an empty folder (ex: *mcp-server-vs-code*) and open it in VS Code.

**Step 2:** Open GitHub Copilot Chat in VS Code and [switch to Agent mode](https://code.visualstudio.com/docs/copilot/chat/chat-agent-mode). You should see the available tools.

![List Neon MCP Server tools in GitHub Copilot](/docs/guides/github-copilot-neon-mcp-server/github-copilot-neon-mcp-server-tools.png)

**Step 3:** Ask Copilot something like "_Create an Azure function with an HTTP trigger_”:

![Create an Azure function in GitHub Copilot](/docs/guides/github-copilot-neon-mcp-server/github-copilot-create-azure-functions.png)

Copilot will generate a REST API using Azure Functions in JavaScript with a basic setup to run the functions locally.

**Step 4:** Next, you can ask to list existing Neon projects:

![list existing Neon projects in GitHub Copilot](/docs/guides/github-copilot-neon-mcp-server/github-copilot-list-neon-mcp-server-tools.png)

**Step 5:** Try out different prompts to fetch the connection string for the chosen database and set it to the Azure Functions settings. Then ask to create a sample Customer table and so on. Or you can even prompt to create a new [database branch](/docs/introduction/branching) on Neon.

![Fetch data from Neon in GitHub Copilot](/docs/guides/github-copilot-neon-mcp-server/github-copilot-fetch-neon-connection-string.png)

**Step 6:** Finally, you can prompt Copilot to update the Azure functions code to fetch data from the table:

![Update Azure Functions in GitHub Copilot](/docs/guides/github-copilot-neon-mcp-server/github-copilot-update-azure-functions.png)

## Combine the Azure MCP Server

Neon MCP gives GitHub Copilot full access to your database schema, so it can help you write SQL queries, connect to the database, and build API endpoints. But when you add **[Azure MCP](https://github.com/Azure/azure-mcp?tab=readme-ov-file)** into the mix, Copilot can also understand your Azure services—like Blob Storage, Queues, and Azure AI.

You can run both **Neon MCP** and **Azure MCP** at the same time to create a full cloud-native developer experience. For example:

- Use **Neon MCP** for serverless Postgres with branching and instant scale.
- Use **Azure MCP** to connect to other Azure services from the same Copilot chat.

Even better: **Azure MCP is evolving**. In newer versions, you can spin up **Azure Functions and other services directly from Copilot chat**, without ever leaving your editor.

Copilot pulls context from both MCP servers, which means smarter code suggestions and faster development. You can mix and match based on your stack and let Copilot help you build real, working apps in minutes.

## Final Thoughts

With GitHub Copilot, Neon MCP server, and Azure Functions, you're no longer writing backend code line by line. It is so fast to build APIs. You're _orchestrating_ services with intent. This is not the future—it’s something you can use today.

## Resources

- [Neon on Azure](/docs/manage/azure)
- [Neon MCP Server](/docs/ai/neon-mcp-server)

<NeedHelp />


# Get started with Claude Desktop and Neon MCP Server

---
title: 'Get started with Claude Desktop and Neon MCP Server'
subtitle: 'Enable natural language interaction with your Neon Postgres databases using LLMs'
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-02-06T00:00:00.000Z'
updatedOn: '2025-02-06T00:00:00.000Z'
---

Imagine managing your database with natural language. Instead of complex SQL, you can simply ask your AI assistant to "create a new table for customer orders" or "show me last quarter's sales figures." This is the power of the [Model Context Protocol (MCP)](https://github.com/modelcontextprotocol), an open standard for AI interaction with external systems.

This guide will introduce you to [Neon's MCP server](https://github.com/neondatabase/mcp-server-neon), which allows you to use Large Language Models (LLMs) for intuitive database management. At its core, Neon MCP server allows tools like Claude to easily communicate with the [Neon API](https://api-docs.neon.tech/reference/getting-started-with-neon-api).

With Neon's MCP server and an LLM like Claude, you can simplify workflows, improve productivity, and manage your Postgres databases more naturally. Let's explore how this approach can make database management easier and more efficient.

<Admonition type="important" title="Neon MCP Server Security Considerations">
The Neon MCP Server grants powerful database management capabilities through natural language requests. **Always review and authorize actions requested by the LLM before execution.** Ensure that only authorized users and applications have access to the Neon MCP Server.

The Neon MCP Server is intended for local development and IDE integrations only. **We do not recommend using the Neon MCP Server in production environments.** It can execute powerful operations that may lead to accidental or unauthorized changes.

For more information, see [MCP security guidance →](/docs/ai/neon-mcp-server#mcp-security-guidance).
</Admonition>

## Understanding MCP

The **Model Context Protocol (MCP)** is a standard that helps LLMs communicate with external tools, like databases and APIs. It's like a translator, making it easier to connect LLMs to services and data. For the Neon MCP server, it's the protocol that lets Claude (and other LLMs) understand and control your Neon databases through the Neon API.

MCP follows a client-server architecture, where a host application can connect to multiple servers. The key components include:

- **Host**: These are LLM applications, such as Claude Desktop or integrated development environments (IDEs), that initiate connections to MCP servers
- **Client**: These reside within the host application and maintain one-to-one connections with individual servers
- **Server**: These programs provide context, tools, and prompts to clients, enabling access to external data and functionalities

### Why use MCP?

Traditionally, connecting AI models to different data sources required developers to create custom code for each integration. This fragmented approach led to increased development time, maintenance burdens, and limited interoperability between AI models and tools. MCP tackles this challenge by providing a standardized protocol that simplifies integration, accelerates development, and enhances the capabilities of AI assistants.

### What is Neon MCP server?

**Neon's MCP server** is an open-source tool that lets LLMs like **Claude manage your Neon databases using natural language by interacting with the Neon API.** It translates your simple English instructions into **Neon API calls**.

Examples of natural language commands that are converted to **Neon API actions**:

- **Create a Postgres database called `my-database`**: Calls the Neon API to create a database
- **Add a column `created_at` to the 'users' table in project `my-project`**: Uses the Neon API to run an SQL command
- **List all my Neon projects**: Calls the Neon API to fetch a project list

### Why use Neon MCP server?

Neon MCP server, combined with Neon, offers:

- **Simple Setup:** Easily connect LLMs to **Neon API**.
- **Natural Language:** Manage databases without direct **Neon API** coding.
- **Empowering Non-Developers**: Intuitive database interaction for everyone.

<Admonition type="important">
The Neon MCP server's ability to execute arbitrary commands from natural language requests requires careful attention to security.  Always review and approve actions before they are committed.  Grant access only to authorized users and applications.
</Admonition>

<MCPTools />

These actions enable any MCP Host to interact with various functionalities of the **Neon platform via the Neon API.** Certain tools, especially database migration ones, are tailored for AI agent and LLM usage, leveraging Neon’s branching for safe preview and commit.

## Setting up Neon MCP server

We'll use Claude Desktop to interact with Neon MCP server. Here's how to set it up:

### Prerequisites

- **Node.js (>= v18):** Install from [nodejs.org](https://nodejs.org/).
- **Claude Desktop:** Install Anthropic's [Claude Desktop](https://claude.ai/download).
- **Neon Account:** Sign up for a free Neon account at [neon.tech](https://neon.tech).
- **Neon API Key (for Local MCP server)::** Get your [Neon API Key](/docs/manage/api-keys#creating-api-keys).

### Setting up Neon MCP Server in Claude

You have two options for connecting Claude to the Neon MCP Server:

1. **Remote MCP Server (Preview):** Connect to Neon's managed MCP server using OAuth for authentication. This method is more convenient as it eliminates the need to manage API keys in Claude. Additionally, you will automatically receive the latest features and improvements as soon as they are released.

2. **Local MCP Server:** Run the Neon MCP server locally on your machine, authenticating with a Neon API key.

### Option 1: Setting up the remote hosted Neon MCP Server

<Admonition type="note">
By default, the Remote MCP Server connects to your personal Neon account. To connect to an organization's account, you must authenticate with an API key. For more information, see [API key-based authentication](/docs/ai/neon-mcp-server#api-key-based-authentication).
</Admonition>

1. Open Claude desktop and navigate to **Settings**.
   ![Claude settings](/guides/images/claude_mcp/claude_settings.png)
2. Under the **Developer** tab, click **Edit Config** (On Windows, it's under File -> Settings -> Developer -> Edit Config) to open the location of configuration file (`claude_desktop_config.json`).
   ![Claude config](/guides/images/claude_mcp/claude_developer_config.png)
3. Open the `claude_desktop_config.json` file in a text editor of your choice.
4. Add the "Neon" server entry within the `mcpServers` object:

   ```json
   {
     "mcpServers": {
       "Neon": {
         "command": "npx",
         "args": ["-y", "mcp-remote@latest", "https://mcp.neon.tech/mcp"]
       }
     }
   }
   ```

5. Save the configuration file and **restart** Claude Desktop.
6. An OAuth window will open in your browser. Follow the prompts to authorize Claude to access your Neon account.
   ![Neon OAuth window](/docs/guides/neon-oauth-window.png)
7. After authorization, you can start using the Neon MCP server with Claude.

<Admonition type="note">
The remote hosted MCP server is in preview due to the [new OAuth MCP specification](https://spec.modelcontextprotocol.io/specification/2025-03-26/basic/authorization/), expect potential changes as we continue to refine the OAuth integration.
</Admonition>

### Option 2: Setting up the Local Neon MCP Server

This method runs the Neon MCP server locally on your machine, using a Neon API key for authentication.

1.  Open your terminal.
2.  Run the following command to install the Local Neon MCP server for use with Claude Desktop:

    ```bash
    npx @neondatabase/mcp-server-neon init $NEON_API_KEY
    ```

    > Make sure to replace `$NEON_API_KEY` with your actual Neon API key. You can generate one through the Neon Console by following the instructions in [Creating API keys](/docs/manage/api-keys#creating-api-keys).

    You will be prompted to install the required dependencies. Type `y` to proceed. You should see output similar to this:

    ```bash
    npx @neondatabase/mcp-server-neon init napi_xxxx
    Need to install the following packages:
    @neondatabase/mcp-server-neon@0.x.x
    Ok to proceed? (y) y

    Config written to: /Users/USERNAME/Library/Application Support/Claude/claude_desktop_config.json
    The Neon MCP server will start automatically the next time you open Claude.
    ```

3.  Restart Claude Desktop. You can do so by quitting the Claude Desktop and opening it again.

### Verifying the connection

You can verify that the connection to the Neon MCP server either remote or local is successful by following these steps:

1. In Claude click on the search and tools icon to see the available tools.
   ![Claude available tools](/guides/images/claude_mcp/claude_available_tools.png)
2. You should see the Neon MCP server's tools listed. Click on the **neon** tool to see the available tools in detail.
   ![Claude list available tools](/guides/images/claude_mcp/claude_list_available_tools.png)
3. Claude is now connected to Neon's remote MCP server.

Ask Claude `"List my Neon projects"`. If it works, you'll see your projects listed by Claude, fetched using the **Neon API**. For example, you might see output similar to this:

![Claude output](/guides/images/claude_mcp/claude_list_project.png)

## Using Neon MCP server

Neon MCP server lets you manage Neon via **Neon API calls**

### Neon platform operations

- **List databases:** `"What databases do I have in my Neon project?"`
- **Create a new Neon project**: `"Create a new Neon named my-project"`
- **Create a new database**: `"Create a new database called my-database in the Neon project named my-project"`

### Simple SQL queries

- **Insert records"** `"Create a table named posts with 20 records."`
- **Query a table:** `"Show me 10 posts from 'posts' table in database 'my-database' of project 'my-project'"` (Triggers `run_sql` action to execute query)

### Schema exploration

- **List tables:** `"What tables are in database 'my-database' of project 'my-project'?"` (Triggers `get_database_tables` action to get table list)
- **Table schema:** `"Show schema of 'posts' table in database 'my-database' of project 'my-project'"` (Triggers `describe_table_schema` action to describe schema)

### Quick example: Neon MCP server in action

Imagine you want to add a column to a table in your Neon project. Instead of writing SQL migrations and directly calling the Neon API, with Neon MCP server and Claude, you can say: `"In my social network Neon project, edit the posts table and add a deleted_at column."`

<Video  
sources={[{src: "/videos/pages/doc/neon-mcp.mp4",type: "video/mp4",}]}
width={960}
height={1080}
/>

Using Neon MCP server, Claude will:

1. **Confirm project:** Check which project you are referring to.
2. **Check schema:** Look at the `posts` table structure.
3. **Make migration:** Create the SQL to add the column
4. **Preview changes:** Show you the changes in a safe, temporary branch leveraging Neon's branching feature.
5. **Apply changes:** After you approve, apply the change to your database.
6. **Confirm success:** Tell you the column is added and prompt you to commit the migration.

This shows how Neon MCP server simplifies and makes database management safer with natural language, all powered by the **Neon API** under the hood.

## Real-world use cases

Neon MCP server can be used in various scenarios. Here are just a few possibilities:

- **SaaS apps:** Faster development with natural language database management
- **Dev/Test:** Quick database setup for testing
- **AI agents:** Simple database backend for AI using natural language
- **Internal tools:** Data access for non-technical teams via natural language interaction

## Security considerations

When Claude uses the Neon MCP tool, you'll see an authorization prompt: "Allow tool from "neon"?"

![Claude output](/guides/images/claude_mcp/claude_allow_tool.png)

For your security, review the tool's purpose before permitting the operation to proceed. Remember that LLMs can sometimes produce unexpected results, so careful monitoring is always recommended.

## Conclusion

Neon MCP server makes database management conversational and easier by enabling natural language interaction with the Neon API. It simplifies tasks, automates processes, and opens new ways to use AI with databases.

## Resources

- [MCP Protocol](https://modelcontextprotocol.org)
- [Neon Docs](/docs)
- [Neon API Reference](https://api-docs.neon.tech/reference/getting-started-with-neon-api)
- [Neon API Keys](/docs/manage/api-keys#creating-api-keys)
- [Neon MCP server GitHub](https://github.com/neondatabase/mcp-server-neon)

<NeedHelp/>


# Neon Database Toolkit for AI Agents

---
title: Neon Database Toolkit for AI Agents
subtitle: Rapidly provision, manage, and interact with Neon Postgres databases in your AI agent workflows
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-01-29T00:00:00.000Z'
updatedOn: '2025-01-29T00:00:00.000Z'
---

The AI landscape is poised for its next "ChatGPT moment" - not with smarter chatbots, but with **autonomous AI agents** that complete real-world tasks. These next-gen assistants go beyond answering questions to independently booking flights, analyzing data trends, and even managing cloud infrastructure.

## The AI Agent revolution

- **Task automation**: Agents now execute multi-step workflows using natural language instructions
- **Environmental awareness**: They interact with apps/APIs like digital employees
- **Persistent memory**: Maintain context across sessions like human colleagues

## The Database challenge

For these agents to thrive, they need infrastructure that matches their dynamic nature. This is where tools like Neon's database toolkit come in. The `@neondatabase/toolkit` simplifies data management for AI agents, providing them with an efficient way to create, manage, and interact with Postgres databases.

As agents evolve from simple helpers to full digital coworkers, their success hinges on infrastructure that's as agile as their programming. Tools like Neon's toolkit aren't just supporting this revolution - they're building the foundation for AI's next evolutionary leap.

This architectural shift isn't just about cost - it enables fundamentally new agent capabilities. When every database interaction becomes as lightweight as an API call, agents can experiment freely, chain operations without hesitation, and manage infrastructure as intuitively as humans manage browser tabs.
There is one thing that is certain: The agent revolution won't be built on legacy infrastructure. Tools like Neon aren't just keeping pace with AI - they're redefining what's possible by making database operations as fluid as the agents themselves.

## Neon: The Perfect Database for AI Agents

Neon's serverless Postgres databases are built to be the ideal partner for AI agents. They offer a powerful, scalable, and cost-effective way to manage data. Here are some key benefits of using Neon databases for AI agents:

| Capability           | Agent Benefit                                                                                                                                                                |
| -------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Databases in seconds | Neon lets you create fully working Postgres databases almost instantly. No more waiting around – your agent can start working with data right away.                          |
| Scales easily        | Neon's serverless design automatically adjusts to your agent's needs, whether it's a quiet period or a busy time. You only pay for what you actually use, which saves money. |
| Full Postgres power  | Your agents get all the reliability and features of Postgres, a database trusted by developers everywhere.                                                                   |
| Micro Workloads      | Neon's cost-effective design is perfect for agents that only need a database for specific tasks. You don't have to pay for database resources when they aren't being used.   |
| Cost-optimized       | Ideal for AI agents with varying workloads, which is great for agents that have quiet periods or only need a database for specific tasks.                                    |

The `@neondatabase/toolkit` takes all the best things about Neon and puts them into an easy-to-use software development kit (SDK), designed to make database interaction super easy for AI agents.

The toolkit can simplify the process of creating a Neon project and running SQL queries. This can also be especially helpful in test environments, where you don't want to manually set up a Neon project each time.

## Getting Started with `@neondatabase/toolkit`

<Admonition type="note">
The `@neondatabase/toolkit` is in the early stages of development, so some features might change in the future.
</Admonition>

### Installation:

Add the toolkit to your project:

<CodeTabs labels={["npm", "yarn", "pnpm", "Deno"]}>

```bash
npm install @neondatabase/toolkit
```

```bash
yarn add @neondatabase/toolkit
```

```bash
pnpm add @neondatabase/toolkit
```

```bash
deno add jsr:@neon/toolkit
```

</CodeTabs>

### Usage

Let's see how quickly you can get an AI agent up and running with a Neon database using the toolkit. Imagine your agent needs a database to keep track of user information. The AI agent itself will provide the instructions (SQL) to set up the database structure. Here’s how it works with `@neondatabase/toolkit`:

```typescript
import { NeonToolkit } from '@neondatabase/toolkit';

// Start the toolkit with your Neon API Key
const toolkit = new NeonToolkit(process.env.NEON_API_KEY!);

async function runAgentWorkflow() {
  // Create a Neon Project (Database) - Ready instantly!
  const project = await toolkit.createProject();
  console.log(`Project created: ${project.id}`);

  // AI Agent Provides SQL to Create a Table
  const agentCreateTableQuery = `
    CREATE TABLE IF NOT EXISTS users (
      id UUID PRIMARY KEY,
      username VARCHAR(50) NOT NULL,
      email VARCHAR(255) UNIQUE NOT NULL,
      signup_date TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
    );
  `;

  // Run the AI Agent's SQL to create the 'users' table
  await toolkit.sql(project, agentCreateTableQuery);
  console.log("Table 'users' created using the AI agent's SQL.");

  // AI Agent Provides SQL to Add Data
  const agentInsertUserQuery = `
    INSERT INTO users (id, username, email)
    VALUES (gen_random_uuid(), 'agentUser1', 'agent.user1@example.com'),
           (gen_random_uuid(), 'agentUser2', 'agent.user2@example.com');
  `;

  // Run the AI Agent's SQL to add user data
  await toolkit.sql(project, agentInsertUserQuery);
  console.log("User data added using the AI agent's SQL.");

  // Perform more SQL operations as needed...

  // Delete the Project (Database) - Clean up when the agent is finished
  await toolkit.deleteProject(project);
  console.log(`Project ${project.id} deleted.`);
}

runAgentWorkflow().catch(console.error);
```

**Key takeaways from this example**

- **Quick setup:** Just a few lines of code to get a fully working database ready for your agent.
- **Clear and simple:** The code is easy to understand, showing how the toolkit focuses on making things simple for developers and agents.
- **Automatic cleanup:** The `deleteProject` function makes sure resources are freed up when the agent is done.

## Conclusion

`@neondatabase/toolkit` takes away the complexity of databases, letting you concentrate on building smarter and more powerful AI agents. It’s the fastest way to connect your agents to data, helping them learn, think, and perform using the power of serverless Postgres.

For more advanced uses, remember that the toolkit gives you access to the complete [Neon API](https://api-docs.neon.tech/reference/getting-started-with-neon-api) through `toolkit.apiClient`. This allows for more detailed customization and integration.

## Resources

- [@neondatabase/toolkit on npm](https://www.npmjs.com/package/@neondatabase/toolkit)
- [@neon/toolkit on JSR](https://jsr.io/@neon/toolkit)
- [Neon Documentation](/docs/introduction)
- [Neon API Reference](https://api-docs.neon.tech/reference/getting-started-with-neon-api)
- [Neon API keys](/docs/manage/api-keys#creating-api-keys)

<NeedHelp/>


# Query Postgres in Next.js Server Actions

---
title: Query Postgres in Next.js Server Actions
subtitle: Learn how to query Postgres in Next.js with Server Actions
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-05-13T13:24:36.612Z'
updatedOn: '2024-05-13T13:24:36.612Z'
---

In this guide, you will learn the process of creating a simple web application using Next.js Server Actions that allows you to capture user input via forms, and insert them into Postgres via `@neondatabase/serverless` and `pg`.

To create a Neon project and access it from an Next.js application:

- [Create a Neon project](#create-a-neon-project)
- [Create a Next.js project and add dependencies](#create-a-nextjs-project-and-add-dependencies)
- [Store your Neon credentials](#store-your-neon-credentials)
- [Create a Form with Server Actions](#create-a-form-with-server-actions)
- [Implement Next.js Server Actions](#implement-nextjs-server-actions)
- [Run the app](#run-the-app)

## Create a Neon project

If you do not have one already, create a Neon project. Save your connection details including your password. They are required when defining connection settings.

1. Navigate to the [Projects](https://console.neon.tech/app/projects) page in the Neon Console.
2. Click **New Project**.
3. Specify your project settings and click **Create Project**.

## Create a Next.js project and add dependencies

1. Create an Next.js project if you do not have one. For instructions, see [Automatic Installation](https://nextjs.org/docs/getting-started/installation#automatic-installation) in the Next.js documentation.

2. Add project dependencies using one of the following commands:

   <CodeTabs reverse={true} labels={["node-postgres", "Neon serverless driver"]}>

   ```shell
   npm install pg
   ```

   ```shell
   npm install @neondatabase/serverless
   ```

   </CodeTabs>

## Store your Neon credentials

Add a `.env` file to your project directory and add your Neon connection string to it. You can find the connection string for your database in the **Connection Details** widget on the Neon **Dashboard**. For more information, see [Connect from any application](/docs/connect/connect-from-any-app).

```shell shouldWrap
DATABASE_URL="postgres://[user]:[password]@[neon_hostname]/[dbname]"
```

## Create a Form with Server Actions

Create a simple form that allows users to input a comment. For now, add an action named `create` that you will create in next step with Next.js server actions.

```tsx
// File: app/page.tsx

export default function Page() {
  return (
    <form action={create}>
      <input type="text" placeholder="write a comment" name="comment" />
      <button type="submit">Submit</button>
    </form>
  );
}
```

## Implement Next.js Server Actions

Now, let's add the server action to insert the data into your Postgres.

<CodeTabs reverse={true} labels={["node-postgres", "Neon serverless driver"]}>

```tsx {3,6-16}
// File: app/page.tsx

import { Client } from 'pg';

export default function Page() {
  async function create(formData: FormData) {
    'use server';
    // Create a client instance using `node-postgres`
    const client = new Client(`${process.env.DATABASE_URL}`);
    await client.connect();
    // Create the comments table if it does not exist
    await client.query('CREATE TABLE IF NOT EXISTS comments (comment TEXT)');
    const comment = formData.get('comment');
    // Insert the comment from the form into the Postgres (powered by Neon)
    await client.query('INSERT INTO comments (comment) VALUES ($1)', [comment]);
  }
  return (
    <form action={create}>
      <input type="text" placeholder="write a comment" name="comment" />
      <button type="submit">Submit</button>
    </form>
  );
}
```

```tsx {3,6-15}
// File: app/page.tsx

import { neon } from '@neondatabase/serverless';

export default function Page() {
  async function create(formData: FormData) {
    'use server';
    // Create an instance of Neon's TS/JS driver
    const sql = neon(`${process.env.DATABASE_URL}`);
    // Create the comments table if it does not exist
    await sql('CREATE TABLE IF NOT EXISTS comments (comment TEXT)');
    const comment = formData.get('comment');
    // Insert the comment from the form into the Postgres (powered by Neon)
    await sql('INSERT INTO comments (comment) VALUES ($1)', [comment]);
  }
  return (
    <form action={create}>
      <input type="text" placeholder="write a comment" name="comment" />
      <button type="submit">Submit</button>
    </form>
  );
}
```

</CodeTabs>

## Run the app

Execute the following command to run your application locally:

```shell
npm run dev
```

<NeedHelp />


# How to upload to S3 in Next.js and save references in Postgres

---
title: How to upload to S3 in Next.js and save references in Postgres
subtitle: Let users upload files directly to S3 by creating presigned URLs in Next.js and saving the references in a Postgres database.
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-05-16T00:00:00.000Z'
updatedOn: '2024-05-16T00:00:00.000Z'
---

In this guide, you will learn how to add a feature to a Next.js app that allows users to upload files to Amazon S3, and insert the references to them in Postgres (powered by Neon) via `pg` and `@neondatabase/serverless`.

## Steps

- [Create a Neon project](#create-a-neon-project)
- [Store your Neon credentials](#store-your-neon-credentials)
- [Create an Amazon S3 Bucket](#create-an-amazon-s3-bucket)
- [Create access keys for IAM users (in AWS)](#create-access-keys-for-iam-users-in-aws)
- [Create a new Next.js application](#create-a-new-nextjs-application)
- [Create a Presigned URL with Amazon S3 SDK](#create-a-presigned-url-with-amazon-s3-sdk)
- [Save Reference to S3 objects in Postgres](#save-reference-to-s3-objects-in-postgres)
- [Upload to Presigned URL with in-browser JavaScript](#upload-to-presigned-url-with-in-browser-javascript)
- [Run the app](#run-the-app)

## Create a Neon project

If you do not have one already, create a Neon project.

1. Navigate to the [Projects](https://console.neon.tech/app/projects) page in the Neon Console.
2. Click **New Project**.
3. Specify your project settings and click **Create Project**.
4. Copy the database connection string to add to your Next.js app later. The connection string looks like `postgres://[user]:[password]@[neon_hostname]/[dbname]` and can be found in the **Connection Details** widget on the Neon **Dashboard**.

## Create an Amazon S3 Bucket

Open the [Amazon S3 Bucket](https://console.aws.amazon.com/s3), and click **Create bucket**.

![](/guides/images/s3-1.png)

Enter a repository name, say `my-custom-bucket-0` for example. Copy the bucket name to be used as **AWS_S3_BUCKET_NAME** in your application.

```shell shouldWrap
AWS_S3_BUCKET_NAME="my-custom-bucket-0"
```

![](/guides/images/s3-2.png)

In the **Policy** section, use the following json to define the actions allowed with the bucket:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "PublicReadGetObject",
      "Effect": "Allow",
      "Principal": "*",
      "Action": ["s3:GetObject"],
      "Resource": "arn:aws:s3:::my-custom-bucket-0/*"
    }
  ]
}
```

In the **CORS** section, use the following json to define the actions allowed with the bucket:

```json
[
  {
    "AllowedHeaders": ["*"],
    "AllowedMethods": ["GET", "PUT"],
    "AllowedOrigins": ["*"],
    "ExposeHeaders": [],
    "MaxAgeSeconds": 9000
  }
]
```

Finally, complete the bucket creation process by clicking the **Create bucket** at the end.

## Create access keys for IAM users (in AWS)

In the navigation bar on the upper right in your AWS account, click on your name, and then choose **Security credentials**.

![](/guides/images/iam-1.png)

Scroll down to **Access keys** and click on **Create access key**.

![](/guides/images/iam-2.png)

Again, click on **Create access key**.

![](/guides/images/iam-3.png)

Copy the Access key and Secret access key, you will add them to your Next.js project later.

```shell shouldWrap
AWS_KEY_ID="..."
AWS_SECRET_ACCESS_KEY=".../...+"
```

![](/guides/images/iam-4.png)

## Create a new Next.js application

Let’s get started by creating a new Next.js project. Open your terminal and run the following command:

```shell shouldWrap
npx create-next-app@latest my-app
```

When prompted, choose:

- `Yes` when prompted to use TypeScript.
- `No` when prompted to use ESLint.
- `Yes` when prompted to use Tailwind CSS.
- `No` when prompted to use `src/` directory.
- `Yes` when prompted to use App Router.
- `No` when prompted to customize the default import alias (`@/*`).

Once that is done, move into the project directory and start the app in developement mode by executing the following command:

```shell shouldWrap
cd my-app
npm run dev
```

The app should be running on [localhost:3000](http://localhost:3000). Stop the development server to install the libraries necessary to build the application:

```shell shouldWrap
npm install @aws-sdk/client-s3 @aws-sdk/s3-request-presigner @neondatabase/serverless
```

The command installed the following libraries:

- `@aws-sdk/client-s3`: AWS SDK for JavaScript S3 Client for Node.js, Browser and React Native.
- `@aws-sdk/s3-request-presigner`: SDK to generate signed url for S3.
- `@neondatabase/serverless`: Neon's PostgreSQL driver for JavaScript and TypeScript.

Now, create a `.env` file at the root of your project. You are going to add the credentials you obtained earlier.

It should look something like this:

```shell shouldWrap
# AWS Environment Variables
AWS_KEY_ID="..."
AWS_SECRET_ACCESS_KEY=".../...+"
AWS_S3_BUCKET_NAME="...-bucket-0"

# Postgres (powered by Neon) Environment Variable
DATABASE_URL="postgresql://neondb_owner:...@...-pooler.us-east-2.aws.neon.tech/neondb?sslmode=require&channel_binding=require"
```

Now, let's move on to creating an API route to obtain a presigned URL to upload objects to.

## Create a Presigned URL with Amazon S3 SDK

[Presigned URLs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-presigned-url.html) allow you to upload large chunks of data directly at the source (here, `Amazon S3`).

This saves you from a couple limitations of a server-based upload operation:

- maximum request payload restrictions (on a hosting service, especially in serverless)
- huge RAM required to process multiple large file buffers at the same time

You will create an API endpoint that accepts the file name and it's content type to be uploaded via a presigned URL. In Next.js, you can create an API endpoint by creating a `route.ts` file at any directory level inside the `app` directory. To use `/api/presigned` as the desired API route, create a file `app/api/presigned/route.ts` with the following code:

```tsx
// File: app/api/presigned/route.ts

import { NextResponse, type NextRequest } from 'next/server';

export async function GET(request: NextRequest) {
  const accessKeyId = process.env.AWS_KEY_ID;
  const secretAccessKey = process.env.AWS_SECRET_ACCESS_KEY;
  const s3BucketName = process.env.AWS_S3_BUCKET_NAME;
  if (!accessKeyId || !secretAccessKey || !s3BucketName) {
    return new Response(null, { status: 500 });
  }
  const searchParams = request.nextUrl.searchParams;
  const fileName = searchParams.get('fileName');
  const contentType = searchParams.get('contentType');
  if (!fileName || !contentType) {
    return new Response(null, { status: 500 });
  }
}
```

The code above defines a `GET` handler that validates the presence of all the environment variables required, and the file name and it's content type.

Next, append the following code to return a JSON from the endpoint containing the presigned URL as `signedUrl`:

```tsx {4,5,20-34}
// File: app/api/presigned/route.ts

import { NextResponse, type NextRequest } from 'next/server';
import { getSignedUrl } from '@aws-sdk/s3-request-presigner';
import { S3Client, PutObjectCommand } from '@aws-sdk/client-s3';

export async function GET(request: NextRequest) {
  const accessKeyId = process.env.AWS_KEY_ID;
  const secretAccessKey = process.env.AWS_SECRET_ACCESS_KEY;
  const s3BucketName = process.env.AWS_S3_BUCKET_NAME;
  if (!accessKeyId || !secretAccessKey || !s3BucketName) {
    return new Response(null, { status: 500 });
  }
  const searchParams = request.nextUrl.searchParams;
  const fileName = searchParams.get('fileName');
  const contentType = searchParams.get('contentType');
  if (!fileName || !contentType) {
    return new Response(null, { status: 500 });
  }
  const client = new S3Client({
    region: 'eu-north-1',
    credentials: {
      accessKeyId,
      secretAccessKey,
    },
  });
  const command = new PutObjectCommand({
    Bucket: s3BucketName,
    Key: fileName,
    ContentType: contentType,
  });
  const signedUrl = await getSignedUrl(client, command, { expiresIn: 3600 });
  if (signedUrl) return NextResponse.json({ signedUrl });
  return new Response(null, { status: 500 });
}
```

The code above creates an S3 client using the `@aws-sdk/client-s3` SDK. Then, it uses the `getSignedUrl` utility (from `@aws-sdk/s3-request-presigner`) to sign the URL.

Now, let's move on to building an endpoint to insert the reference to the uploaded object in Postgres.

## Save Reference to S3 objects in Postgres

You will create an API endpoint that accepts the URL to the publicly accessible object. In this example, we'll create a table in Postgres, and associate the object URL with a user, for demonstration purposes. To use `/api/user/image` as the desired API route, create a file `app/api/user/image/route.ts` with the following code:

<CodeTabs reverse={true} labels={["node-postgres", "postgres.js", "Neon serverless driver"]}>

```tsx {3,10-11,14,16,18-21}
// File: app/api/user/image/route.ts

import { Client } from 'pg';
import { NextResponse, type NextRequest } from 'next/server';

export async function POST(request: NextRequest) {
  const { objectUrl } = await request.json();
  if (!process.env.DATABASE_URL) return new Response(null, { status: 500 });
  // Create a client instance using `node-postgres`
  const client = new Client(process.env.DATABASE_URL);
  await client.connect();
  try {
    // Create the user table if it does not exist
    await client.query('CREATE TABLE IF NOT EXISTS "user" (name TEXT, image TEXT)');
    // Mock call to get the user
    const user = 'rishi'; // getUser();
    // Insert the user name and the reference to the image into the user table
    await client.query('INSERT INTO "user" (name, image) VALUES ($1, $2)', [user, objectUrl]);
    return NextResponse.json({ code: 1 });
  } catch (e) {
    return NextResponse.json({
      code: 0,
      message: e instanceof Error ? e.message : e?.toString(),
    });
  }
}
```

```tsx {3,10-11,14,16,18-21}
// File: app/api/user/image/route.ts

import postgres from 'postgres';
import { NextResponse, type NextRequest } from 'next/server';

export async function POST(request: NextRequest) {
  const { objectUrl } = await request.json();
  if (!process.env.DATABASE_URL) return new Response(null, { status: 500 });
  // Create a client instance
  const sql = postgres(process.env.DATABASE_URL, { ssl: 'require' });
  try {
    // Create the user table if it does not exist
    await sql`CREATE TABLE IF NOT EXISTS "user" (name TEXT, image TEXT)`;
    // Mock call to get the user
    const user = 'rishi'; // getUser();
    // Insert the user name and the reference to the image into the user table
    await sql`INSERT INTO "user" (name, image) VALUES (${user}, ${objectUrl})`;
    return NextResponse.json({ code: 1 });
  } catch (e) {
    return NextResponse.json({
      code: 0,
      message: e instanceof Error ? e.message : e?.toString(),
    });
  }
}
```

```tsx {3,12,14,16-19}
// File: app/api/user/image/route.ts

import { neon } from '@neondatabase/serverless';
import { NextResponse, type NextRequest } from 'next/server';

export async function POST(request: NextRequest) {
  const { objectUrl } = await request.json();
  if (!process.env.DATABASE_URL) return new Response(null, { status: 500 });
  const sql = neon(process.env.DATABASE_URL);
  try {
    // Create the user table if it does not exist
    await sql('CREATE TABLE IF NOT EXISTS "user" (name TEXT, image TEXT)');
    // Mock call to get the user
    const user = 'rishi'; // getUser();
    // Insert the user name and the reference to the image into the user table
    await sql('INSERT INTO "user" (name, image) VALUES ($1, $2)', [user, objectUrl]);
    return NextResponse.json({ code: 1 });
  } catch (e) {
    return NextResponse.json({
      code: 0,
      message: e instanceof Error ? e.message : e?.toString(),
    });
  }
}
```

</CodeTabs>

The code above defines a POST endpoint, which first validates the presence of `DATABASE_URL` environment variable. Further, it creates a table named `user` if it does not exist, and inserts the record for a user named `rishi` with the object URL.

Now, let's move on to learning how to call these APIs in the front-end built with React.

## Upload to Presigned URL with in-browser JavaScript

With the API routes defined, the flow to upload the objects and save references to it in the database, is in three steps:

### 1. Accept a file from the user

Using the HTML `<input />` element, accept a file from the user to be uploaded to S3. Attach a listener to change in the file attached to upload programtically.

```tsx
// File: app/page.tsx

'use client';

import { ChangeEvent } from 'react';

export default function Home() {
  const uploadFile = (e: ChangeEvent<HTMLInputElement>) => {
    const file: File | null | undefined = e.target.files?.[0];
    if (!file) return;
    const reader = new FileReader();
    reader.onload = async (event) => {
      const fileData = event.target?.result;
      if (fileData) {
        // Fetch presigned URL and save reference in Postgres (powered by Neon)
      }
    };
    reader.readAsArrayBuffer(file);
  };
  return <input onChange={uploadFile} type="file" />;
}
```

### 2. Fetch the Presigned URL using the file name and type

Perform a GET call to `/api/presigned` API route with the file name and type as the query params. Obtain the presigned URL, and then upload the file as a Blob to it.

```tsx {15-28}
// File: app/page.tsx

'use client';

import { ChangeEvent } from 'react';

export default function Home() {
  const uploadFile = (e: ChangeEvent<HTMLInputElement>) => {
    const file: File | null | undefined = e.target.files?.[0];
    if (!file) return;
    const reader = new FileReader();
    reader.onload = async (event) => {
      const fileData = event.target?.result;
      if (fileData) {
        const presignedURL = new URL('/api/presigned', window.location.href);
        presignedURL.searchParams.set('fileName', file.name);
        presignedURL.searchParams.set('contentType', file.type);
        fetch(presignedURL.toString())
          .then((res) => res.json())
          .then((res) => {
            const body = new Blob([fileData], { type: file.type });
            fetch(res.signedUrl, {
              body,
              method: 'PUT',
            }).then(() => {
              // Save reference to the object in Postgres (powered by Neon)
            });
          });
      }
    };
    reader.readAsArrayBuffer(file);
  };
  return <input onChange={uploadFile} type="file" />;
}
```

### 3. Insert the reference to the object in the Postgres

Perform a `POST` to the `/api/user/image` route, with the presigned URL configured to **not contain the query parameters**. The stripped URL is an absolute reference to the publicly available object uploaded.

```tsx {26-32}
// File: app/page.tsx

'use client';

import { ChangeEvent } from 'react';

export default function Home() {
  const uploadFile = (e: ChangeEvent<HTMLInputElement>) => {
    const file: File | null | undefined = e.target.files?.[0];
    if (!file) return;
    const reader = new FileReader();
    reader.onload = async (event) => {
      const fileData = event.target?.result;
      if (fileData) {
        const presignedURL = new URL('/api/presigned', window.location.href);
        presignedURL.searchParams.set('fileName', file.name);
        presignedURL.searchParams.set('contentType', file.type);
        fetch(presignedURL.toString())
          .then((res) => res.json())
          .then((res) => {
            const body = new Blob([fileData], { type: file.type });
            fetch(res.signedUrl, {
              body,
              method: 'PUT',
            }).then(() => {
              fetch('/api/user/image', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                  objectUrl: res.signedUrl.split('?')[0],
                }),
              });
            });
          });
      }
    };
    reader.readAsArrayBuffer(file);
  };
  return <input onChange={uploadFile} type="file" />;
}
```

## Run the app

Execute the following command to run your application locally:

```shell
npm run dev
```

You should now be able to go through the entire workflow of selecting a file, uploading it to S3, and referencing it later by saving it in the database.

<NeedHelp />


# Building a Job Queue System with Node.js, Bull, and Neon Postgres

---
title: Building a Job Queue System with Node.js, Bull, and Neon Postgres
subtitle: Learn how to implement a job queue system to handle background tasks efficiently using Node.js, Bull, and Neon Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2025-03-16T00:00:00.000Z'
updatedOn: '2025-03-16T00:00:00.000Z'
---

Job queues are essential components in modern applications. Queues enable you to handle resource-intensive or time-consuming tasks asynchronously. This approach improves application responsiveness by moving heavy processing out of the request-response cycle.

In this guide, we'll walk through building a job queue system using Node.js, Bull (a Redis-backed queue library), and Neon Postgres to process jobs efficiently at scale.

## Prerequisites

To follow the steps in this guide, you will need the following:

- [Node.js 18](https://nodejs.org/en) or later
- A [Neon](https://console.neon.tech/signup) account
- [Redis](https://redis.io/download) installed locally
- Basic understanding of JavaScript and PostgreSQL

## Understanding Job Queues

Job queues allow applications to offload time-consuming tasks to be processed in the background. Some common use cases include:

- Email delivery
- Image or video processing
- Data aggregation and reporting
- Webhook delivery
- Regular maintenance tasks

Here's how our architecture will work:

1. The main application adds jobs to the queue
2. Bull manages the queue in Redis
3. Worker processes consume jobs from the queue
4. Job statuses and results are stored in Neon Postgres
5. The application can check job status and retrieve results

This separation improves system performance, reliability, and scalability. It also allows for better error handling, retry logic, monitoring and even user experience as the application can respond quickly to user requests regardless of the job processing time.

## Create a Neon project

First, let's set up a new Neon Postgres database to store our job metadata.

1. Navigate to the [Neon Console](https://console.neon.tech/app/projects) and create a new project.

2. Choose a name for your project, for example "job-queue-system".

3. After creating the project, you'll see the connection details. Save the connection string, you'll need it to connect to your database.

4. Using the SQL Editor in the Neon Console or your preferred PostgreSQL client, create the tables for our job queue system:

```sql
CREATE TABLE jobs (
  id SERIAL PRIMARY KEY,
  job_id VARCHAR(255) UNIQUE NOT NULL,
  queue_name VARCHAR(100) NOT NULL,
  data JSONB,
  status VARCHAR(50) NOT NULL,
  result JSONB,
  error TEXT,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  started_at TIMESTAMP,
  completed_at TIMESTAMP,
  attempts INT DEFAULT 0,
  max_attempts INT DEFAULT 3
);

CREATE INDEX idx_jobs_status ON jobs(status);
CREATE INDEX idx_jobs_queue_name ON jobs(queue_name);
```

This jobs table will track:

- Unique job identifiers
- The queue a job belongs to
- Job data/parameters
- Current status (pending, active, completed, failed)
- Results or errors
- Timestamps for tracking job lifecycle
- Retry information

## Set up your Node.js project

Now that our database is ready, let's create a Node.js project:

1. Create a new directory for your project:

```bash
mkdir job-queue-system
cd job-queue-system
```

2. Initialize a new Node.js project:

```bash
npm init -y
```

3. Install the required dependencies:

```bash
npm install bull pg dotenv express
```

These packages provide:

- `bull`: Queue management with Redis
- `pg`: PostgreSQL client for Node.js
- `dotenv`: Environment variable management
- `express`: Web framework to create a simple API for our example

4. Create a `.env` file to store your configuration:

```
# Database
DATABASE_URL=postgres://[user]:[password]@[hostname]/[database]?sslmode=require&channel_binding=require

# Redis
REDIS_URL=redis://localhost:6379

# Application
PORT=3000
```

Replace the `DATABASE_URL` with your Neon connection string.

## Integrate Bull for job processing

Bull is a Node.js library that implements a fast queue system based on Redis.

If you don't have Redis installed, you can run it using Docker:

```bash
docker run -d -p 6379:6379 redis
```

Let's set up the basic queue structure for our job queue system.

Create a file named `src/queues/index.js`:

```javascript
// src/queues/index.js
const Bull = require('bull');
const dotenv = require('dotenv');

dotenv.config();

// Create queues
const emailQueue = new Bull('email', process.env.REDIS_URL);
const imageProcessingQueue = new Bull('image-processing', process.env.REDIS_URL);
const dataExportQueue = new Bull('data-export', process.env.REDIS_URL);

// Export the queues to be used elsewhere in the application
module.exports = {
  emailQueue,
  imageProcessingQueue,
  dataExportQueue,
};
```

What the queues represent in this context is a way to group similar jobs together. This file creates three different queues for different types of jobs. In a production application, you might have many more queues for various tasks.

Now, let's create a utility to add jobs to these queues. Create a file named `src/utils/queueHelper.js`:

```javascript
// src/utils/queueHelper.js
const { Pool } = require('pg');
const dotenv = require('dotenv');

dotenv.config();

// Initialize PostgreSQL connection pool
const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  ssl: {
    rejectUnauthorized: false,
  },
});

// Helper function to add a job to a queue and record it in Postgres
async function addJob(queue, data, options = {}) {
  const client = await pool.connect();

  try {
    await client.query('BEGIN');

    // Add job to Bull queue
    const job = await queue.add(data, options);

    // Record job in Postgres
    const result = await client.query(
      `INSERT INTO jobs (job_id, queue_name, data, status)
       VALUES ($1, $2, $3, $4)
       RETURNING id`,
      [job.id.toString(), queue.name, JSON.stringify(data), 'pending']
    );

    await client.query('COMMIT');

    return {
      jobId: job.id,
      dbId: result.rows[0].id,
    };
  } catch (error) {
    await client.query('ROLLBACK');
    throw error;
  } finally {
    client.release();
  }
}

// Helper function to update job status in Postgres
async function updateJobStatus(jobId, queueName, updates) {
  const client = await pool.connect();

  try {
    // Build the SET clause based on provided updates
    const setClauses = [];
    const values = [jobId, queueName];
    let paramIndex = 3;

    for (const [key, value] of Object.entries(updates)) {
      setClauses.push(`${key} = $${paramIndex}`);
      values.push(value);
      paramIndex++;
    }

    if (setClauses.length === 0) {
      return;
    }

    const setClause = setClauses.join(', ');

    await client.query(
      `UPDATE jobs
       SET ${setClause}
       WHERE job_id = $1 AND queue_name = $2`,
      values
    );
  } finally {
    client.release();
  }
}

module.exports = {
  addJob,
  updateJobStatus,
};
```

This helper file provides two main functions:

1. `addJob`: Adds a job to a Bull queue and records it in our Neon Postgres database
2. `updateJobStatus`: Updates a job's status in the Neon Postgres as it progresses through the queue

## Create the job processor

Now, let's create processors for each type of job. We'll create processors for email sending, image processing, and data exports. A processor is a function that takes a job object from Bull, processes it, and updates the job status in Postgres.

First, let's set up our directory structure:

```bash
mkdir -p src/processors
```

Create a file for email processing at `src/processors/emailProcessor.js`:

```javascript
// src/processors/emailProcessor.js
const { updateJobStatus } = require('../utils/queueHelper');

async function sendEmail(to, subject, body) {
  // In a real application, you'd use a library like Nodemailer
  // This is a simplified example
  console.log(`Sending email to ${to}`);
  console.log(`Subject: ${subject}`);
  console.log(`Body: ${body}`);

  // Simulate network delay
  await new Promise((resolve) => setTimeout(resolve, 1000));

  // Return success
  return { delivered: true, timestamp: new Date() };
}

// The processor function takes a job object from Bull
async function processEmailJob(job) {
  try {
    // Update job status in Postgres
    await updateJobStatus(job.id, 'email', {
      status: 'active',
      started_at: new Date().toISOString(),
    });

    // Extract job data
    const { to, subject, body } = job.data;

    // Process the job
    const result = await sendEmail(to, subject, body);

    // Update job status on success
    await updateJobStatus(job.id, 'email', {
      status: 'completed',
      result: JSON.stringify(result),
      completed_at: new Date().toISOString(),
    });

    // Return the result
    return result;
  } catch (error) {
    // Update job status on failure
    await updateJobStatus(job.id, 'email', {
      status: 'failed',
      error: error.message,
      completed_at: new Date().toISOString(),
    });

    // Re-throw the error for Bull to handle
    throw error;
  }
}

module.exports = processEmailJob;
```

Similarly, create a processor for image processing at `src/processors/imageProcessor.js`:

```javascript
// src/processors/imageProcessor.js
const { updateJobStatus } = require('../utils/queueHelper');

async function processImage(imageUrl, options) {
  // In a real application, you'd use libraries like Sharp
  // This is a simplified example
  console.log(`Processing image from ${imageUrl}`);
  console.log('Options:', options);

  // Simulate CPU-intensive task
  await new Promise((resolve) => setTimeout(resolve, 3000));

  // Return processed image details
  return {
    processed: true,
    originalUrl: imageUrl,
    processedUrl: `processed-${imageUrl}`,
    dimensions: { width: 800, height: 600 },
    timestamp: new Date(),
  };
}

async function processImageJob(job) {
  try {
    // Update job status in Postgres
    await updateJobStatus(job.id, 'image-processing', {
      status: 'active',
      started_at: new Date().toISOString(),
    });

    // Extract job data
    const { imageUrl, options } = job.data;

    // Process the job
    const result = await processImage(imageUrl, options);

    // Update job status on success
    await updateJobStatus(job.id, 'image-processing', {
      status: 'completed',
      result: JSON.stringify(result),
      completed_at: new Date().toISOString(),
    });

    // Return the result
    return result;
  } catch (error) {
    // Update job status on failure
    await updateJobStatus(job.id, 'image-processing', {
      status: 'failed',
      error: error.message,
      completed_at: new Date().toISOString(),
    });

    // Re-throw the error for Bull to handle
    throw error;
  }
}

module.exports = processImageJob;
```

Now, let's create a processor for data exports at `src/processors/dataExportProcessor.js`:

```javascript
// src/processors/dataExportProcessor.js
const { updateJobStatus } = require('../utils/queueHelper');
const { Pool } = require('pg');
const dotenv = require('dotenv');

dotenv.config();

// Initialize PostgreSQL connection pool
const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  ssl: {
    rejectUnauthorized: false,
  },
});

async function exportData(query, format) {
  console.log(`Executing query: ${query}`);
  console.log(`Export format: ${format}`);

  // Actually execute the query against Neon Postgres
  const result = await pool.query(query);

  // Simulate file creation
  await new Promise((resolve) => setTimeout(resolve, 2000));

  // Return export details
  return {
    records: result.rowCount,
    format: format,
    downloadUrl: `https://example.com/exports/${Date.now()}.${format}`,
    timestamp: new Date(),
  };
}

async function processDataExportJob(job) {
  try {
    // Update job status in Postgres
    await updateJobStatus(job.id, 'data-export', {
      status: 'active',
      started_at: new Date().toISOString(),
    });

    // Extract job data
    const { query, format } = job.data;

    // Process the job
    const result = await exportData(query, format);

    // Update job status on success
    await updateJobStatus(job.id, 'data-export', {
      status: 'completed',
      result: JSON.stringify(result),
      completed_at: new Date().toISOString(),
    });

    // Return the result
    return result;
  } catch (error) {
    // Update job status on failure
    await updateJobStatus(job.id, 'data-export', {
      status: 'failed',
      error: error.message,
      completed_at: new Date().toISOString(),
    });

    // Re-throw the error for Bull to handle
    throw error;
  }
}

module.exports = processDataExportJob;
```

Each processor function follows a similar pattern:

- Update the job status to `active` when processing starts
- Extract job data from the Bull job object
- Process the job
- Update the job status to `completed` on success or `failed` on error
- Return the result

The core logic of each processor is kept separate from the job queue management, which allows for easier testing, maintenance, and reuse.

## Implement PostgreSQL job tracking

With the processors in place, let's create a module to retrieve job information from PostgreSQL. Create a file named `src/utils/jobsRepository.js`:

```javascript
// src/utils/jobsRepository.js
const { Pool } = require('pg');
const dotenv = require('dotenv');

dotenv.config();

// Initialize PostgreSQL connection pool
const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  ssl: {
    rejectUnauthorized: false,
  },
});

async function getJobById(jobId) {
  const result = await pool.query('SELECT * FROM jobs WHERE job_id = $1', [jobId]);

  return result.rows[0] || null;
}

async function getJobsByStatus(status, limit = 100, offset = 0) {
  const result = await pool.query(
    'SELECT * FROM jobs WHERE status = $1 ORDER BY created_at DESC LIMIT $2 OFFSET $3',
    [status, limit, offset]
  );

  return result.rows;
}

async function getJobsByQueue(queueName, limit = 100, offset = 0) {
  const result = await pool.query(
    'SELECT * FROM jobs WHERE queue_name = $1 ORDER BY created_at DESC LIMIT $2 OFFSET $3',
    [queueName, limit, offset]
  );

  return result.rows;
}

async function getJobStats() {
  const result = await pool.query(`
    SELECT 
      queue_name,
      status,
      COUNT(*) as count
    FROM jobs
    GROUP BY queue_name, status
    ORDER BY queue_name, status
  `);

  return result.rows;
}

module.exports = {
  getJobById,
  getJobsByStatus,
  getJobsByQueue,
  getJobStats,
};
```

This module provides several functions to query job information from our Postgres database:

1. `getJobById`: Retrieve a specific job by its ID
2. `getJobsByStatus`: Get jobs filtered by their status
3. `getJobsByQueue`: Get jobs from a specific queue
4. `getJobStats`: Get statistics about jobs across all queues

These functions will be used by the API to provide job status and statistics to the user.

## Build retry and error handling

Bull provides built-in retry capabilities. Let's set up our worker with proper retry and error handling. Create a file named `src/worker.js`:

```javascript
// src/worker.js
const { emailQueue, imageProcessingQueue, dataExportQueue } = require('./queues');

const processEmailJob = require('./processors/emailProcessor');
const processImageJob = require('./processors/imageProcessor');
const processDataExportJob = require('./processors/dataExportProcessor');
const { updateJobStatus } = require('./utils/queueHelper');

// Set up processors with retry logic
emailQueue.process(processEmailJob);
imageProcessingQueue.process(processImageJob);
dataExportQueue.process(processDataExportJob);

// Global error handlers for each queue
const setupErrorHandlers = (queue) => {
  queue.on('failed', async (job, err) => {
    console.error(`Job ${job.id} in ${queue.name} queue failed:`, err.message);

    // Update job status and increment attempt count
    await updateJobStatus(job.id, queue.name, {
      status: job.attemptsMade >= job.opts.attempts ? 'failed' : 'waiting',
      attempts: job.attemptsMade,
      error: err.message,
    });
  });

  queue.on('completed', async (job, result) => {
    console.log(`Job ${job.id} in ${queue.name} queue completed`);
  });

  queue.on('stalled', async (job) => {
    console.warn(`Job ${job.id} in ${queue.name} queue has stalled`);

    // Update job status
    await updateJobStatus(job.id, queue.name, {
      status: 'stalled',
    });
  });
};

// Set up error handlers for all queues
setupErrorHandlers(emailQueue);
setupErrorHandlers(imageProcessingQueue);
setupErrorHandlers(dataExportQueue);

console.log('Worker started processing jobs...');
```

This worker file:

1. Imports all our queues and job processors
2. Assigns each processor to its respective queue
3. Sets up error handlers for failed, completed, and stalled jobs
4. Updates the job status in Postgres based on these events

## Set up a simple API

Let's create a simple Express API to add jobs to the queue and check their status. Create a file named `src/api.js`:

```javascript
// src/api.js
const express = require('express');
const { emailQueue, imageProcessingQueue, dataExportQueue } = require('./queues');
const { addJob } = require('./utils/queueHelper');
const {
  getJobById,
  getJobsByStatus,
  getJobsByQueue,
  getJobStats,
} = require('./utils/jobsRepository');

const app = express();
app.use(express.json());
const port = process.env.PORT || 3000;

// Add email job
app.post('/jobs/email', async (req, res) => {
  try {
    const { to, subject, body } = req.body;

    // Validate input
    if (!to || !subject || !body) {
      return res.status(400).json({ error: 'Missing required fields' });
    }

    // Add job to queue with 3 retry attempts
    const job = await addJob(
      emailQueue,
      { to, subject, body },
      {
        attempts: 3,
        backoff: {
          type: 'exponential',
          delay: 1000,
        },
      }
    );

    res.status(201).json({
      message: 'Email job added to queue',
      jobId: job.jobId,
    });
  } catch (error) {
    console.error('Error adding email job:', error);
    res.status(500).json({ error: 'Failed to add job to queue' });
  }
});

// Add image processing job
app.post('/jobs/image', async (req, res) => {
  try {
    const { imageUrl, options } = req.body;

    // Validate input
    if (!imageUrl) {
      return res.status(400).json({ error: 'Missing required fields' });
    }

    // Add job to queue with 2 retry attempts
    const job = await addJob(
      imageProcessingQueue,
      { imageUrl, options },
      {
        attempts: 2,
        backoff: {
          type: 'fixed',
          delay: 5000,
        },
      }
    );

    res.status(201).json({
      message: 'Image processing job added to queue',
      jobId: job.jobId,
    });
  } catch (error) {
    console.error('Error adding image processing job:', error);
    res.status(500).json({ error: 'Failed to add job to queue' });
  }
});

// Add data export job
app.post('/jobs/export', async (req, res) => {
  try {
    const { query, format } = req.body;

    // Validate input
    if (!query || !format) {
      return res.status(400).json({ error: 'Missing required fields' });
    }

    // Add job to queue with 3 retry attempts
    const job = await addJob(
      dataExportQueue,
      { query, format },
      {
        attempts: 3,
        backoff: {
          type: 'exponential',
          delay: 2000,
        },
      }
    );

    res.status(201).json({
      message: 'Data export job added to queue',
      jobId: job.jobId,
    });
  } catch (error) {
    console.error('Error adding data export job:', error);
    res.status(500).json({ error: 'Failed to add job to queue' });
  }
});

// Get job status
app.get('/jobs/:id', async (req, res) => {
  try {
    const job = await getJobById(req.params.id);

    if (!job) {
      return res.status(404).json({ error: 'Job not found' });
    }

    res.json(job);
  } catch (error) {
    console.error('Error fetching job:', error);
    res.status(500).json({ error: 'Failed to fetch job' });
  }
});

// Get jobs by status
app.get('/jobs/status/:status', async (req, res) => {
  try {
    const limit = parseInt(req.query.limit) || 100;
    const offset = parseInt(req.query.offset) || 0;
    const jobs = await getJobsByStatus(req.params.status, limit, offset);

    res.json(jobs);
  } catch (error) {
    console.error('Error fetching jobs by status:', error);
    res.status(500).json({ error: 'Failed to fetch jobs' });
  }
});

// Get jobs by queue
app.get('/jobs/queue/:queue', async (req, res) => {
  try {
    const limit = parseInt(req.query.limit) || 100;
    const offset = parseInt(req.query.offset) || 0;
    const jobs = await getJobsByQueue(req.params.queue, limit, offset);

    res.json(jobs);
  } catch (error) {
    console.error('Error fetching jobs by queue:', error);
    res.status(500).json({ error: 'Failed to fetch jobs' });
  }
});

// Get job stats
app.get('/stats', async (req, res) => {
  try {
    const stats = await getJobStats();
    res.json(stats);
  } catch (error) {
    console.error('Error fetching job stats:', error);
    res.status(500).json({ error: 'Failed to fetch job stats' });
  }
});

app.listen(port, () => {
  console.log(`API server listening on port ${port}`);
});
```

This API provides endpoints to:

1. Add jobs to the different queues
2. Check the status of a specific job
3. List jobs by status or queue
4. Get job statistics

We will later add a Bull Board dashboard to monitor the job queues in real-time and use these endpoints to test our job queue system.

## Create the main entry point

With all the above in place, let's create the main entry point for our application. Create a file named `src/index.js`:

```javascript
// src/index.js
const dotenv = require('dotenv');
dotenv.config();

// In a production environment, you would typically run the API and worker in separate processes
// For simplicity, we're starting both in the same file
const startMode = process.env.START_MODE || 'all';

if (startMode === 'all' || startMode === 'api') {
  require('./api');
  console.log('API server started');
}

if (startMode === 'all' || startMode === 'worker') {
  require('./worker');
  console.log('Worker process started');
}
```

Update your `package.json` to include start scripts:

```json
{
  "scripts": {
    "start": "node src/index.js",
    "start:api": "START_MODE=api node src/index.js",
    "start:worker": "START_MODE=worker node src/index.js"
  }
}
```

## Run the application

Now you can run the application:

```bash
npm start
```

If you were to navigate to `http://localhost:3000`, you would see the API server running.

This will start both the API server and the worker process. In a production environment, you might want to run them separately:

```bash
# Start the API server
npm run start:api

# Start the worker in a different terminal
npm run start:worker
```

## Monitor your job queue

To monitor your job queue in real-time, you can use Bull UI tools like [Bull Board](https://github.com/felixmosh/bull-board).

Install Bull Board:

```bash
npm install @bull-board/express @bull-board/api
```

Then add the following to your `src/api.js` file:

```javascript
// Add at the top of the file
const { createBullBoard } = require('@bull-board/api');
const { BullAdapter } = require('@bull-board/api/bullAdapter');
const { ExpressAdapter } = require('@bull-board/express');

// Add before app.use(express.json())
const serverAdapter = new ExpressAdapter();
serverAdapter.setBasePath('/admin/queues');

createBullBoard({
  queues: [
    new BullAdapter(emailQueue),
    new BullAdapter(imageProcessingQueue),
    new BullAdapter(dataExportQueue),
  ],
  serverAdapter,
});

app.use('/admin/queues', serverAdapter.getRouter());
```

Now you can access the Bull Board dashboard at `http://localhost:3000/admin/queues`.

## Testing Your Job Queue System

Now that you've set up your job queue system, let's test it to ensure everything works correctly. We'll create several test jobs, monitor their progress, and examine the results.

First, make sure your system is running with both the API server and worker process:

```bash
npm start
```

Using a tool like cURL or Postman, you can send requests to your API to create new jobs:

### 1. Create an Email Job

```bash
curl -X POST http://localhost:3000/jobs/email \
  -H "Content-Type: application/json" \
  -d '{
    "to": "test@example.com",
    "subject": "Test Email",
    "body": "This is a test email from our job queue system."
  }'
```

You should receive a response like:

```json
{
  "message": "Email job added to queue",
  "jobId": "1"
}
```

### 2. Create an Image Processing Job

```bash
curl -X POST http://localhost:3000/jobs/image \
  -H "Content-Type: application/json" \
  -d '{
    "imageUrl": "https://example.com/sample-image.jpg",
    "options": {
      "resize": { "width": 800, "height": 600 },
      "format": "webp"
    }
  }'
```

### 3. Create a Data Export Job

```bash
curl -X POST http://localhost:3000/jobs/export \
  -H "Content-Type: application/json" \
  -d '{
    "query": "SELECT * FROM jobs LIMIT 10",
    "format": "csv"
  }'
```

### Monitor Job Progress

After creating the jobs, you can monitor their progress in several ways:

#### Check Job Status via API

Use the job ID returned when you created the job to check its status:

```bash
curl http://localhost:3000/jobs/1 | jq
```

You should see the job details, including its current status (pending, active, completed, or failed):

```json
{
  "id": 1,
  "job_id": "1",
  "queue_name": "email",
  "data": {
    "to": "test@example.com",
    "subject": "Test Email",
    "body": "This is a test email from our job queue system."
  },
  "status": "completed",
  "result": { "delivered": true, "timestamp": "2025-03-16T11:32:47.123Z" },
  "error": null,
  "created_at": "2025-03-16T11:32:45.678Z",
  "started_at": "2025-03-16T11:32:46.789Z",
  "completed_at": "2025-03-16T11:32:47.890Z",
  "attempts": 1,
  "max_attempts": 3
}
```

#### View Jobs by Status

To see all jobs with a specific status:

```bash
curl http://localhost:3000/jobs/status/completed
```

This will return a list of all completed jobs. You can also check for "pending", "active", or "failed" jobs.

#### Check Queue Statistics

To see statistics about all your queues:

```bash
curl http://localhost:3000/stats | jq
```

This will show you a breakdown of job counts by queue and status:

```json
[
  { "queue_name": "email", "status": "completed", "count": "1" },
  { "queue_name": "image-processing", "status": "active", "count": "1" },
  { "queue_name": "data-export", "status": "pending", "count": "1" }
]
```

### Bull Board Dashboard

If you've set up Bull Board as described earlier, you can visit `http://localhost:3000/admin/queues` in your browser to see a visual dashboard of all your queues and jobs.

![Bull Board Dashboard](/guides/images/job-queue-system/bull-board.png)

This dashboard provides a real-time view of your queues, including active, waiting, and completed jobs.

## Verifying Database Records

To check that the job information is being correctly stored in your Neon Postgres database, you can use the Neon SQL Editor or any PostgreSQL client to run queries:

```sql
SELECT * FROM jobs;
```

You can also check processing times for completed jobs:

```sql
SELECT
  job_id,
  queue_name,
  status,
  EXTRACT(EPOCH FROM (completed_at - started_at)) AS processing_time_seconds
FROM jobs
WHERE status = 'completed';
```

This query will show you the processing time for each completed job in seconds.

## Conclusion

In this guide, you've built a job queue system using Node.js, Bull, and Neon Postgres. This system can handle different types of background tasks, retry failed jobs, and track job status and results in a PostgreSQL database.

The combination of Bull's queue management backed by Redis and Neon's serverless Postgres for persistent job tracking provides a scalable and reliable solution for background processing. It is a great foundation for building more complex job processing systems in your applications.

You can extend this system by adding more specialized queues, extending the monitoring, implementing user notifications, or integrating with other services in your architecture.

## Additional resources

- [Bull Documentation](https://github.com/OptimalBits/bull/blob/master/REFERENCE.md)
- [Neon Postgres Documentation](/docs)
- [Node.js PostgreSQL Client (pg)](https://node-postgres.com/)
- [Redis Documentation](https://redis.io/documentation)

<NeedHelp />


# Building a Nuxt.js app with a Vercel and Neon branching workflow

---
title: 'Building a Nuxt.js app with a Vercel and Neon branching workflow'
subtitle: 'Automate database branching for every preview deployment using the native Neon Vercel Integration'
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-07-14T00:00:00.000Z'
updatedOn: '2025-07-14T00:00:00.000Z'
---

[Nuxt.js](https://nuxt.com) is an open-source, progressive framework built on [Vue.js](https://vuejs.org/) that simplifies web development. It enhances Vue with versatile rendering options, including default [universal rendering (SSR)](https://nuxt.com/docs/guide/concepts/rendering#universal-rendering) for fast initial loads and strong SEO, and [client-side rendering](https://nuxt.com/docs/guide/concepts/rendering#client-side-rendering) for highly interactive applications. Nuxt also supports advanced strategies like [hybrid rendering](https://nuxt.com/docs/guide/concepts/rendering#hybrid-rendering) to mix modes per-route.

As your Nuxt application grows, managing database changes for new features can be challenging. How do you test a feature that requires database schema changes without disrupting your live application? This is where the integration between [Vercel](https://vercel.com) and [Neon](https://neon.com) comes in. Vercel is a deployment platform, and Neon provides a serverless Postgres database. Together, they offer seamless [**database branching**](/branching).

When you enable the integration, every time you push a new feature branch, Vercel automatically creates a preview deployment. Simultaneously, Neon creates an isolated copy of your database just for that branch. This gives you a safe, sandboxed environment to develop and test with realistic data, without any risk to your live application.

In this guide, you will learn how to:

- Set up a Nuxt.js project and connect it to a GitHub repository.
- Install and configure the Vercel-managed Neon Integration.
- Define a database schema and manage migrations with Drizzle ORM.
- Build a full-stack Todo application with Nuxt.js server routes.
- Test the automated branching workflow: Push a new feature branch and see Vercel and Neon automatically create a sandboxed environment with its own database branch.
- Safely merge schema changes into production.

## Prerequisites

Before you begin, ensure you have the following:

- Node.js `v18.0` or later.
- A Vercel account
- A Neon account. The Vercel integration can create one for you if you don't have one.

<Steps>

## Initialize Nuxt.js project

First, create a new Nuxt.js application and push it to a GitHub repository, which Vercel will use for deployments.

1.  **Create a Nuxt.js app:**
    Open your terminal and run the following command:

    ```bash
    npm create nuxt nuxt-neon-vercel-todo
    ```

    When prompted, choose the following options:
    - **Package manager:** `npm`
    - **Initialize git repository:** `Yes`
    - **Install official modules:** `No`

    You should see output similar to this:

    ```text
    > npm create nuxt nuxt-neon-vercel-todo
    Need to install the following packages:
    create-nuxt@3.26.0
    Ok to proceed? (y) y

           .d$b.
          i$$A$$L  .d$b
        .$$F` `$$L.$$A$$.
       j$$'    `4$$:` `$$.
      j$$'     .4$:    `$$.
     j$$`     .$$:      `4$L
    :$$:____.d$$:  _____.:$$:
    `4$$$$$$$$P` .i$$$$$$$$P`

    ℹ Welcome to Nuxt!                                                                                            nuxi 9:46:56 AM
    ℹ Creating a new project in nuxt-neon-vercel-todo.                                                            nuxi 9:46:58 AM

    ✔ Which package manager would you like to use?
    npm
    ◐ Installing dependencies...                                                                                   nuxi 9:47:02 AM
    npm WARN deprecated node-domexception@1.0.0: Use your platform's native DOMException instead
    npm WARN deprecated @types/parse-path@7.1.0: This is a stub types definition. parse-path provides its own type definitions, so you do not need this installed.

    > postinstall
    > nuxt prepare

    ✔ Types generated in .nuxt                                                                                    nuxi 9:48:15 AM

    added 751 packages, and audited 753 packages in 1m

    171 packages are looking for funding
    run `npm fund` for details

    found 0 vulnerabilities
    ✔ Installation completed.                                                                                     nuxi 9:48:15 AM

    ✔ Initialize git repository?
    Yes
    ℹ Initializing git repository...                                                                              nuxi 9:48:18 AM

    Initialized empty Git repository in /nuxt-neon-vercel-todo/.git/

    ✔ Would you like to install any of the official modules?
    No
    ```

2.  Navigate into the project directory

    ```bash
    cd nuxt-neon-vercel-todo
    ```

3.  Push the initial project to a GitHub repository:

    [Create a new repository on GitHub](https://docs.github.com/en/repositories/creating-and-managing-repositories/quickstart-for-repositories) (e.g., `nuxt-neon-vercel-todo`). Then, run the following commands in your terminal, replacing `<your-github-repo-url>` with your repository's URL.

    ```bash
    git add .
    git commit -m "Initial commit"
    git remote add origin <your-github-repo-url>
    git push -u origin main
    ```

To begin, you will set up the Vercel-managed Neon Integration. This is a crucial first step that provisions a Neon database and automatically injects the `DATABASE_URL` environment variable into your Nuxt.js application. This setup streamlines both local development and Vercel deployments.

## Create a Vercel project

1.  Login to [Vercel dashboard](https://vercel.com/dashboard).
2.  Click on **Add New** and select **Project**.
3.  Select the GitHub repository you created earlier (`nuxt-neon-vercel-todo`).
4.  Vercel will automatically detect that this is a Nuxt.js application and configure the build settings accordingly.
5.  Click **Deploy** to create your Vercel project.

    ![Create Nuxt.js project in Vercel](/docs/guides/nuxt-vercel-create-project.png)

## Configure the Vercel Neon integration

Install the Neon integration from the Vercel Marketplace. This process will create a Neon project and link it to your Vercel account.

1.  Navigate to the [Neon integration page on the Vercel Marketplace](https://vercel.com/marketplace/neon) and click **Install**.
2.  Accept the terms, pick a region & plan, then name your database. (Remember: a "Database" in Vercel is a Project in Neon.)

## Connect the database to your Vercel project

With the integration installed, you need to connect your `nuxt-neon-vercel-todo` Vercel project to your new Neon database. This is where you enable the automated branching feature.

1.  From the **Storage** tab in your Vercel dashboard, select your Neon database.
2.  Click the **Connect Project** tab and select your `nuxt-neon-vercel-todo` project from the dropdown.
3.  **Enable Preview Branches:** In the **Deployments Configuration** section, ensure the **Preview** toggle is enabled. This is the key step that instructs Vercel to create a new Neon database branch for every Preview Deployment.
    ![Connect Vercel Project and enable Preview deployments](/docs/guides/nuxt-vercel-neon-connect-project.png)
4.  Click **Connect**.

Vercel will now automatically add the `DATABASE_URL` environment variable to your project, making it available in Production, Preview, and Development environments.

Now that your Vercel project is connected to Neon, you can start defining your database schema and managing migrations.

## Add database schema and migrations

To manage your database schema and migrations, you will use [Drizzle ORM](https://orm.drizzle.team) with the Neon serverless driver. Drizzle ORM provides a type-safe way to define your database schema and run migrations.

1.  Install Drizzle dependencies

    ```bash
    npm install drizzle-orm @neondatabase/serverless
    npm install -D drizzle-kit
    ```

2.  **Define the schema:**

    Create a new file at `server/db/schema.ts` to define the `todos` table.

    ```typescript
    import { pgTable, serial, text, boolean, timestamp } from 'drizzle-orm/pg-core';

    export const todos = pgTable('todos', {
      id: serial('id').primaryKey(),
      text: text('text').notNull(),
      completed: boolean('completed').default(false).notNull(),
      createdAt: timestamp('created_at').defaultNow().notNull(),
    });
    ```

    The `todos` table has the following fields:
    - `id`: A unique identifier for each todo item.
    - `text`: The text of the todo item.
    - `completed`: A boolean indicating whether the todo is completed.
    - `createdAt`: A timestamp for when the todo was created.

3.  **Configure Drizzle Kit:**

    Create `drizzle.config.ts` in your project root. Drizzle Kit will use this to connect to your database to generate and run migrations.

    ```typescript
    import { defineConfig } from 'drizzle-kit';

    export default defineConfig({
      schema: './server/db/schema.ts',
      out: './server/db/migrations',
      dialect: 'postgresql',
      dbCredentials: {
        url: process.env.DATABASE_URL!,
      },
    });
    ```

4.  **Add migration scripts to `package.json`:**

    ```json
    {
      "scripts": {
        // ... other scripts
        "db:generate": "drizzle-kit generate",
        "db:migrate": "drizzle-kit migrate",
        "build": "nuxt build && npm run db:migrate"
      }
    }
    ```

    > The build script is modified to run `db:migrate` after building the Nuxt application. This ensures that any pending migrations are applied to the database before the application starts.

5.  **Generate the initial migration:**

    To run migrations locally or in the Vercel build step, you need the `DATABASE_URL` available. Create a `.env` file in your root directory.

    Now, go to your Vercel project's **Settings** -> **Environment Variables**, reveal the value for `DATABASE_URL`, and copy it into your `.env` file.

    ```ini
    DATABASE_URL="postgres://..."
    ```

    Now, generate your first migration file:

    ```bash
    npm run db:generate
    ```

    This creates a new SQL file in the `server/db/migrations` directory.

6.  **Commit the changes:**
    Add all the new files to git and commit them.

    ```bash
    git add .
    git commit -m "feat: setup drizzle and initial schema"
    git push
    ```

    Pushing to `main` will trigger a production deployment on Vercel. The `npm run build` command will execute `npm run db:migrate`, applying your initial migration to the main database branch.

## Build the Todo application

With the database configured, let's build the Todo application. This will include creating server routes for CRUD operations and a simple frontend to interact with the database.

    <Admonition type="warning" title="Security Note">
    The following code is a basic example and does not include authentication or input validation. In a production application, you should implement proper security measures to protect your API endpoints and database.
    </Admonition>

1.  **Create a Drizzle client utility:**

    Create `server/utils/db.ts` to instantiate and export the Drizzle client.

    ```typescript
    import { drizzle } from 'drizzle-orm/neon-http';
    import { neon } from '@neondatabase/serverless';
    import * as schema from '~/server/db/schema';

    const sql = neon(process.env.DATABASE_URL!);
    export const db = drizzle(sql, { schema });
    ```

2.  **Create API endpoints:**

    Nuxt automatically scans the `server/api` directory to create your API.
    - **GET `/api/todos`:** Create `server/api/todos.get.ts`

      ```typescript
      import { db } from '~/server/utils/db';
      import { todos } from '~/server/db/schema';
      import { desc } from 'drizzle-orm';

      export default defineEventHandler(async () => {
        return await db.select().from(todos).orderBy(desc(todos.createdAt));
      });
      ```

      The `GET` endpoint retrieves all todo items from the database, ordered by creation date.

    - **POST `/api/todos`:** Create `server/api/todos.post.ts`

      ```typescript
      import { db } from '~/server/utils/db';
      import { todos } from '~/server/db/schema';

      export default defineEventHandler(async (event) => {
        const body = await readBody(event);
        const [newTodo] = await db.insert(todos).values({ text: body.text }).returning();
        return newTodo;
      });
      ```

      The `POST` endpoint creates a new todo item with the provided text.

    - **PUT `/api/todos/:id`:** Create `server/api/todos/[id].put.ts`

      ```typescript
      import { db } from '~/server/utils/db';
      import { todos } from '~/server/db/schema';
      import { eq } from 'drizzle-orm';

      export default defineEventHandler(async (event) => {
        const id = getRouterParam(event, 'id');
        const body = await readBody(event);
        const [updatedTodo] = await db
          .update(todos)
          .set({ completed: body.completed })
          .where(eq(todos.id, Number(id)))
          .returning();
        return updatedTodo;
      });
      ```

      The `PUT` endpoint updates the completion status of a todo item by its ID.

    - **DELETE `/api/todos/:id`:** Create `server/api/todos/[id].delete.ts`

      ```typescript
      import { db } from '~/server/utils/db';
      import { todos } from '~/server/db/schema';
      import { eq } from 'drizzle-orm';

      export default defineEventHandler(async (event) => {
        const id = getRouterParam(event, 'id');
        await db.delete(todos).where(eq(todos.id, Number(id)));
        return { message: 'Todo deleted' };
      });
      ```

      The `DELETE` endpoint deletes a todo item by its ID.

3.  **Build the user interface:**

    Replace the content of `app.vue` with the following code to create frontend for the todo app.

    ```vue
    <template>
      <div class="container">
        <h1>Todo List</h1>
        <p>
          A Nuxt.js app with a serverless Postgres database from Neon, deployed on Vercel with
          automated database branching.
        </p>
        <form @submit.prevent="addTodo">
          <input v-model="newTodoText" placeholder="Add a new todo item" required />
          <button type="submit">Add Todo</button>
        </form>
        <ul>
          <li v-for="todo in todos" :key="todo.id">
            <span :class="{ completed: todo.completed }" @click="toggleTodo(todo)">
              {{ todo.text }}
            </span>
            <button class="remove-btn" @click="removeTodo(todo)">Remove</button>
          </li>
        </ul>
      </div>
    </template>

    <script setup>
    const { data: todos, refresh } = await useFetch('/api/todos');
    const newTodoText = ref('');

    const addTodo = async () => {
      if (!newTodoText.value.trim()) return;
      await $fetch('/api/todos', {
        method: 'POST',
        body: { text: newTodoText.value },
      });
      newTodoText.value = '';
      await refresh();
    };

    const toggleTodo = async (todo) => {
      await $fetch(`/api/todos/${todo.id}`, {
        method: 'PUT',
        body: { completed: !todo.completed },
      });
      await refresh();
    };

    const removeTodo = async (todo) => {
      await $fetch(`/api/todos/${todo.id}`, {
        method: 'DELETE',
      });
      await refresh();
    };
    </script>

    <style>
    body {
      background-color: #f4f4f9;
      color: #333;
    }
    .container {
      max-width: 600px;
      margin: 2rem auto;
      font-family: sans-serif;
      background: #fff;
      padding: 2rem;
      border-radius: 8px;
      box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
    }
    h1 {
      text-align: center;
      color: #000000;
    }
    p {
      text-align: center;
      margin-bottom: 2rem;
    }
    form {
      display: flex;
      margin-bottom: 1rem;
    }
    input {
      flex-grow: 1;
      padding: 0.75rem;
      border: 1px solid #ccc;
      border-radius: 4px;
    }
    button {
      padding: 0.75rem 1.5rem;
      border: none;
      background-color: #00b04f;
      color: white;
      border-radius: 4px;
      cursor: pointer;
      margin-left: 0.5rem;
    }
    ul {
      list-style: none;
      padding: 0;
    }
    li {
      display: flex;
      justify-content: space-between;
      align-items: center;
      padding: 0.75rem;
      border-bottom: 1px solid #eee;
    }
    li span {
      cursor: pointer;
    }
    .completed {
      text-decoration: line-through;
      color: #aaa;
    }
    .remove-btn {
      background-color: #ff4d4d;
      color: white;
      border: none;
      padding: 0.4rem 0.8rem;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
    ```

    The above code creates a simple Todo application. It includes a form to add new todos, a list to display existing todos, and buttons to toggle completion status and remove todos. It uses the API endpoints you created earlier to interact with the database.

4.  **Run the application locally:**

    Start your Nuxt.js application locally to test the functionality.

    ```bash
    npm run dev
    ```

    Open your browser and navigate to [http://localhost:3000](http://localhost:3000). You should see your Todo application running. You can add, toggle, and remove todos, which will interact with your Neon database.

    ![Nuxt Todo Application](/docs/guides/nuxt-todo-app.png)

5.  **Commit and push the application code:**

    ```bash
    git add .
    git commit -m "build: complete todo application"
    git push
    ```

## The branching workflow in action

To demonstrate the automated branching workflow, you will add a new feature to your Todo application that requires a database schema change. This will show how Vercel and Neon work together to create a safe, isolated environment for testing new features. For this example, you will add a `priority` field to the todos.

1.  **Create a new feature branch in git:**

    Run the following command to create a new branch. This will allow you to work on the new feature without affecting the main branch.

    ```bash
    git checkout -b feature/add-priority
    ```

2.  **Modify the database schema:**

    Update `server/db/schema.ts` to include a `priority` field.

    ```typescript
    import { pgTable, serial, text, boolean, timestamp } from 'drizzle-orm/pg-core'; // [!code --]
    import { pgTable, serial, text, boolean, timestamp, pgEnum } from 'drizzle-orm/pg-core'; // [!code ++]

    export const priorityEnum = pgEnum('priority', ['low', 'medium', 'high']); // [!code ++]

    export const todos = pgTable('todos', {
      id: serial('id').primaryKey(),
      text: text('text').notNull(),
      completed: boolean('completed').default(false).notNull(),
      priority: priorityEnum().default('medium').notNull(), // [!code ++]
      createdAt: timestamp('created_at').defaultNow().notNull(),
    });
    ```

3.  **Generate a new migration:**

    Run the `db:generate` command again. It will compare your updated schema to the database state and create a new migration file.

    ```bash
    npm run db:generate
    ```

4.  Update `app.vue` with the new priority field in the UI. Copy the following code to replace the existing content in `app.vue`.

    ```vue
    <template>
      <div class="container">
        <h1>Todo List</h1>
        <p>
          A Nuxt.js app with a serverless Postgres database from Neon, deployed on Vercel with
          automated database branching.
        </p>
        <form @submit.prevent="addTodo">
          <div class="form-row">
            <input v-model="newTodoText" placeholder="Add a new todo item" required />
            <select v-model="newTodoPriority" class="priority-select">
              <option value="low">Low Priority</option>
              <option value="medium">Medium Priority</option>
              <option value="high">High Priority</option>
            </select>
            <button type="submit">Add Todo</button>
          </div>
        </form>
        <ul>
          <li v-for="todo in todos" :key="todo.id" :class="`priority-${todo.priority}`">
            <div class="todo-content">
              <span :class="{ completed: todo.completed }" @click="toggleTodo(todo)">
                {{ todo.text }}
              </span>
              <span class="priority-badge" :class="`priority-${todo.priority}`">
                {{ todo.priority }}
              </span>
            </div>
            <button class="remove-btn" @click="removeTodo(todo)">Remove</button>
          </li>
        </ul>
      </div>
    </template>

    <script setup>
    const { data: todos, refresh } = await useFetch('/api/todos');
    const newTodoText = ref('');
    const newTodoPriority = ref('medium');

    const addTodo = async () => {
      if (!newTodoText.value.trim()) return;
      await $fetch('/api/todos', {
        method: 'POST',
        body: {
          text: newTodoText.value,
          priority: newTodoPriority.value,
        },
      });
      newTodoText.value = '';
      newTodoPriority.value = 'medium';
      await refresh();
    };

    const toggleTodo = async (todo) => {
      await $fetch(`/api/todos/${todo.id}`, {
        method: 'PUT',
        body: { completed: !todo.completed },
      });
      await refresh();
    };

    const removeTodo = async (todo) => {
      await $fetch(`/api/todos/${todo.id}`, {
        method: 'DELETE',
      });
      await refresh();
    };
    </script>

    <style>
    body {
      background-color: #f4f4f9;
      color: #333;
    }
    .container {
      max-width: 600px;
      margin: 2rem auto;
      font-family: sans-serif;
      background: #fff;
      padding: 2rem;
      border-radius: 8px;
      box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
    }
    h1 {
      text-align: center;
      color: #000000;
    }
    p {
      text-align: center;
      margin-bottom: 2rem;
    }
    form {
      margin-bottom: 1rem;
    }
    .form-row {
      display: flex;
      gap: 0.5rem;
      align-items: stretch;
    }
    input {
      flex-grow: 1;
      padding: 0.75rem;
      border: 1px solid #ccc;
      border-radius: 4px;
    }
    .priority-select {
      padding: 0.75rem;
      border: 1px solid #ccc;
      border-radius: 4px;
      background: white;
      min-width: 140px;
    }
    button {
      padding: 0.75rem 1.5rem;
      border: none;
      background-color: #00b04f;
      color: white;
      border-radius: 4px;
      cursor: pointer;
    }
    ul {
      list-style: none;
      padding: 0;
    }
    li {
      display: flex;
      justify-content: space-between;
      align-items: center;
      padding: 0.75rem;
      border-bottom: 1px solid #eee;
      border-left: 4px solid #ddd;
    }
    li.priority-high {
      border-left-color: #ff4444;
    }
    li.priority-medium {
      border-left-color: #ffaa00;
    }
    li.priority-low {
      border-left-color: #44ff44;
    }
    .todo-content {
      display: flex;
      align-items: center;
      gap: 0.75rem;
      flex-grow: 1;
    }
    li span {
      cursor: pointer;
    }
    .completed {
      text-decoration: line-through;
      color: #aaa;
    }
    .priority-badge {
      font-size: 0.75rem;
      padding: 0.25rem 0.5rem;
      border-radius: 12px;
      text-transform: uppercase;
      font-weight: bold;
      cursor: default !important;
    }
    .priority-badge.priority-high {
      background-color: #ffe6e6;
      color: #cc0000;
    }
    .priority-badge.priority-medium {
      background-color: #fff3e0;
      color: #e65100;
    }
    .priority-badge.priority-low {
      background-color: #e8f5e8;
      color: #2e7d32;
    }
    .remove-btn {
      background-color: #ff4d4d;
      color: white;
      border: none;
      padding: 0.4rem 0.8rem;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
    ```

5.  Update the `server/api/todos.post.ts` endpoint to handle the new `priority` field:

    ```typescript
    import { db } from '~/server/utils/db';
    import { todos } from '~/server/db/schema';

    export default defineEventHandler(async (event) => {
      const body = await readBody(event);
      const [newTodo] = await db
        .insert(todos)
        .values({
          text: body.text,
          priority: body.priority || 'medium', // [!code ++]
        })
        .returning();
      return newTodo;
    });
    ```

6.  **Push the feature branch to GitHub:**

    Commit the schema and migration changes and push the new branch.

    ```bash
    git add .
    git commit -m "feat: add priority field to todos"
    git push -u origin feature/add-priority
    ```

7.  **Check the Preview deployment:**
    - **Vercel:** Go to your project in Vercel. You will see a new **Preview deployment** being built for the `feature/add-priority` branch.
    - **Neon:** Go to your project in the Neon Console and click on the **Branches** tab. You will see a new branch has been automatically created, named something like `preview/feature/add-priority`. This branch is an instant, copy-on-write clone of your main branch.

    When Vercel built the preview deployment, the `npm run build` command ran `db:migrate` against the **new preview database branch**, applying the migration to add the `priority` column. Your preview URL is now running the new feature code against a database with the updated schema, all without touching production.

    ![Preview deployment](/docs/guides/nuxt-vercel-neon-preview-deployment.png)

8.  **Merge the pull request:**

    On GitHub, create and merge the pull request for your feature branch.
    Merging to `main` will trigger a new **Production Deployment** in Vercel. The build process will run again, but this time `db:migrate` will run against your main production database, safely applying the new schema change.

</Steps>

## Summary

You have successfully configured a Nuxt.js application with a fully automated and safe database branching workflow. For every pull request, Vercel and Neon now work together to create an isolated preview environment, complete with its own copy-on-write database branch.

This workflow allows developers to make and test schema changes with confidence, eliminating a common bottleneck in the development lifecycle and accelerating your team's ability to ship features safely.

## Resources

- **Vercel-managed Neon Integration:** [Vercel Marketplace](https://vercel.com/marketplace/neon)
- **Nuxt.js Documentation:** [nuxt.com](https://nuxt.com)
- **Drizzle ORM:** [orm.drizzle.team](https://orm.drizzle.team)
- **Neon Documentation:** [neon.tech/docs](/docs)

<NeedHelp/>


# Manage Neon with OpenTofu

---
title: Manage Neon with OpenTofu
subtitle: Use OpenTofu to provision and manage your Neon projects, branches, endpoints, roles, databases, and other resources as code.
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-05-26T00:00:00.000Z'
updatedOn: '2025-05-26T00:00:00.000Z'
---

[OpenTofu](https://opentofu.org) is an open-source infrastructure as code (IaC) tool, forked from Terraform, that allows you to define and provision cloud resources in a declarative configuration language. By codifying infrastructure, OpenTofu enables consistent, repeatable, and automated deployments, significantly reducing manual errors. It is a community-driven alternative governed by the Linux Foundation.

This guide will show you how to use **OpenTofu to manage your Neon projects**, including your branches, databases, and compute endpoints. By using OpenTofu with Neon, you get better control, can track changes, and automate your database setup.

Neon can be managed using the following community-developed Terraform provider, which is compatible with OpenTofu:

**Terraform Provider Neon - Maintainer: Dmitry Kisler**

- [GitHub repository](https://github.com/kislerdm/terraform-provider-neon)

<Admonition type="note">
This provider is a Terraform provider compatible with OpenTofu. It is not maintained or officially supported by Neon. Use at your own discretion. If you have questions about the provider, please contact the project maintainer.
</Admonition>

## Provider usage notes

- **Provider upgrades**: When using `tofu init -upgrade` to update a provider, be aware that changes in the provider’s schema or defaults can lead to unintended resource replacements. This may occur when certain attributes are altered or reset.

  To avoid unintended resource replacements which can result in data loss:
  - Review the provider’s changelog for any breaking changes that might affect your resources before upgrading to a new version.
  - For CI pipelines and auto-approved pull requests, only use `tofu init`. Running `tofu init -upgrade` should be done manually followed by plan reviews.
  - Run `tofu plan` before applying any changes to detect potential differences and review the behavior of resource updates.
  - Use [lifecycle protections](https://opentofu.org/docs/language/meta-arguments/lifecycle/) on critical resources to ensure they're not recreated unintentionally.
  - Explicitly define all critical resource parameters in your OpenTofu configurations, even if they had defaults previously.
  - On Neon paid plans, you can enable branch protection to prevent unintended deletion of branches and projects. To learn more, see [Protected branches](/docs/guides/protected-branches).

- **Provider maintenance**: As Neon enhances existing features and introduces new ones, the [Neon API](https://api-docs.neon.tech/reference/getting-started-with-neon-api) will continue to evolve. These changes may not immediately appear in community-maintained providers. If you notice that a provider requires an update, please reach out to the maintainer by opening an issue or contributing to the provider's GitHub repository.

## Prerequisites

Before you begin, ensure you have the following:

1.  **OpenTofu CLI installed:** If you don't have OpenTofu installed, download and install it from the [official OpenTofu website](https://opentofu.org/docs/intro/install/).
2.  **Neon Account:** You'll need a Neon account. If you don't have one, sign up at [neon.tech](https://console.neon.tech/signup).
3.  **Neon API key:** Generate an API key from the Neon Console. Navigate to your Account Settings > API Keys. This key is required for the provider to authenticate with the Neon API. Learn more about creating API keys in [Manage API keys](/docs/manage/api-keys).

## Set up the OpenTofu Neon provider

1.  **Create a project directory:**
    Create a new directory for your OpenTofu project and navigate into it.

    ```shell
    mkdir neon-opentofu-project
    cd neon-opentofu-project
    ```

2.  **Create a `main.tf` file:**
    This file will contain your OpenTofu configuration. Start by declaring the required Neon provider. OpenTofu can use providers from the tofu registry.

    ```tofu
    tofu {
      required_providers {
        neon = {
          source  = "kislerdm/neon"
        }
      }
    }

    provider "neon" {}
    ```

3.  **Initialize OpenTofu:**
    Run the `tofu init` command in your project directory. This command downloads and installs the Neon provider.
    ```shell
    tofu init
    ```

## Configure authentication

The Neon provider needs your Neon API key to manage resources. You can configure it in two ways:

1.  **Directly in the provider block (Less secure):**
    For quick testing, you can **hardcode your API key** directly within `provider "neon"` block. However, this method isn't recommended for production environments or shared configurations. A more secure alternative is to retrieve the API key from a secrets management service like [AWS Secrets Manager](https://aws.amazon.com/secrets-manager/) or [HashiCorp Vault](https://developer.hashicorp.com/vault), and then update your provider block to reflect this.

    ```tofu
    provider "neon" {
      api_key = "<YOUR_NEON_API_KEY>"
    }
    ```

2.  **Using environment variables:**
    The provider will automatically use the `NEON_API_KEY` environment variable if set.

    ```shell
    export NEON_API_KEY="<YOUR_NEON_API_KEY>"
    ```

    If the environment variable is set, you can leave the `provider "neon"` block empty:

    ```tofu
    provider "neon" {}
    ```

<Admonition type="note">
The following sections primarily detail the creation of Neon resources. To manage existing resources, use the `tofu import` command or `import` blocks. More information can be found in the [Import Existing Neon Resources](#import-existing-neon-resources-with-opentofu) section.
</Admonition>

## Manage Neon resources with OpenTofu

This section provides examples of how to manage Neon resources using OpenTofu. You can create and manage projects, branches, endpoints, roles, databases, and more.

### Managing projects

A Neon project is the top-level container for your Postgres databases, branches, and endpoints.

```tofu
resource "neon_project" "my_app_project" {
  name       = "my-application-project"
  pg_version = 16
  region_id  = "aws-us-east-1"

  # Configure default branch settings (optional)
  branch {
    name          = "production"
    database_name = "app_db"
    role_name     = "app_admin"
  }

  # Configure default endpoint settings (optional)
  default_endpoint_settings {
    autoscaling_limit_min_cu = 0.25
    autoscaling_limit_max_cu = 1.0
    # suspend_timeout_seconds  = 300
  }
}
```

This configuration creates a new Neon project.

**Key `neon_project` attributes:**

- `name`: (Optional) Name of the project.
- `pg_version`: (Optional) PostgreSQL version (e.g., 14, 15, 16, 17).
- `region_id`: (Optional) The region where the project will be created (e.g., `aws-us-east-1`).
  > For up-to-date information on available regions, see [Neon Regions](/docs/introduction/regions).
- `branch {}`: (Optional) Block to configure the default primary branch.

**Output project details:**
You can output computed values like the project ID or connection URI:

```tofu
output "project_id" {
  value = neon_project.my_app_project.id
}

output "project_connection_uri" {
  description = "Default connection URI for the primary branch (contains credentials)."
  value       = neon_project.my_app_project.connection_uri
  sensitive   = true
}

output "project_default_branch_id" {
  value = neon_project.my_app_project.default_branch_id
}

output "project_database_user" {
  value = neon_project.my_app_project.database_user
}
```

For more attributes and options on managing projects, refer to the [Provider's documentation](https://github.com/kislerdm/terraform-provider-neon/blob/master/docs/resources/project.md).

### Managing branches

You can create branches from the primary branch or any other existing branch.

```tofu
resource "neon_branch" "dev_branch" {
  project_id = neon_project.my_app_project.id
  name       = "feature-x-development"
  parent_id  = neon_project.my_app_project.default_branch_id # Branch from the project's primary branch

  # Optional: Create a protected branch
  # protected = "yes"

  # Optional: Create from a specific LSN or timestamp of the parent
  # parent_lsn = "..."
  # parent_timestamp = 1678886400 # Unix epoch
}
```

**Key `neon_branch` attributes:**

- `project_id`: (Required) ID of the parent project.
- `name`: (Optional) Name for the new branch.
- `parent_id`: (Optional) ID of the parent branch. If not specified, defaults to the project's primary branch.
- `protected`: (Optional, String: "yes" or "no") Set to protect the branch.
- `parent_lsn`: (Optional) LSN of the parent branch to create from.
- `parent_timestamp`: (Optional) Timestamp of the parent branch to create from.

> `protected` attribute is only available for paid plans. It allows you to protect branches from deletion or modification.

For more attributes and options on managing branches, refer to the [Provider's documentation](https://github.com/kislerdm/terraform-provider-neon/blob/master/docs/resources/branch.md).

### Managing endpoints

Endpoints provide connection strings to access your branches. Each branch can have multiple read-only endpoints but only one read-write endpoint.

Before creating an endpoint, you must first create a **branch** for it to connect to. Here's how to create a read-write endpoint for your `dev_branch`:

```tofu
resource "neon_endpoint" "dev_endpoint" {
  project_id = neon_project.my_app_project.id
  branch_id  = neon_branch.dev_branch.id
  type       = "read_write" # "read_write" or "read_only"

  autoscaling_limit_min_cu = 0.25
  autoscaling_limit_max_cu = 0.5
  # suspend_timeout_seconds  = 600

  # Optional: Enable connection pooling
  # pooler_enabled = true
}

output "dev_endpoint_host" {
  value = neon_endpoint.dev_endpoint.host
}
```

**Key `neon_endpoint` attributes:**

- `project_id`: (Required) ID of the parent project.
- `branch_id`: (Required) ID of the branch this endpoint connects to.
- `type`: (Optional) `read_write` (default) or `read_only`. A branch can only have one `read_write` endpoint.
- `autoscaling_limit_min_cu`/`autoscaling_limit_max_cu`: (Optional) Compute units for autoscaling.
- `suspend_timeout_seconds`: (Optional) Inactivity period before suspension. Only available for paid plans.
- `pooler_enabled`: (Optional) Enable connection pooling.

<Admonition type="note">
It is not possible currently to change the endpoint type after creation. The `type` attribute is immutable, meaning you cannot modify it once the endpoint is created. This includes changing from `read_write` to `read_only` or vice versa. This is a limitation of the Neon API and the provider's current implementation. You must destroy the existing endpoint and create a new one with the desired type.
</Admonition>

For more attributes and options on managing endpoints, refer to the [Provider's documentation](https://github.com/kislerdm/terraform-provider-neon/blob/master/docs/resources/endpoint.md)

### Managing roles

Roles (users) are managed per branch. Before creating a role, ensure you have a branch created. Follow the [Managing Branches](#managing-branches) section for details.

```tofu
resource "neon_role" "app_user" {
  project_id = neon_project.my_app_project.id
  branch_id  = neon_branch.dev_branch.id
  name       = "application_user"
}

output "app_user_password" {
  value     = neon_role.app_user.password
  sensitive = true
}
```

**Key `neon_role` attributes:**

- `project_id`: (Required) ID of the parent project.
- `branch_id`: (Required) ID of the branch for this role.
- `name`: (Required) Name of the role.
- `password`: (Computed, Sensitive) The generated password for the role.

For more attributes and options on managing roles, refer to the [Provider's documentation](https://github.com/kislerdm/terraform-provider-neon/blob/master/docs/resources/role.md)

### Managing databases

Databases are also managed per branch. Follow the [Managing Branches](#managing-branches) section for details on creating a branch.

```tofu
resource "neon_database" "service_db" {
  project_id = neon_project.my_app_project.id
  branch_id  = neon_branch.dev_branch.id
  name       = "service_specific_database"
  owner_name = neon_role.app_user.name
}
```

**Key `neon_database` attributes:**

- `project_id`: (Required) ID of the parent project.
- `branch_id`: (Required) ID of the branch for this database.
- `name`: (Required) Name of the database.
- `owner_name`: (Required) Name of the role that will own this database.

For more attributes and options on managing databases, refer to the [Provider's documentation](https://github.com/kislerdm/terraform-provider-neon/blob/master/docs/resources/database.md)

### Managing API keys

You can manage Neon API keys themselves using OpenTofu.

```tofu
resource "neon_api_key" "ci_cd_key" {
  name = "automation-key-for-ci"
}

output "ci_cd_api_key_value" {
  description = "The actual API key token."
  value       = neon_api_key.ci_cd_key.key
  sensitive   = true
}
```

**Key `neon_api_key` attributes:**

- `name`: (Required) A descriptive name for the API key.
- `key`: (Computed, Sensitive) The generated API key token.

### Advanced: Project permissions

Share project access with other users.

```tofu
resource "neon_project_permission" "share_with_colleague" {
  project_id = neon_project.my_app_project.id
  grantee    = "colleague@example.com"
}
```

### Advanced: JWKS URL for RLS

Configure JWKS URL for Row Level Security authorization.

```tofu
resource "neon_jwks_url" "auth_provider_jwks" {
  project_id    = neon_project.my_app_project.id
  # Use the default role from the project, or specify custom roles
  role_names    = [neon_project.my_app_project.database_user]
  provider_name = "YourAuthProviderName" # e.g., "clerk"
  jwks_url      = "<https://<YOUR_AUTH_PROVIDER_JWKS_URL>" # Replace with your actual JWKS URL
}
```

> For a list of supported providers, see [Neon RLS: Supported Providers](/docs/guides/neon-rls#supported-providers).

For more attributes and options on managing JWKS URLs, refer to the [Provider's documentation](https://github.com/kislerdm/terraform-provider-neon/blob/master/docs/resources/jwks_url.md)

### Advanced: VPC endpoint management (for Neon private networking)

These resources are used for organizations with Scale or Enterprise plans requiring private networking.

#### Assign VPC endpoint to organization

```tofu
resource "neon_vpc_endpoint_assignment" "org_vpc_endpoint" {
  org_id          = "your-neon-organization-id" # Replace with your actual Org ID
  region_id       = "aws-us-east-1"             # Neon region ID
  vpc_endpoint_id = "vpce-xxxxxxxxxxxxxxxxx"    # Your AWS VPC Endpoint ID
  label           = "main-aws-vpc-endpoint"
}
```

For more attributes and options on managing VPC endpoints, refer to the [Provider's documentation](https://github.com/kislerdm/terraform-provider-neon/blob/master/docs/resources/vpc_endpoint_assignment.md)

#### Restrict project to VPC endpoint

```tofu
resource "neon_vpc_endpoint_restriction" "project_to_vpc" {
  project_id      = neon_project.my_app_project.id
  vpc_endpoint_id = neon_vpc_endpoint_assignment.org_vpc_endpoint.vpc_endpoint_id
  label           = "restrict-my-app-project-to-vpc"
}
```

For more attributes and options on managing VPC endpoint restrictions, refer to the [Provider's documentation](https://github.com/kislerdm/terraform-provider-neon/blob/master/docs/resources/vpc_endpoint_restriction.md)

## Apply the configuration with OpenTofu

Once you have defined your resources:

1.  **Format and validate:**

    ```shell
    tofu fmt
    tofu validate
    ```

2.  **Plan:**
    Run `tofu plan` to see what actions OpenTofu will take.

    ```shell
    tofu plan -out=tfplan
    ```

3.  **Apply:**
    Run `tofu apply` to create the resources in Neon.
    ```shell
    tofu apply tfplan
    ```
    OpenTofu will ask for confirmation. Type `yes` to confirm.

You have now successfully created and managed Neon resources using OpenTofu! You can continue to modify your `main.tf` file to add, change, or remove resources as needed. After making changes, repeat the `tofu plan` and `tofu apply` steps to update your resources on Neon.

## Import existing Neon resources with OpenTofu

If you have existing Neon resources that were created outside of OpenTofu (e.g., via the Neon Console or API directly), you can bring them under OpenTofu's management. This allows you to manage their lifecycle with code moving forward. OpenTofu supports both the CLI import command and declarative import blocks, similar to tofu 1.5.0+.

Both methods involve telling OpenTofu about an existing resource and associating it with a `resource` block in your configuration.

### Set up your OpenTofu configuration for import

Ensure your OpenTofu environment is configured for the Neon provider as described previously:

1.  Define the provider in your `main.tf`.
2.  Run `tofu init`.
3.  Configure authentication for the Neon provider.

### Neon resource IDs for import

When importing Neon resources, you need to know the specific ID format for each resource type. Always refer to the "Import" section of the specific resource's documentation page on the [Provider's GitHub: `kislerdm/terraform-provider-neon`](https://github.com/kislerdm/terraform-provider-neon/tree/master/docs/resources) for the exact ID format.

Common formats:

- **`neon_project`:** Project ID (e.g., `my-application-project-tofu-actual-id`).
- **`neon_branch`:** Branch ID (e.g., `br-dev-branch-tofu-actual-id`).
- **`neon_endpoint`:** Endpoint ID (e.g., `ep-dev-endpoint-tofu-actual-id`).
- **`neon_role`:** Composite ID: `<project_id>/<branch_id>/<role_name>`.
- **`neon_database`:** Composite ID: `<project_id>/<branch_id>/<database_name>`.
- **`neon_api_key` and `neon_jwks_url`:** These do not support import.

### Order of import for dependent resources

When importing resources that depend on each other, it's important to import them in the order of their dependencies:

```plaintext
Project -> Branch -> Endpoint -> Role -> Database
```

### Method 1: Using the `tofu import` CLI command

For each Neon resource you want to import:

1.  **Write a resource block:** Add a corresponding minimal `resource` block to your OpenTofu configuration file (e.g., `main.tf`).
2.  **Run `tofu import`:** Execute the import command: `tofu import <tofu_resource_address> <neon_resource_id>`.

#### Example: Importing resources using `tofu import` CLI

In this example, we'll import the resources we defined earlier in the [Manage Neon Resources](#manage-neon-resources-with-opentofu) section. This needs a project, a branch, an endpoint, a role, and a database already created in your Neon account. These resources will now be imported into a new OpenTofu configuration.

##### Define the HCL resource blocks

In your `main.tf` file, define the resource blocks for the existing resources. You can start with minimal definitions, as OpenTofu will populate the actual values during the import process. You primarily need to define the resource type and a name for OpenTofu to use. OpenTofu will populate the actual attribute values from the live resource into its state file during the import. You'll then use `tofu plan` to see these and update your HCL to match or to define your desired state.

For required attributes (like `project_id` for a branch), you'll either need to hardcode the known ID or reference a resource that will also be imported.

```tofu
tofu {
  required_providers {
    neon = {
      source  = "kislerdm/neon"
    }
  }
}

provider "neon" {}

# --- Project ---
resource "neon_project" "my_app_project" {}

# --- Development Branch ---
# Requires project_id. We'll reference the project we're about to import.
# The actual value of neon_project.my_app_project.id will be known after its import.
resource "neon_branch" "dev_branch" {
  project_id = neon_project.my_app_project.id
  name       = "feature-x-development"
}

# --- Development Branch Endpoint ---
# Requires project_id and branch_id.
resource "neon_endpoint" "dev_endpoint" {
  project_id = neon_project.my_app_project.id
  branch_id  = neon_branch.dev_branch.id
}

# --- Application User Role on Development Branch ---
# Requires project_id, branch_id, and name.
resource "neon_role" "app_user" {
  project_id = neon_project.my_app_project.id
  branch_id  = neon_branch.dev_branch.id
  name       = "application_user"
}

# --- Service Database on Development Branch ---
# Requires project_id, branch_id, name, and owner_name.
resource "neon_database" "service_db" {
  project_id = neon_project.my_app_project.id
  branch_id  = neon_branch.dev_branch.id
  name       = "service_specific_database"
  owner_name = neon_role.app_user.name
}
```

Here's a breakdown of the minimal HCL and why certain attributes are included:

- **`neon_project.my_app_project`**:
  - This block defines the OpenTofu resource for your main Neon project.
  - No attributes are strictly required _in the HCL_ for the import command itself, as the project is imported using its unique Neon Project ID. Adding a `name` attribute matching the existing project can aid readability but isn't essential for the import operation.

- **`neon_branch.dev_branch`**:
  - This defines the OpenTofu resource for your development branch.
  - It requires `project_id` in the HCL to link it to the (to-be-imported) project resource within OpenTofu.
  - The `name` attribute should also be specified in the HCL, matching the existing branch's name, as it's a key identifier.
  - The branch is imported using its unique Neon Branch ID.

- **`neon_endpoint.dev_endpoint`**:
  - This block defines the OpenTofu resource for the endpoint on your development branch.
  - It requires both `project_id` and `branch_id` in the HCL to correctly associate it with the imported project and development branch resources within OpenTofu.
  - Other attributes like `type` (which defaults if unspecified) or autoscaling limits will be read from the live resource during import.
  - The endpoint is imported using its unique Neon Endpoint ID.

- **`neon_role.app_user`**:
  - This defines the OpenTofu resource for an application user role.
  - The HCL requires `project_id` and `branch_id` to link to the respective imported OpenTofu resources.
  - The `name` attribute must be specified in the HCL and match the existing role's name.

- **`neon_database.service_db`**:
  - This defines the OpenTofu resource for a service-specific database.
  - The HCL requires `project_id` and `branch_id` to link to the imported OpenTofu resources.
  - The `name` attribute must be specified in the HCL and match the existing database's name.
  - The `owner_name` should also be included, linking to the OpenTofu role resource (e.g., `neon_role.app_user.name`) that owns this database.

All other configurable attributes will be populated into OpenTofu's state file from the live Neon resource during the `tofu import` process. You will then refine your HCL by reviewing the `tofu plan` output.

#### Run the import commands in order

1.  **Import the project:**

    ```shell
    tofu import neon_project.my_app_project "actual_project_id_from_neon"
    ```

    You can retrieve the project ID via Neon Console/CLI/API. Learn more: [Manage projects](/docs/manage/projects#project-settings)

    Example output:

    ```shell
    tofu import neon_project.my_app_project damp-recipe-88779456
    ```

    ```text
    neon_project.my_app_project: Importing from ID "damp-recipe-88779456"...
    neon_project.my_app_project: Import prepared!
      Prepared neon_project for import
    neon_project.my_app_project: Refreshing state... [id=damp-recipe-88779456]

    Import successful!

    The resources that were imported are shown above. These resources are now in
    your OpenTofu state and will henceforth be managed by OpenTofu.
    ```

2.  **Import the development branch:**

    ```shell
    tofu import neon_branch.dev_branch "actual_dev_branch_id_from_neon"
    ```

    You can retrieve the branch ID via Neon Console/CLI/API. Learn more: [Manage branches](/docs/manage/branches)

    The following image shows the branch ID in the Neon Console:
    ![Neon Console Branch ID](/docs/guides/neon-console-branch-id.png)

    Example output:

    ```shell
    tofu import neon_branch.dev_branch br-orange-bonus-a4v00wjl
    ```

    ```text
    neon_branch.dev_branch: Importing from ID "br-orange-bonus-a4v00wjl"...
    neon_branch.dev_branch: Import prepared!
      Prepared neon_branch for import
    neon_branch.dev_branch: Refreshing state... [id=br-orange-bonus-a4v00wjl]

    Import successful!

    The resources that were imported are shown above. These resources are now in
    your OpenTofu state and will henceforth be managed by OpenTofu.
    ```

3.  **Import the development compute endpoint:**

    ```shell
    tofu import neon_endpoint.dev_endpoint "actual_dev_endpoint_id_from_neon"
    ```

    You can retrieve the endpoint ID via Neon Console/CLI/API. Learn more: [Manage computes](/docs/manage/computes).

    The following image shows the endpoint ID in the Neon Console:
    ![Neon Console Compute Endpoint ID](/docs/guides/neon-console-compute-endpoint-id.png)

    Example output:

    ```shell
    tofu import neon_endpoint.dev_endpoint ep-blue-cell-a4xzunwf
    ```

    ```text
    neon_endpoint.dev_endpoint: Importing from ID "ep-blue-cell-a4xzunwf"...
    neon_endpoint.dev_endpoint: Import prepared!
      Prepared neon_endpoint for import
    neon_endpoint.dev_endpoint: Refreshing state... [id=ep-blue-cell-a4xzunwf]

    Import successful!

    The resources that were imported are shown above. These resources are now in
    your OpenTofu state and will henceforth be managed by OpenTofu.
    ```

4.  **Import the application user role:**

    ```shell
    tofu import neon_role.app_user "actual_project_id_from_neon/actual_dev_branch_id_from_neon/application_user"
    ```

    > Replace `application_user` with the actual name of the role you want to import.

    Example output:

    ```shell
    tofu import neon_role.app_user "damp-recipe-88779456/br-orange-bonus-a4v00wjl/application_user"
    ```

    ```text
    neon_role.app_user: Importing from ID "damp-recipe-88779456/br-orange-bonus-a4v00wjl/application_user"...
    neon_role.app_user: Import prepared!
      Prepared neon_role for import
    neon_role.app_user: Refreshing state... [id=damp-recipe-88779456/br-orange-bonus-a4v00wjl/application_user]

    Import successful!

    The resources that were imported are shown above. These resources are now in
    your OpenTofu state and will henceforth be managed by OpenTofu.
    ```

5.  **Import the service database:**

    ```shell
    tofu import neon_database.service_db "actual_project_id_from_neon/actual_dev_branch_id_from_neon/service_specific_database"
    ```

    > Replace `service_specific_database` with the actual name of the database you want to import.

    Example output:

    ```shell
    tofu import neon_database.service_db "damp-recipe-88779456/br-orange-bonus-a4v00wjl/service_specific_database"
    ```

    ```text
    neon_database.service_db: Importing from ID "damp-recipe-88779456/br-orange-bonus-a4v00wjl/service_specific_database"...
    neon_database.service_db: Import prepared!
      Prepared neon_database for import
    neon_database.service_db: Refreshing state... [id=damp-recipe-88779456/br-orange-bonus-a4v00wjl/service_specific_database]

    Import successful!

    The resources that were imported are shown above. These resources are now in
    your OpenTofu state and will henceforth be managed by OpenTofu.
    ```

    After importing all resources, your OpenTofu state file (`terraform.tfstate`) will now contain the imported resources, and you can manage them using OpenTofu. Follow the [Reconcile your HCL with the imported state](#reconcile-your-hcl-with-the-imported-state) section to update your HCL files with the attributes that were populated during the import.

### Method 2: Using `import` Blocks

OpenTofu also supports a declarative way to import existing resources using `import` blocks in your `.tf` files. This method is similar to the `tofu import` command but allows you to define the import process directly in your configuration file.

**The process with `import` Blocks:**

For each existing Neon resource you want to bring under OpenTofu management, you'll define two blocks in your `.tf` file:

- A standard `resource "resource_type" "resource_name" {}` block. For the initial import, this block can be minimal. It primarily tells OpenTofu the type and name of the resource in your configuration.
- An `import {}` block:
  - `to = resource_type.resource_name`: This refers to the OpenTofu address of the `resource` block you defined above.
  - `id = "neon_specific_id"`: This is the actual ID of the resource as it exists in Neon (e.g., project ID, branch ID, or composite ID for roles/databases).

**Example using `import` blocks:**

In this example, we'll import the resources we defined earlier in the [Manage Neon Resources](#manage-neon-resources-with-opentofu) section. This needs a project, a branch, an endpoint, a role, and a database already created in your Neon account. These resources will now be imported into a new OpenTofu configuration.

Let's say we have the following existing Neon resources and their IDs:

- Project `my_app_project` ID: `damp-recipe-88779456`
- Branch `dev_branch` ID: `br-orange-bonus-a4v00wjl`
- Endpoint `dev_endpoint` ID: `ep-blue-cell-a4xzunwf`
- Role `application_user`
- Database `service_specific_database`

You would add the following to your `main.tf`:

```tofu
tofu {
  required_providers {
    neon = {
      source  = "kislerdm/neon"
    }
  }
}

provider "neon" {
  # API key configured via environment variable or directly
}

# --- Project Import ---
import {
  to = neon_project.my_app_project
  id = "damp-recipe-88779456" # Replace with your actual Project ID
}

resource "neon_project" "my_app_project" {
  # Minimal definition for import.
  # After import and plan, you'll populate this with actual/desired attributes.
}

# --- Development Branch Import ---
import {
  to = neon_branch.dev_branch
  id = "br-orange-bonus-a4v00wjl" # Replace with your actual Branch ID
}

resource "neon_branch" "dev_branch" {
  project_id = neon_project.my_app_project.id # Links to the TF resource
  name       = "feature-x-development"        # Should match existing branch name
}

# --- Development Branch Endpoint Import ---
import {
  to = neon_endpoint.dev_endpoint
  id = "ep-blue-cell-a4xzunwf" # Replace with your actual Endpoint ID
}

resource "neon_endpoint" "dev_endpoint" {
  project_id = neon_project.my_app_project.id
  branch_id  = neon_branch.dev_branch.id      # Links to the TF resource
}

# --- Application User Role on Development Branch Import ---
import {
  to = neon_role.app_user
  # ID format: project_id/branch_id/role_name
  id = "damp-recipe-88779456/br-orange-bonus-a4v00wjl/application_user"
}

resource "neon_role" "app_user" {
  project_id = neon_project.my_app_project.id
  branch_id  = neon_branch.dev_branch.id
  name       = "application_user"             # Must match existing role name
}

# --- Service Database on Development Branch Import ---
import {
  to = neon_database.service_db
  # ID format: project_id/branch_id/name
  id = "damp-recipe-88779456/br-orange-bonus-a4v00wjl/service_specific_database"
}

resource "neon_database" "service_db" {
  project_id = neon_project.my_app_project.id
  branch_id  = neon_branch.dev_branch.id
  name       = "service_specific_database"    # Must match existing database name
  owner_name = neon_role.app_user.name        # Links to the TF role resource
}
```

<Admonition type="important">
You need to replace the IDs in the `import` blocks with the actual IDs of your existing Neon resources. The `to` field in each `import` block refers to the corresponding `resource` block defined in your configuration. The above configuration is a minimal example to get you started with the import process.
</Admonition>

### Reconcile your HCL with the imported state

After importing your resources using either method, you need to ensure that your HCL configuration accurately reflects the current state of the imported resources. This is an iterative process where you will:

1.  **Run `tofu plan`:**

    ```shell
    tofu plan
    ```

2.  **Understanding the plan output:**
    OpenTofu will compare your HCL `resource` blocks against the detailed state just imported from Neon.
    - The plan will likely propose to **add many attributes** to your HCL blocks. These are the actual current values of your Neon resources.
    - You might see "update in-place" actions, for example, for `neon_endpoint` it might show `+ branch_id = "your-branch-id"`. This is normal as OpenTofu reconciles the explicit configuration (where `branch_id` might be a reference that has now resolved to a concrete ID) with the imported state.

3.  **Update your HCL (`main.tf`):**
    Carefully review the `tofu plan` output. Your primary goal is to update your HCL `resource` blocks to accurately match the actual, imported state of your resources, or to define your desired state if you intend to make changes. Copy the relevant attributes and their values from the plan output into your HCL.

4.  **Repeat `tofu plan`:**
    After updating your HCL, run `tofu plan` again. Iterate until `tofu plan` shows "No changes. Your infrastructure matches the configuration." or only shows changes you intentionally want to make.

### Verify and reconcile

Once your HCL is fully updated, `tofu plan` should report:

```text
No changes. Your infrastructure matches the configuration.

OpenTofu has compared your real infrastructure against your configuration and found no
differences, so no changes are needed.
```

This confirms that your Neon resources are now successfully managed by OpenTofu.

## Destroying resources with OpenTofu

To remove the resources managed by OpenTofu:

```shell
tofu destroy
```

OpenTofu will ask for confirmation.

## Resources

- [OpenTofu Documentation](https://opentofu.org/docs/)
- [GitHub repository](https://github.com/kislerdm/terraform-provider-neon)
- [Terraform Registry](https://registry.terraform.io/providers/kislerdm/neon)
- [OpenTofu Registry](https://search.opentofu.org/provider/kislerdm/neon/latest)
- [Manage Neon with tofu](/docs/reference/tofu)

<NeedHelp/>


# Distributed hyperparameter tuning with Optuna, Neon Postgres, and Kubernetes

---
title: Distributed hyperparameter tuning with Optuna, Neon Postgres, and Kubernetes
subtitle: Use Neon Postgres to orchestrate multi-node hyperparameter tuning for your scikit-learn, XGBoost, PyTorch, and TensorFlow/Keras models on a Kubernetes cluster
author: sam-harri
enableTableOfContents: true
createdAt: '2024-10-28T00:00:00.000Z'
updatedOn: '2024-10-28T00:00:00.000Z'
---

In this guide, you'll learn how to set up distributed hyperparameter tuning for machine learning models across multiple nodes using Kubernetes. You'll use Optuna, a bayesian optimization library, to fine-tune models built with popular libraries like scikit-learn, XGBoost, PyTorch, and TensorFlow/Keras.

To orchestrate all the trials, you'll use Neon Postgres, a serverless postgres database. The combination of Neon Postgres, Kubernetes, and Docker allows for scalable, distributed hyperparameter tuning, simplifying the orchestration and management of complex machine learning workflows.

## Prerequisites

Before you begin, ensure you have the following tools and services set up:

- `Neon Serverless Postgres`: To provision and manage your serverless PostgreSQL database. If you don't have an account yet, [sign up here](https://console.neon.tech/signup).
- `Minikube`: For running a local Kubernetes cluster. You can install it by following the official [Minikube installation guide](https://minikube.sigs.k8s.io/docs/start).
- `kubectl`: Kubernetes command-line tool for interacting with your cluster. Follow the [kubectl installation instructions](https://kubernetes.io/docs/tasks/tools/) to get started.
- `Docker`: For containerizing your applications. If you don't have it installed, check out the [Docker installation guide](https://docs.docker.com/engine/install/).
- `Python`: To create, train, and optimize machine learning models. You can download Python from the [official website](https://www.python.org/downloads/).

## Overview

Hyperparameters are essential to machine learning model performance. Unlike parameters learned during training, hyperparameters—like learning rates, batch sizes, or the number of layers in a neural network—need to be set in advance. Tuning these hyperparameters effectively can greatly improve model performance, squeezing out the last bit of accuracy or reducing training time.

Bayesian optimization offers an efficient method for hyperparameter tuning. Unlike traditional approaches like grid or random search, Bayesian optimization builds a probabilistic model of the objective function to help it sample which hyperparameters to test next, which reduces the number of experiments needed, saving time and compute.

Distributing hyperparameter tuning across multiple nodes allows each trial to run independently on its own machine, enabling multiple configurations to be tested simultaneously and speeding up the search process. However, these nodes need to be coordinated, and a serverless database like Neon Postgres is perfect. Neon offers a pay-as-you-go model that minimizes costs during idle periods,but scales when the workload demands it. This database will maintain the state of the hyperparameter tuning process, storing the results of each trial and coordinating the distribution of new trials to available nodes.

These nodes are managed using Kubernetes, a container orchestration platform, which enables the same task to run concurrently across multiple nodes, allowing each node to handle a separate trial independently. It can also manage resources per node, and reboot nodes that fail, allowing for a fault tolerant training process.

In this guide, you will combine Optuna, Kubernetes, and Neon Postgres to create a scalable and cost-effective system for distributed tuning of your PyTorch, TensorFlow/Keras, scikit-learn, and XGBoost models.

## Hyperparameter Tuning

Optuna organizes hyperparameter tuning into studies, which are collections of trials. A study represents a single optimization run, and each trial within the study corresponds to a set of hyperparameters to be evaluated. Optuna uses a study to manage the optimization process, keeping track of the trials, their results, and the best hyperparameters found so far.

When you create a study, you can specify various parameters like the study name, the direction of optimization (minimize or maximize), and the storage backend. The storage backend is where Optuna stores the study data, including the trials and their results. By using a persistent storage backend like Neon Postgres, you can save the state of the optimization process, allowing all nodes to access the same study and coordinate the tuning process.

A study is created like so :

```python {4,5}
if __name__ == "__main__":
    study = optuna.create_study(
        study_name="sklearn_example",
        storage=os.environ["DATABASE_URL"],
        load_if_exists=True,
        direction="maximize",
    )
    study.optimize(objective, n_trials=100)
```

In the case of the distributed training, the `load_if_exists` parameter is set to `True` to load an existing study if it already exists, allowing nodes to join the optimization process.

Based on your machine learning library of choice, you can define an `objective` function that takes a `trial` object as input and returns a metric to optimize. Let's dive into each library and see how to define the `objective` function for scikit-learn, XGBoost, PyTorch, and TensorFlow/Keras models using test datasets.

To follow along, name your python script `hyperparam_optimization.py`.

### sklearn

ScikitLearn has a very wide range of models in its library, but in this you will be comparing a Support Vector Classifier and a Random Forest Classifier, and their hyperparameter configurations. For the SVC, you will optimize the strength of the regularization parameter `C`, while for the RF, you will optimize the maximum depth of the trees `max_depth`.

```python
import os
import optuna
import sklearn.datasets
import sklearn.ensemble
import sklearn.model_selection
import sklearn.svm

def objective(trial):
    iris = sklearn.datasets.load_iris()
    x, y = iris.data, iris.target

    classifier_name = trial.suggest_categorical("classifier", ["SVC", "RandomForest"])
    if classifier_name == "SVC":
        svc_c = trial.suggest_float("svc_c", 1e-10, 1e10, log=True)
        classifier_obj = sklearn.svm.SVC(C=svc_c, gamma="auto")
    else:
        rf_max_depth = trial.suggest_int("rf_max_depth", 2, 32, log=True)
        classifier_obj = sklearn.ensemble.RandomForestClassifier(
            max_depth=rf_max_depth, n_estimators=10
        )

    score = sklearn.model_selection.cross_val_score(classifier_obj, x, y, n_jobs=-1, cv=3)
    accuracy = score.mean()
    return accuracy


if __name__ == "__main__":
    study = optuna.create_study(
        study_name="sklearn_example",
        storage=os.environ["DATABASE_URL"],
        load_if_exists=True,
        direction="maximize",
    )
    study.optimize(objective, n_trials=100)
    print(study.best_trial)
```

### xgboost

Gradient Boosting is king in the world of tabular data, and XGBoost is one of the most popular libraries for this task. However, these models are especially sensitive to hyperparameter choice. In this example, you will optimize the booster type, regularization weights, sampling ratios, and tree complexity parameters.

```python
import numpy as np
import os
import optuna
import sklearn.datasets
import sklearn.metrics
from sklearn.model_selection import train_test_split
import xgboost as xgb


def objective(trial):
    (data, target) = sklearn.datasets.load_breast_cancer(return_X_y=True)
    train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)
    dtrain = xgb.DMatrix(train_x, label=train_y)
    dvalid = xgb.DMatrix(valid_x, label=valid_y)

    param = {
        "verbosity": 0,
        "objective": "binary:logistic",
        # use exact for small dataset.
        "tree_method": "exact",
        # defines booster, gblinear for linear functions.
        "booster": trial.suggest_categorical("booster", ["gbtree", "gblinear", "dart"]),
        # L2 regularization weight.
        "lambda": trial.suggest_float("lambda", 1e-8, 1.0, log=True),
        # L1 regularization weight.
        "alpha": trial.suggest_float("alpha", 1e-8, 1.0, log=True),
        # sampling ratio for training data.
        "subsample": trial.suggest_float("subsample", 0.2, 1.0),
        # sampling according to each tree.
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.2, 1.0),
    }

    if param["booster"] in ["gbtree", "dart"]:
        # maximum depth of the tree, signifies complexity of the tree.
        param["max_depth"] = trial.suggest_int("max_depth", 3, 9, step=2)
        # minimum child weight, larger the term more conservative the tree.
        param["min_child_weight"] = trial.suggest_int("min_child_weight", 2, 10)
        param["eta"] = trial.suggest_float("eta", 1e-8, 1.0, log=True)
        # defines how selective algorithm is.
        param["gamma"] = trial.suggest_float("gamma", 1e-8, 1.0, log=True)
        param["grow_policy"] = trial.suggest_categorical("grow_policy", ["depthwise", "lossguide"])

    if param["booster"] == "dart":
        param["sample_type"] = trial.suggest_categorical("sample_type", ["uniform", "weighted"])
        param["normalize_type"] = trial.suggest_categorical("normalize_type", ["tree", "forest"])
        param["rate_drop"] = trial.suggest_float("rate_drop", 1e-8, 1.0, log=True)
        param["skip_drop"] = trial.suggest_float("skip_drop", 1e-8, 1.0, log=True)

    bst = xgb.train(param, dtrain)
    preds = bst.predict(dvalid)
    pred_labels = np.rint(preds)
    accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)
    return accuracy

if __name__ == "__main__":
    study = optuna.create_study(
        study_name="xgboost_example",
        storage=os.environ["DATABASE_URL"],
        load_if_exists=True,
        direction="maximize",
    )
    study.optimize(objective, n_trials=100)
```

### PyTorch

PyTorch is now the defacto library for deep learning research, and its flexibility makes it a popular choice for many machine learning tasks. In this example, you will optimize the number of layers, hidden units, and dropout ratios in a feedforward neural network for the FashionMNIST dataset, a popular benchmark for image classification.

```python
import os
import optuna
from optuna.trial import TrialState
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.utils.data
from torchvision import datasets
from torchvision import transforms


DEVICE = torch.device("cpu")
BATCHSIZE = 128
CLASSES = 10
DIR = os.getcwd()
EPOCHS = 10
N_TRAIN_EXAMPLES = BATCHSIZE * 30
N_VALID_EXAMPLES = BATCHSIZE * 10


def define_model(trial):
    n_layers = trial.suggest_int("n_layers", 1, 3)
    layers = []

    in_features = 28 * 28
    for i in range(n_layers):
        out_features = trial.suggest_int("n_units_l{}".format(i), 4, 128)
        layers.append(nn.Linear(in_features, out_features))
        layers.append(nn.ReLU())
        p = trial.suggest_float("dropout_l{}".format(i), 0.2, 0.5)
        layers.append(nn.Dropout(p))

        in_features = out_features
    layers.append(nn.Linear(in_features, CLASSES))
    layers.append(nn.LogSoftmax(dim=1))

    return nn.Sequential(*layers)


def get_mnist():
    train_loader = torch.utils.data.DataLoader(
        datasets.FashionMNIST(DIR, train=True, download=True, transform=transforms.ToTensor()),
        batch_size=BATCHSIZE,
        shuffle=True,
    )
    valid_loader = torch.utils.data.DataLoader(
        datasets.FashionMNIST(DIR, train=False, transform=transforms.ToTensor()),
        batch_size=BATCHSIZE,
        shuffle=True,
    )

    return train_loader, valid_loader


def objective(trial):
    model = define_model(trial).to(DEVICE)

    # Generate the optimizers.
    optimizer_name = trial.suggest_categorical("optimizer", ["Adam", "RMSprop", "SGD"])
    lr = trial.suggest_float("lr", 1e-5, 1e-1, log=True)
    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)

    train_loader, valid_loader = get_mnist()

    for epoch in range(EPOCHS):
        model.train()
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)

            optimizer.zero_grad()
            output = model(data)
            loss = F.nll_loss(output, target)
            loss.backward()
            optimizer.step()

        model.eval()
        correct = 0
        with torch.no_grad():
            for batch_idx, (data, target) in enumerate(valid_loader):
                if batch_idx * BATCHSIZE >= N_VALID_EXAMPLES:
                    break
                data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)
                output = model(data)
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()

        accuracy = correct / min(len(valid_loader.dataset), N_VALID_EXAMPLES)

        trial.report(accuracy, epoch)

        if trial.should_prune():
            raise optuna.exceptions.TrialPruned()

    return accuracy


if __name__ == "__main__":
    study = optuna.create_study(
        study_name="pytorch_example",
        storage=os.environ["DATABASE_URL"],
        load_if_exists=True,
        direction="maximize",
    )
    study.optimize(objective, n_trials=100, timeout=600)
```

### tfkeras

While PyTorch is the go-to library for research, Keras with the TensorFlow backend is popular for its simplicity and ease of use. In this example, you will optimize the number of filters, kernel size, strides, activation functions, and learning rate in a convolutional neural network for the MNIST dataset.

```python
import urllib
import os

import optuna
from tensorflow.keras.backend import clear_session
from tensorflow.keras.datasets import mnist
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Flatten
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import RMSprop

N_TRAIN_EXAMPLES = 3000
N_VALID_EXAMPLES = 1000
BATCHSIZE = 128
CLASSES = 10
EPOCHS = 10


def objective(trial):
    clear_session()

    (x_train, y_train), (x_valid, y_valid) = mnist.load_data()
    img_x, img_y = x_train.shape[1], x_train.shape[2]
    x_train = x_train.reshape(-1, img_x, img_y, 1)[:N_TRAIN_EXAMPLES].astype("float32") / 255
    x_valid = x_valid.reshape(-1, img_x, img_y, 1)[:N_VALID_EXAMPLES].astype("float32") / 255
    y_train = y_train[:N_TRAIN_EXAMPLES]
    y_valid = y_valid[:N_VALID_EXAMPLES]
    input_shape = (img_x, img_y, 1)

    model = Sequential()
    model.add(
        Conv2D(
            filters=trial.suggest_categorical("filters", [32, 64]),
            kernel_size=trial.suggest_categorical("kernel_size", [3, 5]),
            strides=trial.suggest_categorical("strides", [1, 2]),
            activation=trial.suggest_categorical("activation", ["relu", "linear"]),
            input_shape=input_shape,
        )
    )
    model.add(Flatten())
    model.add(Dense(CLASSES, activation="softmax"))

    learning_rate = trial.suggest_float("learning_rate", 1e-5, 1e-1, log=True)
    model.compile(
        loss="sparse_categorical_crossentropy",
        optimizer=RMSprop(learning_rate=learning_rate),
        metrics=["accuracy"],
    )

    model.fit(
        x_train,
        y_train,
        validation_data=(x_valid, y_valid),
        shuffle=True,
        batch_size=BATCHSIZE,
        epochs=EPOCHS,
        verbose=False,
    )

    score = model.evaluate(x_valid, y_valid, verbose=0)
    return score[1]

if __name__ == "__main__":
    study = optuna.create_study(
        study_name="tfkeras_example",
        storage=os.environ["DATABASE_URL"],
        load_if_exists=True,
        direction="maximize",
    )
    study.optimize(objective, n_trials=100, timeout=600)
```

## Creating the Docker Image

To run the hyperparameter tuning process in a Kubernetes cluster, you'll need to containerize your application using Docker by creating a Docker image. The Docker image will contain your Python code, dependencies, and the necessary configuration files to run.

```Dockerfile
FROM python:3.10-slim-buster

WORKDIR /usr/src/

RUN pip install --no-cache-dir optuna psycopg2-binary OTHER_DEPENDENCIES

COPY hyperparam_optimization.py .
```

Depending on the machine learning library you're using, you'll need to install the appropriate dependencies in the Docker image. Each of the examples above requires the `optuna`, and `psycopg2-binary` packages, but you will need additional dependencies for each of the examples :

- For scikit-learn, you'll need to install `scikit-learn`
- For XGBoost, you'll need to install `xgboost`
- For PyTorch, you'll need to install `torch` and `torchvision`
- For TensorFlow/Keras, you'll need to install `tensorflow`

You'll want to build the Docker image later, once the Kubernetes cluster is set up, so that the image is available to the Kubernetes nodes.

## Setting up Kubernetes

To run distributed hyperparameter tuning across multiple nodes, you'll need a Kubernetes cluster. For this guide, you'll use Minikube to set up a local Kubernetes cluster on your machine. Minikube is a lightweight Kubernetes distribution, making it easy to get started with Kubernetes development.

To start Minikube, run the following command:

```bash
minikube start
```

This command will create a new Kubernetes cluster using the default settings. Once Minikube is up and running, you can interact with the cluster using the `kubectl` command-line tool.

To check the status of your cluster, run:

```bash
kubectl cluster-info
```

This command will display information about the Kubernetes cluster, including the API server address and the cluster services.

Now that minikube is running, you can build your Docker using:

```bash
eval "$(minikube docker-env)"
docker image build -t "optuna-kubernetes:example" .
```

Note the `eval "$(minikube docker-env)"` command, which sets the Docker environment variables to point to the Minikube Docker daemon. This allows you to build the Docker image inside the Minikube cluster, making it available to the Kubernetes nodes.

To submit jobs to the Kubernetes cluster, you'll need to create a Kubernetes manifest file. This file describes the task you want to run, including the container image, command, and environment variables. You can define the number of parallel jobs to run and the restart policy for the job.

To allow the Job to access the Neon Postgres database, you'll need to create a Kubernetes Secret containing the database credentials. You can create the secret from a `.env` file containing the database URL like so:

```bash
kubectl create secret generic optuna-postgres-secrets --from-env-file=.env
```

where your `.env` file contains the database URL from the Neon Console:

```bash
DATABASE_URL=YOUR_DATABASE_URL
```

Or from the raw string in the Neon Console like so:

```bash
kubectl create secret generic optuna-postgres-secrets \
    --from-literal=DATABASE_URL=YOUR_DATABASE_URL
```

Now, you can create the Kubernetes Job manifest file:

```yaml
---
apiVersion: batch/v1
kind: Job
metadata:
  name: worker
spec:
  parallelism: 3
  template:
    spec:
      restartPolicy: OnFailure
      containers:
        - name: worker
          image: optuna-kubernetes:example
          imagePullPolicy: IfNotPresent
          command:
            - python
            - hyperparam_optimization.py
          envFrom:
            - secretRef:
                name: optuna-postgres-secrets
```

Here, the manifest tells Kubernetes to launch 3 parallel jobs, each running the `hyperparam_optimization.py` script inside the Docker container, and if the job fails, Kubernetes will restart it automatically. The `envFrom` field specifies that the Job should use the `optuna-postgres-secrets` secret you just created to access the Neon Postgres database.

Finally, you can submit the Job to the Kubernetes cluster using the following command:

```bash
kubectl apply -f k8s-manifests.yaml
```

Now, if you run the following command, you should see the Job running in the Kubernetes cluster:

```bash
kubectl get jobs
```

and if you run the following command, you should see the 3 pods running the job:

```bash
kubectl get pods
```

## Monitoring

To monitor your Kubernetes cluster, you can use the Kubernetes Dashboard, a web-based UI for managing and monitoring your cluster. To access the Kubernetes Dashboard, run the following command:

```bash
minikube dashboard
```

Likewise, you can monitor logs of the running pods to monitor using a tool like `stern`, which allows you to tail logs from multiple pods at once:

```bash
stern .
```

Here, you can see that the first pod creates a new study, and the other pods join the existing study. Then, each pods runs its trial, logs the result, and creates new a trial based on the results of the previous ones in the database.

![Stern Logs](/guides/images/optuna-hyperprameter-kubernetes/k8s-example-logs.png)

To show off the power of Kubernetes fault tolerance, you can delete one of the pods, and see that the job is automatically restarted on a new pod. First, find all the running pods and chose one to delete:

```bash
kubectl get pods
```

Then, delete the pod:

```bash
kubectl delete pod <POD_NAME>
```

In the stern logs, you can see the pod getting removed, and a new pod being created to replace it, all while continuing the same study as before.

![Delete Pod Stern Logs](/guides/images/optuna-hyperprameter-kubernetes/deletepod-logs.png)

## Conclusion

Now, you have successfully set up distributed hyperparameter tuning using Optuna, Neon Postgres, and Kubernetes. By leveraging Kubernetes to manage multiple nodes running hyperparameter tuning jobs, you can speed up the optimization process and find the best hyperparameters for your machine learning models more efficiently. This kind of task, which sees bursts of database activity followed by long periods of inactivity, is well-suited to a serverless database like Neon Postgres, which can scale dynamically to any workload, then back to zero.

To take this to the next step, you can leverage cloud Kubernetes services like Azure Kubernetes Service (AKS) or Amazon Elastic Kubernetes Service (EKS). These services offer managed Kubernetes clusters that can scale to hundreds of nodes to run your jobs at scale. You can also integrate with cloud storage services like Azure Blob Storage or Amazon S3 to store your training data and model checkpoints, making it easier to manage large datasets and distributed training workflows.


# Using Payload CMS with Neon Postgres to Build an E-commerce Store in Next.js

---
title: Using Payload CMS with Neon Postgres to Build an E-commerce Store in Next.js
subtitle: Build your own E-commerce Store in a Next.js application with Payload CMS and Postgres (powered by Neon).
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-06-06T00:00:00.000Z'
updatedOn: '2024-06-06T00:00:00.000Z'
---

In this guide, you will learn how to set up a serverless Postgres database with Neon, configure Payload CMS with Postgres, and seed the Postgres database using the pre-populated information in Payload CMS Ecommerce template.

## Prerequisites

To follow the steps in this guide, you will need the following:

- [Node.js 18](https://nodejs.org/en) or later
- A [Neon](https://console.neon.tech/signup) account

## Steps

- [Provisioning a Serverless Postgres database powered by Neon](#provisioning-a-serverless-postgres-database-powered-by-neon)
- [Create a new Payload CMS application with Next.js](#create-a-new-payload-cms-application-with-nextjs)
- [Seed your Postgres database](#seed-your-postgres-database)
- [Build and Test your E-commerce Store (locally)](#build-and-test-your-e-commerce-store-locally)
- [Scale-to-zero with Postgres (powered by Neon)](#scale-to-zero-with-postgres-powered-by-neon)

## Provisioning a Serverless Postgres database powered by Neon

Using a serverless Postgres database powered by Neon lets you scale down to zero, which helps you save on compute costs.

To get started, go to the [Neon console](https://console.neon.tech/app/projects) and create a project.

Enable the **Connection pooling** toggle on the **Connection Details** panel to obtain the Postgres connection string.

![](/guides/images/payload/98592ce7-3b8a-411b-a769-a0b89eaac8a3.png)

All Neon connection strings have the following format:

```bash
postgres://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>?sslmode=require&channel_binding=require
```

- `<user>` is the database user.
- `<password>` is the database user’s password.
- `<endpoint_hostname>.neon.tech` is the host with `neon.tech` as the [top-level domain (TLD)](https://www.cloudflare.com/en-gb/learning/dns/top-level-domain/).
- `<port>` is the Neon port number. The default port number is 5432.
- `<dbname>` is the name of the database. **neondb** is the default database created with each Neon project if you do not define your own.
- `?sslmode=require&channel_binding=require` are optional query parameters that enforce [SSL](https://www.cloudflare.com/en-gb/learning/ssl/what-is-ssl/) mode and channel binding for better security when connecting to the Postgres instance.

Save the connecting string somewhere safe. You will use it later to configure the `POSTGRES_URL` variable.

## Create a new Payload CMS application with Next.js

Let's begin with creating a Payload CMS backend to serve all the content for your e-commerce store in Next.js. Open your terminal and run the following command:

```bash
npx create-payload-app@latest payload-neon-ecommerce-store
```

`npx create-payload-app` is the recommended way to scaffold a Payload CMS + Next.js project quickly.

When prompted, choose the following:

![Welcome to Payload command line](/guides/images/payload/6c1f1650-7cc6-4b37-b293-611ba32dc6cc.png)

- `ecommerce` as the project template.
- `PostgreSQL (beta)` as the database.
- The connection string you obtained earlier as the PostgreSQL connection string: `postgres://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>?sslmode=require&channel_binding=require&channel_binding=require` .

Once that's done, change to the project directory and start the app:

```bash
cd payload-neon-ecommerce-store
yarn && yarn dev
```

![E-commerce template page](/guides/images/payload/e736400e-e52a-4b28-bb61-7f10fa7c2bc4.png)

The app should be running on [localhost:3000](http://localhost:3000). Let's keep the development server running as we work through the next steps.

Next, let's move on to adding e-commerce seed data to your Neon Postgres database.

## Seed your Postgres database

The `ecommerce` template is pre-configured with a seed database. To add the seed data to your Postgres database, navigate to the Payload CMS admin console at [localhost:3000/admin](http://localhost:3000/admin). Enter only the required attributes: **Email**, **Password**, **Name** and **Role**.

![Sign-in page](/guides/images/payload/fd54ff4f-400b-43fb-a08d-6f4fb0f8dd99.png)

Once you are inside the admin view, click **Seed your database** to start the process of seeding your database with e-commerce data.

![Welcome to your dashboard page](/guides/images/payload/086ae87d-d994-4fbf-b2fd-031ac711a4d1.png)

Once you see the following message in your terminal, you are done with adding seed data to the database.

```bash
INFO (payload): Seeded database successfully!
```

Now, let's move on to building the application and previewing it in action.

## Build and Test your E-commerce Store (Locally)

To test the e-commerce store in action, prepare a build and run the preview server using the following command:

```bash
yarn build && yarn serve
```

The app should now be running on [localhost:3000](http://localhost:3000). Navigate to http://localhost:3000/products/online-course to view the product display page of **Online Course** product.

![Online course page](/guides/images/payload/906a90a5-a17c-4573-8e45-87b67606f0c6.png)

Congratulations! You have now completed creating your own e-commerce store ✨

## Scale-to-zero with Postgres (powered by Neon)

Interestingly, during the entire process of setting up this app, you were using Neon's **Scale-to-zero** feature, which places your Postgres compute endpoint into an idle state when the database is inactive for more than 5 minutes. Click the **Operations** button in your Neon console sidebar to see when the compute was started and automatically suspended to reduce compute usage.

![Neon Monitoring page](/guides/images/payload/74a2aa54-6d28-4f47-b181-077957df6779.png)

## Summary

In this guide, you learned how to build an e-commerce store in Next.js using Payload CMS and a serverless Postgres database (powered by Neon). Furthermore, using Postgres (powered by Neon) allowed you to save on cloud compute resources when the compute endpoint that runs Postgres was idle for more than 5 minutes.

<NeedHelp />


# Real-Time Notifications using pg_notify with Neon Postgres

---
title: Real-Time Notifications using pg_notify with Neon Postgres
subtitle: A step-by-step guide describing how to implement real-time notifications using pg_notify in Postgres
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-07-02T13:24:36.612Z'
updatedOn: '2024-07-02T13:24:36.612Z'
---

This step-by-step guide shows how you can implement real-time notifications in Postgres (powered by Neon). Real-time notifications provide a way to instantly notify users in an application. With [pg_notify](https://www.postgresql.org/docs/current/sql-notify.html) and [Postgres triggers](https://www.postgresql.org/docs/current/triggers.html), you can create a webhook-like system to invoke external services on specific database operations.

## Prerequisites

To follow the steps in this guide, you will need the following:

- [Node.js 18](https://nodejs.org/en/blog/announcements/v18-release-announce) or later
- A [Neon](https://console.neon.tech/signup) account

## Steps

- [Provisioning a Postgres database powered by Neon](#provisioning-a-postgres-database-powered-by-neon)
- [Creating a new Node.js application](#creating-a-new-nodejs-application)
- [Set up triggers](#set-up-triggers)
- [Set up a Notifications Listener](#set-up-a-notifications-listener)
- [Notify using triggers](#notify-using-triggers)

## Provisioning a Postgres database powered by Neon

To get started, go to the [Neon console](https://console.neon.tech/app/projects) and enter the name of your choice as the project name.

You will then be presented with a dialog that provides a connecting string of your database. Make sure to **uncheck** the **Pooled connection checkbox** on the top right of the dialog and the connecting string automatically updates in the box below it.

![](/guides/images/pg-notify/index.png)

All Neon connection strings have the following format:

```bash
postgres://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>
```

- `user` is the database user.
- `password` is the database user’s password.
- `endpoint_hostname` is the host with neon.tech as the [TLD](https://www.cloudflare.com/en-gb/learning/dns/top-level-domain/).
- `port` is the Neon port number. The default port number is 5432.
- `dbname` is the name of the database. “neondb” is the default database created with each Neon project.
- `?sslmode=require&channel_binding=require` optional query parameters that enforce the [SSL](https://www.cloudflare.com/en-gb/learning/ssl/what-is-ssl/) mode and channel binding while connecting to the Postgres instance for better security.

Save this connecting string somewhere safe to be used as the `DATABASE_URL` further in the guide. Proceed further in this guide to create a Node.js application.

## Creating a new Node.js application

To start building the application, create a new Node.js project. Open your terminal and run the following command:

```bash
npm init -y
```

Further, execute the following command to install the dependencies to read the environment variables and connect to Postgres:

```bash
npm install pg dotenv
```

The libraries installed include:

- `pg`: A Postgres client for Node.js.
- `dotenv`: A library for handling environment variables.

Now, let's move on to setting up event triggers that will send notifications upon insertion of a row in a specific table.

## Set up triggers

To set up event triggers for a specific table (say `my_table`), you will define a trigger function called `my_trigger_function`. Create a file named `setup.js` with the following code:

```js shouldWrap
// File: setup.js

// Load all the environment variables
require('dotenv').config();

const { Client } = require('pg');

const connectionString = process.env.DATABASE_URL;

const client = new Client({ connectionString });

async function setupTrigger() {
  try {
    // Connect to Postgres
    await client.connect();
    // Create a my_table if it does not already exist
    await client.query(`CREATE TABLE IF NOT EXISTS
    my_table (id SERIAL PRIMARY KEY, message TEXT)`);
    // Define the my_trigger_function function to send notifications
    await client.query(`
    CREATE OR REPLACE FUNCTION my_trigger_function() RETURNS trigger AS $$
    BEGIN
      PERFORM pg_notify('channel_name', NEW.message);
      RETURN NEW;
    END;
    $$ LANGUAGE plpgsql;`);
    // Create the my_trigger to call the my_trigger_function after each insert
    await client.query(`
    CREATE TRIGGER my_trigger
    AFTER INSERT ON my_table
    FOR EACH ROW
    EXECUTE FUNCTION my_trigger_function();`);
    console.log('Event triggers setup complete.');
    await client.end();
  } catch (e) {
    console.log(e);
  }
}

setupTrigger().catch(console.log);
```

In the code above, `my_trigger_function` function uses `pg_notify` to send a notification to the channel named `my_channel` with the content of the newly inserted `message`. Finally, it creates a trigger named `my_trigger` that will execute this function after each insertion in table named `my_table`. This ensures that any new message inserted into the table triggers the notification.

To execute the script as above, run the following command:

```bash
node setup.js
```

Now, let's move to setting up a notifications listener in Node.js.

## Set up a Notifications Listener

To listen for notifications in Node.js intended for the channel named `my_channel`, create a file `listen.js` with the following code:

```js
// File: listen.js

// Load all the environment variables
require('dotenv').config();

const { Client } = require('pg');

const connectionString = process.env.DATABASE_URL;

const client = new Client({ connectionString });

async function listenToNotifications() {
  try {
    // Connect to Postgres
    await client.connect();
    // Listen to specific channel in Postgres
    // Attach a listener to notifications received
    client.on('notification', (msg) => {
      console.log('Notification received', msg.payload);
    });
    await client.query('LISTEN channel_name');
    console.log('Listening for notifications on my_channel');
  } catch (e) {
    console.log(e);
  }
}

listenToNotifications().catch(console.log);
```

The code above begins with importing `pg` and loading all the enviroment variables into scope. Further, it initializes a client connection to your Postgres. In the `listenToNotifications` function, it sets up a listener to notifications using `client.on('notification', ...)` callback. To invoke the callback, it starts listening for notifications to channel named `my_channel`, using `LISTEN my_channel` command.

To keep listening to the notifications, you would want to keep running the following command:

```bash
node listen.js
```

Now, let's insert a row to invoke the triggers that will notify the listeners.

## Notify using triggers

To notify the listeners, you will use Postgres triggers. To programtically trigger an event that will be created upon insertion into the table named `my_table`, create a file `send.js` with the following code:

```js
// File: send.js

// Load all the environment variables
require('dotenv').config();

const { Client } = require('pg');

const connectionString = process.env.DATABASE_URL;

const client = new Client({ connectionString });

async function insertRow(message) {
  try {
    // Connect to Postgres
    await client.connect();
    // Insert a row into Postgres table
    await client.query('INSERT INTO my_table (message) VALUES ($1)', [message]);
    console.log("Inserted a row in the 'my_table' table.");
    await client.end();
  } catch (e) {
    console.log(e);
  }
}

insertRow('Hello, world!').catch(console.log);
```

The code above begins with importing `pg` and loading all the enviroment variables into scope. Further, it initializes a client connection to your Postgres. In the `insertRow` function, it simply inserts a row into the table named `my_table`.

To execute the script as above, run the following command:

```bash
node send.js
```

<Admonition type="note" title="Note">
By default, Neon scales to zero after 5 minutes of inactivity, which ends any running sessions. As a result, `NOTIFY` and `LISTEN` commands only persist for the duration of the current session and are lost when the session ends.
</Admonition>

## Summary

In this guide, you learned how to receive and send real-time notifications using `pg_notify` in Serverless Postgres. Using Postgres triggers, you can selectively listen to changes happening in specific database table(s), and perform a function that invokes `pg_notify` to send out the notifications to the connected listeners.

## Source code

You can find the source code for the application described in this guide on GitHub.

<DetailIconCards>
<a href='https://github.com/neondatabase/examples/tree/main/with-nodejs-pg-notify' description='pg_notify with Node.js and Neon' icon='github'>Set up notifications using pg_notify with Node.js and Neon</a>
</DetailIconCards>

<NeedHelp />


# Comparing Text Search Strategies pg_search vs. tsvector vs. External Engines

---
title: Comparing Text Search Strategies pg_search vs. tsvector vs. External Engines
subtitle: Choosing the Right Search Approach for Your Application with PostgreSQL and Neon
author: bobbyiliev
enableTableOfContents: true
createdAt: '2025-04-06T00:00:00.000Z'
updatedOn: '2025-04-06T00:00:00.000Z'
---

When implementing search in your application, you need to choose the right text search approach. This guide compares PostgreSQL's built-in [`tsvector`](/guides/full-text-search), the [`pg_search`](docs/extensions/pg_search) extension, and external search engines to help you select the best option for your needs.

## Built-in PostgreSQL `tsvector` for Text Search

PostgreSQL includes native full-text search capabilities using the `tsvector` data type and `tsquery` search expressions. The `tsvector` data type is a specialized data structure that PostgreSQL uses to represent documents in a form optimized for search.

This built-in functionality works for basic search needs without requiring any additional extensions. It's perfect for smaller applications or when you don't require advanced search features.

### Example of Using `tsvector`

To use `tsvector`, you need to create a table with a column to store the search vector. You can then convert text into the `tsvector` format and create an index for efficient searching. This allows your queries to run faster, even as your dataset grows.

```sql
CREATE TABLE articles (
    id SERIAL PRIMARY KEY,
    title TEXT,
    content TEXT,
    search_vector tsvector
);

-- Update the search vector
UPDATE articles
SET search_vector = to_tsvector('english', title || ' ' || content);

-- Create an index on the search vector
CREATE INDEX idx_search_vector ON articles USING GIN (search_vector);

-- Query using tsquery
SELECT id, title
FROM articles
WHERE search_vector @@ to_tsquery('english', 'database & performance');
```

This example shows how to:

- Create a table with a column to store search vectors
- Convert text to the `tsvector` format using the `to_tsvector` function
- Create a GIN index to speed up searches
- Search for articles containing both "database" and "performance"

While `tsvector` meets basic search needs, it has limitations with relevance ranking, handling typos, and complex search patterns. It’s suitable for small to medium-sized datasets where advanced features aren’t necessary.

### Pros of `tsvector`

- **Simplicity**: No need for any extensions, just native PostgreSQL functionality.
- **Integrated with Postgres**: Works seamlessly within the same database, avoiding the need for additional services.
- **Low overhead**: Since `tsvector` is part of Postgres, it doesn’t require a separate system to maintain.

### Cons of `tsvector`

- **Limited relevance ranking**: It doesn't automatically rank search results based on relevance.
- **No typo tolerance**: Exact matches are required, so if a user misspells a search term, it won’t return relevant results.
- **Complex queries**: Handling more complex queries like fuzzy matching or phrase proximity is not possible out-of-the-box.

## Extending `tsvector` with `pg_search`

The `pg_search` extension builds on PostgreSQL's search capabilities by adding better relevance ranking with the BM25 algorithm, fuzzy matching for handling typos, and more flexible search options.

This extension is particularly useful for applications that require more sophisticated search features without needing a separate search engine.

With `pg_search`, you get features such as:

- **Relevance Ranking**: The BM25 algorithm provides an automatic ranking of search results based on how relevant they are to the query.
- **Fuzzy Matching**: It allows for typo tolerance, meaning searches will return relevant results even when a user makes small errors.
- **Phrase Search**: You can search for exact phrases or words that are close together, which can be especially useful in content-heavy applications.
- **Flexible Queries**: `pg_search` supports more complex search expressions like partial word matches, stemming, and stop-word filtering.

### Enabling `pg_search` on Neon

<Admonition type="note" title="pg_search on Neon">

`pg_search` is currently only available on Neon projects created in an [AWS region](/docs/introduction/regions#aws-regions).

</Admonition>

Adding `pg_search` to your Neon database is simple - just run this single SQL command:

```sql
CREATE EXTENSION IF NOT EXISTS pg_search;
```

That's all it takes to enable the extension. You can then use the `pg_search` features in your queries. The setup is straightforward, and you don’t need any additional infrastructure to get started.

### Example of Using `pg_search`

After enabling the extension, you can create search indexes and run more sophisticated queries:

```sql
-- Create a BM25 index on multiple columns
CREATE INDEX article_search_idx ON articles
USING bm25 (id, title, content)
WITH (key_field='id');

-- Simple keyword search
SELECT title
FROM articles
WHERE title @@@ 'database';

-- Handling typos with fuzzy matching
SELECT title
FROM articles
WHERE id @@@ paradedb.match('title', 'database', distance => 1);

-- Sorting by relevance score
SELECT title, paradedb.score(id) AS relevance
FROM articles
WHERE content @@@ 'performance'
ORDER BY paradedb.score(id) DESC;
```

This code shows how to:

- Create a BM25 index that covers multiple columns
- Perform a basic keyword search using the `@@@` operator
- Find results even when the search term has typos
- Sort results by relevance so the most relevant results appear first

### Why Use `pg_search` on Neon?

Using `pg_search` on Neon gives you:

- **Better search capabilities**: With ranking, typo tolerance, and complex query options, `pg_search` provides much more functionality than `tsvector`.
- **A fully managed PostgreSQL experience**: You don’t need to set up or maintain a separate search service. Neon handles everything for you, from scaling to backups.
- **Data consistency**: Since the search index is part of your PostgreSQL database, there’s no need to worry about synchronizing data between separate systems.
- **Simple architecture**: With `pg_search` running on Neon, you avoid the complexity of managing an external search engine while still getting advanced search features.

`pg_search` is a great choice for applications that need more advanced search features but want to avoid the complexity of managing a separate search engine.

## External Search Engines (e.g., Elasticsearch)

External search engines like Elasticsearch provide specialized search features for complex use cases and very large datasets. These engines are designed to scale out across many servers and handle high-performance, low-latency search queries.

While these engines offer powerful capabilities, they come with trade-offs:

- **You need to set up and maintain additional infrastructure**: External search engines require managing separate servers or cloud services, which can increase operational overhead.
- **You must keep your database and search index synchronized**: Ensuring that your external search engine stays in sync with your PostgreSQL database can introduce complexity, especially as your data changes.
- **Increased complexity**: Managing another system means additional configuration, monitoring, and troubleshooting.
- **Higher operational costs**: Running an external search engine comes with extra costs, both in terms of infrastructure and developer time.

External search engines like Elasticsearch provide powerful features such as:

- Distributed search: Handles large-scale search queries across many machines.
- Complex querying: Offers advanced querying capabilities such as aggregations, nested fields, and more.
- Real-time indexing: Updates search results in real time as new data is indexed.

### Use Cases for External Search Engines

You might choose an external search engine when:

- **You have extremely large datasets** (billions of records) that require distributed search across multiple servers.
- **Your search needs include specialized features** not available in PostgreSQL, such as advanced analytics, geographic search, or machine learning integration.
- **You have the resources** to manage additional infrastructure and complexity.

## Comparison Summary

| Feature               | `tsvector`             | `pg_search` on Neon | External Engines                                   |
| --------------------- | ---------------------- | ------------------- | -------------------------------------------------- |
| **Setup**             | Built into Postgres    | Easy setup on Neon  | Separate system to install, configure and maintain |
| **Relevance Ranking** | Basic                  | BM25 ranking        | Advanced ranking options                           |
| **Typo Tolerance**    | No                     | Yes                 | Yes                                                |
| **Query Flexibility** | Limited                | Good                | Extensive                                          |
| **Scaling**           | Limited by Postgres    | Managed by Neon     | Requires manual scaling                            |
| **Cost**              | Included with Postgres | Included with Neon  | Additional infrastructure costs                    |
| **Maintenance**       | Minimal                | Handled by Neon     | Requires ongoing maintenance                       |

## Which Option Should You Choose?

With all these options available, how do you choose the right one for your application? Here are some guidelines:

### Use built-in `tsvector` when:

- You have simple search requirements
- Your dataset is small to medium-sized
- You want to use only built-in PostgreSQL features

### Use `pg_search` on Neon when:

- You need better search relevance and typo tolerance
- You want to avoid setting up separate systems
- You prefer a managed database experience
- Your search needs are important but don't require specialized features

### Consider external search engines when:

- You have extremely large datasets (billions of records)
- Your search needs include specialized features not available in PostgreSQL
- You have the resources to manage additional infrastructure

For most web applications, content sites, and e-commerce platforms, `pg_search` on Neon provides a great balance between search features and simplicity. It extends PostgreSQL's capabilities without requiring you to manage separate systems or synchronize data.

## Conclusion

When choosing a search strategy, start with the simplest option that meets your needs. For many applications, `pg_search` on Neon offers a great middle ground - better search features than native PostgreSQL without the complexity of a separate search system.

Selecting the right search approach allows you to provide good search functionality to your users while keeping your application architecture as simple as possible.

<NeedHelp />


# Building an End-to-End Full-Text Search Experience With pg_search on Neon

---
title: Building an End-to-End Full-Text Search Experience With pg_search on Neon
subtitle: A guide to building a full-text search experience with pg_search on Neon
author: bobbyiliev
enableTableOfContents: true
createdAt: '2025-04-06T00:00:00.000Z'
updatedOn: '2025-04-06T00:00:00.000Z'
---

Full-text search is an essential component for applications that need to search through large text collections. While many developers use dedicated search engines like Elasticsearch, Neon's `pg_search` extension lets you build search capabilities directly in your Postgres database, eliminating the need for additional services.

This guide shows you how to build a search solution with `pg_search` on Neon, from database setup to creating a responsive search interface for a developer snippet manager. You'll learn how to create and query search indexes, highlight relevant search results, and build a simple interface for searching code snippets.

## Prerequisites

To follow this guide, you'll need:

- A [Neon](https://console.neon.tech/signup) account
- Basic SQL knowledge
- Familiarity with a backend language (we use Node.js in our examples but you can adapt it to your preferred language)
- Basic understanding of HTML, CSS, and JavaScript for the frontend

## What is `pg_search`?

The [`pg_search`](/docs/extensions/pg_search) extension adds full-text search capabilities to Postgres using the BM25 scoring algorithm—the same approach used by modern search engines. It offers:

- Fast and relevant search results
- Easy integration with your existing database
- Support for fuzzy matching to handle typos
- Advanced filtering capabilities and no need for additional services

This means you can implement search without adding complexity to your infrastructure.

## Enabling `pg_search` on Neon

<Admonition type="note" title="pg_search on Neon">

`pg_search` is currently only available on Neon projects created in an [AWS region](/docs/introduction/regions#aws-regions).

</Admonition>

First, let's enable the `pg_search` extension on your Neon database. Connect to your database using the Neon SQL Editor or a client like `psql` and run the following command:

```sql
CREATE EXTENSION IF NOT EXISTS pg_search;
```

This adds the `pg_search` functionality to your database.

Once enabled, you'll have access to new operators and functions for full-text search that will make building a snippet search tool much easier.

## Understanding how `pg_search` works

Before diving into implementation, it's helpful to understand the two key components that make `pg_search` efficient:

1. **BM25 scoring** calculates how relevant each result is based on:
   - Word frequency within a document (how often a search term appears)
   - Word rarity across all documents (uncommon terms get higher scores)
   - Document length (adjusts scores so longer documents don't automatically rank higher)

2. **Inverted indexes** map words to the documents containing them, making searches fast by directly finding relevant documents instead of scanning everything. Think of it like the index at the back of a book that tells you exactly which pages contain specific topics.

These components are particularly valuable for code snippet search, where developers often need to quickly locate specific algorithms, functions, or techniques across a large collection of code snippets.

## Setting up a database for our snippet manager

For the purposes of this guide, we'll create a simple code snippet manager. This will allow developers to store, search, and categorize code snippets across different programming languages.

The following SQL creates our database structure:

```sql
CREATE TABLE languages (
  language_id SERIAL PRIMARY KEY,
  name TEXT NOT NULL UNIQUE
);

CREATE TABLE users (
  user_id SERIAL PRIMARY KEY,
  username TEXT NOT NULL UNIQUE,
  email TEXT NOT NULL UNIQUE
);

CREATE TABLE snippets (
  snippet_id SERIAL PRIMARY KEY,
  title TEXT NOT NULL,
  description TEXT,
  code TEXT NOT NULL,
  language_id INTEGER REFERENCES languages(language_id),
  user_id INTEGER REFERENCES users(user_id),
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE tags (
  tag_id SERIAL PRIMARY KEY,
  name TEXT NOT NULL UNIQUE
);

CREATE TABLE snippet_tags (
  snippet_id INTEGER REFERENCES snippets(snippet_id),
  tag_id INTEGER REFERENCES tags(tag_id),
  PRIMARY KEY (snippet_id, tag_id)
);
```

This schema creates five tables:

- `languages` for categorizing snippets by programming language
- `users` to track who created each snippet
- `snippets` for storing the actual code snippets and their metadata
- `tags` for categorizing snippets (algorithms, utilities, etc.)
- `snippet_tags` for the many-to-many relationship between snippets and tags

Now let's add some sample data to work with, which will help us demonstrate the search capabilities of our snippet manager.

The following SQL inserts a few sample records into our tables:

```sql
-- Add programming languages
INSERT INTO languages (name) VALUES
('JavaScript'),
('Python'),
('Go'),
('SQL'),
('TypeScript');

-- Add users
INSERT INTO users (username, email) VALUES
('devguru', 'dev@example.com'),
('codedojo', 'dojo@example.com'),
('scriptpro', 'pro@example.com');

-- Add tags
INSERT INTO tags (name) VALUES
('algorithm'),
('utility'),
('frontend'),
('database'),
('middleware');

-- Add code snippets
INSERT INTO snippets (title, description, code, language_id, user_id) VALUES
('Quick Sort Implementation', 'Efficient implementation of the quicksort algorithm',
'function quickSort(arr) {
  if (arr.length <= 1) {
    return arr;
  }

  const pivot = arr[Math.floor(arr.length / 2)];
  const left = arr.filter(x => x < pivot);
  const middle = arr.filter(x => x === pivot);
  const right = arr.filter(x => x > pivot);

  return [...quickSort(left), ...middle, ...quickSort(right)];
}',
1, 1),

('Database Connection Pool', 'Reusable database connection pool using the pg library',
'const { Pool } = require("pg");

const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  ssl: true,
  max: 20,
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000,
});

module.exports = {
  query: (text, params) => pool.query(text, params),
  getClient: async () => {
    const client = await pool.connect();
    return client;
  }
};',
1, 2),

('Simple API Middleware', 'Express middleware for API authentication',
'function authMiddleware(req, res, next) {
  const authHeader = req.headers.authorization;

  if (!authHeader || !authHeader.startsWith("Bearer ")) {
    return res.status(401).json({ error: "Unauthorized" });
  }

  const token = authHeader.split(" ")[1];

  try {
    const decoded = jwt.verify(token, process.env.JWT_SECRET);
    req.user = decoded;
    next();
  } catch (error) {
    return res.status(401).json({ error: "Invalid token" });
  }
}',
1, 3),

('Binary Search Algorithm', 'Efficient binary search implementation in Python',
'def binary_search(arr, target):
    left, right = 0, len(arr) - 1

    while left <= right:
        mid = (left + right) // 2

        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            left = mid + 1
        else:
            right = mid - 1

    return -1  # Target not found',
2, 1),

('SQL Transaction Helper', 'Helper function for managing SQL transactions',
'async function withTransaction(callback) {
  const client = await pool.connect();

  try {
    await client.query("BEGIN");
    const result = await callback(client);
    await client.query("COMMIT");
    return result;
  } catch (error) {
    await client.query("ROLLBACK");
    throw error;
  } finally {
    client.release();
  }
}',
1, 2);

-- Add snippet tags
INSERT INTO snippet_tags (snippet_id, tag_id) VALUES
(1, 1),  -- Quick Sort: algorithm
(2, 4),  -- Database Connection Pool: database
(3, 5),  -- Simple API Middleware: middleware
(4, 1),  -- Binary Search: algorithm
(5, 4);  -- SQL Transaction Helper: database
```

This gives us a foundation of coding snippets across different languages and categories to demonstrate search capabilities.

## Creating search indexes

Now that we have our data, we need to create BM25 indexes to enable efficient searching.

For a code snippet manager, these indexes are essential since they will allow developers to quickly find relevant code:

```sql
CREATE INDEX snippet_search_idx ON snippets
USING bm25 (snippet_id, title, description, code)
WITH (key_field='snippet_id');

CREATE INDEX language_search_idx ON languages
USING bm25 (language_id, name)
WITH (key_field='language_id');

CREATE INDEX tag_search_idx ON tags
USING bm25 (tag_id, name)
WITH (key_field='tag_id');
```

Let's break down what this does:

- The first index enables searching across snippet titles, descriptions, and actual code
- The language index allows searching for programming languages
- The tag index enables searching for specific categories like "algorithm" or "utility"
- Each index specifies `key_field` to identify which column uniquely identifies each row

These indexes make searching efficient by pre-processing and organizing the text data for quick lookups, which is key when developers need to search through potentially thousands of code snippets.

## Basic search queries with `pg_search`

Now with our indexes in place, let's perform some searches using the `@@@` operator, which is the main search operator provided by `pg_search`.

### Simple keyword search

Find snippets that mention "connection" in any of the indexed fields:

```sql
SELECT snippet_id, title, description
FROM snippets
WHERE title @@@ 'connection' OR description @@@ 'connection' OR code @@@ 'connection'
ORDER BY paradedb.score(snippet_id) DESC;
```

This query searches for the term "connection" across multiple fields. The `paradedb.score()` function returns the relevance score of each match, allowing us to show the most relevant snippets first. This is particularly useful for developers who often need to find code examples based on certain keywords or concepts.

### Exact phrase search

When you need to find an exact sequence of words, such as a specific function signature, use double quotes around the phrase:

```sql
SELECT snippet_id, title, description
FROM snippets
WHERE code @@@ '"function authMiddleware"'
ORDER BY paradedb.score(snippet_id) DESC;
```

This searches for the exact phrase "`function authMiddleware`" in the code. Without the quotes, it would find snippets containing both words in any order or position, which could lead to less precise results when looking for specific function definitions. You can give it a try with other phrases to see how it works.

### Fuzzy matching for typos

Naturally everyone makes mistakes while typing, and developers are no exception. Typos are common, especially when searching for function names or variable names. The `@@@` operator supports fuzzy matching to help find relevant results even with minor errors:

```sql
SELECT snippet_id, title
FROM snippets
WHERE title @@@ paradedb.match('title', 'binary serch', distance => 1);
```

This would find "`Binary Search Algorithm`" even though "`search`" was misspelled as "`serch`".

The `distance => 1` parameter allows for one character difference, making your search more forgiving and practical for real-world use. You can tweak the distance parameter to allow for more or fewer errors based on your needs.

### Combining search with filters

You can combine text search with standard SQL filtering to narrow down results by language or tag:

```sql
SELECT s.snippet_id, s.title, l.name AS language
FROM snippets s
JOIN languages l ON s.language_id = l.language_id
WHERE s.code @@@ 'function' AND l.name = 'JavaScript'
ORDER BY paradedb.score(s.snippet_id) DESC;
```

This query finds JavaScript snippets that contain the word "function" in their code. It demonstrates how you can combine full-text search with traditional SQL conditions, which is especially useful when developers want to narrow their search to a specific programming language.

### Highlighting search results

To help developers quickly identify relevant code sections, you can highlight the matching terms:

```sql
SELECT
  snippet_id,
  title,
  paradedb.snippet(code) AS code_highlight
FROM snippets
WHERE code @@@ 'pool';
```

This wraps matched terms in `<b></b>` tags by default:

```
 snippet_id |          title           |                       code_highlight
------------+--------------------------+---------------------------------------------------------------
         2  | Database Connection Pool | const { <b>Pool</b> } = require("pg");\n\nconst <b>pool</b> = new <b>Pool</b>({...
         5  | SQL Transaction Helper   | ... const client = await <b>pool</b>.connect(); ...
```

You can customize the highlighting with different tags to match your UI:

```sql
SELECT
  snippet_id,
  title,
  paradedb.snippet(code, start_tag => '<code class="highlight">', end_tag => '</code>') AS code_highlight
FROM snippets
WHERE code @@@ 'pool';
```

This feature is particularly valuable in a code snippet manager as it allows developers to quickly see where their search terms appear in potentially lengthy code blocks.

## Building the Search API and Frontend

Now that we've set up the `pg_search` indexes and the backend database, let's integrate the search functionality into a simple API and frontend. This section walks you through setting up a Node.js API to handle search queries and a React-based frontend to display the results.

Start by creating a new directory for your project:

```bash
mkdir snippet-search
cd snippet-search
```

Then you are ready to create the backend API and frontend application.

### 1. Setting Up the API

We'll create a simple Node.js API to handle incoming search requests. The API will query the Neon Postgres database using the `pg_search` extension and return the results to the frontend.

#### Install Required Dependencies

First, we need a few packages to set up the backend. These include Express for the server, pg for interacting with the Postgres database, and `dotenv` for managing environment variables like your Neon database connection string.

```bash
mkdir snippet-search-api
cd snippet-search-api
npm init -y
npm install express pg dotenv cors
```

#### Create the Database Connection

We'll start by setting up the database connection in a separate file (`db.js`). This makes it easier to manage the connection and reuse it in other parts of the application.

```javascript
// db.js - Database connection
const { Pool } = require('pg');
require('dotenv').config();

const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  ssl: true,
});

module.exports = pool;
```

In this file, we are creating a connection pool to manage connections to the Neon database. Make sure your `DATABASE_URL` is stored in a `.env` file like so:

```
DATABASE_URL=postgres://[username]:[password]@[endpoint]/[dbname]
```

For more information on how to work with Neon and Node.js, check out the [Neon documentation](/docs/guides/node).

#### Search Logic

Next, we'll write the search logic in a `searchService.js` file. This function will handle querying the database, applying the search filters, and returning the relevant results.

```javascript
// searchService.js - Search functionality
const db = require('./db');

async function searchSnippets({ query, language, tag, page = 1, limit = 10 }) {
  const offset = (page - 1) * limit;

  let searchQuery = `
    SELECT 
      s.snippet_id, 
      s.title, 
      s.description,
      l.name AS language,
      u.username AS created_by,
      paradedb.snippet(s.code) AS code_highlight,
      paradedb.score(s.snippet_id) AS relevance
    FROM snippets s
    JOIN languages l ON s.language_id = l.language_id
    JOIN users u ON s.user_id = u.user_id
  `;

  let whereConditions = [];
  let queryParams = [];
  let paramIndex = 1;

  if (query) {
    whereConditions.push(
      `(s.title @@@ $${paramIndex} OR s.description @@@ $${paramIndex} OR s.code @@@ $${paramIndex})`
    );
    queryParams.push(query);
    paramIndex++;
  }

  if (language) {
    whereConditions.push(`l.name = $${paramIndex}`);
    queryParams.push(language);
    paramIndex++;
  }

  if (tag) {
    searchQuery += ` JOIN snippet_tags st ON s.snippet_id = st.snippet_id JOIN tags t ON st.tag_id = t.tag_id`;
    whereConditions.push(`t.name = $${paramIndex}`);
    queryParams.push(tag);
    paramIndex++;
  }

  if (whereConditions.length > 0) {
    searchQuery += ` WHERE ${whereConditions.join(' AND ')}`;
  }

  searchQuery += ` ORDER BY relevance DESC LIMIT $${paramIndex} OFFSET $${paramIndex + 1}`;
  queryParams.push(limit, offset);

  const results = await db.query(searchQuery, queryParams);

  let countQuery = `
    SELECT COUNT(*) 
    FROM snippets s
    JOIN languages l ON s.language_id = l.language_id
  `;

  if (tag) {
    countQuery += ` JOIN snippet_tags st ON s.snippet_id = st.snippet_id JOIN tags t ON st.tag_id = t.tag_id`;
  }

  if (whereConditions.length > 0) {
    countQuery += ` WHERE ${whereConditions.join(' AND ')}`;
  }

  const countResult = await db.query(countQuery, queryParams.slice(0, -2));
  const total = parseInt(countResult.rows[0].count);

  const snippetIds = results.rows.map((row) => row.snippet_id);

  if (snippetIds.length > 0) {
    const tagsQuery = `
      SELECT st.snippet_id, array_agg(t.name) as tags
      FROM snippet_tags st
      JOIN tags t ON st.tag_id = t.tag_id
      WHERE st.snippet_id = ANY($1)
      GROUP BY st.snippet_id
    `;

    const tagsResult = await db.query(tagsQuery, [snippetIds]);

    const snippetTags = {};
    tagsResult.rows.forEach((row) => {
      snippetTags[row.snippet_id] = row.tags;
    });

    results.rows.forEach((row) => {
      row.tags = snippetTags[row.snippet_id] || [];
    });
  }

  return {
    results: results.rows,
    pagination: {
      total,
      page: parseInt(page),
      limit: parseInt(limit),
      totalPages: Math.ceil(total / limit),
    },
  };
}

module.exports = {
  searchSnippets,
};
```

This function receives search parameters and constructs a SQL query with the appropriate filters. It also handles pagination by calculating `limit` and `offset` values, so only a subset of results is returned at a time.

#### Set Up the Express Server

Now, let's set up the Express server in a `server.js` file. This file will expose a simple `/api/search` endpoint that accepts GET requests with query parameters, handles them using the `searchSnippets` function, and returns the search results.

```javascript
// server.js - Express server
const express = require('express');
const cors = require('cors');
const { searchSnippets } = require('./searchService');

const app = express();
const port = process.env.PORT || 3000;

app.use(cors());
app.use(express.json());

app.get('/api/search', async (req, res) => {
  try {
    const { query, language, tag, page, limit } = req.query;

    if (!query && !language && !tag) {
      return res.status(400).json({ error: 'At least one search parameter is required' });
    }

    const result = await searchSnippets({
      query,
      language,
      tag,
      page: parseInt(page || 1),
      limit: parseInt(limit || 10),
    });

    res.json(result);
  } catch (error) {
    console.error('Search error:', error);
    res.status(500).json({ error: 'An error occurred while searching' });
  }
});

app.listen(port, () => {
  console.log(`Server running on port ${port}`);
});
```

This server listens for search requests on the `/api/search` endpoint. It supports query parameters like `query`, `language`, and `tag`, and it passes these to the `searchSnippets` service to fetch and return the results.

### 2. Frontend: React Search Interface

Now that we have the API set up, let's create a simple frontend in React to allow users to perform searches.

#### Set Up the React App

Create a new React app and install the necessary dependencies:

```bash
npx create-react-app snippet-search-ui
cd snippet-search-ui
npm install axios highlight.js
```

#### Create the API Service

In the `src/services/api.js` file, we'll set up an Axios service to make requests to the backend API.

```javascript
// src/services/api.js
import axios from 'axios';

const API_URL = 'http://localhost:3000/api';

// Function to search snippets
export const searchSnippets = async (params) => {
  const response = await axios.get(`${API_URL}/search`, { params });
  return response.data;
};

export const getLanguages = async () => {
  // Mock API call to fetch languages
  return [
    { id: 1, name: 'JavaScript' },
    { id: 2, name: 'Python' },
    { id: 3, name: 'Go' },
    { id: 4, name: 'SQL' },
    { id: 5, name: 'TypeScript' },
  ];
};

export const getTags = async () => {
  // Mock API call to fetch tags
  return [
    { id: 1, name: 'algorithm' },
    { id: 2, name: 'utility' },
    { id: 3, name: 'frontend' },
    { id: 4, name: 'database' },
    { id: 5, name: 'middleware' },
  ];
};
```

#### Build the Search Form Component

We need a form component where users can enter a search query, select a language, or filter by tags. The form will trigger the search when submitted.

```jsx
// src/components/SearchForm.js
import React, { useState, useEffect } from 'react';
import { getLanguages, getTags } from '../services/api';

const SearchForm = ({ onSearch }) => {
  const [query, setQuery] = useState('');
  const [language, setLanguage] = useState('');
  const [tag, setTag] = useState('');
  const [languages, setLanguages] = useState([]);
  const [tags, setTags] = useState([]);

  useEffect(() => {
    const loadFilters = async () => {
      const languagesData = await getLanguages();
      const tagsData = await getTags();
      setLanguages(languagesData);
      setTags(tagsData);
    };

    loadFilters();
  }, []);

  const handleSubmit = (e) => {
    e.preventDefault();
    onSearch({ query, language, tag });
  };

  return (
    <form onSubmit={handleSubmit}>
      <input
        type="text"
        value={query}
        onChange={(e) => setQuery(e.target.value)}
        placeholder="Search..."
      />
      <select value={language} onChange={(e) => setLanguage(e.target.value)}>
        <option value="">All Languages</option>
        {languages.map((lang) => (
          <option key={lang.id} value={lang.name}>
            {lang.name}
          </option>
        ))}
      </select>
      <select value={tag} onChange={(e) => setTag(e.target.value)}>
        <option value="">All Tags</option>
        {tags.map((tag) => (
          <option key={tag.id} value={tag.name}>
            {tag.name}
          </option>
        ))}
      </select>
      <button type="submit">Search</button>
    </form>
  );
};

export default SearchForm;
```

#### Code Snippet Component

Let's now create a component that displays a single code snippet:

```jsx
// src/components/CodeSnippet.js
import React from 'react';

const CodeSnippet = ({ snippet }) => {
  return (
    <div>
      <h3>{snippet.title}</h3>
      <pre>{snippet.code}</pre>
      <p>{snippet.description}</p>
    </div>
  );
};

export default CodeSnippet;
```

#### Display Search Results

Finally, let's put everything together in the main `App` component. We'll handle search requests, display the results, and paginate.

```jsx
// src/App.js
import React, { useState } from 'react';
import SearchForm from './components/SearchForm';
import CodeSnippet from './components/CodeSnippet';
import { searchSnippets } from './services/api';

function App() {
  const [results, setResults] = useState([]);
  const [loading, setLoading] = useState(false);

  const handleSearch = async (params) => {
    setLoading(true);
    try {
      const response = await searchSnippets(params);
      setResults(response.results);
    } catch (error) {
      console.error('Error during search', error);
    } finally {
      setLoading(false);
    }
  };

  return (
    <div>
      <SearchForm onSearch={handleSearch} />
      {loading && <p>Loading...</p>}
      <div>
        {results.map((snippet) => (
          <CodeSnippet key={snippet.snippet_id} snippet={snippet} />
        ))}
      </div>
    </div>
  );
}

export default App;
```

This app handles searches, displays results, and allows users to filter based on query terms, languages, and tags.

You can also add styling to make the search form and results look better. For example, you can use CSS or a library like Tailwind CSS to style the components. For simplicity, we won't cover styling in this guide, but feel free to customize the UI to your liking.

### 3. Dockerizing the Application and Using Docker Compose

To deploy and run the search API and frontend in isolated containers, we can use Docker. In this section, we'll show you how to dockerize both the backend API and the frontend, and use Docker Compose to orchestrate the entire application.

#### Dockerizing the Backend API

To begin, let's create a Dockerfile for the backend API. This file will define the steps to build a container image for the Node.js API.

1. Create a `Dockerfile` in the root of the `snippet-search-api` directory:

   ```dockerfile
   FROM node:20-alpine

   WORKDIR /usr/src/app

   COPY package*.json ./

   RUN npm install

   COPY . .

   EXPOSE 3000

   CMD ["node", "server.js"]
   ```

   This Dockerfile uses the official Node.js image, sets up the working directory, installs dependencies, and exposes port 3000 (the port the API will listen on).

2. Create a `.dockerignore` file to avoid copying unnecessary files to the Docker image:

   ```plaintext
   node_modules
   npm-debug.log
   .env
   ```

#### Dockerizing the Frontend

Next, we'll dockerize the frontend React application. We'll create a separate `Dockerfile` for the frontend.

1. **Create a `Dockerfile` in the `snippet-search-ui` directory:**

   ```dockerfile
   FROM node:20-alpine

   WORKDIR /app

   COPY package*.json ./

   RUN npm install

   COPY . .

   RUN npm run build

   FROM nginx:alpine

   COPY --from=0 /app/build /usr/share/nginx/html

   EXPOSE 80

   CMD ["nginx", "-g", "daemon off;"]
   ```

   This Dockerfile does the following:
   - It first builds the React app using the Node.js image.
   - Then, it uses an Nginx image to serve the build files, ensuring that the app is ready for production.

#### Docker Compose Setup

Now, we'll use Docker Compose to run both the backend API and the frontend together in one command. We'll create a `compose.yml` file in the root directory of the project.

1. Create a `compose.yml` file in the root directory of the project, e.g. `snippet-search` which contains both `snippet-search-api` and `snippet-search-ui` directories:

   ```yaml
   services:
   # Backend API service
   api:
     build:
     context: ./snippet-search-api
     container_name: snippet-api
     environment:
       - DATABASE_URL=postgres://[username]:[password]@[endpoint]/[dbname]
     ports:
       - '3000:3000'
     networks:
       - snippet-network

   # Frontend service
   frontend:
     build:
     context: ./snippet-search-ui
     container_name: snippet-frontend
     ports:
       - '80:80'
     networks:
       - snippet-network

   networks:
   snippet-network:
     driver: bridge
   ```

   This file defines three services:
   - **`api`**: The backend service, built from the `snippet-search-api` directory. It expects the `DATABASE_URL` environment variable to connect to the Neon database. You should replace `[username]`, `[password]`, and `[endpoint]` with your actual Neon database credentials or use a `.env` file to manage these variables securely.
   - **`frontend`**: The React frontend service, built from the `snippet-search-ui` directory. It will serve the static build files via Nginx.

#### Building and Running the Application

With the Dockerfiles and `compose.yml` file in place, we can now build and start all the services with Docker Compose.

1. Build the images and start the containers:

   ```bash
   docker compose up --build
   ```

   This command will:
   - Build the Docker images for the backend API and the frontend.
   - Create and start the containers for the backend API, frontend, and Postgres database.

2. Access the application:
   - The backend API will be available at `http://localhost:3000`.
   - The frontend React app will be served at `http://localhost`.

3. Shut down the application:

   If you want to stop the containers, run:

   ```bash
   docker compose down
   ```

## Performance optimization tips

When working with larger collections containing thousands of code snippets, you can optimize `pg_search` performance with these adjustments:

### Configure PostgreSQL settings

Adjust these settings for better performance on large datasets:

```sql
-- Allocate more memory for index building
SET maintenance_work_mem = '1GB';  -- Adjust based on your compute size

-- Enable parallel workers
SET max_parallel_workers_per_gather = 4;
SET max_parallel_workers = 8;
```

These settings help Postgres use more system resources effectively. In Neon, `maintenance_work_mem` is set based on your compute size, don't exceed 50-60% of your compute's available RAM. For larger code repositories, these optimizations can significantly speed up both index creation and search queries.

### Pre-warm your indexes

When you restart your database or after creating indexes, you can pre-load them into memory for faster queries:

```sql
-- Install the extension
CREATE EXTENSION IF NOT EXISTS pg_prewarm;

-- Pre-warm indexes
SELECT pg_prewarm('snippet_search_idx');
```

Pre-warming loads index data into memory, eliminating disk read latency for initial searches. This is particularly valuable for snippet search, as it ensures the first developer searching after a system restart gets fast results.

## Conclusion

With `pg_search` on Neon, you can build a powerful code snippet search system directly within your Postgres database. This approach eliminates the need for separate search services while providing excellent search performance and features tailored to developers' needs.

The BM25 algorithm and inverted indexes ensure your searches are both fast and relevant, while the integration with standard SQL gives you powerful filtering and sorting capabilities.

For repositories with millions of snippets, only index columns you actually search on (this keeps index size manageable). Also consider partial indexes if you only search active or public snippets and monitor index size and rebuild periodically for optimal performance.

<NeedHelp />


# Using pgAdmin4 with a Hosted Postgres

---
title: Using pgAdmin4 with a Hosted Postgres
subtitle: A comprehensive guide on how to manage your Postgres database using pgAdmin4.
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-11-29T00:00:00.000Z'
updatedOn: '2024-11-29T00:00:00.000Z'
---

pgAdmin4 is a powerful web-based administration tool for managing PostgreSQL databases. This guide will walk you through the steps to set up and use pgAdmin4 with a hosted Postgres database, enabling you to perform various database operations efficiently.

## Table of Contents

- [Setting Up pgAdmin4](#setting-up-pgadmin4)
- [Connecting to Your Hosted Postgres Database](#connecting-to-your-hosted-postgres-database)
- [Basic Operations in pgAdmin4](#basic-operations-in-pgadmin4)

## Setting Up pgAdmin4

1. **Download and Install pgAdmin4**: If you haven't already, download pgAdmin4 from the [official website](https://www.pgadmin.org/download/). Follow the installation instructions for your operating system.

2. **Launch pgAdmin4**: Open pgAdmin4 from your applications menu or web browser.

## Provisioning a Serverless Postgres

To get started, go to the [Neon console](https://console.neon.tech/app/projects) and enter the name of your choice as the project name.

![](/guides/images/pg-notify/index.png)

All Neon connection strings have the following format:

```bash
postgres://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>
```

- `user` is the database user.
- `password` is the database user’s password.
- `endpoint_hostname` is the host with neon.tech as the [TLD](https://www.cloudflare.com/en-gb/learning/dns/top-level-domain/).
- `port` is the Neon port number. The default port number is 5432.
- `dbname` is the name of the database. “neondb” is the default database created with each Neon project.
- `?sslmode=require&channel_binding=require` optional query parameters that enforce the [SSL](https://www.cloudflare.com/en-gb/learning/ssl/what-is-ssl/) mode and channel binding while connecting to the Postgres instance for better security.

You will be using these connecting string components further in the guide. Proceed further in this guide to connect pgAdmin4 to your Postgres.

## Connecting to Your Hosted Postgres Database

1. **Open pgAdmin4**: Once pgAdmin4 is running, you will see the dashboard.

2. **Create a New Server Connection**:
   - Right-click on "Servers" in the left sidebar and select "Create" > "Server...".
   - In the "Create - Server" dialog, enter a name for your server connection.

3. **Configure Connection Settings**:
   - Go to the "Connection" tab.
   - Enter the following details:
     - **Host**: The endpoint of your hosted Postgres database (e.g., `ep-...us-east-2.aws.neon.tech`).
     - **Port**: The port number (default is `5432`).
     - **Maintenance database**: Your database name.
     - **Username**: Your database username.
     - **Password**: Your database password (you can save the password if desired).

4. **Save the Connection**: Click "Save" to create the server connection. You should now see your server listed in the left sidebar.

![](/guides/images/pg-notify/pgAdmin4.png)

## Basic Operations in pgAdmin4

### 1. Running SQL Queries

- Click on your database in the left sidebar.
- Click on the "Query Tool" icon (or right-click the database and select "Query Tool").
- Enter your SQL queries in the editor and click the "Execute" button (play icon) to run them.

### 2. Managing Tables

- Expand your database in the left sidebar, then expand the "Schemas" > "public" > "Tables" section.
- Right-click on "Tables" to create a new table or manage existing ones.

### 3. Importing and Exporting Data

- To import data, right-click on a table and select "Import/Export".
- Follow the prompts to upload a CSV file or export data to a file.

## Conclusion

pgAdmin4 is an essential tool for managing your hosted Postgres database. With its user-friendly interface, you can easily perform various database operations, from creating databases and tables to running complex queries. By following this guide, you should be well-equipped to utilize pgAdmin4 effectively.

<NeedHelp />


# Zero downtime schema migrations with pgroll

---
title: Zero downtime schema migrations with pgroll
subtitle: A comprehensive guide to using pgroll for safe, reversible Postgres migrations
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-06-30T00:00:00.000Z'
updatedOn: '2025-06-30T00:00:00.000Z'
---

Database schema migrations are a critical but often risky part of application development. Traditional migration tools can lock tables, cause downtime, and make rollbacks difficult, especially for applications that require high availability. [`pgroll`](https://github.com/xataio/pgroll) is an open-source CLI tool that solves this problem for Postgres, enabling zero-downtime, reversible schema changes.

This guide will walk you through understanding `pgroll`, and how to use it effectively in your development workflow to ensure safe, continuous database migrations without service interruptions.

## What is `pgroll`?

`pgroll` is an open-source command-line tool for Postgres that enables **zero-downtime, reversible schema migrations** by allowing multiple schema versions to coexist during updates, so client applications remain uninterrupted even during breaking changes. It manages complex migrations safely without locking the database and supports instant rollbacks if needed.

### Key features

- **Zero downtime migrations:** `pgroll` employs an expand/contract workflow, ensuring changes are applied without taking your application offline or locking database tables.
- **Instant, reversible changes:** Active migrations can be instantly rolled back with a single command, providing a critical safety net for production deployments.
- **Multi-version schema support:** `pgroll` allows old and new versions of your schema to coexist simultaneously. This decouples application and database deployments, as new application versions can use the new schema while legacy versions continue to function on the old one.
- **Declarative migrations:** You define the _desired end state_ of your schema in simple `yaml` or `json` files. `pgroll` handles the complex, lock-safe SQL execution required to achieve that state.
- **Automated data backfilling:** When adding constraints like `NOT NULL` to a column with existing data, `pgroll` automates the entire backfilling process in the background without blocking writes.

`pgroll` is the ideal solution for environments with high-availability requirements where schema changes must be deployed frequently and safely.

### Why not traditional migration strategies?

To appreciate `pgroll`'s approach, it helps to understand the trade-offs of conventional migration methods. Schema migrations in Postgres typically follow one of two strategies:

#### Strategy 1: Scheduled downtime (The maintenance window)

This method prioritizes operational simplicity at the cost of service availability. It is only viable for applications where scheduled downtime is acceptable.

**Process:**

1.  **Halt service:** Stop application servers to prevent all database writes.
2.  **Apply migration:** Execute the migration script, which often acquires `ACCESS EXCLUSIVE` locks.
3.  **Deploy new code:** Deploy the application version compatible with the new schema.
4.  **Restore service:** Restart application servers.

**Challenges:**

- **Service interruption:** Unacceptable for high-availability systems.
- **High-risk, high-pressure event:** Any failure during the migration extends the outage.
- **Difficult rollbacks:** Reverting a failed migration is operationally complex, often requiring a database restore.

#### Strategy 2: Manual Zero-downtime migration (The expand/contract pattern)

This advanced strategy avoids downtime but transfers complexity to the application layer and development teams.

**Process:**

1.  **Expand phase:** Apply only backward-compatible changes (e.g., add a new column as `NULL`). Deploy new application code that handles both schema versions, often requiring complex dual-write logic.
2.  **Transition phase:** Run a custom script to backfill data into the new column, usually in small batches to avoid table locks.
3.  **Contract phase:** Once data is migrated and consistent, apply the breaking change (e.g., add a `NOT NULL` constraint).
4.  **Cleanup:** Deploy a final application version that removes the dual-write logic and run another migration to drop the old column.

**Challenges:**

- **Engineering overhead:** This multi-stage process is slow and requires development effort to manage dual-writes, backfills, and feature flags.
- **Operational complexity:** The process is error-prone and requires coordination across multiple deployments.
- **Data consistency risks:** Bugs in the application's backfill or dual-write logic can lead to silent data corruption.

### How `pgroll` solves these problems

`pgroll` transforms the complex, manual migration process into a simple, automated one. It achieves this by codifying the **expand/contract** pattern, allowing you to focus on defining _what_ you want to change, while `pgroll` handles _how_ to apply it safely.

#### The `pgroll` migration lifecycle

A typical migration with `pgroll` involves a clear, two-phase process that separates database changes from application deployment, ensuring safety and reversibility.

**Step 1: Define your migration**

You start by creating a declarative migration file in `yaml` or `json` that defines the desired schema changes.

**Step 2: Start the migration (`pgroll start`) - The "Expand" phase**

Running `pgroll start <migration-file>` initiates the migration.

- **What happens:** `pgroll` applies only _additive_ (non-breaking) changes. For breaking changes like adding a `NOT NULL` constraint, it creates a temporary helper column, backfills data, and sets up triggers to keep both old and new columns synchronized.
- **The result:** A new, versioned schema is created and becomes accessible. The old schema version remains fully operational.

**Step 3: Deploy your new application code**

With the new schema available, you can safely deploy your new application.

- **What you do:** Configure your new application instances to use the new schema version by setting their `search_path` connection parameter. You can get the latest schema name by running `pgroll latest schema`. Learn more about this in the [Connecting your application to the new schema version](#step-5-connecting-your-application-to-the-new-schema-version) section.
- **The key benefit:** During this phase, both old and new application versions can run concurrently against their respective schema versions, enabling phased rollouts like canary or blue-green deployments.

**Step 4: Complete the migration (`pgroll complete`) - The "Contract" phase**

Once your new application is stable and no traffic is hitting instances that use the old schema, you finalize the process.

- **What happens:** Running `pgroll complete` performs the "contract" steps. It removes the old schema version, drops temporary columns and triggers, and makes the schema changes permanent.
- **The result:** The migration is complete, and the database schema is now in its final, clean state.

Optionally, you can also run `pgroll rollback` at any point before completing the migration to revert to the previous schema version. This is a critical safety feature that allows you to quickly undo changes if issues arise during the migration process.

![Migration Flow Diagram](https://raw.githubusercontent.com/xataio/pgroll/main/docs/img/schema-changes-flow@2x.png)

> _Image source: [pgroll GitHub repository](https://github.com/xataio/pgroll/blob/main/docs/img/schema-changes-flow@2x.png)_

#### How `pgroll` manages multiple schema versions

For each migration, `pgroll` creates a new, versioned schema (e.g., `public_01_initial`, `public_02_add_column`). These schemas do not contain the physical tables themselves but rather [views](/postgresql/postgresql-views) that point to the underlying tables in your main schema (e.g., `public`).

This abstracts the schema's structure. For example, when you rename a column, the new version schema's view presents the column with its new name, while the old version schema's view continues to show the old name. This allows different application versions to interact with the same underlying data through different schema _lenses_, completely unaware of the ongoing migration.

![Multiple schema versions diagram](https://raw.githubusercontent.com/xataio/pgroll/main/docs/img/migration-schemas@2x.png)

> _Image source: [pgroll GitHub repository](https://github.com/xataio/pgroll/blob/main/docs/img/migration-schemas@2x.png)_

## Getting started

Now that you understand the basics, let's dive into using `pgroll` for schema migrations in a Neon Postgres database. This guide will take you through installing and setting up `pgroll`, creating your first migration, and understanding how to manage schema changes safely.

### Prerequisites

- **`pgroll` CLI installed**: Follow the [installation instructions](#step-1-installation) below.
- **Neon Account and Project**: A Neon account and a project with a running Postgres database. You can create a free Neon account and project at [pg.new](https://pg.new).

### Step 1: Installation

You can install `pgroll` using various methods depending on your operating system and preferences. The recommended way is to use the pre-built binaries available for major platforms.

If you are on macOS, you can install `pgroll` using Homebrew:

```bash shouldWrap
brew tap xataio/pgroll
brew install pgroll
```

If you prefer to install from source, ensure you have Go installed and run:

```bash shouldWrap
go install github.com/xataio/pgroll@latest
```

If you need a pre-compiled binary for your platform, please refer to [`pgroll` installation instructions](https://pgroll.com/docs/latest/installation).

### Step 2: Initialize `pgroll`

`pgroll` requires a dedicated schema (by default, `pgroll`) to store its internal state. Initialize it by running the following command:

```bash shouldWrap
pgroll init --postgres-url "postgresql://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>?sslmode=require&channel_binding=require"
```

> Replace `<user>`, `<password>`, `<endpoint_hostname>`, `<port>`, and `<dbname>` with your Neon database connection details. You can find these in the [Neon Console](https://console.neon.tech) under your project's **Connect** section. Learn more: [Connect from any application](/docs/connect/connect-from-any-app)

### Step 3: Your first migration

<Admonition type="important" title="Working with an existing database?">
The following steps start by creating new tables. If you are applying `pgroll` to a database that already contains tables, you must first create a baseline of your existing schema. Please follow the instructions in the **[Onboarding an Existing Database](#onboarding-an-existing-database-baseline)** section.
</Admonition>

Migrations in `pgroll` are defined declaratively in `yaml` or `json` files. This means you specify _what_ you want the end state of your schema to be, and `pgroll` handles the complex steps of _how_ to get there safely.

Let's create a `users` table. Save the following content to a file named `migrations/01_create_users.yaml`:

```yaml
# A list of one or more schema change operations
operations:
  # The first operation is to create a table
  - create_table:
      # The name of the table to create
      name: users
      # A list of column definitions for the table
      columns:
        - name: id
          type: serial
          pk: true
        - name: name
          type: varchar(255)
          unique: true
        - name: description
          type: text
          nullable: true
```

#### Understanding the migration syntax

Let's quickly break down the file you just created:

- `operations`: This is the top-level key for a list of actions `pgroll` will perform. A single migration file can contain multiple operations.
- `create_table`: This is a specific `pgroll` operation. It defines a new table and its properties.
- `columns`: Inside `create_table`, this array defines each column's `name`, `type`, and any constraints like `pk` (primary key), `unique`, or `nullable`.

This declarative approach is what allows `pgroll` to analyze the changes, manage locks intelligently, and perform migrations without downtime. For a complete list of all supported actions, such as `alter_column` or `drop_index`, see the official **[pgroll operations reference](https://pgroll.com/docs/latest/operations)**.

<Admonition type="note" title="Coming from an ORM or SQL Scripts?">
You don't always have to write these YAML files by hand. `pgroll` can automatically generate migrations from standard SQL files. We'll cover how to use this feature with tools like Drizzle in the [Generating migrations from ORMs](#generating-migrations-with-orms) section.
</Admonition>

Since this is the first migration, there's no "old" schema to preserve compatibility with, so we can start and complete it in one step using the `--complete` flag.

```bash shouldWrap
pgroll start migrations/01_create_users.yaml --postgres-url "postgresql://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>?sslmode=require&channel_binding=require" --complete
```

### Step 4: A breaking change (add `NOT NULL` constraint)

Now, let's make the `description` column non-nullable. This is a classic breaking change, as it introduces two immediate challenges that would cause downtime with a traditional migration tool:

1.  **Existing data:** The `users` table may already contain rows where `description` is `NULL`, which would violate the new constraint.
2.  **Live application:** Your running application code is still operating under the assumption that the column is nullable and may attempt to insert `NULL` values, which would result in runtime errors.

This is precisely the type of scenario `pgroll` is designed to handle without disrupting your service. To perform this migration, we will use `pgroll`'s ability to create a new schema version that temporarily allows `NULL` values while we backfill existing data. In this case, we must provide an `up` SQL expression to tell `pgroll` how to backfill any existing `NULL` values and a `down` expression to revert the changes in case of a rollback.

Create a new migration file named `migrations/02_make_description_not_null.yaml` with the following content:

```yaml
operations:
  - alter_column:
      table: users
      column: description
      nullable: false
      up: SELECT CASE WHEN description IS NULL THEN 'No description provided' ELSE description END
      down: description
```

We'll now start the migration using `pgroll start`, which will perform the "expand" phase. This phase prepares the database for the breaking change without applying it yet.

```bash shouldWrap
pgroll start migrations/02_make_description_not_null.yaml --postgres-url "postgresql://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>?sslmode=require&channel_binding=require"
```

At this point, `pgroll` has performed the "expand" phase:

1.  It created a temporary column `_pgroll_new_description` on the `users` table.
2.  It backfilled this new column using your `up` SQL, converting `NULL`s to a valid string.
3.  It created triggers to transparently sync writes between `description` and `_pgroll_new_description`.
4.  It created a new schema version, `public_02_make_description_not_null`, whose view exposes `_pgroll_new_description` as `description`.

Your old applications can continue using the previous schema version, while you deploy new applications configured to use the new version.

### Step 5: Connecting your application to the new schema version

The key to a zero-downtime rollout is updating your application to point to the new schema version. This is done by setting the `search_path` for the database connection.

First, you can get the name of the latest schema version directly from `pgroll`. This is ideal for use in CI/CD pipelines:

```bash shouldWrap
export PGROLL_SCHEMA_VERSION=$(pgroll latest --with-schema --postgres-url "postgresql://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>?sslmode=require&channel_binding=require")
echo $PGROLL_SCHEMA_VERSION
# Example output: public_02_make_description_not_null
```

You would then pass this environment variable (`PGROLL_SCHEMA_VERSION`) to your application during deployment.

#### Example: Configuring a TypeScript/Drizzle Application

To connect your application to a new schema version, you must configure your database client to use the correct `search_path`. Since Drizzle ORM does not have a built-in, session-level way to set this, the recommended approach is to wrap your queries within a **transaction**. This ensures the `SET search_path` command is executed for the current session before your application code queries the database.

<Admonition type="warning" title="Session-Based Connection Required">
Setting the `search_path` is a session-level command. This means you must use a database driver that supports persistent, interactive sessions.

For Neon users, the stateless **`drizzle-orm/neon-http` driver is not suitable for this task**. You must use a session-based driver like `postgres-js`, `node-postgres` (`pg`), or the `neon-serverless` driver (which uses WebSockets).
</Admonition>

Here are examples for three popular drivers. In each case, we assume the schema name (e.g., `public_02_make_description_not_null`) is passed to the application via an environment variable like `PGROLL_SCHEMA_VERSION` as shown above.

<CodeTabs reverse={true} labels={["postgres.js", "node-postgres", "Neon serverless driver"]}>

```typescript
import { drizzle } from 'drizzle-orm/postgres-js';
import postgres from 'postgres';
import { users } from './db/schema';
import 'dotenv/config';

// Get the target schema from environment variables
const schema = process.env.PGROLL_SCHEMA_VERSION || 'public';

const client = postgres(process.env.DATABASE_URL!);
const db = drizzle({ client });

async function getUsers() {
  try {
    // Wrap your query in a transaction to set the search_path
    const allUsers = await db.transaction(async (tx) => {
      await tx.execute(`SET search_path TO ${schema}`);
      return tx.select().from(users);
    });

    console.log(`Users from schema '${schema}':`, allUsers);
  } catch (error) {
    console.error('Error fetching users:', error);
  } finally {
    await client.end();
  }
}

getUsers();
```

```typescript
import { drizzle } from 'drizzle-orm/node-postgres';
import { Pool } from 'pg';
import { users } from './db/schema';
import 'dotenv/config';

// Get the target schema from environment variables
const schema = process.env.PGROLL_SCHEMA_VERSION || 'public';

const client = new Pool({
  connectionString: process.env.DATABASE_URL!,
});

const db = drizzle({ client });

async function getUsers() {
  try {
    // Wrap your query in a transaction to set the search_path
    const allUsers = await db.transaction(async (tx) => {
      await tx.execute(`SET search_path TO ${schema}`);
      return tx.select().from(users);
    });

    console.log(`Users from schema '${schema}':`, allUsers);
  } catch (error) {
    console.error('Error fetching users:', error);
  } finally {
    await client.end();
  }
}

getUsers();
```

```typescript
import { Pool, neonConfig } from '@neondatabase/serverless';
import { drizzle } from 'drizzle-orm/neon-serverless';
import { users } from './db/schema';
import ws from 'ws';
import 'dotenv/config';

// Required for WebSocket connections in Node.js
neonConfig.webSocketConstructor = ws;

// Get the target schema from environment variables
const schema = process.env.PGROLL_SCHEMA_VERSION || 'public';

const client = new Pool({
  connectionString: process.env.DATABASE_URL!,
});
const db = drizzle({ client });

async function getUsers() {
  try {
    // Wrap your query in a transaction to set the search_path
    const allUsers = await db.transaction(async (tx) => {
      await tx.execute(`SET search_path TO ${schema}`);
      return tx.select().from(users);
    });

    console.log(`Users from schema '${schema}':`, allUsers);
  } catch (error) {
    console.error('Error fetching users:', error);
  } finally {
    await client.end();
  }
}

getUsers();
```

</CodeTabs>

The key pattern in all these examples is wrapping your database calls in a `db.transaction`. This guarantees that the `SET search_path` command and your actual queries are executed within the same database session, ensuring your application interacts with the correct `pgroll` version schema.

For examples in other languages and frameworks, please refer to the official `pgroll` documentation on [integrating client applications](https://pgroll.com/docs/latest/guides/clientapps).

### Step 6: Complete the migration

Once all your application instances have been updated to use the new schema, you can safely complete the migration.

```bash shouldWrap
pgroll complete --postgres-url "postgresql://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>?sslmode=require&channel_binding=require"
```

`pgroll` will now perform the "contract" phase: drop the old `description` column, rename `_pgroll_new_description` to `description`, apply the `NOT NULL` constraint permanently, and remove the temporary triggers and the old version schema.

### Step 7: Rolling back

If you discover an issue after `start` but before `complete`, you can instantly and safely roll back the changes.

```bash shouldWrap
pgroll rollback --postgres-url "postgresql://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>?sslmode=require&channel_binding=require"
```

This command removes the new version schema and all temporary structures, reverting the database to its exact state before the migration began. This operation has no impact on applications still using the old schema version.

## Integrating `pgroll` into your workflow

`pgroll` is designed to fit seamlessly into modern development practices, including workflows with ORMs and CI/CD pipelines.

### Generating migrations with ORMs

You don't need to write `pgroll` migrations by hand. Most ORMs can generate schema changes as raw SQL, which `pgroll` can then convert into its declarative format.

The key command is `pgroll convert`, which reads SQL statements and translates them into `pgroll`'s YAML or JSON format.

### Example: Drizzle ORM

A typical workflow with Drizzle ORM and `pgroll` involves the following steps:

1.  **Modify your drizzle schema:** Start by making the desired changes to your schema definitions in your project's `db/schema.ts` file.

2.  **Generate the SQL migration:** Use the Drizzle Kit CLI to generate a standard SQL migration file from your schema changes.

    ```shell
    npx drizzle-kit generate
    ```

    This creates a new `.sql` file in your migrations folder.

3.  **Convert to a `pgroll` migration:** Use the `pgroll` CLI to convert the generated SQL file into `pgroll`'s declarative YAML format.

    ```shell
    pgroll convert <path-to-your-drizzle-generated.sql> > <path-to-your-new.yaml>
    ```

    **Crucially, review the output YAML.** For any breaking changes, you will likely need to manually provide the correct `up` and `down` SQL expression to handle data backfilling.

    <Admonition type="important" title="Manual review required">
    The `convert` command is a powerful starting point, but you may need to manually edit the output. For complex changes, `pgroll` often creates `TODO` markers for `up`/`down` expressions that it cannot infer automatically. Always review and complete the generated migration file.
    </Admonition>

4.  **Start the migration:** Apply the migration to your database using the `start` command. This creates the new schema version alongside the old one without causing downtime.

    ```shell
    pgroll start <path-to-your-new.yaml>
    ```

5.  **Test and deploy your new application:**
    - Fetch the new schema name using `pgroll latest schema`.
    - In your CI/CD pipeline, deploy the new version of your application, configuring it to use the new schema via an environment variable (e.g., `PGROLL_SCHEMA_VERSION`).
    - This is the ideal stage for phased rollouts (canary, blue-green), as the old application version continues to run unaffected on the previous schema.

6.  **Validate and finalize:**
    - **If an issue is found,** you can instantly and safely revert the database changes with `pgroll rollback`. This will not affect the running (old) application.
    - **If the new application is stable,** proceed with a full rollout.

7.  **Complete the migration:** Once you are confident that no services are using the old schema, finalize the process by running:
    ```shell
    pgroll complete
    ```
    This removes the old schema version and cleans up all temporary columns and triggers, leaving your database in its new, permanent state.

This workflow of generating and converting SQL can be adapted for other ORMs like Sequelize, TypeORM, or Prisma that can output schema changes as SQL files.

## Onboarding an existing database (`baseline`)

If you want to use `pgroll` on a project with an existing schema, you don't need to recreate its migration history. The `baseline` command establishes a starting point.

```bash shouldWrap
pgroll baseline 01_initial_schema ./migrations
```

This command:

- Records the current schema state as the starting point in `pgroll`'s internal tables.
- Creates an empty placeholder migration file (`01_initial_schema.yaml`).
- Does **not** apply any changes to your database.

You should then use a tool like `pg_dump --schema-only` to capture your current schema DDL and place it inside a `sql` operation within the placeholder file. All future migrations will now build upon this baseline.

## Common migration operations

`pgroll` migrations consist of a list of declarative operations. Below are a few common examples.

<Admonition type="note" title="Refer to the Official Documentation">
The following examples showcase some of the most common use cases, but `pgroll`'s capabilities are far more extensive. It provides a comprehensive suite of declarative operations for fine-grained schema control, including:

- **Table Management:** `create_table`, `drop_table`, and `rename_table`.
- **Column Manipulation:** `add_column`, `drop_column`, and a powerful `alter_column` operation for changing types, nullability, defaults, and comments.
- **Indexes and Constraints:** Full lifecycle management for indexes and constraints, including `create_index`, `drop_index`, `create_constraint`, `drop_constraint`, and `rename_constraint`.
- **Raw SQL Escape Hatch:** An `sql` operation for executing custom DDL or handling advanced scenarios not covered by the declarative operations.

For a complete list of all operations and their detailed parameters, it is highly recommended to consult the official [pgroll Operations Reference](https://pgroll.com/docs/latest/operations).
</Admonition>

#### Create table

To create a new table, you can use the `create_table` operation. This operation allows you to define the table name and its columns, including types and constraints.

```yaml
operations:
  - create_table:
      name: products
      columns:
        - name: id
          type: serial
          pk: true
        - name: name
          type: varchar(255)
          unique: true
        - name: price
          type: decimal(10,2)
```

#### Add column

Creating a new column in an existing table is straightforward with the `add_column` operation. You can specify the column name, type, and any default value.

```yaml
operations:
  - add_column:
      table: reviews
      column:
        name: rating
        type: text
        default: '0'
```

#### Raw SQL (escape hatch)

For operations not natively supported, you can use raw SQL. Be aware that these operations do not come with `pgroll`'s zero-downtime guarantees and should be used with caution.

```yaml
operations:
  - sql:
      up: CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT)
      down: DROP TABLE users
```

## Conclusion

`pgroll` provides a practical solution for Postgres schema migrations by using a declarative, multi-version approach. This method automates complex updates, turning them into a safer and more predictable workflow.

The core benefits are the ability to achieve zero-downtime deployments and perform instant rollbacks, which reduces the risk associated with production schema changes. While this requires adapting deployment strategies to manage the `search_path` in client applications, the trade-off results in a more reliable migration process.

For organizations that prioritize high availability and continuous delivery, `pgroll` offers a valuable framework for evolving database schemas safely.

## Resources

- [pgroll GitHub Repository](https://github.com/xataio/pgroll)
- [pgroll Official Documentation](https://pgroll.com/docs)
- [Introducing pgroll: zero-downtime, reversible, schema migrations for Postgres](https://pgroll.com/blog/introducing-pgroll-zero-downtime-reversible-schema-migrations-for-postgres)
- [Postgres Schema Search Path Documentation](https://www.postgresql.org/docs/current/ddl-schemas.html#DDL-SCHEMAS-PATH)

<NeedHelp/>


# Using LISTEN and NOTIFY for Pub/Sub in PostgreSQL

---
title: Using LISTEN and NOTIFY for Pub/Sub in PostgreSQL
subtitle: A step-by-step guide describing how to use LISTEN and NOTIFY for pub/sub in Postgres
author: vkarpov15
enableTableOfContents: true
createdAt: '2025-03-28T13:24:36.612Z'
updatedOn: '2025-03-28T13:24:36.612Z'
---

PostgreSQL has a built-in mechanism for publish/subscribe (Pub/Sub) communication using the `LISTEN` and `NOTIFY` commands.
This allows different sessions to send messages to each other using Postgres, without needing a separate service like Kafka or RabbitMQ.

## Steps

- Overview
- Set Up a Listener in Node.js
- Send a Message using `NOTIFY`
- Limitations of `LISTEN`/`NOTIFY`

### Overview

At a high level, the following is how `LISTEN` and `NOTIFY` are used.

1. A client subscribes to a notification channel using `LISTEN`.
2. Another client sends a message to that channel using `NOTIFY`.
3. The subscribed client receives the notification asynchronously.

### Set Up a Listener in Node.js

Because of connection pooling, setting up a pub/sub listener in the Neon console is tricky.
But you can create a Node.js script that listens to notifications on the channel `my_channel` as follows.
Note that you do **not** need to explicitly create the `my_channel` channel, subscribing to the channel also creates the channel.

```javascript
const { Client } = require('pg');

const client = new Client({
  connectionString: YOUR CONNECTION STRING HERE
});

async function setupListener() {
  await client.connect();
  await client.query('LISTEN my_channel');
  console.log('Listening for notifications on my_channel...');

  client.on('notification', (msg) => {
    console.log('Received notification:', msg.payload);
  });
}

setupListener().catch(console.error);
```

Make sure to disable connection pooling in your Neon connection string (make sure your connection string does not include `-pooler`).
`LISTEN` and `NOTIFY` are session-specific features and are not compatible with Neon connection pooling.

Run the above script and you should see the following output.
Keep the above script running, you will trigger a notification in the next section.

```
Listening for notifications on my_channel...
```

### Send a Message using NOTIFY

You can send a message to the above Node.js script from the Neon console using the following SQL command.

```sql
NOTIFY my_channel, 'Hello from another session!';
```

After running the above, your Node.js script should print out the following output.

```
Received notification: Hello from another session!
```

You can also use the `pg_notify()` function as follows, which is equivalent to the `NOTIFY` command above.

```sql
SELECT pg_notify('my_channel', 'Hello from pg_notify!');
```

Note that you don't need to explicitly create a channel.

### Limitations of LISTEN/NOTIFY

Postgres `LISTEN` and `NOTIFY` run entirely in memory and do not persist any data.
If there are no listeners when a `NOTIFY` runs, the message disappears and Postgres does not provide a mechanism to replay messages.
While the memory overhead of `LISTEN` is minimal, `LISTEN` can cause performance degradations at scale if notifications start using up too much memory.

There is also no way to ensure that a message was delivered to a listener.
If you need message persistence or guarantees that a message was processed, you should look at dedicated message queues like RabbitMQ or Kafka.

If you are using `LISTEN` and `NOTIFY`, you should disable Neon's [Scale to Zero feature](/docs/introduction/scale-to-zero).
If Neon scales your compute to 0, [it will terminate all listeners](/docs/reference/compatibility#session-context), which may lead to lost messages when your database reactivates.


# Building a Real-Time AI Voice Assistant with ElevenLabs

---
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-12-17T00:00:00.000Z'
updatedOn: '2024-12-17T00:00:00.000Z'
title: Building a Real-Time AI Voice Assistant with ElevenLabs
subtitle: A step-by-step guide to building your own AI Voice Assistant in a Next.js application with ElevenLabs and Postgres
---

Imagine having an AI voice assistant like Iron Man's [J.A.R.V.I.S.](https://en.wikipedia.org/wiki/J.A.R.V.I.S.), capable of understanding and responding to your needs in real-time. In this guide, you will learn how to build your very own real-time AI voice assistant using ElevenLabs, store each conversation in a Postgres database, and index them for faster retrieval.

<Admonition type="note" title="Deal alert">
Take advantage of the [AI Engineer Starter Pack](https://www.aiengineerpack.com) by ElevenLabs to get discounts for the tools used in this guide.
</Admonition>

## Prerequisites

To follow this guide, you’ll need the following:

- [Node.js 18](https://nodejs.org/en) or later
- A [Neon](https://console.neon.tech/signup) account
- A [ElevenLabs](https://elevenlabs.io/) account
- A [Vercel](https://vercel.com) account

## Create a new Next.js application

Let’s get started by creating a new Next.js project with the following command:

```shell shouldWrap
npx create-next-app@latest pulse
```

When prompted, choose:

- `Yes` when prompted to use TypeScript.
- `No` when prompted to use ESLint.
- `Yes` when prompted to use Tailwind CSS.
- `No` when prompted to use `src/` directory.
- `Yes` when prompted to use App Router.
- `No` when prompted to use Turbopack for `next dev`.
- `No` when prompted to customize the default import alias (`@/*`).

Once that is done, move into the project directory and install the necessary dependencies with the following command:

```shell
cd pulse
npm install @11labs/react @neondatabase/serverless motion framer-motion react-feather sonner
npm install -D tsx
```

The libraries installed include:

- `framer-motion`: A library for animations in React.
- `react-feather`: A collection of open-source icons for React.
- `motion`: A library to create animations in React applications.
- `sonner`: A notification library for React to display toast notifications.
- `@11labs/react`: A React library to interact with [ElevenLabs API](https://elevenlabs.io/api).
- `@neondatabase/serverless`: A library to connect and interact with Neon’s serverless Postgres database.

The development-specific libraries include:

- `tsx`: To execute and rebuild TypeScript efficiently.

## Provision a Serverless Postgres

To set up a serverless Postgres, go to the [Neon console](https://console.neon.tech/app/projects) and create a new project. Once your project is created, you will receive a connection string that you can use to connect to your Neon database. The connection string will look like this:

```bash shouldWrap
postgresql://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>?sslmode=require&channel_binding=require
```

Replace `<user>`, `<password>`, `<endpoint_hostname>`, `<port>`, and `<dbname>` with your specific details.

Use this connection string as an environment variable designated as `DATABASE_URL` in the `.env` file.

## Create an AI Agent with ElevenLabs

To create a customizable agent, go to ElevenLabs' [AI Agents](https://elevenlabs.io/app/conversational-ai) and then click on `Create an AI agent` button.

![](/guides/images/pulse/agent-1.png)

Next, give it a personalized name and select the kind of Agent you would want. For demonstration purposes, let's start with a `Blank template`.

![](/guides/images/pulse/agent-2.png)

Next, copy the Agent ID displayed just below the customized name of your agent (here, `Custom`). You will use this Agent ID as the `AGENT_ID` environment variable in your application.

![](/guides/images/pulse/agent-3.png)

Next, go to `Advanced > Client Events` in your Agent settings, and add two events named `agent_response` and `user_transcript`.

![](/guides/images/pulse/agent-4.png)

Finally, go to [API Keys](https://elevenlabs.io/app/settings/api-keys), create an API key and use the value obtained as `XI_API_KEY` enviroment variable in your application.

## Database Schema Setup

Create a file named `schema.tsx` at the root of your project directory with the following code:

```tsx
// File: schema.tsx

import { neon } from '@neondatabase/serverless';
import 'dotenv/config';

const createMessagesTable = async () => {
  if (!process.env.DATABASE_URL) throw new Error(`DATABASE_URL environment variable not found.`);
  const sql = neon(process.env.DATABASE_URL);
  try {
    await sql(
      `CREATE TABLE IF NOT EXISTS messages (created_at SERIAL, id TEXT PRIMARY KEY, session_id TEXT, content_type TEXT, content_transcript TEXT, object TEXT, role TEXT, status TEXT, type TEXT);`
    );
    await sql(
      `CREATE INDEX IF NOT EXISTS idx_session_created_at ON messages (session_id, created_at);`
    );
    console.log('Setup schema succesfully.');
  } catch (error) {
    console.error(error);
    console.log('Failed to set up schema.');
  }
};

createMessagesTable();
```

The code above defines an asynchronous function `createMessagesTable` that connects to a Neon serverless Postgres database using a connection string stored in the `DATABASE_URL` environment variable, creates a `messages` table if it doesn't already exist, and sets up an index on the `session_id` and `created_at` columns for faster retrievals.

To run the migrations, execute the following command:

```
npx tsx schema.tsx
```

If it runs succesfully, you should see `Setup schema succesfully.` in the terminal.

## Build Reusable React Components and Hooks

### 1. Typing Effect Animation

To enhance the user experience by simulating real-time interactions, implement a typing effect in the UI to render AI responses incrementally. Create a file named `useTypingEffect.ts` in the `components` directory with the following code:

```tsx
// File: components/useTypingEffect.ts

import { useEffect, useState } from 'react';

export const useTypingEffect = (text: string, duration: number = 50, isTypeByLetter = false) => {
  const [currentPosition, setCurrentPosition] = useState(0);
  const items = isTypeByLetter ? text.split('') : text.split(' ');
  useEffect(() => {
    setCurrentPosition(0);
  }, [text]);
  useEffect(() => {
    if (currentPosition >= items.length) return;
    const intervalId = setInterval(() => {
      setCurrentPosition((prevPosition) => prevPosition + 1);
    }, duration);
    return () => {
      clearInterval(intervalId);
    };
  }, [currentPosition, items, duration]);
  return items.slice(0, currentPosition).join(isTypeByLetter ? '' : ' ');
};
```

The provided code exports a custom React hook called `useTypingEffect`. This hook simulates a typing effect for a specified text over a given duration, enhancing the user interface by rendering text incrementally.

### 2. Conversation Message

To render each message in the conversation history, you need to dynamically indicate whether the message is from the User or the AI. Create a file named `Message.tsx` in the `components` directory with the following code:

```tsx
// File: components/Message.tsx

import { Cpu, User } from 'react-feather';

export default function ({
  conversationItem,
}: {
  conversationItem: { role: string; formatted: { transcript: string } };
}) {
  return (
    <div className="flex max-w-full flex-row flex-wrap items-start gap-x-3">
      <div className="max-w-max rounded border p-2">
        {conversationItem.role === 'user' ? <User /> : <Cpu />}
      </div>
      <div className="flex flex-col gap-y-2">{conversationItem.formatted.transcript}</div>
    </div>
  );
}
```

The code above exports a React component that renders a message. It conditionally displays a `Cpu` icon for messages from the AI and a `User` icon for messages from the user, along with the message content.

### 3. Various States During AI Interaction

Create a file named `TextAnimation.tsx` in the `components` directory with the following code:

```tsx
// File: components/TextAnimation.tsx

'use client';

import { useTypingEffect } from '@/components/useTypingEffect';
import { motion } from 'framer-motion';
import { useEffect, useState } from 'react';

type AIState = 'idle' | 'listening' | 'speaking';

interface Props {
  onStartListening?: () => void;
  onStopListening?: () => void;
  isAudioPlaying?: boolean;
  currentText: string;
}

export default function AiTalkingAnimation({
  onStartListening,
  onStopListening,
  isAudioPlaying,
  currentText,
}: Props) {
  const [aiState, setAiState] = useState<AIState>('idle');
  const animatedCurrentText = useTypingEffect(currentText, 20);
  const displayedText = useTypingEffect('Click the circle to start the conversation', 20);

  const handleCircleClick = () => {
    if (aiState === 'listening' || aiState === 'speaking') {
      onStopListening?.();
      setAiState('idle');
    } else if (!isAudioPlaying) {
      onStartListening?.();
      setAiState('listening');
    }
  };

  useEffect(() => {
    if (isAudioPlaying) setAiState('speaking');
    else if (aiState === 'speaking' && currentText) setAiState('listening');
  }, [isAudioPlaying]);

  return (
    <div className="bg-gray-100 flex min-h-screen flex-col items-center justify-center p-4">
      <div
        role="button"
        onClick={handleCircleClick}
        className="relative mb-8 cursor-pointer"
        aria-label={aiState === 'listening' ? 'Stop listening' : 'Start listening'}
      >
        <motion.div
          className="from-pink-500 to-violet-600 flex h-20 w-20 items-center justify-center rounded-full bg-gradient-to-br shadow-lg"
          animate={
            aiState === 'idle'
              ? { scale: [1, 1.1, 1] }
              : aiState === 'speaking'
                ? { scale: [1, 1.2, 0.8, 1.2, 1] }
                : {}
          }
          transition={{
            repeat: Infinity,
            ease: 'easeInOut',
            duration: aiState === 'speaking' ? 0.8 : 1.5,
          }}
        />
        {aiState === 'listening' && (
          <svg
            viewBox="0 0 100 100"
            className="absolute left-1/2 top-1/2 h-24 w-24 -translate-x-1/2 -translate-y-1/2"
          >
            <motion.circle
              cx="50"
              cy="50"
              r="48"
              fill="none"
              strokeWidth="4"
              stroke="#8B5CF6"
              transition={{
                duration: 10,
                ease: 'linear',
                repeat: Infinity,
              }}
              strokeLinecap="round"
              initial={{ pathLength: 0, rotate: -90 }}
              animate={{ pathLength: 1, rotate: 270 }}
            />
          </svg>
        )}
      </div>
      <div className="w-full max-w-md rounded-lg bg-white p-6 shadow-lg">
        <p className="text-gray-800 font-mono text-lg" aria-live="polite">
          {aiState === 'listening'
            ? 'Listening...'
            : aiState === 'speaking'
              ? animatedCurrentText
              : displayedText}
        </p>
        {aiState === 'idle' && (
          <motion.div
            animate={{
              opacity: [0, 1, 0],
            }}
            transition={{
              duration: 0.8,
              repeat: Infinity,
              ease: 'easeInOut',
            }}
            className="bg-violet-600 mt-2 h-5 w-2"
          />
        )}
      </div>
    </div>
  );
}
```

The code above exports a React component that creates an interactive UI for the AI voice assistant. It utilizes the `useTypingEffect` hook to simulate a typing effect for the AI's responses and displays different states of interaction, such as "idle," "listening," and "speaking." The component also includes a clickable circle that toggles between starting and stopping the listening state, providing visual feedback through animations.

## Generate a Signed URL for private conversations with ElevenLabs

To create a secure access between user and AI (powered by ElevenLabs), create a new file named `route.ts` in the `app/api/i` directory with the following code:

```tsx
// File: app/api/i/route.ts

export const runtime = 'edge';

export const dynamic = 'force-dynamic';

export const fetchCache = 'force-no-store';

import { NextResponse } from 'next/server';

export async function POST(request: Request) {
  const agentId = process.env.AGENT_ID;
  if (!agentId) throw Error('AGENT_ID is not set or received.');
  const apiKey = process.env.XI_API_KEY;
  if (!apiKey) throw Error('XI_API_KEY is not set or received.');
  try {
    const apiUrl = new URL('https://api.elevenlabs.io/v1/convai/conversation/get_signed_url');
    apiUrl.searchParams.set('agent_id', agentId);
    const response = await fetch(apiUrl.toString(), {
      headers: { 'xi-api-key': apiKey },
    });
    if (!response.ok) throw new Error(response.statusText);
    const data = await response.json();
    return NextResponse.json({ apiKey: data.signed_url });
  } catch (error) {
    // @ts-ignore
    const message = error.message || error.toString();
    return NextResponse.json({ error: message }, { status: 500 });
  }
}
```

The code above defines an API route that generates a signed URL using ElevenLabs API. You will want to use signed URL instead of connecting to a fixed point server so as to allow connection to your personalized, private agents created in ElevenLabs.

## Sync Conversations to a Postgres database

Create a file named `route.ts` in the `app/api/c` directory with the following code:

```tsx
// File: app/api/c/route.ts

export const runtime = 'edge';

export const dynamic = 'force-dynamic';

export const fetchCache = 'force-no-store';

import { neon, neonConfig } from '@neondatabase/serverless';
import { NextResponse } from 'next/server';

neonConfig.poolQueryViaFetch = true;

export async function POST(request: Request) {
  const { id, item } = await request.json();
  if (!id || !item || !process.env.DATABASE_URL) return NextResponse.json({}, { status: 400 });
  const sql = neon(process.env.DATABASE_URL);
  const rows = await sql('SELECT COUNT(*) from messages WHERE session_id = $1', [id]);
  await sql(
    'INSERT INTO messages (created_at, id, session_id, content_type, content_transcript, object, role, status, type) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9) ON CONFLICT DO NOTHING',
    [
      rows[0].count,
      item.id,
      id,
      item.content[0].type,
      item.content[0].transcript,
      item.object,
      item.role,
      item.status,
      item.type,
    ]
  );
  return NextResponse.json({});
}

export async function GET(request: Request) {
  const id = new URL(request.url).searchParams.get('id');
  if (!id || !process.env.DATABASE_URL) return NextResponse.json([]);
  const sql = neon(process.env.DATABASE_URL);
  const rows = await sql('SELECT * from messages WHERE session_id = $1', [id]);
  return NextResponse.json(rows);
}
```

The code above defines two endpoint handlers on `/api/c`:

- A `POST` endpoint that allows you to insert a new message into the `messages` table. It expects a JSON payload containing the `id` of the session and the `item` to be inserted. If the session ID or item is missing, it returns a 400 status code.

- A `GET` endpoint that retrieves all messages associated with a specific session ID. It extracts the session ID from the request URL and queries the `messages` table, returning the results as a JSON response. If the session ID is not provided, it returns an empty array.

## Create the UI for Starting Conversations and Synchronizing Chat History

Create a file named `page.tsx` in the `app/c/[slug]` directory with the following code:

```tsx
// File: app/c/[slug]/page.tsx

'use client';

import { toast } from 'sonner';
import { useParams } from 'next/navigation';
import { useCallback, useEffect, useState } from 'react';
import { type Role, useConversation } from '@11labs/react';

export default function () {
  const { slug } = useParams();
  const [currentText, setCurrentText] = useState('');
  const [messages, setMessages] = useState<any[]>([]);
  const loadConversation = () => {
    fetch(`/api/c?id=${slug}`)
      .then((res) => res.json())
      .then((res) => {
        if (res.length > 0) {
          setMessages(
            res.map((i: any) => ({
              ...i,
              formatted: {
                text: i.content_transcript,
                transcript: i.content_transcript,
              },
            }))
          );
        }
      });
  };
  const conversation = useConversation({
    onError: (error: string) => {
      toast(error);
    },
    onConnect: () => {
      toast('Connected to ElevenLabs.');
    },
    onMessage: (props: { message: string; source: Role }) => {
      const { message, source } = props;
      if (source === 'ai') setCurrentText(message);
      fetch('/api/c', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          id: slug,
          item: {
            type: 'message',
            status: 'completed',
            object: 'realtime.item',
            id: 'item_' + Math.random(),
            role: source === 'ai' ? 'assistant' : 'user',
            content: [{ type: 'text', transcript: message }],
          },
        }),
      }).then(loadConversation);
    },
  });
  const connectConversation = useCallback(async () => {
    toast('Setting up ElevenLabs...');
    try {
      await navigator.mediaDevices.getUserMedia({ audio: true });
      const response = await fetch('/api/i', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
      });
      const data = await response.json();
      if (data.error) return toast(data.error);
      await conversation.startSession({ signedUrl: data.apiKey });
    } catch (error) {
      toast('Failed to set up ElevenLabs client :/');
    }
  }, [conversation]);
  const disconnectConversation = useCallback(async () => {
    await conversation.endSession();
  }, [conversation]);
  const handleStartListening = () => {
    if (conversation.status !== 'connected') connectConversation();
  };
  const handleStopListening = () => {
    if (conversation.status === 'connected') disconnectConversation();
  };
  useEffect(() => {
    return () => {
      disconnectConversation();
    };
  }, [slug]);
  return <></>;
}
```

The code above does the following:

- Defines a `loadConversation` function which calls the `/api/c` route to fetch the conversation history based on the particular slug (i.e. the conversation ID).
- Uses the `useConversation` hook by ElevenLabs to display the toast when the instance is connected, and to sync the real-time message to Postgres using the `onMessage` callback.
- Defines a `connectConversation` function that instantiates a private conversation with the agent after obtaining a signed URL using the `/api/i` route.
- Defines a `disconnectConversation` function that disconnects the ongoing conversation with the agent.
- Creates a `useEffect` handler which on unmount, ends the ongoing conversation with the agent.

Next, import the `TextAnimation` component which displays different state of the conversation, whether AI is listening or speaking (and what if so).

```tsx ins={4,10-15}
'use client';

// ... Existing imports ...
import TextAnimation from '@/components/TextAnimation';

export default function () {
  // ... Existing code ...
  return (
    <>
      <TextAnimation
        currentText={currentText}
        onStopListening={handleStopListening}
        onStartListening={handleStartListening}
        isAudioPlaying={conversation.isSpeaking}
      />
    </>
  );
}
```

Finally, add a `Show Transcript` button that displays the conversation history stored in Neon to the user.

```tsx ins={4,5,9,13-37}
'use client';

// ... Existing imports ...
import { X } from 'react-feather';
import Message from '@/components/Message';

export default function () {
  // ... Existing code ...
  const [isTranscriptOpen, setIsTranscriptOpen] = useState(false);
  return (
    <>
      {/* Existing code */}
      {messages.length > 0 && (
        <button
          className="fixed right-4 top-2 text-sm underline"
          onClick={() => setIsTranscriptOpen(!isTranscriptOpen)}
        >
          Show Transcript
        </button>
      )}
      {isTranscriptOpen && (
        <div className="fixed inset-0 z-50 flex items-center justify-center bg-black bg-opacity-50">
          <div className="max-h-[90%] max-w-[90%] overflow-y-scroll rounded bg-white p-4 text-black shadow-lg">
            <div className="flex flex-row items-center justify-between">
              <span>Transcript</span>
              <button onClick={() => setIsTranscriptOpen(false)}>
                <X />
              </button>
            </div>
            <div className="mt-4 flex flex-col gap-y-4 border-t py-4">
              {messages.map((conversationItem) => (
                <Message key={conversationItem.id} conversationItem={conversationItem} />
              ))}
            </div>
          </div>
        </div>
      )}
    </>
  );
}
```

Now, let's move on to deploying the application to Vercel.

## Deploy to Vercel

The repository is now ready to deploy to Vercel. Use the following steps to deploy:

- Start by creating a GitHub repository containing your app's code.
- Then, navigate to the Vercel Dashboard and create a **New Project**.
- Link the new project to the GitHub repository you've just created.
- In **Settings**, update the **Environment Variables** to match those in your local `.env` file.
- Deploy.

<DetailIconCards>

<a target="_blank" href="https://github.com/neondatabase-labs/pulse" description="A Real-Time AI Voice Assistant" icon="github">Pulse</a>

</DetailIconCards>

## Summary

In this guide, you learned how to build a real-time AI voice assistant using ElevenLabs and Next.js, integrating it with a Postgres database to store and retrieve conversation histories. You explored the process of setting up a serverless database, creating a customizable AI agent, and implementing a user-friendly interface with animations and message handling. By the end, you gained hands-on experience connecting various technologies to create a fully functional AI voice assistant application.

<NeedHelp />


# Query your Postgres Database Using Azure Functions

---
title: Query your Postgres Database Using Azure Functions
subtitle: Learn how to query your Postgres database using Azure Functions
author: adalbert-pungu
enableTableOfContents: true
createdAt: '2024-10-19T21:00:00.000Z'
---

In this guide, we will explore how to query a **Postgres** database hosted on **Neon** using **Azure Functions**. This combination allows you to take advantage of a flexible, high-performance infrastructure without worrying about server management.

## Prerequisites

You will need:

- An [Azure](https://azure.microsoft.com/) account with a subscription to deploy Azure Functions.
- A Neon account. If you don’t have one yet, you can [sign up](https://console.neon.tech/signup).
- Basic knowledge of Node.js and SQL.
- Familiarity with using Visual Studio Code.

## Why Neon?

Neon stands out as a cloud-native Postgres solution with an innovative architecture that separates compute and storage, offering a truly serverless database. This means Neon automatically adjusts its resources based on your application’s needs, making it ideal for projects that require flexible scalability without directly managing the infrastructure. In other words, Neon allows you to accelerate project delivery by focusing solely on development, while having an infrastructure that scales on demand.

> Neon Database is cloud-native Serverless Postgres, meaning it has completly separated storage from compute. This means Neon automatically adjusts its resources based on your application’s needs, making it ideal for projects that require flexible scaling without directly managing the infrastructure. In other words, Neon Database is a fully managed database service that allows you to focus on building your application without worrying about the underlying infrastructure.

Azure Functions are also a serverless compute service, which enables you to run event-driven code without having to manage infrastructure. It is a great choice for building serverless applications, microservices, and APIs. By combining Neon Database with Azure Functions, you can create a powerful and scalable application that can handle a wide range of use cases.

At the same time, Azure Functions enables you to run code in response to events without worrying about the underlying infrastructure. It will create microservices that respond to events, such as **HTTP requests**, without the need to deploy or manage servers.

To illustrate this, we will discuss an example of client management (hotel reservation management), which is a common use case in application development. We will use the technologies mentioned above to query and process data.

## Context

Imagine you are developing a solution to manage hotel reservations. You want to allow users (via an app or website) to view available reservations and interact with a Postgres database hosted on **Neon**.

The application's features will include:

- **View available rooms**: The application will allow users to check available hotel rooms for booking.
- **Add a new reservation**: When a customer makes a reservation, their information will be stored in the Neon.
- **Cancel a reservation**: Customers can cancel a reservation by deleting the corresponding record from the database.

---

## Step 1: Create and Configure the Database on Neon

**Sign up and create the database**

Sign up on [Neon](https://neon.com/) and follow the steps to create a Postgres database. The database will be named **neondb**.

After creating the database, make sure to copy the connection details (such as **host**, **user**, **password**, **database**) somewhere safe, as they will be used to configure **Azure Functions** to connect to **Neon**.

1. **Creating the tables**

   Once the database is created, you should see an option named "SQL Editor" on the left to write and execute queries.

   In the query editor, copy and paste the SQL code below to create the `clients` and `hotels` tables. These are reference tables, as the `reservations` table will refer to these tables via foreign keys:

   ```sql
   CREATE TABLE clients (
       client_id SERIAL PRIMARY KEY,
       first_name VARCHAR(100),
       last_name VARCHAR(100),
       email VARCHAR(100),
       phone_number VARCHAR(20)
   );

   CREATE TABLE hotels (
       hotel_id SERIAL PRIMARY KEY,
       name VARCHAR(100),
       location VARCHAR(100)
   );
   ```

   Here is the SQL script to create the `reservations` table:

   ```sql
   CREATE TABLE reservations (
       reservation_id SERIAL PRIMARY KEY,
       client_id INT NOT NULL,
       hotel_id INT NOT NULL,
       check_in_date DATE NOT NULL,
       check_out_date DATE NOT NULL,
       number_of_guests INT NOT NULL,
       created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
       FOREIGN KEY (client_id) REFERENCES clients(client_id),
       FOREIGN KEY (hotel_id) REFERENCES hotels(hotel_id)
   );
   ```

2. **Inserting test data**

   You can insert some example data into the database to ensure that everything is working fine up to this point.

   Here is the SQL script to insert data into the `clients` table:

   ```sql
   INSERT INTO clients (first_name, last_name, email, phone_number) VALUES
   ('Alice', 'Dupont', 'alice.dupont@example.com', '0123456789'),
   ('Bob', 'Martin', 'bob.martin@example.com', '0987654321'),
   ('Chloé', 'Lefevre', 'chloe.lefevre@example.com', '0147253689');
   ```

   Here is the SQL script to insert data into the `hotels` table:

   ```sql
   INSERT INTO hotels (name, location) VALUES
   ('Hôtel Le Paris', 'Paris'),
   ('Hôtel des Alpes', 'Annecy'),
   ('Hôtel de la Plage', 'Nice');
   ```

   Here is the SQL script to insert data into the `reservations` table:

   ```sql
   INSERT INTO reservations (client_id, hotel_id, check_in_date, check_out_date, number_of_guests) VALUES
   (1, 1, '2024-11-01', '2024-11-05', 2),
   (2, 2, '2024-11-10', '2024-11-15', 1),
   (3, 3, '2024-12-01', '2024-12-10', 4);
   ```

## Step 2: Create an Azure Function to Manage Products

1.  **Sign in to Azure**

    If you don't already have an account, sign up on the Microsoft [Azure](https://portal.azure.com/) portal.

    We will initialize an Azure Functions project where we will create an **HTTP Trigger function** in Visual Studio Code (VS Code) using the **Azure Functions extension**.

2.  **Install the Azure Functions extension**:
    - Open VS Code, or install [Visual Studio Code](https://code.visualstudio.com/) if it's not yet installed.
    - Go to the extensions tab or press `Ctrl+Shift+X`.
    - Search for "Azure Functions" and install the official extension.

3.  **Create an Azure Functions Project**

    Open the command palette or press `Ctrl+Shift+P` to open the command palette.
    - Type `Azure Functions: Create New Project...` and select that option.
    - Choose a directory where you want to create the project.
    - Select the programming language (`JavaScript` in our case).
    - Choose a JavaScript programming model (`Model V4`).
    - Choose a function template, and select `HTTP trigger`.
    - Give your function a name, for example, `manageClients`.

    Once confirmed, the project will be created with some default code.

4.  **Install the Postgres client**

    In the terminal of your Azure Functions project, install either [Neon serverless driver](/docs/serverless/serverless-driver) or the `node-postgres` (`pg`) package, which will be used to connect to Postgres:

    <CodeTabs labels={["Neon serverless driver", "node-postgres"]}>

    ```bash
    npm install @neondatabase/serverless
    ```

    ```bash
    npm install pg
    ```

    </CodeTabs>

5.  **Azure Functions Core Tools**

    Install Azure Functions Core Tools to run functions locally.

    ```bash
    npm install -g azure-functions-core-tools@4 --unsafe-perm true
    ```

    <Admonition type="note" title="suggested folder structure">

    Since there are three tables in the database (`Clients`, `Hotels`, and `Reservations`), using a separate file for each feature or interaction with the database is a good practice to maintain clear and organized code.

    ```
    src/
      ├──index.js
      ├──functions/
      │   ├──manageClients.js
      │   ├──manageHotels.js
      │   └──manageReservations.js
      └──database/
          ├──client.js
          ├──hotel.js
          └──reservation.js
    ```

    </Admonition>

6.  **Configure Environment Variables**

    On the Neon dashboard, go to `Connection string`, select `Node.js`, and click `.env`. Then, click `show password` and copy the database connection string. If you don't click `show password`, you'll copy a connection string without the password (which is masked).

    Create a `.env` file at the root of the project to store your database connection information from the Neon.

    Here's an example of the connection string you'll copy:

    ```bash shouldWrap
    DATABASE_URL='postgresql://neondb_owner:************@ep-quiet-leaf-a85k5wbg.eastus2.azure.neon.tech/neondb?sslmode=require&channel_binding=require'
    ```

7.  **Modify the `local.settings.json` file**

    The `local.settings.json` file is used by Azure Functions for **local executions**. Azure Functions does not directly read the `.env` file. Instead, it relies on `local.settings.json` to inject environment variable values during local execution. In production, you will define the same settings through `App Settings` in the Azure portal.

    Here's an example of the `local.settings.json` file :

    ```JSON
    {
      "IsEncrypted": false,
      "Values": {
        "AzureWebJobsStorage": "",
        "FUNCTIONS_WORKER_RUNTIME": "node",
        "DATABASE_URL": "postgresql://neondb_owner:************@ep-quiet-leaf-a85k5wbg.eastus2.azure.neon.tech/neondb?sslmode=require&channel_binding=require"
      }
    }
    ```

    Install the `dotenv` package by opening the terminal in your Azure Functions project. This package will allow you to load environment variables from the `.env` file:

    ```bash
    npm install dotenv
    ```

8.  **Manage Each Table**

    a. Create a separate file for each table in the `database/` folder.

    Here, you can use either the `neon` package or the `pg` package to connect to the database.

    **Example code for `client.js`**

    <CodeTabs labels={["Neon serverless driver", "node-postgres"]}>

    ```javascript
    import { neon } from '@neondatabase/serverless';
    import dotenv from 'dotenv';

    dotenv.config();

    const sql = neon(process.env.DATABASE_URL);

    const getAllClients = async () => {
      const rows = await sql`SELECT * FROM clients`;
      return rows;
    };

    const addClient = async (first_name, last_name, email, phone_number) => {
      const [newClient] = await sql`
        INSERT INTO clients (first_name, last_name, email, phone_number)
        VALUES (${first_name}, ${last_name}, ${email}, ${phone_number})
        RETURNING *`;
      return newClient;
    };

    export { getAllClients, addClient };
    ```

    ```javascript
    const { Client } = require('pg');
    require('dotenv').config();

    const client = new Client({
      connectionString: process.env.DATABASE_URL,
    });

    const connectDB = async () => {
      if (!client._connected) {
        await client.connect();
      }
    };

    const getAllClients = async () => {
      await connectDB();
      const result = await client.query('SELECT * FROM clients');
      return result.rows;
    };

    const addClient = async (first_name, last_name, email, phone_number) => {
      await connectDB();
      const result = await client.query(
        `INSERT INTO clients (first_name, last_name, email, phone_number)
            VALUES ($1, $2, $3, $4)
            RETURNING *`,
        [first_name, last_name, email, phone_number]
      );
      return result.rows[0];
    };

    module.exports = {
      getAllClients,
      addClient,
    };
    ```

    </CodeTabs>

    **Example code for `hotel.js`**

    <CodeTabs labels={["Neon serverless driver", "node-postgres"]}>

    ```javascript
    import { neon } from '@neondatabase/serverless';
    import dotenv from 'dotenv';

    dotenv.config();

    const sql = neon(process.env.DATABASE_URL);

    const getAllHotels = async () => {
      const rows = await sql`SELECT * FROM hotels`;
      return rows;
    };

    export { getAllHotels };
    ```

    ```javascript
    const { Client } = require('pg');
    require('dotenv').config();

    const client = new Client({
        connectionString: process.env.DATABASE_URL,
    });

    const connectDB = async () => {
        if (!client.\_connected) {
            await client.connect();
        }
    };

    const getAllHotels = async () => {
        await connectDB();
        const result = await client.query('SELECT \* FROM hotels');
        return result.rows;
    };

    module.exports = {
        getAllHotels,
        client,
    };
    ```

    </CodeTabs>

    **Example code for `reservation.js`**

    <CodeTabs labels={["Neon serverless driver", "node-postgres"]}>

    ```javascript
    import { neon } from '@neondatabase/serverless';
    import dotenv from 'dotenv';

    dotenv.config();

    const sql = neon(process.env.DATABASE_URL);

    const getAvailableReservations = async () => {
      const rows = await sql`
            SELECT * FROM reservations WHERE status = ${'available'}
        `;
      return rows;
    };

    export { getAvailableReservations };
    ```

    ```javascript
    const { Client } = require('pg');
    require('dotenv').config();

    const client = new Client({
      connectionString: process.env.DATABASE_URL,
    });

    const connectDB = async () => {
      if (!client._connected) {
        await client.connect();
      }
    };

    const getAvailableReservations = async () => {
      await connectDB();
      const result = await client.query('SELECT * FROM reservations WHERE status = $1', [
        'available',
      ]);
      return result.rows;
    };

    module.exports = {
      getAvailableReservations,
      client,
    };
    ```

    </CodeTabs>

    b. Modify the `functions/` folder by adding the function files:

    In the `functions/` folder, remove the default file, and then add three function management files (`manageClients.js`, `manageHotels.js`, and `manageReservations.js`).

    **Example for `manageClients.js`**

    ```javascript
    // src/functions/manageClients.js
    const { app } = require('@azure/functions');
    const { getAllClients, addClient } = require('../database/client');

    app.http('manageClients', {
      methods: ['GET', 'POST'],
      authLevel: 'anonymous',
      handler: async (request, context) => {
        context.log(`HTTP function processed request for url "${request.url}"`);

        if (request.method === 'GET') {
          try {
            const clients = await getAllClients();
            console.table(clients);
            return {
              body: clients,
            };
          } catch (error) {
            context.log('Error fetching clients:', error);
            return {
              status: 500,
              body: 'Error retrieving clients.',
            };
          }
        }

        if (request.method === 'POST') {
          try {
            const { first_name, last_name, email, phone_number } = await request.json();

            if (!first_name || !last_name || !email || !phone_number) {
              return {
                status: 400,
                body: 'Missing required fields: first_name, last_name, email, phone_number.',
              };
            }

            const newClient = await addClient(first_name, last_name, email, phone_number);
            console.table(newClient);
            return {
              status: 201,
              body: newClient,
            };
          } catch (error) {
            context.log('Error adding client:', error);
            return {
              status: 500,
              body: 'Error adding client.',
            };
          }
        }
      },
    });
    ```

    **Example for `manageHotels.js`**

    ```javascript
    // src/functions/manageHotels.js
    const { app } = require('@azure/functions');
    const { getAllHotels } = require('../database/hotel');

    app.http('manageHotels', {
      methods: ['GET'],
      authLevel: 'anonymous',
      handler: async (request, context) => {
        context.log(`HTTP function processed request for url "${request.url}"`);

        try {
          const hotels = await getAllHotels();
          return {
            body: hotels,
          };
        } catch (error) {
          context.log('Error fetching hotels:', error);
          return {
            status: 500,
            body: 'Error retrieving hotels.',
          };
        }
      },
    });
    ```

    **Example for `manageReservations.js`**

    ```javascript
    // src/functions/manageReservations.js
    const { app } = require('@azure/functions');
    const { getAvailableReservations } = require('../database/reservation');

    app.http('manageReservations', {
      methods: ['GET'],
      authLevel: 'anonymous',
      handler: async (request, context) => {
        context.log(`HTTP function processed request for url "${request.url}"`);

        try {
          const reservations = await getAvailableReservations();
          return {
            body: reservations,
          };
        } catch (error) {
          context.log('Error fetching reservations:', error);
          return {
            status: 500,
            body: 'Error retrieving available reservations.',
          };
        }
      },
    });
    ```

    Feel free to extend this structure to include features such as adding new clients, creating new reservations, or even updating and deleting data, each with **its own file** and **its own logic**.

## Step 3: Test the Function Locally

1. **Run the Function Locally**:
   - Open the integrated terminal in VS Code.
   - Run the following command `npm run start`, which will execute `func start` to start the project and launch the functions:

     ```bash
     npm run start
     ```

2. **Test with a Browser or Postman**:
   - Open a browser and navigate to `http://localhost:7071/api/manageClients` to test your function.
   - You can also use a tool like **Postman** to send **HTTP requests**.

## Step 4: Test and Deploy the Function to Azure

1. **Deploy Your Function**:
   - Open the command palette with `Ctrl+Shift+P` and type `Azure Functions: Deploy to Function App...`.
   - Follow the instructions to select your Azure subscription and choose or create a Function App, then complete the deployment process.

2. **Test the Function**:

   Use a tool like Postman to send an HTTP request to the Azure Function, for example:

   ```arduino
   https://your-azure-function-url?client_id=1234
   ```

   This will return the information of the client with the ID **1234**, if present in the database.

## Conclusion

We have demonstrated how combining Neon and Azure Functions enables the development of fast, scalable applications while reducing the complexity associated with managing infrastructure. With this combination, you can efficiently query your Postgres database without worrying about server maintenance. Moreover, Neon simplifies the scalability of your applications, making it an ideal choice for many modern projects.

## Additional Resources

- [Neon Documentation](/docs) - Comprehensive documentation for Neon's database services, including guides, tutorials, and API references.
- [Azure Functions Documentation](https://learn.microsoft.com/en-us/azure/azure-functions/)

<NeedHelp />


# Queue System using SKIP LOCKED in Neon Postgres

---
title: Queue System using SKIP LOCKED in Neon Postgres
subtitle: A step-by-step guide describing how to structure a tasks table for use as a task queue in Postgres
author: vkarpov15
enableTableOfContents: true
createdAt: '2025-01-10T17:48:36.612Z'
updatedOn: '2025-01-10T17:48:36.612Z'
---

The `SKIP LOCKED` clause allows concurrent transactions to skip rows currently locked by other transactions.
This behavior makes `SKIP LOCKED` ideal for implementing a non-blocking task queue in Postgres.

## Steps

- Create a tasks table
- Insert tasks into the queue
- Fetch tasks using SKIP LOCKED
- Mark tasks as completed
- Track stuck tasks
- Optimize with indexing

## Create a tasks table

First, you need a table to store your tasks.
Each task should have a unique identifier, a status, and a payload containing the task to run and any parameters.
Use the following SQL statement to create the tasks table:

```sql
CREATE TABLE tasks (
  id SERIAL PRIMARY KEY,
  payload JSONB NOT NULL,
  status TEXT DEFAULT 'pending',
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  started_at TIMESTAMP
);
```

This table defines a tasks queue with a `payload` column containing semi-structured task-specific data as a `JSONB`.

## Insert tasks into the queue

Next, populate the tasks table with sample data. Each task is represented as a JSON object in the `payload` column:

```sql
INSERT INTO tasks (payload) VALUES
  ('{"task": "email", "recipient": "user1@example.com"}'),
  ('{"task": "email", "recipient": "user2@example.com"}'),
  ('{"task": "report", "type": "sales"}');
```

You can then verify the rows in the task collection using the following.

```sql
SELECT * FROM tasks;
```

## Fetch tasks using `SKIP LOCKED`

When implementing a task queue, it is important to ensure that only one worker can run a given task, otherwise you may end up with tasks running multiple times.
`FOR UPDATE SKIP LOCKED` ensures that only one process retrieves and locks tasks, while others skip over already-locked rows.

Here’s a query to fetch and lock a single task.

```sql
WITH cte AS (
  SELECT id
  FROM tasks
  WHERE status = 'pending'
  ORDER BY created_at
  LIMIT 1
  FOR UPDATE SKIP LOCKED
)
UPDATE tasks
SET status = 'in_progress', started_at = CURRENT_TIMESTAMP
FROM cte
WHERE tasks.id = cte.id
RETURNING tasks.*;
```

This query:

1. Selects the oldest task with status `pending`.
2. Locks the task row to prevent other transactions from processing it.
3. Updates the task's status to `in_progress` and records the task's `started_at`.

## Mark tasks as completed

After processing a task, you should update the task's status to `completed` to indicate that it's finished:

```sql
UPDATE tasks
SET status = 'completed'
WHERE id = <task id here>;
```

## Track stuck tasks

In production, tasks may get stuck `in_progress` due to worker errors.
To identify tasks that may be hanging, you can query for tasks that have been in progress for more than 5 minutes as follows.

```sql
SELECT *
FROM tasks
WHERE status = 'in_progress'
  AND started_at < NOW() - INTERVAL '5 minutes';
```

## Optimize with indexing

As the number of tasks grows, queries on tasks can get slow.
Adding an index on the `status` and `created_at` columns can help ensure consistent performance for the `SKIP LOCKED` query:

```sql
CREATE INDEX idx_tasks_status_created_at
ON tasks (status, created_at);
```


# Rate Limiting in Postgres

---
title: Rate Limiting in Postgres
subtitle: A step-by-step guide describing how to implement rate limiting in Postgres using advisory locks and counters
author: vkarpov15
enableTableOfContents: true
createdAt: '2025-05-09T13:24:36.612Z'
updatedOn: '2025-05-09T13:24:36.612Z'
---

Rate limiting means limiting the number of requests that can happen in a given time window, like 5 requests per minute or 100 requests per hour.
While rate limiting is often implemented in the application layer, you can actually build effective rate limiting systems directly in Postgres.
You can rate limit a certain Postgres query using a combination of advisory locks and counters.

## Steps

- Use advisory locks to synchronize access
- Create a counter table for rate tracking
- Upsert into the `rate_limits` table
- Implement a basic rate limiter with SQL
- Wrap rate limiting in an SQL function

### Use advisory locks to synchronize access

Advisory locks in Postgres are application-level, user-defined locks that help coordinate access to shared resources without blocking unrelated operations.
The advantage of using advisory locks over transactions for rate limiting is that they allow you to synchronize access to a shared key (like a user's counter) without locking rows.

You can grab an exclusive lock on a given key like this:

```sql
SELECT pg_advisory_xact_lock(hashtext('user_123_rate_limit'));
```

This ensures that only one transaction at a time can modify the counter for `user_123`.

### Create a counter table for rate tracking

Start by creating a table to store rate limit counters.
You'll use this table to track how many requests each key (for example, user id or IP address) has made within a time window.

```sql
CREATE TABLE rate_limits (
  key TEXT PRIMARY KEY,
  count INTEGER NOT NULL DEFAULT 0,
  window_start TIMESTAMPTZ NOT NULL
);
```

Each row represents a rate limit bucket.
`window_start` marks the beginning of the current time window.

### Upsert into the rate_limits table

The key idea behind the `rate_limits` table is to ensure each request either starts a new rate limiting window or increments the count within the current one, if there is a current window.
The logic looks like the following.

1. Try to insert a new counter. If the key doesn't exist yet, insert it with `count = 1` and `window_start = now`.
2. If the key already exists, decide whether the current request is in the same window.
3. If the current time is outside the existing time window (`window_start + window_length <= now`), then reset the count to 1 and start a new window.
4. Otherwise, increment the count and keep the existing window.

Below is the SQL implementation of this logic.

```sql
-- Upsert the counter
INSERT INTO rate_limits (key, count, window_start)
VALUES (rate_limit_key, 1, now)
ON CONFLICT (key) DO UPDATE
SET count = CASE
              WHEN rate_limits.window_start + window_length <= now
                THEN 1
                ELSE rate_limits.count + 1
            END,
    window_start = CASE
                     WHEN rate_limits.window_start + window_length <= now
                       THEN now
                       ELSE rate_limits.window_start
                   END;
```

However, this logic is vulnerable to race conditions.
Two transactions might read the same `window_start` and `count` values at the same time and both think they're within the same rate limit window.
They both calculate `count + 1`, and both write the same new value, which overwrites one of the increments.
That's where advisory locks come in.

### Implement a basic rate limiter with SQL

Below is an implementation of a rate limiter that allows up to 5 requests per minute using advisory locks to avoid any race conditions.
Running the below code 6 times within 1 minute will result in an error: `ERROR: Rate limit exceeded for user_123`

```sql
DO $$
DECLARE
  rate_limit_key TEXT := 'user_123';
  now TIMESTAMPTZ := clock_timestamp();
  max_requests INTEGER := 5;
  window_length INTERVAL := INTERVAL '1 minute';
  current_count INTEGER;
BEGIN
  -- Lock access to the user's counter
  PERFORM pg_advisory_xact_lock(hashtext(rate_limit_key));

  -- Upsert the counter
  INSERT INTO rate_limits (key, count, window_start)
  VALUES (rate_limit_key, 1, now)
  ON CONFLICT (key) DO UPDATE
  SET count = CASE
                WHEN rate_limits.window_start + window_length <= now
                  THEN 1
                  ELSE rate_limits.count + 1
              END,
      window_start = CASE
                       WHEN rate_limits.window_start + window_length <= now
                         THEN now
                         ELSE rate_limits.window_start
                     END;

  -- Read current count
  SELECT count INTO current_count FROM rate_limits WHERE key = rate_limit_key;

  IF current_count > max_requests THEN
    RAISE EXCEPTION 'Rate limit exceeded for %', rate_limit_key;
  END IF;
END $$;
```

### Wrap rate limiting in an SQL function

The above SQL query hard-codes the rate limit key, `max_requests`, and other parameters, making it difficult to reuse.
To make this logic reusable, you can put the logic in an SQL function as follows.

```sql
CREATE OR REPLACE FUNCTION check_rate_limit(rate_key TEXT, max_requests INTEGER, window_seconds INTEGER)
RETURNS VOID AS $$
DECLARE
  now TIMESTAMPTZ := clock_timestamp();
  window_length INTERVAL := make_interval(secs => window_seconds);
  current_count INTEGER;
BEGIN
  PERFORM pg_advisory_xact_lock(hashtext(rate_key));

  INSERT INTO rate_limits (key, count, window_start)
  VALUES (rate_key, 1, now)
  ON CONFLICT (key) DO UPDATE
  SET count = CASE
                WHEN rate_limits.window_start + window_length <= now
                  THEN 1
                  ELSE rate_limits.count + 1
              END,
      window_start = CASE
                       WHEN rate_limits.window_start + window_length <= now
                         THEN now
                         ELSE rate_limits.window_start
                     END;

  SELECT count INTO current_count FROM rate_limits WHERE key = rate_key;

  IF current_count > max_requests THEN
    RAISE EXCEPTION 'Rate limit exceeded for %', rate_key;
  END IF;
END;
$$ LANGUAGE plpgsql;
```

Then you can call the function as follows.
The following query checks whether `user_123` has made more than 5 requests in the last minute.

```sql
SELECT check_rate_limit('user_123', 5, 60);
```

You can reuse this function in conjunction with other queries.
For example, if you want to make sure `user_123` can only read the `activity_feed` table 5 times per minute, you can use the following.

```sql
SELECT check_rate_limit('user_123', 5, 60);

SELECT * FROM activity_feed ORDER BY created_at DESC LIMIT 50;
```


# Building a Full-Stack Portfolio Website with a RAG Powered Chatbot

---
title: Building a Full-Stack Portfolio Website with a RAG Powered Chatbot
subtitle: Develop a modern React portfolio website featuring a chatbot powered by pgvector, Neon Postgres, FastAPI, and OpenAI.
author: sam-harri
enableTableOfContents: true
createdAt: '2024-10-17T00:00:00.000Z'
updatedOn: '2024-10-17T00:00:00.000Z'
---

In this guide, you will build a full-stack portfolio website using `React` for the frontend and `FastAPI` for the backend, featuring a Retrieval-Augmented Generation (RAG) chatbot that leverages `pgvector` on `Neon`'s serverless Postgres to store and retrieve embeddings created with `OpenAI`'s embedding model.

This project is perfect for showcasing technical skills through a portfolio that not only demonstrates front-end and back-end capabilities but also integrates AI to answer questions about your experience.

## Prerequisites

Before you start, ensure that you have the following tools and services ready:

- `pip`: This is required for installing and managing Python packages, including [uv](https://docs.astral.sh/uv/) for creating virtual environments. You can check if `pip` is installed by running the following command:
  ```bash
  pip --version
  ```
- Neon serverless Postgres : You will need a Neon account for provisioning and scaling your `PostgreSQL` database. If you don't have an account yet, [sign up here](https://console.neon.tech/signup).
- Node.js: Needed for developing the frontend using React, you can download it following the [official installation guide](https://nodejs.org/en/learn/getting-started/how-to-install-nodejs).
- OpenAI API key: You need access to the OpenAI API for generating embeddings, you can [sign up here](https://platform.openai.com/signup).

## Setting up the Backend

Follow these steps to set up your backend for the full-stack portfolio website:

1.  Create the project structure.

    Since this is a full-stack project, your backend and frontend will be in separate directories within a single parent folder. Begin by creating the parent folder and moving into it

    ```bash
    mkdir portfolio_project
    cd portfolio_project
    ```

2.  Create a `uv` Python virtual environment.

    If you don't already have uv installed, you can install it with:

    ```bash
    pip install uv
    ```

    Once `uv` is installed, create a new project:

    ```bash
    uv init portfolio_backend
    ```

    This will create a new project directory called `portfolio_backend`. Open this directory in your code editor of your choice.

3.  Set up the virtual environment.

        You will now create and activate a virtual environment in which your project's dependencies will be installed.

        <CodeTabs labels={["Linux/macOS", "Windows"]}>

            ```bash
            uv venv
            source .venv/bin/activate
            ```

            ```bash
            uv venv
            .venv\Scripts\activate
            ```

        </CodeTabs>

        You should see `(portfolio_backend)` in your terminal now, this means that your virtual environment is activated.

4.  Install dependencies.

    Next, add all the necessary dependencies for your project:

    ```bash
    uv add fastapi asyncpg uvicorn loguru python-dotenv openai pgvector
    ```

    where each package does the following:
    - `FastAPI` : A Web / API framework
    - `AsyncPG` : An asynchronous PostgreSQL client
    - `Uvicorn` : An ASGI server for our app
    - `Loguru` : A logging library
    - `Python-dotenv` : To load environment variables from a .env file
    - `openai` : The OpenAI API client for generating embeddings and chatbot responses
    - `pgvector` : A Python client for working with pgvector in PostgreSQL

5.  Create the project structure.

    Create the following directory structure to organize your project files:

    ```md
    portfolio_backend
    ├── src/
    │ ├── database/
    │ │ └── postgres.py
    │ ├── models/
    │ │ └── product_models.py
    │ ├── routes/
    │ │ └── product_routes.py
    │ └── main.py
    ├── .env  
    ├── .python-version
    ├── README.md  
    ├── pyproject.toml  
    └── uv.lock
    ```

## Setting up your Database

In this section, you will set up the `pgvector` extension using Neon's console, add the database's schema, and create the database connection pool and lifecycle management logic in FastAPI.

First, add pgvector to Postgres:

```sql
CREATE EXTENSION IF NOT EXISTS vector;
```

Next, add the schema to your database:

```sql
CREATE TABLE portfolio_embeddings (
    id SERIAL PRIMARY KEY,
    content TEXT NOT NULL,
    embedding VECTOR(1536) NOT NULL
);
```

This table will store the embeddings generated by OpenAI for the chatbot responses. The `content` column will store the text for which the embedding was generated, and the `embedding` column will store the 1536-dimensional vector. An embedding is a representation of text in a high-dimensional space that captures the meaning of the text, allowing you to compare and search for similar text.

With your schema in place, you're now ready to connect to your database in the FastAPI application. To do this you must create a `.env` file in the root of the project to hold environment-specific variables, such as the connection string to your Neon PostgreSQL database, and API keys.

```bash
DATABASE_URL=postgres://user:password@your-neon-hostname.neon.tech/neondb?sslmode=require&channel_binding=require
OPENAI_API_KEY=your-api-key
OPENAI_ORG_ID=your-org-id
OPENAI_PROJECT_ID=your-project-id
```

Make sure to replace the placeholders (user, password, your-neon-hostname, etc.) with your actual Neon database credentials, which are available in the console, and to fetch the OpenAI key from the OpenAI console.

In your project, the `database.py` file manages the connection to `PostgreSQL` using `asyncpg` and its connection pool, which is a mechanism for managing and reusing database connections efficiently. With this, you can use asynchronous queries, allowing the application to handle multiple requests concurrently.

```python
import os
import asyncpg
from loguru import logger
from typing import Optional
from pgvector.asyncpg import register_vector

conn_pool: Optional[asyncpg.Pool] = None


async def init_postgres() -> None:
    """
    Initialize the PostgreSQL connection pool
    """
    global conn_pool
    try:
        logger.info("Initializing PostgreSQL connection pool...")

        async def initalize_vector(conn):
            await register_vector(conn)

        conn_pool = await asyncpg.create_pool(
            dsn=os.getenv("DATABASE_URL"), init=initalize_vector
        )
        logger.info("PostgreSQL connection pool created successfully.")

    except Exception as e:
        logger.error(f"Error initializing PostgreSQL connection pool: {e}")
        raise


async def get_postgres() -> asyncpg.Pool:
    """
    Get a reference to the PostgreSQL connection pool.

    Returns
    -------
    asyncpg.Pool
        The connection pool object to the PostgreSQL database.
    """
    global conn_pool
    if conn_pool is None:
        logger.error("Connection pool is not initialized.")
        raise ConnectionError("PostgreSQL connection pool is not initialized.")
    try:
        return conn_pool
    except Exception as e:
        logger.error(f"Failed to return PostgreSQL connection pool: {e}")
        raise


async def close_postgres() -> None:
    """
    Close the PostgreSQL connection pool.
    """
    global conn_pool
    if conn_pool is not None:
        try:
            logger.info("Closing PostgreSQL connection pool...")
            await conn_pool.close()
            logger.info("PostgreSQL connection pool closed successfully.")
        except Exception as e:
            logger.error(f"Error closing PostgreSQL connection pool: {e}")
            raise
    else:
        logger.warning("PostgreSQL connection pool was not initialized.")

```

`init_postgres` is responsible for opening the connection pool to the `PostgreSQL` database and `close_postgres` is responsible for gracefully closing all connections in the pool when the `FastAPI` app shuts down to properly manage the lifecycle of the database.

Throughout your API you will also need access to the pool to get connection instances and run queries. `get_postgres` returns the active connection pool. If the pool is not initialized, an error is raised.

## Defining the Pydantic Models

Now, you will create the models that represent the data structures used in your API. These models will be used to validate the request and response data in your API endpoints. The API will use these models to serialize and deserialize data between the client and the server. For your API, you will need to complete a chat request and add embeddings to the database.

```python
from pydantic import BaseModel
from typing import Optional


class PortfolioEntryCreate(BaseModel):
    content: str


class PortfolioEntryResponse(BaseModel):
    id: int
    content: str
    embedding: Optional[list[float]]


class QueryRequest(BaseModel):
    query: str


class QueryResponse(BaseModel):
    content: str
    similarity: float
```

Each of the models represent the following:

- `PortfolioEntryCreate`: Represents the input for creating a new embedding
- `PortfolioEntryResponse`: Represents the output of a created embedding
- `QueryRequest`: Represents a question to ask the chatbot
- `QueryResponse`: Represents the response from the chatbot

## Creating the API Endpoints

In this section, you will create the API routes for adding new portfolio entries and chatting with the chatbot. The `add-portfolio-entry` route will add a new portfolio entry to the database and store its embedding. The `chat` route will use the stored embeddings to generate a response to a user query.

A typical RAG chatbot workflow involves first creating an embedding for the user query, then finding the most similar embeddings in the database, and finally using the context of these embeddings to generate a response. Here, embeddings are generated using OpenAI's text-embedding-3-small model, and the similarity between embeddings is calculated using the `<=>` operator in PostgreSQL, which represents the cosine similarity between two vectors. The chatbot response is then generated using OpenAI's GPT-4o-mini model.

```python
from fastapi import APIRouter, HTTPException, Depends
from models.models import PortfolioEntryCreate, PortfolioEntryResponse, QueryRequest
from database.postgres import get_postgres
import asyncpg
from typing import List
import os
from openai import OpenAI
import numpy as np

router = APIRouter()

client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    organization=os.getenv("OPENAI_ORG_ID"),
    project=os.getenv("OPENAI_PROJECT_ID"),
)


async def generate_embedding(content: str) -> List[float]:
    """
    Generate an embedding for the given content using OpenAI API.
    """
    try:
        content = content.replace("\n", " ")
        response = client.embeddings.create(
            input=[content],
            model="text-embedding-3-small",
        )
        return response.data[0].embedding
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error generating embedding: {e}")


@router.post("/add-entry/", response_model=PortfolioEntryResponse)
async def add_portfolio_entry(
    entry: PortfolioEntryCreate, pool: asyncpg.Pool = Depends(get_postgres)
):
    """
    Add a new portfolio entry and store its embedding in PostgreSQL.
    """
    try:
        embedding = await generate_embedding(entry.content)

        embedding_np = np.array(embedding)

        async with pool.acquire() as conn:
            row = await conn.fetchrow(
                """
                INSERT INTO portfolio_embeddings (content, embedding)
                VALUES ($1, $2)
                RETURNING id, content, embedding
                """,
                entry.content,
                embedding_np,
            )
            if row:
                return PortfolioEntryResponse(
                    id=row["id"], content=row["content"], embedding=row["embedding"]
                )
            else:
                raise HTTPException(
                    status_code=500, detail="Failed to insert entry into the database."
                )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to add entry: {e}")


@router.post("/chat/")
async def chat(query: QueryRequest, pool: asyncpg.Pool = Depends(get_postgres)):
    """
    Chat with the portfolio chatbot by retrieving relevant information from stored embeddings
    and using it as context to generate a response.
    """
    try:
        query_embedding = await generate_embedding(query.query)

        query_embedding_np = np.array(query_embedding)

        async with pool.acquire() as conn:
            rows = await conn.fetch(
                """
                SELECT content, embedding <=> $1 AS similarity
                FROM portfolio_embeddings
                ORDER BY similarity
                LIMIT 5
                """,
                query_embedding_np,
            )

            context = "\n".join([row["content"] for row in rows])

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "You are YOUR NAME, and you are answering questions as yourself, using 'I' and 'my' in your responses. "
                        "You should respond to questions about your portfolio, skills, experience, and education. For example, you should answer questions about specific technologies you've worked with, such as Java, React, or other tools. "
                        "If you have relevant experience with a technology, describe it concisely. For example, if asked about Java, describe your experience using it. "
                        "You should also answer questions about your education, including your experience at school, and your work in relevant industries. "
                        "However, if a question is completely unrelated to your professional experience, such as questions about recipes, trivia, or non-technical personal matters, respond with: 'That question isn't relevant to my experience or skills.' "
                        "Focus on answering technical and career-related questions, but only reject questions that are clearly off-topic."
                        "If they ask you about a technology you havent used, you can say: 'I haven't worked with that technology yet, but I'm always eager to learn new things.'"
                        "Answer any personal questions that are related to technology, like 'What are our favorite languages?' or 'What technology/language/anything tech are you most excited about?'"
                    ),
                },
                {
                    "role": "user",
                    "content": f"Context: {context}\n\nQuestion: {query.query}",
                },
            ],
            max_tokens=200,
            stream=False,
        )

        return {"response": response.choices[0].message.content}

    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Failed to process chat request: {e}"
        )

```

In the `chat` route, the chatbot is sent the text obtained from RAG and the user questions, but is also sent a system message that sets the context for the chatbot. You can customize this message to provide additional context or instructions to the chatbot, and to guide the chatbot's responses to your liking.

## Running the Application

After setting up the database, models, and API routes, the next step is to run the `FastAPI` application.

The `main.py` file defines the `FastAPI` application, manages the database lifecycle, and includes the routes you created above.

```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
from database.postgres import init_postgres, close_postgres
from routes.routes import router
import dotenv
import uvicorn


@asynccontextmanager
async def lifespan(app: FastAPI):
    dotenv.load_dotenv()
    await init_postgres()
    yield
    await close_postgres()


app: FastAPI = FastAPI(lifespan=lifespan, title="FastAPI Portfolio RAG ChatBot API")
app.include_router(router)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

```

Since you will be connecting your application from the frontend, you will need to allow CORS (Cross Origin Resource Sharing). The `CORSMiddleware` is added such that the API can accept requests from any origin, including your React app.

To run the application, use uvicorn CLI with the following command:

```bash
uvicorn main:app --host 0.0.0.0 --port 8000
```

## Adding Embeddings to the Database

You can now add embeddings to the database by sending a POST request to the `/add-entry` endpoint with the content you want to store. To do this, you can use a tool like `curl`,`Postman`, `httpie`, or any other HTTP client. For example, using `httpie`:

```bash shouldWrap
http POST http://localhost:8000/add-entry/ content="YOUR CONTENT"
```

For the best results, you should add a variety of portfolio entries that cover different aspects of your experience and skills. This will help the chatbot generate more accurate responses to user queries.

## Testing the Chatbot

Using `httpie`, you can test the chatbot by sending a POST request to the `/chat` endpoint with a user query. For example:

```bash shouldWrap
http POST http://localhost:8000/chat/ query="What is your favorite programming language?"
```

The chatbot will respond with a relevant answer based on the embeddings stored in the database, something like:

```json shouldWrap
{
  "response": "My favorite programming language right now is Rust, with Python coming in a close second. I love the performance and safety features of Rust, and the simplicity and readability of Python."
}
```

## Setting up the Frontend

Now that the backend is set up and running, it's time to set up the frontend using React. The frontend will be responsible for interacting with the FastAPI backend and displaying your portfolio, as well as allowing users to ask questions to the RAG-powered chatbot.

1. Clone the frontend repository.

   First, go back to the parent directory (portfolio_project) you created at the beginning of this guide and clone the frontend repository:

   ```bash
   cd ..
   git clone https://github.com/sam-harri/portfolio_frontend
   ```

   Then, open up that new directory in your code editor.

2. Install the dependencies.

   Once you have cloned the frontend repository, install the necessary dependencies using `npm`:

   ```bash
   npm install
   ```

   This will install all the packages specified in the `package.json` file, which are required for the React app to run.

3. Update the frontend content.

   Now, you will update the content of your portfolio, such as your bio, projects, and skills, and experience to match your personal details. Each of the section in the portfolio is a separate component that you can modify to include your own information.
   These include:
   - Landing: The landing page of the portfolio, which includes your name, bio, and a profile picture.
   - Experience: A section that lists your work experience, including the company name, logo, your position, and a brief description of your role.
   - Skills: A section that lists your technical skills, such as programming languages, frameworks, and tools you are proficient in. Find the logos of your technologies at [Devicon](https://devicon.dev/)
   - Projects: A section that lists your projects, including the project name, description, and a link to the project's GitHub repository or live demo.

   The chatbot component is responsible for sending user queries to the backend and displaying the chatbot responses, and can be found in the `Chatbot.tsx` file.

## Running the Frontend

Once you've updated the content, you can start the frontend development server to preview your portfolio website.

To start the development server, run the following command:

```bash
npm run dev
```

This will start the React development server at http://localhost:3000. Open your browser and navigate to that address to view your portfolio website.

Now, with the React app running, take a look at how the website appears. Ensure that the design, content, and overall presentation are what you expect. You can interact with the chatbot (which will not yet be functional until the backend is also running) to check the layout and form submission.

To test the chatbot functionality, you will need to have the backend running as well. To do this, open another console window, navigate to the `portfolio_backend` directory, and run the FastAPI application using the `uvicorn` command as shown earlier.
Once the API is running, you can interact with the chatbot on the frontend to test the chatbot functionality, try out some prompts, tweak the responses, and see how the chatbot performs.

## Optional: Running the Full-Stack Project Using Docker and Docker Compose

In this optional section, you will containerize both the frontend and backend of your full-stack portfolio website using Docker. By using Docker, you can ensure that your application runs consistently across different environments without worrying about dependencies and setups. Docker Compose will allow you to orchestrate running both services (frontend and backend) with a single command.

The only prerequisite for this section is having Docker installed on your machine. If you don't have it installed, you can follow the [Docker installation guide](https://docs.docker.com/engine/install/).

1. Dockerize the backend.

   To Dockerize the backend, you will create a `Dockerfile` in the `portfolio_backend` directory. This file will define the steps to build the Docker image for your FastAPI application.

   ```Dockerfile
   FROM python:3.10-slim

   ENV PYTHONDONTWRITEBYTECODE=1
   ENV PYTHONUNBUFFERED=1

   WORKDIR /app

   COPY . /app

   RUN pip install uv

   RUN uv pip install -r pyproject.toml --system

   WORKDIR /app/src

   EXPOSE 8000

   CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
   ```

   This Dockerfile:
   - Copies your FastAPI app into the container
   - Installs all necessary Python dependencies
   - Exposes port 8000, which is where FastAPI will run
   - Runs the FastAPI app using uvicorn

2. Dockerize the frontend.

   Dockerizing the frontend is similar to the backend. Create a `Dockerfile` in the root of the `portfolio_frontend` directory:

   ```Dockerfile
   FROM node:18-alpine AS base

   WORKDIR /app

   COPY package.json yarn.lock* package-lock.json* pnpm-lock.yaml* ./

   RUN \
   if [ -f yarn.lock ]; then yarn --frozen-lockfile; \
   elif [ -f package-lock.json ]; then npm ci; \
   elif [ -f pnpm-lock.yaml ]; then corepack enable pnpm && pnpm i --frozen-lockfile; \
   else echo "Lockfile not found." && exit 1; \
   fi

   COPY . .

   RUN npm run build

   RUN npm install -g serve

   EXPOSE 3000

   CMD ["serve", "-s", "dist"]
   ```

   This Dockerfile:
   - Uses Node.js to install frontend dependencies
   - Builds the React application
   - Serves the static build using the serve package
   - Exposes port 3000 where the React app will be available

3. Set up Docker Compose.

   Docker Compose simplifies the process of running multiple containers together. You can define both the frontend and backend in a single configuration and run them together with a single command.

   Below is the `docker-compose.yml` file, placed at the root of the project, which sets up both the services:

   ```yaml
   services:
   api:
     build:
     context: portfolio_backend/
     dockerfile: Dockerfile
     ports:
       - '8000:8000'
     env_file:
       - portfolio_backend/.env
   nextjs-app:
     build:
       context: portfolio_frontend/
       dockerfile: Dockerfile
     ports:
       - '3000:3000'
     environment:
       - NODE_ENV=production
     depends_on:
       - api
   ```

4. Run the application with Docker Compose.

   Once your Dockerfiles and Docker Compose file are ready, you can bring up the entire stack using:

   ```bash
   docker-compose up --build
   ```

   This command will:
   - Build the Docker images for both the backend and frontend
   - Start both the FastAPI backend (on port 8000) and the React frontend (on port 3000)
   - Automatically manage the service dependencies (the frontend will wait until the backend is up before starting)

   Using Docker and Docker Compose to run your full-stack portfolio website simplifies the process of managing dependencies and ensures consistency across different environments. You can now run your entire application, both frontend and backend, in isolated containers with a single command. This setup is also beneficial if you plan to deploy your application to production in a cloud environment or if you want to share the project with others who can run it without manual installation steps.

## Conclusion

You have successfully built and deployed a full-stack portfolio website powered by React, FastAPI, pgvector, Neon Postgres, and OpenAI. By leveraging OpenAI embeddings and the RAG-powered chatbot, you added an AI-driven layer to your portfolio that can dynamically answer questions about your projects, skills, and experience.

The next steps in the project could include deploying the application to a cloud platform like AWS, Azure, or Google Cloud, adding more features to the chatbot, or customizing the frontend design to match your personal style. You can also extend the chatbot's capabilities by fine-tuning it on more data or using a different model for generating responses.


# Scale your Django application with Neon Postgres Read Replicas

---
title: Scale your Django application with Neon Postgres Read Replicas
subtitle: Learn how to scale Django applications with Neon Postgres Read Replicas
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2024-10-20T00:00:00.000Z'
updatedOn: '2024-10-20T00:00:00.000Z'
---

[Neon read replicas](/docs/introduction/read-replicas) are independent read-only compute instances that can significantly enhance database performance and scalability. By distributing read operations across these replicas, you can reduce latency and improve overall system responsiveness, especially for read-heavy applications. A standout feature of Neon is that adding a read replica doesn't require extra storage. This makes it a cost-effective way to scale your database, suitable for businesses of all sizes.

This guide explains how to integrate Neon read replicas into your Django application. You'll learn how to configure your Django database router to direct read operations to these replicas, optimizing your database performance and overall application speed.

## Prerequisites

Before you begin, make sure you have:

- A Neon account and project. If you don't have one, sign up for a Neon account and create a project by following the [Getting started guide](/docs/get-started/signing-up).
- Basic knowledge of [Django](https://docs.djangoproject.com/en) and Python.
- [Python](https://www.python.org/downloads/) installed on your local machine.

## Build the note-taking app

To demonstrate how to use Neon read replicas with Django, we'll build a simple note-taking application that uses a Neon database. We'll then update the application to use a read replica for read operations, improving the application's performance and scalability.

### Part 1: Build the initial note-taking app with a single database

#### Set up the project

Create a new Django project and app:

```bash
python -m venv venv
source venv/bin/activate # On Windows, use venv\Scripts\activate
pip install django psycopg2-binary
django-admin startproject django_notes
cd django_notes
python manage.py startapp notes
```

This creates a new virtual environment, installs Django, and sets up a new Django project called `django_notes`. We also create a new app called `notes`, which will contain the logic for managing notes. For the database driver, we use `psycopg2-binary` to connect to a PostgreSQL database.

#### Update settings

In `django_notes/settings.py`, add the following:

```python
INSTALLED_APPS = [
    # ... other apps
    'notes',
]

DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'your_neon_database_name',
        'USER': 'your_username',
        'PASSWORD': 'your_password',
        'HOST': 'your_neon_host',
        'PORT': '5432',
    }
}
```

The `INSTALLED_APPS` array is updated to include the `notes` app we just created. The `DATABASES` dictionary is also updated to use a PostgreSQL database on Neon.

#### Create the Note model

In `notes/models.py`, add:

```python
from django.db import models

class Note(models.Model):
    title = models.CharField(max_length=200)
    content = models.TextField()
    created_at = models.DateTimeField(auto_now_add=True)

    def __str__(self):
        return self.title
```

The `Note` model defines the structure for storing notes in the database. Each note will have a `title`, `content`, and a `created_at` field, which is automatically populated with the current timestamp when a note is created.

#### Create views

In `notes/views.py`, add:

```python
from django.shortcuts import render, redirect, get_object_or_404
from .models import Note
from django.views.decorators.http import require_http_methods

@require_http_methods(["GET", "POST"])
def create_note(request):
    if request.method == "POST":
        title = request.POST.get('title')
        content = request.POST.get('content')
        Note.objects.create(title=title, content=content)
        return redirect('list_notes')
    return render(request, 'notes/create_note.html')

def list_notes(request):
    notes = Note.objects.all().order_by('-created_at')
    return render(request, 'notes/list_notes.html', {'notes': notes})

@require_http_methods(["POST"])
def delete_note(request, note_id):
    note = get_object_or_404(Note, id=note_id)
    note.delete()
    return redirect('list_notes')
```

Three views are defined here:

- `create_note`: Handles both displaying the note creation form (GET request) and saving the new note (POST request).
- `list_notes`: Fetches and displays all the notes ordered by the creation time, showing the newest ones first.
- `delete_note`: Handles the deletion of a specific note based on its ID.

#### Set up URLs

In `django_notes/urls.py`:

```python
from django.urls import path
from notes import views

urlpatterns = [
    path('', views.list_notes, name='list_notes'),
    path('create/', views.create_note, name='create_note'),
    path('delete/<int:note_id>/', views.delete_note, name='delete_note'),
]
```

We define the URL patterns for the project. The default route displays the list of notes, `/create/` serves the note creation form, and `/delete/<note_id>/` handles the deletion of a specific note.

#### Create templates

Create `notes/templates/notes/base.html`:

```html
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>{% block title %}Django Notes{% endblock %}</title>
    <style>
      body {
        font-family: Arial, sans-serif;
        line-height: 1.6;
        margin: 0;
        padding: 20px;
      }
      .container {
        max-width: 800px;
        margin: 0 auto;
      }
      h1 {
        color: #333;
      }
      form {
        margin-bottom: 20px;
      }
      input[type='text'],
      textarea {
        width: 100%;
        padding: 10px;
        margin-bottom: 10px;
      }
      button {
        background-color: #4caf50;
        color: white;
        padding: 10px 15px;
        border: none;
        cursor: pointer;
      }
      ul {
        list-style-type: none;
        padding: 0;
      }
      li {
        background: #f4f4f4;
        margin-bottom: 10px;
        padding: 10px;
      }
    </style>
  </head>
  <body>
    <div class="container">{% block content %}{% endblock %}</div>
  </body>
</html>
```

This is the base HTML template, which provides the layout and structure for the pages. Other templates will extend this layout, using the `block` tags to insert page-specific content.

Create `notes/templates/notes/create_note.html`:

```html
{% extends 'notes/base.html' %} {% block title %}Create Note{% endblock %} {% block content %}
<h1>Create a New Note</h1>
<form method="post">
  {% csrf_token %}
  <input type="text" name="title" placeholder="Title" required />
  <textarea name="content" placeholder="Content" required></textarea>
  <button type="submit">Create Note</button>
</form>
<a href="{% url 'list_notes' %}">Back to Notes</a>
{% endblock %}
```

This template displays the form to create a new note. It extends the base template and includes a form with fields for the title and content. Upon submission, the form sends a `POST` request to the server.

Create `notes/templates/notes/list_notes.html`:

```html
{% extends 'notes/base.html' %} {% block title %}Notes List{% endblock %} {% block content %}
<h1>Notes</h1>
<a href="{% url 'create_note' %}">Create New Note</a>
<ul>
  {% for note in notes %}
  <li>
    <h3>{{ note.title }}</h3>
    <p>{{ note.content }}</p>
    <small>Created at: {{ note.created_at }}</small>
    <div style="display: flex; justify-content: flex-end;">
      <form method="post" action="{% url 'delete_note' note.id %}" style="display: inline;">
        {% csrf_token %}
        <button
          type="submit"
          onclick="return confirm('Are you sure you want to delete this note?');"
        >
          Delete
        </button>
      </form>
    </div>
  </li>
  {% empty %}
  <li>No notes yet.</li>
  {% endfor %}
</ul>
{% endblock %}
```

This template displays a list of all notes. Each note shows its title, content, and creation time. A delete button is also provided next to each note, allowing for easy deletion.

#### Run migrations and start the server

```bash
python manage.py makemigrations
python manage.py migrate
python manage.py runserver
```

These commands generate and apply the database migrations for the Note model and start the development server, allowing you to access the app in your browser.

Visit `http://localhost:8000` to test the note-taking app.

![Django Notes App](/docs/guides/django_notes_app.png)

### Part 2: Use a read replica for read-only operations

#### Create a read replica on Neon

To create a read replica:

1. In the Neon Console, select **Branches**.
2. Select the branch where your database resides.
3. Click **Add Read Replica**.
4. On the **Add new compute** dialog, select **Read replica** as the **Compute type**.
5. Specify the **Compute size settings** options. You can configure a **Fixed Size** compute with a specific amount of vCPU and RAM (the default) or enable autoscaling by configuring a minimum and maximum compute size. You can also configure the **Suspend compute after inactivity** setting, which is the amount of idle time after which your read replica compute is automatically suspended. The default setting is 5 minutes.
   <Admonition type="note">
   The compute size configuration determines the processing power of your database. More vCPU and memory means more processing power but also higher compute costs. For information about compute costs, see [Billing metrics](/docs/introduction/billing).
   </Admonition>
6. When you finish making selections, click **Create**.

Your read replica compute is provisioned and appears on the **Computes** tab of the **Branches** page.

Navigate to the **Dashboard** page, select the branch where the read replica compute was provisioned, and set the compute option to **Replica** to obtain the read replica connection string:

![Read replica connection string](/docs/guides/read_replica_connection_string.png)

#### Set up database routing for read replicas

Create a new file `notes/db_router.py`:

```python
class PrimaryReplicaRouter:
    def db_for_read(self, model, **hints):
        return 'replica'

    def db_for_write(self, model, **hints):
        return 'default'

    def allow_relation(self, obj1, obj2, **hints):
        return True

    def allow_migrate(self, db, app_label, model_name=None, **hints):
        return True
```

This `PrimaryReplicaRouter` class defines the routing logic for database operations. The `db_for_read method` routes all read operations to the 'replica' database, while `db_for_write` directs write operations to the 'default' database. The `allow_relation` and `allow_migrate` methods are set to return `True`, allowing all relations and migrations across databases.

Update `django_notes/settings.py`:

```python
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'your_database_name',
        'USER': 'your_username',
        'PASSWORD': 'your_password',
        'HOST': 'your_primary_host',
        'PORT': '5432',
    },
    'replica': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'your_database_name',
        'USER': 'your_username',
        'PASSWORD': 'your_password',
        'HOST': 'your_read_replica_host',
        'PORT': '5432',
    }
}

DATABASE_ROUTERS = ['notes.db_router.PrimaryReplicaRouter']
```

In the `settings.py` file, we define two database connections: `'default'` for the primary database and `'replica'` for the read replica. Both use the PostgreSQL engine and share the same database name, but have different host addresses. The `DATABASE_ROUTERS` setting tells Django to use our custom `PrimaryReplicaRouter` for database routing decisions.

With these configurations in place, Django will automatically route read queries to the read replica and write queries to the primary database, effectively distributing the database load and potentially improving your application's performance.

## Conclusion

By leveraging Neon's read replicas in your Django application, you can significantly improve your application's performance and scalability. Django's database router makes it easy to set up and use read replicas without having to manually manage multiple database connections in your application code.

This setup allows you to distribute your read load across one or more read replicas while ensuring that all write operations are performed on the primary database. Monitor your application's performance and adjust the number of read replicas as needed to handle your specific load requirements.

You can find the source code for this application on GitHub:

<DetailIconCards>
<a href="https://github.com/dhanushreddy291/neon-read-replica-django" description="
Learn how to scale Django applications with Neon Postgres Read Replicas" icon="github">Use read replicas with Django</a>
</DetailIconCards>

<NeedHelp/>


# Scale your Next.js application with Drizzle ORM and Neon Postgres Read Replicas

---
title: Scale your Next.js application with Drizzle ORM and Neon Postgres Read Replicas
subtitle: Learn how to scale Next.js applications with Drizzle ORM's withReplicas() function and Neon Postgres Read Replicas
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2024-10-14T00:00:00.000Z'
updatedOn: '2024-10-14T00:00:00.000Z'
---

[Neon read replicas](/docs/introduction/read-replicas) are independent read-only compute instances that can significantly enhance database performance and scalability. By distributing read operations across these replicas, you can reduce latency and improve overall system responsiveness, especially for read-heavy applications.

A key advantage of Neon's architecture is that adding a read replica doesn't require additional storage, making it a highly efficient scaling solution. This cost-effective approach is ideal for businesses of all sizes that need to improve database performance without increasing storage costs.

This guide demonstrates how to leverage Neon read replicas to efficiently scale Next.js applications using Drizzle ORM. You'll learn how to configure your Drizzle database client to work with read replicas, enabling you to optimize your database operations and improve overall application performance.

## Prerequisites

- A Neon account and a Project. If you don't have one, you can sign up for a Neon account and create a project by following the [Getting Started guide](/docs/get-started/signing-up).
- Basic knowledge of [Next.js](https://nextjs.org/docs) and TypeScript
- [Node.js](https://nodejs.org/en/download/package-manager) and npm installed on your local machine

## Build the Polling app

To demonstrate how to use Neon read replicas with Drizzle in Next.js, we'll build a simple Polling application that uses a Neon database. We'll then update the application to use a read replica for read operations, improving the application's performance and scalability.

### Part 1: Build the initial Polling app with a single database

#### Set up the project

Create a new Next.js project with all the default options:

```bash
npx create-next-app@latest polling-app
cd polling-app
```

#### Install required packages

Install Drizzle ORM and the PostgreSQL driver:

```bash
npm install drizzle-orm pg dotenv
npm install -D @types/pg drizzle-kit
```

#### Create the database schema

Create a new file `db/schema.ts`:

```typescript
import { pgTable, serial, text } from 'drizzle-orm/pg-core';

export const pollTable = pgTable('votes', {
  id: serial('id').primaryKey(),
  option: text('option').notNull(),
  ipAddress: text('ip_address').notNull(),
});
```

This code defines the schema for our `votes` table. It has an auto-incrementing `id`, an `option` field for the vote choice, and an `ipAddress` field to track unique voters.

#### Set up the Drizzle client

Create a new file `db/drizzle.ts`:

```typescript
import { drizzle } from 'drizzle-orm/node-postgres';
import { Pool } from 'pg';
import 'dotenv/config';

const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
});

export const db = drizzle(pool);
```

This code sets up the Drizzle ORM client. It creates a connection pool using the `DATABASE_URL` from our environment variables and initializes Drizzle with this pool.

#### Update `.env`

Add the Neon database connection string:

```
DATABASE_URL=postgres://your-username:your-password@your-neon-host/your-database
```

This environment variable stores the connection string for your Neon database.

#### Set up migrations and create tables

Add the following scripts to your `package.json`:

```json
{
  "scripts": {
    ...
    "migration:generate": "drizzle-kit generate",
    "migration:migrate": "drizzle-kit migrate"
  }
}
```

These scripts allow you to generate and run database migrations using [Drizzle Kit](https://www.npmjs.com/package/drizzle-kit).

Create a new file `drizzle.config.ts` in the root of your project:

```typescript
import type { Config } from 'drizzle-kit';
import 'dotenv/config';

export default {
  schema: './db/schema.ts',
  out: './db/migrations',
  dialect: 'postgresql',
  dbCredentials: {
    url: process.env.DATABASE_URL!,
  },
} satisfies Config;
```

This configuration file tells Drizzle Kit where to find your schema, where to output migrations, and which database to connect to.

To generate your first migration based on your schema, run:

```bash
npm run migration:generate
```

To apply the migration and create the table in your Neon database, run:

```bash
npm run migration:migrate
```

#### Create the API routes

Create a new file `app/api/vote/route.ts`:

```typescript
import { db } from '@/db/drizzle';
import { pollTable } from '@/db/schema';
import { NextRequest, NextResponse } from 'next/server';
import { eq, sql } from 'drizzle-orm';

export async function POST(req: NextRequest) {
  const ipAddress = req.headers.get('x-forwarded-for');

  if (ipAddress == null) {
    return NextResponse.json({ message: 'IP address not found!' });
  }

  const { option } = await req.json();

  const existingVote = await db
    .select()
    .from(pollTable)
    .where(eq(pollTable.ipAddress, ipAddress))
    .execute();

  if (existingVote.length > 0) {
    return NextResponse.json({ message: 'You have already voted!' });
  }

  // Insert a new vote
  await db.insert(pollTable).values({
    option: option,
    ipAddress: ipAddress,
  });

  return NextResponse.json({ message: 'Vote submitted successfully!' });
}

export async function GET() {
  const options = await db
    .select({
      count: sql<number>`cast(count(*) as int)`,
      option: pollTable.option,
    })
    .from(pollTable)
    .groupBy(pollTable.option)
    .execute();
  return NextResponse.json(options);
}
```

This file defines two API routes:

- The POST route handles new votes, checking for existing votes from the same IP and inserting new votes.
- The GET route retrieves the current vote counts for each option.

#### Create the frontend

Add [shadcn-ui](https://ui.shadcn.com/) to the project for styling:

```bash
npx shadcn@latest init -d
npx shadcn@latest add button card
```

Update `app/page.tsx`:

```typescript
"use client"

import { useState, useEffect } from "react"
import { Button } from "@/components/ui/button"
import { Card, CardContent, CardHeader, CardTitle } from "@/components/ui/card"

export default function Component() {
  const [votes, setVotes] = useState({ Python: 0, JavaScript: 0 });
  const [loading, setLoading] = useState(false);

  useEffect(() => {
    // Fetch the current poll results when the component mounts
    const fetchPollResults = async () => {
      try {
        const response = await fetch("/api/vote");
        const data = await response.json();

        setVotes({
          Python: data.find((option: { option: string }) => option.option === "Python")?.count || 0,
          JavaScript: data.find((option: { option: string }) => option.option === "JavaScript")?.count || 0,
        });

      } catch (error) {
        console.error("Error fetching poll results:", error)
      }
    }

    fetchPollResults()
  }, [])

  const handleVote = async (option: "Python" | "JavaScript") => {
    setLoading(true)
    try {
      const response = await fetch("/api/vote", {
        method: "POST",
        headers: {
          "Content-Type": "application/JavaScripton",
        },
        body: JSON.stringify({ option }),
      })

      if (!response.ok) {
        throw new Error("Error submitting vote")
      }

      const data = await response.json()
      if (data.message === "Vote submitted successfully!") {
        setVotes((prevVotes) => ({
          ...prevVotes,
          [option]: prevVotes[option] + 1,
        }))
      } else {
        alert(data.message)
      }

    } catch (error) {
      console.error("Error submitting vote:", error)
    } finally {
      setLoading(false)
    }
  }

  const totalVotes = votes.Python + votes.JavaScript
  const PythonPercentage = totalVotes ? (votes.Python / totalVotes) * 100 : 0
  const JavaScriptPercentage = totalVotes ? (votes.JavaScript / totalVotes) * 100 : 0

  return (
    <div className="flex items-center justify-center min-h-screen">
      <Card className="w-full max-w-md mx-auto bg-gradient-to-br from-purple-50 to-indigo-50 dark:from-gray-900 dark:to-gray-800 shadow-lg">
        <CardHeader className="pb-2">
          <CardTitle className="text-3xl font-bold text-center text-purple-700 dark:text-purple-300">
            What is your favorite programming language?
          </CardTitle>
        </CardHeader>
        <CardContent className="space-y-6 p-6">
          <div className="space-y-4">
            <Button
              onClick={() => handleVote("Python")}
              className="w-full justify-between text-lg font-semibold py-6 bg-gradient-to-r from-purple-500 to-indigo-500 hover:from-purple-600 hover:to-indigo-600 transition-all duration-300"
              disabled={loading}
            >
              <span>Python</span>
              <span className="bg-white text-purple-700 px-3 py-1 rounded-full text-sm">
                {votes.Python}
              </span>
            </Button>
            <div className="h-3 bg-gray-200 rounded-full overflow-hidden dark:bg-gray-700">
              <div
                className="h-full bg-gradient-to-r from-purple-500 to-indigo-500 transition-all duration-300 ease-in-out"
                style={{ width: `${PythonPercentage}%` }}
              />
            </div>
          </div>
          <div className="space-y-4">
            <Button
              onClick={() => handleVote("JavaScript")}
              className="w-full justify-between text-lg font-semibold py-6 bg-gradient-to-r from-indigo-500 to-purple-500 hover:from-indigo-600 hover:to-purple-600 transition-all duration-300"
              disabled={loading}
            >
              <span>JavaScript</span>
              <span className="bg-white text-indigo-700 px-3 py-1 rounded-full text-sm">
                {votes.JavaScript}
              </span>
            </Button>
            <div className="h-3 bg-gray-200 rounded-full overflow-hidden dark:bg-gray-700">
              <div
                className="h-full bg-gradient-to-r from-indigo-500 to-purple-500 transition-all duration-300 ease-in-out"
                style={{ width: `${JavaScriptPercentage}%` }}
              />
            </div>
          </div>
          <p className="text-center text-sm font-medium text-gray-600 dark:text-gray-300 mt-4">
            Total votes: {totalVotes}
          </p>
        </CardContent>
      </Card>
    </div>
  )
}
```

This component creates a user interface for the polling app. It includes:

- State management for votes and loading status
- An effect hook to fetch initial poll results
- A function to handle new votes
- A UI with buttons for voting and progress bars to show results

#### Run the application

```bash
npm run dev
```

![Polling app](/docs/guides/drizzle_polling_demo_app.png)

Visit [`http://localhost:3000`](http://localhost:3000) to test the polling app.

### Part 2: Use a read replica for read-only operations

#### Create a read replica on Neon

To create a read replica:

1. In the Neon Console, select **Branches**.
2. Select the branch where your database resides.
3. Click **Add Read Replica**.
4. On the **Add new compute** dialog, select **Read replica** as the **Compute type**.
5. Specify the **Compute size settings** options. You can configure a **Fixed Size** compute with a specific amount of vCPU and RAM (the default) or enable autoscaling by configuring a minimum and maximum compute size. You can also configure the **Suspend compute after inactivity** setting, which is the amount of idle time after which your read replica compute is automatically suspended. The default setting is 5 minutes.
   <Admonition type="note">
   The compute size configuration determines the processing power of your database. More vCPU and memory means more processing power but also higher compute costs. For information about compute costs, see [Billing metrics](/docs/introduction/billing).
   </Admonition>
6. When you finish making selections, click **Create**.

Your read replica compute is provisioned and appears on the **Computes** tab of the **Branches** page.

Navigate to the **Dashboard** page, select the branch where the read replica compute was provisioned, and set the compute option to **Replica** to obtain the read replica connection string:

![Read replica connection string](/docs/guides/read_replica_connection_string.png)

#### Update the Drizzle client

Modify `db/drizzle.ts`:

```typescript
import { drizzle } from 'drizzle-orm/node-postgres';
import { withReplicas } from 'drizzle-orm/pg-core';
import { Pool } from 'pg';
import 'dotenv/config';

const primaryDb = drizzle(
  new Pool({
    connectionString: process.env.DATABASE_URL!,
  })
);
const read = drizzle(
  new Pool({
    connectionString: process.env.READ_REPLICA_URL!,
  })
);

export const db = withReplicas(primaryDb, [read]);
```

This setup uses Drizzle's [`withReplicas`](https://orm.drizzle.team/docs/read-replicas) function to create a single database client that can handle both primary and read replica connections. It automatically routes read queries to the read replica and write queries to the primary database.

#### Update `.env`

Add the read replica connection string:

```
DATABASE_URL=postgres://your-username:your-password@your-neon-primary-host/your-database
READ_REPLICA_URL=postgres://your-username:your-password@your-neon-read-replica-host/your-database
```

These environment variables store the connection strings for both your primary database and the read replica.

<Admonition type="note">
   You can also pass an array of read replica connection strings if you want to use multiple read replicas. Neon supports adding multiple read replicas to a database branch.

```javascript
const primaryDb = drizzle(
  new Pool({
    connectionString: process.env.DATABASE_URL,
  })
);
const read1 = drizzle(
  new Pool({
    connectionString: process.env.READ_REPLICA_URL_1,
  })
);
const read2 = drizzle(
  new Pool({
    connectionString: process.env.READ_REPLICA_URL_2,
  })
);
const db = withReplicas(primaryDb, [read1, read2]);
```

  </Admonition>

If you want to read from the primary compute and bypass read replicas, you can use the `$primary()` key:

```javascript
const posts = await db.$primary().post.findMany();
```

You can find the source code for the application described in this guide on GitHub.

<DetailIconCards>
<a href="https://github.com/dhanushreddy291/neon-read-replica-drizzle" description="Use read replicas with Drizzle and Next.js" icon="github">Use read replicas with Drizzle</a>
</DetailIconCards>

## Conclusion

By leveraging Neon's read replicas with Drizzle in your Next.js application, you can significantly improve your application's performance and scalability. Drizzle makes it easy to set up and use read replicas without having to manually manage multiple database connections in your application code.

This setup allows you to distribute your read load across one or more read replicas while ensuring that all write operations are performed on the primary database. Monitor your application's performance and adjust the number of read replicas as needed to handle your specific load requirements.

<NeedHelp/>


# Scale your .NET application with Entity Framework and Neon Postgres Read Replicas

---
title: Scale your .NET application with Entity Framework and Neon Postgres Read Replicas
subtitle: Learn how to scale .NET applications with Entity Framework's DbContext and Neon Postgres Read Replicas
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2024-10-13T00:00:00.000Z'
updatedOn: '2024-10-13T00:00:00.000Z'
---

[Neon read replicas](/docs/introduction/read-replicas) are independent read-only compute instances that perform read operations on the same data as your primary read-write compute. A key advantage of Neon's architecture is that adding a read replica to a Neon project doesn't require additional storage, making it an efficient scaling solution.

This guide demonstrates how to leverage Neon read replicas to efficiently scale .NET applications using Entity Framework Core. You'll learn how to configure your DbContext to work with read replicas, enabling you to optimize your database operations and improve overall application performance.

## Prerequisites

- A Neon account and a Project. If you don't have one, you can sign up for a Neon account and create a project by following the [Getting Started guide](/docs/get-started/signing-up).
- Basic knowledge of .NET Core
- Dotnet SDK installed on your local machine. You can download it from the [official .NET website](https://dotnet.microsoft.com/download).
- Dotnet Entity Framework Core CLI tools installed. You can install them by running the following command:

  ```bash
  dotnet tool install --global dotnet-ef
  ```

## Build the Todo app

To demonstrate how to use Neon read replicas with Entity Framework Core, we'll build a simple Todo application that uses a Neon database. We'll then update the application to use a read replica for read operations, improving the application's performance and scalability. This is just a simple example to demonstrate the concept, and you can apply the same principles to more complex applications.

### Part 1: Build the initial Todo app with a single database

#### Set up the project

Create a new .NET Core Web API project using the following commands:

```bash
dotnet new webapi -n TodoApi
cd TodoApi
```

#### Delete the WeatherForecast files

Delete the files `WeatherForecast.cs` and `Controllers/WeatherForecastController.cs` as we won't be using them:

```bash
rm WeatherForecast.cs Controllers/WeatherForecastController.cs
```

#### Install required packages

Install Entity Framework Core Design and Npgsql packages:

<Admonition type="tip" title="Best Practice">
Ensure you install package versions that match your .NET version. You can verify your .NET version at any time by running `dotnet --version`.
</Admonition>

```bash
dotnet add package Microsoft.EntityFrameworkCore.Design --version 6.0.4
dotnet add package Npgsql.EntityFrameworkCore.PostgreSQL --version 6.0.4
```

#### Create the Todo model

Create a new file `Models/Todo.cs`:

```csharp
namespace TodoApi.Models
{
    public class Todo
    {
        public int Id { get; set; }
        public string? Name { get; set; }
        public bool IsComplete { get; set; }
    }
}
```

#### Create the database context

Create a new file `Data/TodoDbContext.cs`:

```csharp
using Microsoft.EntityFrameworkCore;
using TodoApi.Models;

namespace TodoApi.Data
{
    public class TodoDbContext : DbContext
    {
        public TodoDbContext(DbContextOptions<TodoDbContext> options) : base(options) { }
        public DbSet<Todo> Todos => Set<Todo>();
    }
}
```

#### Update `appsettings.json` / `appsettings.Development.json`:

Add the connection string:

```json
{
  "ConnectionStrings": {
    "TodoDbConnection": "Host=your-neon-host;Database=your-db;Username=your-username;Password=your-password"
  }
}
```

#### Create the TodoController

Create a new file `Controllers/TodoController.cs`:

```csharp
using Microsoft.AspNetCore.Mvc;
using Microsoft.EntityFrameworkCore;
using TodoApi.Data;
using TodoApi.Models;

namespace TodoApi.Controllers
{
    [ApiController]
    [Route("api/[controller]")]
    public class TodoController : ControllerBase
    {
        private readonly TodoDbContext _context;

        public TodoController(TodoDbContext context)
        {
            _context = context;
        }

        [HttpGet]
        public async Task<ActionResult<IEnumerable<Todo>>> GetTodos()
        {
            return await _context.Todos.ToListAsync();
        }

        [HttpGet("{id}")]
        public async Task<ActionResult<Todo>> GetTodo(int id)
        {
            var todo = await _context.Todos.FindAsync(id);
            if (todo == null)
            {
                return NotFound();
            }
            return todo;
        }

        [HttpPost]
        public async Task<ActionResult<Todo>> PostTodo(Todo todo)
        {
            _context.Todos.Add(todo);
            await _context.SaveChangesAsync();
            return CreatedAtAction(nameof(GetTodo), new { id = todo.Id }, todo);
        }

        [HttpPut("{id}")]
        public async Task<IActionResult> PutTodo(int id, Todo todo)
        {
            if (id != todo.Id)
            {
                return BadRequest();
            }
            _context.Entry(todo).State = EntityState.Modified;
            await _context.SaveChangesAsync();
            return NoContent();
        }

        [HttpDelete("{id}")]
        public async Task<IActionResult> DeleteTodo(int id)
        {
            var todo = await _context.Todos.FindAsync(id);
            if (todo == null)
            {
                return NotFound();
            }
            _context.Todos.Remove(todo);
            await _context.SaveChangesAsync();
            return NoContent();
        }
    }
}
```

This controller defines CRUD operations (Create, Read, Update, Delete) for Todo items using HTTP requests. It uses `TodoDbContext` to interact with the database.

#### Update `Program.cs` with the following content:

```csharp
using Microsoft.EntityFrameworkCore;
using TodoApi.Data;

var builder = WebApplication.CreateBuilder(args);

builder.Services.AddControllers();

builder.Services.AddDbContext<TodoDbContext>(opt =>
    opt.UseNpgsql(builder.Configuration.GetConnectionString("TodoDbConnection")));

builder.Services.AddEndpointsApiExplorer();
builder.Services.AddSwaggerGen();

var app = builder.Build();

if (app.Environment.IsDevelopment())
{
    app.UseSwagger();
    app.UseSwaggerUI();
}

app.UseAuthorization();
app.MapControllers();

if (app.Environment.IsDevelopment())
{
    app.Run("http://localhost:5001");
}
else
{
    app.UseHttpsRedirection();
    app.Run();
}
```

This code configures the application to use Entity Framework Core with a PostgreSQL database. It registers `TodoDbContext` with the application's services and sets up the database connection using the connection string from `appsettings.json` / `appsettings.Development.json`.

#### Create Migrations

Run the following commands to create and apply the initial migration:

```bash
dotnet ef migrations add InitialCreate
dotnet ef database update
```

#### Run the application

Start the application:

```bash
dotnet run
```

Visit the Swagger UI at [`http://localhost:5001/swagger`](http://localhost:5001/swagger) to test the API.

![Swagger UI](/docs/guides/dotnet_ef_todo_swagger_demo.png)

### Part 2: Use a read replica for read-only operations

#### Create a read replica on Neon

To create a read replica:

1. In the Neon Console, select **Branches**.
2. Select the branch where your database resides.
3. Click **Add Read Replica**.
4. On the **Add new compute** dialog, select **Read replica** as the **Compute type**.
5. Specify the **Compute size settings** options. You can configure a **Fixed Size** compute with a specific amount of vCPU and RAM (the default) or enable autoscaling by configuring a minimum and maximum compute size. You can also configure the **Suspend compute after inactivity** setting, which is the amount of idle time after which your read replica compute is automatically suspended. The default setting is 5 minutes.
   <Admonition type="note">
   The compute size configuration determines the processing power of your database. More vCPU and memory means more processing power but also higher compute costs. For information about compute costs, see [Billing metrics](/docs/introduction/billing).
   </Admonition>
6. When you finish making selections, click **Create**.

Your read replica compute is provisioned and appears on the **Computes** tab of the **Branches** page.

Navigate to the **Dashboard** page, select the branch where the read replica compute was provisioned, and set the compute option to **Replica** to obtain the read replica connection string:

![Read replica connection string](/docs/guides/read_replica_connection_string.png)

#### Update the TodoDbContext

Modify `Data/TodoDbContext.cs` to include separate read and write contexts:

```csharp
using Microsoft.EntityFrameworkCore;
using TodoApi.Models;

namespace TodoApi.Data
{
    public class TodoDbContext : DbContext
    {
        public TodoDbContext(DbContextOptions<TodoDbContext> options) : base(options) { }
        public DbSet<Todo> Todos => Set<Todo>();
    }

    public class TodoDbReadContext : DbContext
    {
        public TodoDbReadContext(DbContextOptions<TodoDbReadContext> options) : base(options) { }
        public DbSet<Todo> Todos => Set<Todo>();
    }
}
```

#### Update Program.cs

Modify `Program.cs` to include both read and write contexts:

```csharp
using Microsoft.EntityFrameworkCore;
using TodoApi.Data;

var builder = WebApplication.CreateBuilder(args);

builder.Services.AddControllers();

builder.Services.AddDbContext<TodoDbContext>(opt =>
    opt.UseNpgsql(builder.Configuration.GetConnectionString("TodoDbConnection")));
builder.Services.AddDbContext<TodoDbReadContext>(opt =>
    opt.UseNpgsql(builder.Configuration.GetConnectionString("TodoDbConnectionRead")));

builder.Services.AddEndpointsApiExplorer();
builder.Services.AddSwaggerGen();

var app = builder.Build();

if (app.Environment.IsDevelopment())
{
    app.UseSwagger();
    app.UseSwaggerUI();
}

app.UseAuthorization();
app.MapControllers();

if (app.Environment.IsDevelopment())
{
    app.Run("http://localhost:5001");
}
else
{
    app.UseHttpsRedirection();
    app.Run();
}
```

#### Update `appsettings.json` / `appsettings.Development.json`

Add the read replica connection string:

```json
{
  "ConnectionStrings": {
    "TodoDbConnection": "Host=your-neon-primary-host;Database=your-db;Username=your-username;Password=your-password",
    "TodoDbConnectionRead": "Host=your-neon-read-replica-host;Database=your-db;Username=your-username;Password=your-password"
  }
}
```

#### Update the TodoController

Modify `Controllers/TodoController.cs` to use separate read and write contexts:

```csharp
using Microsoft.AspNetCore.Mvc;
using Microsoft.EntityFrameworkCore;
using TodoApi.Data;
using TodoApi.Models;

namespace TodoApi.Controllers
{
    [ApiController]
    [Route("api/[controller]")]
    public class TodoController : ControllerBase
    {
        private readonly TodoDbContext _writeContext;
        private readonly TodoDbReadContext _readContext;

        public TodoController(TodoDbContext writeContext, TodoDbReadContext readContext)
        {
            _writeContext = writeContext;
            _readContext = readContext;
        }

        [HttpGet]
        public async Task<ActionResult<IEnumerable<Todo>>> GetTodos()
        {
            return await _readContext.Todos.ToListAsync();
        }

        [HttpGet("{id}")]
        public async Task<ActionResult<Todo>> GetTodo(int id)
        {
            var todo = await _readContext.Todos.FindAsync(id);
            if (todo == null)
            {
                return NotFound();
            }
            return todo;
        }

        [HttpPost]
        public async Task<ActionResult<Todo>> PostTodo(Todo todo)
        {
            _writeContext.Todos.Add(todo);
            await _writeContext.SaveChangesAsync();
            return CreatedAtAction(nameof(GetTodo), new { id = todo.Id }, todo);
        }

        [HttpPut("{id}")]
        public async Task<IActionResult> PutTodo(int id, Todo todo)
        {
            if (id != todo.Id)
            {
                return BadRequest();
            }
            _writeContext.Entry(todo).State = EntityState.Modified;
            await _writeContext.SaveChangesAsync();
            return NoContent();
        }

        [HttpDelete("{id}")]
        public async Task<IActionResult> DeleteTodo(int id)
        {
            var todo = await _writeContext.Todos.FindAsync(id);
            if (todo == null)
            {
                return NotFound();
            }
            _writeContext.Todos.Remove(todo);
            await _writeContext.SaveChangesAsync();
            return NoContent();
        }
    }
}
```

<Admonition type="tip" title="Did you know?">
You can use dotnet-ef migrations even with multiple db contexts. You can specify the context to use by passing the `--context` option to the `dotnet ef` command.
</Admonition>

The Todo API is now set up to use separate read and write contexts, leveraging Neon's read replica feature. Read operations (`GET` requests) will use the read replica, while write operations (`POST`, `PUT`, `DELETE`) will use the primary database.

You can find the source code for the application described in this guide on GitHub.

<DetailIconCards>
<a href="https://github.com/dhanushreddy291/neon-read-replica-entity-framework" description="Use read replicas with Entity Framework Core" icon="github">Read Replicas in .NET EF</a>
</DetailIconCards>

## Conclusion

This setup allows you to distribute your read load across one or more read replicas while ensuring that all write operations are performed on the primary database. Monitor your application's performance and adjust the number of read replicas as needed to handle your specific load requirements. With Neon, you can quickly scale out with as many read replicas as you need.

<NeedHelp/>


# Scale your Laravel application with Neon Postgres Read Replicas

---
title: Scale your Laravel application with Neon Postgres Read Replicas
subtitle: Learn how to scale Laravel applications with Neon Postgres Read Replicas
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2024-10-20T00:00:00.000Z'
updatedOn: '2024-10-20T00:00:00.000Z'
---

## Introduction

[Neon read replicas](/docs/introduction/read-replicas) are independent read-only compute instances that can significantly enhance database performance and scalability. By distributing read operations across these replicas, you can reduce latency and improve overall system responsiveness, especially for read-heavy applications.

A key advantage of Neon's architecture is that adding a read replica doesn't require additional storage, making it a highly efficient scaling solution. This cost-effective approach is ideal for businesses of all sizes that need to improve database performance without increasing storage costs.

This guide demonstrates how to leverage Neon read replicas to efficiently scale Laravel applications. You'll learn how to configure your Laravel database connections to work with read replicas, enabling you to optimize your database operations and improve overall application performance. We'll use a simple URL shortener application as an example.

## Prerequisites

- A Neon account and a Project. If you don't have one, you can sign up for a Neon account and create a project by following the [Getting Started guide](/docs/get-started/signing-up).
- Basic knowledge of [Laravel](https://laravel.com/docs) and PHP
- [Composer](https://getcomposer.org/) installed on your local machine
- [PHP](https://www.php.net/manual/en/install.php) installed on your local machine

## Build the URL Shortener app

To demonstrate how to use Neon read replicas with Laravel, we'll build a simple URL shortener application that uses a Neon database. We'll then update the application to use a read replica for read operations, improving the application's performance and scalability.

### Part 1: Build the initial URL Shortener app with a single database

#### Set up the project

Create a new Laravel project:

```bash
laravel new url-shortener

 ┌ Would you like to install a starter kit? ────────────────────┐
 │ No starter kit                                               │
 └──────────────────────────────────────────────────────────────┘

 ┌ Which testing framework do you prefer? ──────────────────────┐
 │ Pest                                                         │
 └──────────────────────────────────────────────────────────────┘

 ┌ Would you like to initialize a Git repository? ────────┐
 │ Yes                                                    │
 └────────────────────────────────────────────────────────┘

 ┌ Which database will your application use? ────────────┐
 │ PostgreSQL                                            │
 └───────────────────────────────────────────────────────┘

 ┌ Default database updated. Would you like to run the default database migrations? ┐
 │ No                                                                               │
 └─────────────────────────────────────────────────────────────────────────────────┘
 cd url-shortener
```

#### Configure the database connection

Update your `.env` file with your Neon database credentials:

```
DB_CONNECTION=pgsql
DB_HOST=your-neon-host
DB_PORT=5432
DB_DATABASE=your-database-name
DB_USERNAME=your-username
DB_PASSWORD=your-password
```

#### Create the database schema

Create a new migration for the `urls` table:

```bash
php artisan make:migration create_urls_table
```

Edit the migration file in `database/migrations`:

```php
<?php

use Illuminate\Database\Migrations\Migration;
use Illuminate\Database\Schema\Blueprint;
use Illuminate\Support\Facades\Schema;

class CreateUrlsTable extends Migration
{
    public function up()
    {
        Schema::create('urls', function (Blueprint $table) {
            $table->id();
            $table->string('original_url');
            $table->string('short_code')->unique();
            $table->timestamps();
        });
    }

    public function down()
    {
        Schema::dropIfExists('urls');
    }
}
```

Run the migration:

```bash
php artisan migrate
```

<Admonition type="important">
Neon supports both direct and pooled database connection strings, which can be copied from the **Connection Details** widget on your Neon Project Dashboard. A pooled connection string connects your application to the database via a PgBouncer connection pool, allowing for a higher number of concurrent connections. However, using a pooled connection string for migrations can be prone to errors. For this reason, we recommend using a direct (non-pooled) connection when performing migrations. For more information about direct and pooled connections, see [Connection pooling](/docs/connect/connection-pooling).
</Admonition>

#### Create the model

Create a new model for the URL:

```bash
php artisan make:model Url
```

Edit `app/Models/Url.php`:

```php
<?php

namespace App\Models;

use Illuminate\Database\Eloquent\Factories\HasFactory;
use Illuminate\Database\Eloquent\Model;

class Url extends Model
{
    use HasFactory;

    protected $fillable = ['original_url', 'short_code'];
}
```

#### Create the controller

Create a new controller for handling URL operations:

```bash
php artisan make:controller UrlController
```

Edit `app/Http/Controllers/UrlController.php`:

```php
<?php

namespace App\Http\Controllers;

use App\Models\Url;
use Illuminate\Http\Request;
use Illuminate\Support\Str;

class UrlController extends Controller
{
    public function shorten(Request $request)
    {
        $request->validate([ 'url' => 'required|url' ]);

        $url = Url::create([
            'original_url' => $request->url,
            'short_code' => Str::random(6),
        ]);

        return response()->json([ 'short_url' => url($url->short_code) ], 201);
    }

    public function redirect($shortCode)
    {
        $url = Url::where('short_code', $shortCode)->firstOrFail();
        return redirect($url->original_url);
    }
}
```

#### Set up the routes

Edit `routes/web.php`:

```php
<?php

use App\Http\Controllers\UrlController;
use Illuminate\Support\Facades\Route;

Route::get('/', function () { return view('home'); });
Route::post('/shorten', [UrlController::class, 'shorten']);
Route::get('/{shortCode}', [UrlController::class, 'redirect']);
```

#### Create a simple frontend

Create a new blade template `resources/views/home.blade.php`:

```html
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>URL Shortener</title>
    <script src="https://cdn.tailwindcss.com"></script>
  </head>
  <body class="bg-gray-100 flex min-h-screen items-center justify-center">
    <div class="w-96 rounded-lg bg-white p-8 shadow-md">
      <h1 class="mb-4 text-2xl font-bold">URL Shortener</h1>
      <form id="urlForm" class="mb-4">
        <input
          type="url"
          id="urlInput"
          placeholder="Enter URL to shorten"
          required
          class="mb-2 w-full rounded border p-2"
        />
        <button type="submit" class="bg-blue-500 hover:bg-blue-600 w-full rounded p-2 text-white">
          Shorten URL
        </button>
      </form>
      <div id="result" class="hidden">
        <p>
          Shortened URL:
          <a id="shortUrl" href="#" target="_blank" class="text-blue-500"></a>
        </p>
      </div>
    </div>

    <script>
      document.getElementById('urlForm').addEventListener('submit', async (e) => {
        e.preventDefault();
        const url = document.getElementById('urlInput').value;
        const response = await fetch('/shorten', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'X-CSRF-TOKEN': '{{ csrf_token() }}',
          },
          body: JSON.stringify({ url }),
        });
        const data = await response.json();
        document.getElementById('shortUrl').href = data.short_url;
        document.getElementById('shortUrl').textContent = data.short_url;
        document.getElementById('result').classList.remove('hidden');
      });
    </script>
  </body>
</html>
```

#### Run the application

Start the Laravel development server:

```bash
php artisan serve
```

Visit `http://localhost:8000` to test the URL shortener app.

![Laravel URL Shortener](/docs/guides/laravel_url_shortener.png)

### Part 2: Use a read replica for read-only operations

#### Create a read replica on Neon

To create a read replica:

1. In the Neon Console, select **Branches**.
2. Select the branch where your database resides.
3. Click **Add Read Replica**.
4. On the **Add new compute** dialog, select **Read replica** as the **Compute type**.
5. Specify the **Compute size settings** options. You can configure a **Fixed Size** compute with a specific amount of vCPU and RAM (the default) or enable autoscaling by configuring a minimum and maximum compute size. You can also configure the **Suspend compute after inactivity** setting, which is the amount of idle time after which your read replica compute is automatically suspended. The default setting is 5 minutes.
   <Admonition type="note">
   The compute size configuration determines the processing power of your database. More vCPU and memory means more processing power but also higher compute costs. For information about compute costs, see [Billing metrics](/docs/introduction/billing).
   </Admonition>
6. When you finish making selections, click **Create**.

Your read replica compute is provisioned and appears on the **Computes** tab of the **Branches** page.

Navigate to the **Dashboard** page, select the branch where the read replica compute was provisioned, and set the compute option to **Replica** to obtain the read replica connection string:

![Read replica connection string](/docs/guides/read_replica_connection_string.png)

#### Update the database configuration

Edit `config/database.php` to add the read replica configuration:

```php
'pgsql' => [
    'driver' => 'pgsql',
    'read' => [
        'host' => env('DB_READ_HOST'),
    ],
    'write' => [
        'host' => env('DB_WRITE_HOST'),
    ],
    'sticky'    => true,
    'port' => env('DB_PORT', '5432'),
    'database' => env('DB_DATABASE', 'laravel'),
    'username' => env('DB_USERNAME', 'root'),
    'password' => env('DB_PASSWORD', ''),
    'charset' => env('DB_CHARSET', 'utf8'),
    'prefix' => '',
    'prefix_indexes' => true,
    'search_path' => 'public',
    'sslmode' => 'prefer',
]
```

<Admonition type="info">
Now that you've completed the database migrations, you can leverage the pooled connection string for both read and write operations. This streamlines your database interactions. However, it's worth noting that for future migrations, it's best practice to use the direct connection string. This approach helps avoid potential complications that might arise during the migration process.
</Admonition>

Update your `.env` file with the read replica host:

```
DB_READ_HOST=your-neon-read-replica-host
DB_WRITE_HOST=your-neon-primary-host
```

## Automatic Query Routing with Eloquent

One of the great features of [Laravel's Eloquent ORM](https://laravel.com/docs/11.x/eloquent) is that it automatically routes queries to the appropriate database connection based on the type of query. This means that after configuring your read replica, you don't need to make any changes to your existing controller or model code.
Here's how Eloquent handles different types of queries:

- Read Operations: `SELECT` queries are automatically routed to the read replica.
- Write Operations: `INSERT`, `UPDATE`, and `DELETE` queries are sent to the primary (write) database.

This automatic routing happens transparently, allowing you to scale your application without modifying your application logic.

<Admonition type="tip">
  Laravel offers powerful flexibility in managing database connections. While automatic query routing is convenient, you can easily override it when needed. For instance, to explicitly use the write connection for querying the urls table, you can do the following:
  ```php
  <?php
  use Illuminate\Support\Facades\DB;
  ...
  
  $url = DB::connection('pgsql::write')->table('urls')->where('short_code', $shortCode)->first();
  ```
</Admonition>

You can find the source code for the application described in this guide on GitHub.
<DetailIconCards>
<a href="https://github.com/dhanushreddy291/neon-read-replica-laravel" description="
Learn how to scale Laravel applications with Neon Postgres Read Replicas" icon="github">Use read replicas with Laravel</a>
</DetailIconCards>

## Conclusion

By leveraging Neon's read replicas with Laravel, you can significantly improve your application's performance and scalability. Laravel's database configuration makes it easy to set up and use read replicas without having to manually manage multiple database connections in your application code.

This setup allows you to distribute your read load across one or more read replicas while ensuring that all write operations are performed on the primary database. Monitor your application's performance and adjust the number of read replicas as needed to handle your specific load requirements.

By implementing read replicas in your Laravel application, you're taking a significant step toward building a more scalable and performant system that can handle increased traffic and data loads.

<NeedHelp/>


# Building Real-Time Comments with a Serverless Postgres

---
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2025-01-07T00:00:00.000Z'
updatedOn: '2025-01-07T00:00:00.000Z'
title: Building Real-Time Comments with a Serverless Postgres
subtitle: A guide to building your own real-time comments in a Next.js application with Ably LiveSync and Postgres.
---

Can a serverless Postgres database really handle the demands of a real-time application? The answer lies in pairing it with the right publish-subscribe model. In this guide, you will learn how to combine the real-time capabilities of Ably LiveSync with the structured power of Neon Postgres to build a optimistic and scalable comment system in your Next.js application.

## Prerequisites

To follow this guide, you’ll need the following:

- [Node.js 18](https://nodejs.org/en) or later
- A [Neon](https://console.neon.tech/signup) account
- An [Ably](https://ably.com) account
- A [Vercel](https://vercel.com) account

## Create the Next.js application

Let’s get started by cloning the Next.js project with the following command:

```shell shouldWrap
git clone https://github.com/neondatabase-labs/ably-livesync-neon
```

Once that is done, move into the project directory and install the necessary dependencies with the following command:

```shell shouldWrap
cd ably-livesync-neon
npm install
```

The libraries installed include:

- `ws`: A WebSocket library for Node.js.
- `ably`: A real-time messaging and data synchronization library.
- `@neondatabase/serverless`: A serverless Postgres client designed for Neon.
- `@prisma/client`: Prisma’s auto-generated client for interacting with your database.
- `@prisma/adapter-neon`: A Prisma adapter for connecting with Neon serverless Postgres.
- `@ably-labs/models`: A library for working with data models and real-time updates in Ably.

The development-specific libraries include:

- `tsx`: A fast TypeScript runtime for development and rebuilding.
- `prisma`: A toolkit for Prisma schema management, migrations, and generating clients.

Once that's done, copy the `.env.example` to `.env` via the following command:

```shell shouldWrap
cp .env.example .env
```

## Provision a Serverless Postgres

To set up a serverless Postgres, go to the [Neon console](https://console.neon.tech/app/projects) and create a new project. Once your project is created, you will receive a connection string that you can use to connect to your Neon database. The connection string will look like this:

```bash shouldWrap
postgresql://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>?sslmode=require&channel_binding=require
```

Replace `<user>`, `<password>`, `<endpoint_hostname>`, `<port>`, and `<dbname>` with your specific details.

Use this connection string as an environment variable designated as `DATABASE_URL` in the `.env` file.

## Set up Ably LiveSync with Neon Postgres

Sign in into the [Ably Dashboard](https://ably.com/login), and click on `+ Create new app`.

![](/guides/images/real-time-comments/create.png)

Next, name the application and select `Data Sync` as the type of application you are building.

![](/guides/images/real-time-comments/name.png)

Next, go to the **Integrations** tab and click on `+ New integration rule`.

![](/guides/images/real-time-comments/rule.png)

Next, click the `Choose` button in the **Postgres (Alpha)** card.

![](/guides/images/real-time-comments/postgres.png)

Next, in the integration rule, enter the `DATABASE_URL` environment variable value obtained and proceed.

![](/guides/images/real-time-comments/values.png)

Finally, go to the **API Keys** tab and copy the API Key which has the capabilities as `channel metadata, history, presence, privileged headers, publish, push admin, push subscribe, statistics, subscribe`.

![](/guides/images/real-time-comments/api_keys.png)

Use this API Key as an environment variable designated as `NEXT_PUBLIC_ABLY_API_KEY` in the `.env` file.

## Set up Database Schema

In the file named `schema.tsx`, you would see the following code:

```tsx
// File: schema.tsx

import 'dotenv/config';
import { WebSocket } from 'ws';
import { neon, neonConfig } from '@neondatabase/serverless';

neonConfig.webSocketConstructor = WebSocket;
neonConfig.poolQueryViaFetch = true;

async function prepare() {
  if (!process.env.DATABASE_URL) throw new Error('DATABASE_URL environment variable not found.');
  const sql = neon(process.env.DATABASE_URL);
  await Promise.all([
    sql`CREATE TABLE IF NOT EXISTS nodes (id TEXT PRIMARY KEY, expiry TIMESTAMP WITHOUT TIME ZONE NOT NULL);`,
    sql`CREATE TABLE IF NOT EXISTS outbox (sequence_id  serial PRIMARY KEY, mutation_id  TEXT NOT NULL, channel TEXT NOT NULL, name TEXT NOT NULL, rejected boolean NOT NULL DEFAULT false, data JSONB, headers JSONB, locked_by TEXT, lock_expiry TIMESTAMP WITHOUT TIME ZONE, processed BOOLEAN NOT NULL DEFAULT false);`,
  ]);
  await sql`CREATE OR REPLACE FUNCTION public.outbox_notify() RETURNS trigger AS $$ BEGIN PERFORM pg_notify('ably_adbc'::text, ''::text); RETURN NULL; EXCEPTION WHEN others THEN RAISE WARNING 'unexpected error in %s: %%', SQLERRM; RETURN NULL; END; $$ LANGUAGE plpgsql;`;
  await sql`CREATE OR REPLACE TRIGGER public_outbox_trigger AFTER INSERT ON public.outbox FOR EACH STATEMENT EXECUTE PROCEDURE public.outbox_notify();`;
  console.log('Database schema set up succesfully.');
}

prepare();
```

The code above defines a function that connects to a Neon serverless Postgres database using a `DATABASE_URL` environment variable and sets up the necessary schema for the real-time application. It creates two tables, `nodes` and `outbox`, to store data and manage message processing, respectively. A trigger function, `outbox_notify`, is implemented to send notifications using `pg_notify` whenever new rows are inserted into the `outbox` table. This ensures the database is ready for real-time updates and WebSocket-based communication.

To run the schema against your Neon Postgres, execute the following command:

```
npm run db
```

If it runs succesfully, you should see `Database schema set up succesfully.` in the terminal.

## Set up Prisma for Neon Postgres

In the directory `lib/prisma`, you would see the following code in `index.ts` file:

```tsx
// File: lib/prisma/index.ts

import { neonConfig, Pool } from '@neondatabase/serverless';
import { PrismaNeon } from '@prisma/adapter-neon';
import { PrismaClient } from '@prisma/client';
import { WebSocket } from 'ws';

declare global {
  var prisma: PrismaClient | undefined;
}

const connectionString = `${process.env.DATABASE_URL}`;

neonConfig.webSocketConstructor = WebSocket;
neonConfig.poolQueryViaFetch = true;

const pool = new Pool({ connectionString });
const adapter = new PrismaNeon(pool);
const prisma = global.prisma || new PrismaClient({ adapter });

if (process.env.NODE_ENV === 'development') global.prisma = prisma;

export default prisma;
```

The code above sets up a Prisma client for Neon Postgres. It configures the Neon database connection using the `@neondatabase/serverless` library, with WebSocket and `fetch` support to execute queries. A global `prisma` instance is created using the `PrismaNeon` adapter, ensuring reuse in development to avoid multiple instances. Finally, the configured `prisma` client is exported for use throughout the application.

In the same directory, you would see the following code in the `api.ts` file:

```tsx
// File: lib/prisma/api.ts

import prisma from '@/lib/prisma';
import { Prisma, PrismaClient } from '@prisma/client';
import * as runtime from '@prisma/client/runtime/library';

export type Author = {
  id: number;
  image: string;
  username: string;
};

export type Comment = {
  id: number;
  postId: number;
  author: Author;
  content: string;
  createdAt: Date;
  optimistic?: boolean;
};

export type Post = {
  id: number;
  title: string;
  content: string;
  comments: Comment[];
};

export async function getPosts(): Promise<Post[]> {
  return await prisma.post.findMany({
    include: {
      comments: {
        include: {
          author: true,
        },
      },
    },
  });
}

export async function getPost(id: number): Promise<[Post, number]> {
  return await prisma.$transaction(async (tx) => {
    const post = await getPostTx(tx, id);
    type r = { nextval: number };
    const [{ nextval }] = await tx.$queryRaw<
      r[]
    >`SELECT nextval('outbox_sequence_id_seq')::integer`;
    return [post, nextval];
  });
}

async function getPostTx(tx: TxClient, id: number) {
  return await tx.post.findUniqueOrThrow({
    where: { id },
    include: {
      comments: {
        include: {
          author: true,
        },
        orderBy: {
          createdAt: 'asc',
        },
      },
    },
  });
}

export async function getRandomUser() {
  const count = await prisma.user.count();
  return await prisma.user.findFirstOrThrow({
    skip: Math.floor(Math.random() * count),
  });
}

export type TxClient = Omit<PrismaClient, runtime.ITXClientDenyList>;

export async function addComment(
  tx: TxClient,
  mutationId: string,
  postId: number,
  authorId: number,
  content: string
): Promise<Prisma.outboxCreateInput> {
  const comment = await tx.comment.create({
    data: { postId, authorId, content },
    include: { author: true },
  });
  return {
    mutation_id: mutationId,
    channel: `post:${comment.postId}`,
    name: 'addComment',
    data: comment,
    headers: {},
  };
}

export async function editComment(
  tx: TxClient,
  mutationId: string,
  id: number,
  content: string
): Promise<Prisma.outboxCreateInput> {
  await tx.comment.findUniqueOrThrow({ where: { id } });
  const comment = await tx.comment.update({
    where: { id },
    data: { content },
    include: { author: true },
  });
  return {
    mutation_id: mutationId,
    channel: `post:${comment.postId}`,
    name: 'editComment',
    data: comment,
    headers: {},
  };
}

export async function deleteComment(
  tx: TxClient,
  mutationId: string,
  id: number
): Promise<Prisma.outboxCreateInput> {
  const comment = await tx.comment.delete({
    where: { id },
  });
  return {
    mutation_id: mutationId,
    channel: `post:${comment.postId}`,
    name: 'deleteComment',
    data: comment,
    headers: {},
  };
}

export async function withOutboxWrite(
  op: (tx: TxClient, ...args: any[]) => Promise<Prisma.outboxCreateInput>,
  ...args: any[]
) {
  return await prisma.$transaction(async (tx) => {
    const { mutation_id, channel, name, data, headers } = await op(tx, ...args);
    await tx.outbox.create({
      data: { mutation_id, channel, name, data, headers },
    });
  });
}
```

The code above interacts with the Postgres database using Prisma to manage comments. It implements operations like fetching, adding, editing, and deleting comments, with an emphasis on ensuring these operations are recorded in the `outbox` table for the event-driven system to capturing changes and reflect them in rest of the web clients. Let's understand each function in the code above:

- `withOutboxWrite()`: **This higher-order function wraps any operation that modifies the database (such as adding, editing, or deleting a comment) and ensures that the change is also written to the outbox table.** It first performs the operation, retrieves the necessary outbox details, and then writes the entry to the outbox table within the same transaction.

- `getPosts()`: Fetches all posts from the database, along with their associated comments and the authors of those comments. The function returns a list of posts, each containing its comments and authors.

- `getPost(id: number): Promise<[Post, number]>`: Fetches a single post by its ID, along with the associated comments and authors. Additionally, it executes a raw SQL query within a transaction to get the next value from a PostgreSQL sequence (`outbox_sequence_id_seq`), returning this value alongside the post. This ensures that the operation has both the requested post and sequence number, which may be used in event-driven systems for ordering.

- `getPostTx(tx: TxClient, id: number)`: A helper function used by `getPost()` to retrieve a post within a transaction (`tx`). It ensures the post's comments are fetched in ascending order of their creation timestamp.

- `getRandomUser()`: Retrieves a random user from the database. The function first counts the total number of users and then selects one randomly based on the count.

- `TxClient`: This type represents a transaction client, which is essentially a modified version of the PrismaClient excluding certain methods that are restricted during transactions (`ITXClientDenyList`).

- `addComment()`: Adds a new comment to a post within a transaction. The function takes in several parameters, such as the transaction client (`tx`), mutation ID, post ID, author ID, and comment content. It returns an `outbox` entry that can be used in an event-driven system for tracking the mutation. The outbox entry includes details like the mutation ID, channel (based on the post), event name (`addComment`), and the newly created comment.

- `editComment()`: Edits an existing comment. It accepts the transaction client (`tx`), mutation ID, comment ID, and new content. After updating the comment, it returns an outbox entry similar to `addComment()`, but with the event name `editComment`.

- `deleteComment()`: Deletes a comment. It takes in the transaction client (`tx`), mutation ID, and the comment ID to be deleted. Like the other mutation functions, it returns an outbox entry, but with the event name `deleteComment`.

## Create a Real-Time Data Model Client with Ably

In the directory `lib/models`, you would see the following code in `modelsClient.ts` file:

```tsx
// File: lib/models/modelsClient.ts

import ModelsClient from '@ably-labs/models';
import { Realtime } from 'ably';

let client: ModelsClient;

export const modelsClient = () => {
  const ably = new Realtime({ key: process.env.NEXT_PUBLIC_ABLY_API_KEY });
  if (!client) client = new ModelsClient({ ably });
  return client;
};
```

In the code above, a function `modelsClient` is defined which initializes and returns a singleton instance of the `ModelsClient` from the `@ably-labs/models` library, using an Ably Realtime connection. It ensures that the client is only instantiated once, leveraging the Ably API key stored in environment variables to create the Realtime connection.

In the same directory, you would see the following code in the `mutations.ts` file:

```tsx
// File: lib/models/mutations.ts

import { ConfirmedEvent, OptimisticEvent } from '@ably-labs/models';
import cloneDeep from 'lodash/cloneDeep';
import type { Post as PostType } from '@/lib/prisma/api';
import type { Author as AuthorType } from '@/lib/prisma/api';
import { Comment } from '@/lib/prisma/api';

export async function addComment(
  mutationId: string,
  author: AuthorType,
  postId: number,
  content: string
) {
  const response = await fetch('/api/comments', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ mutationId, authorId: author.id, postId, content }),
  });
  if (!response.ok)
    throw new Error(
      `POST /api/comments: ${response.status} ${JSON.stringify(await response.json())}`
    );
  return response.json();
}

export async function editComment(mutationId: string, id: number, content: string) {
  const response = await fetch(`/api/comments/${id}`, {
    method: 'PUT',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ mutationId, content }),
  });
  if (!response.ok)
    throw new Error(
      `PUT /api/comments/:id: ${response.status} ${JSON.stringify(await response.json())}`
    );
  return response.json();
}

export async function deleteComment(mutationId: string, id: number) {
  const response = await fetch(`/api/comments/${id}`, {
    method: 'DELETE',
    headers: { 'x-mutation-id': mutationId },
  });
  if (!response.ok)
    throw new Error(
      `DELETE /api/comments/:id: ${response.status} ${JSON.stringify(await response.json())}`
    );
  return response.json();
}

export function merge(existingState: PostType, event: OptimisticEvent | ConfirmedEvent): PostType {
  // Optimistic and confirmed events use the same merge function logic.
  // The models function keeps track of the state before events are applied
  // to make sure the rollback of unconfirmed events works, we need to clone
  // the state here. Our state contains an array of objects so we don't use
  // the regular object spread operator.
  const state = cloneDeep(existingState);
  switch (event.name) {
    case 'addComment':
      const newComment = event.data! as Comment;
      state.comments.push(newComment);
      break;
    case 'editComment':
      const editComment = event.data! as Comment;
      const editIdx = state.comments.findIndex((c) => c.id === editComment.id);
      state.comments[editIdx] = editComment;
      break;
    case 'deleteComment':
      const { id } = event.data! as { id: number };
      const deleteIdx = state.comments.findIndex((c) => c.id === id);
      state.comments.splice(deleteIdx, 1);
      break;
    default:
      console.error('unknown event', event);
  }
  state.comments.sort((a, b) => new Date(a.createdAt).getTime() - new Date(b.createdAt).getTime());
  return state;
}
```

In the code above, three asynchronous functions to handle CRUD operations for comments are defined:

- **addComment**: Sends a POST request to add a new comment, including the author's details and content, identified by a `mutationId`.
- **editComment**: Sends a PUT request to update an existing comment's content by its `id`.
- **deleteComment**: Sends a DELETE request to remove a comment by its `id`.

Each function validates the server response and throws an error for unsuccessful requests. Additionally, the `merge` function handles state updates by applying optimistic or confirmed events, ensuring that the state reflects comment additions, edits, or deletions accurately.

In the same directory, you would see the following code in the `hook.ts` file:

```tsx
// File: lib/models/hook.ts

'use client';

import { useEffect, useState } from 'react';
import { Model, SyncReturnType } from '@ably-labs/models';
import { modelsClient } from './modelsClient';
import { merge } from '@/lib/models/mutations';
import type { Post as PostType } from '@/lib/prisma/api';

export type ModelType = Model<(id: number) => SyncReturnType<PostType>>;

export async function getPost(id: number) {
  const response = await fetch(`/api/posts/${id}`, {
    method: 'GET',
    headers: { 'Content-Type': 'application/json' },
  });
  if (!response.ok)
    throw new Error(
      `GET /api/posts/:id: ${response.status} ${JSON.stringify(await response.json())}`
    );
  const { sequenceId, data } = (await response.json()) as {
    sequenceId: string;
    data: PostType;
  };
  return { sequenceId, data };
}

export const useModel = (id: number | null): [PostType | undefined, ModelType | undefined] => {
  const [postData, setPostData] = useState<PostType>();
  const [model, setModel] = useState<ModelType>();

  useEffect(() => {
    if (!id) return;
    const model: ModelType = modelsClient().models.get({
      channelName: `post:${id}`,
      sync: async () => getPost(id),
      merge,
    });
    setModel(model);
  }, [id]);

  useEffect(() => {
    if (!id || !model) return;
    const getPost = async (id: number) => await model.sync(id);
    getPost(id);
  }, [id, model]);

  useEffect(() => {
    if (!model) return;
    const subscribe = (err: Error | null, data?: PostType | undefined) => {
      if (err) return console.error(err);
      setPostData(data);
    };
    model.subscribe(subscribe);

    return () => model.unsubscribe(subscribe);
  }, [model]);

  return [postData, model];
};
```

In the code above, the following function and hook are defined:

1. **getPost**: Fetches the initial post data and its sequence ID from an API endpoint.
2. **useModel**: Manages a model instance and its associated state. The hook returns the synchronized post data and model instance for use in components. It:
   - Initializes the model with a channel name, `sync` function (to fetch data), and merge logic.
   - Synchronizes the model with the latest data when the `id` changes.
   - Subscribes to real-time updates from the model, updating the component state accordingly.

## API Routes for Comments and Real-Time Synchronization

To perform server-side operations as a result of user interaction on the webpage, a common pattern is to use API routes. In Next.js, API Routes (aka Endpoint Handlers) can be created by creating a file named `route.ts` in any directory in the `app` directory.

To allow dynamic updates and the ability to delete comments, you would see the following code in `route.ts` file in the `app/api/comments/[id]` directory:

```tsx
// File: app/api/comments/[id]/route.ts

export const dynamic = 'force-dynamic';

export const fetchCache = 'force-no-store';

import { NextRequest, NextResponse } from 'next/server';
import { withOutboxWrite, editComment, deleteComment } from '@/lib/prisma/api';

export async function PUT(request: NextRequest, { params }: { params: Promise<{ id: string }> }) {
  try {
    const id = Number((await params).id);
    const comment: { mutationId: string; content: string } = await request.json();
    const data = await withOutboxWrite(editComment, comment.mutationId, id, comment.content);
    return NextResponse.json({ data });
  } catch (error) {
    console.error('failed to update comment', error);
    return NextResponse.json({ message: 'failed to update comment', error }, { status: 500 });
  }
}

export async function DELETE(
  request: NextRequest,
  { params }: { params: Promise<{ id: string }> }
) {
  try {
    const id = Number((await params).id);
    const mutationId = request.headers.get('x-mutation-id') || 'missing';
    const data = await withOutboxWrite(deleteComment, mutationId, id);
    return NextResponse.json({ data });
  } catch (error) {
    console.error('failed to delete comment', error);
    return NextResponse.json({ message: 'failed to delete comment', error }, { status: 500 });
  }
}
```

In the code above, there are two endpoints, `PUT` and `DELETE`, both of which parse the `id` param in the request. The `PUT` endpoint extracts the comment properties (`mutationId`, `content`) to edit the comment in Postgres and sync the changes to the rest of the web clients that are actively looking to stream comment changes in real-time.

To allow the ability to insert comments, you would see the following code in `route.ts` file in the `app/api/comments` directory:

```tsx
// File: app/api/comments/route.ts

export const dynamic = 'force-dynamic';

export const fetchCache = 'force-no-store';

import { NextRequest, NextResponse } from 'next/server';
import { withOutboxWrite, addComment } from '@/lib/prisma/api';

export async function POST(request: NextRequest) {
  try {
    const comment: {
      mutationId: string;
      postId: number;
      authorId: number;
      content: string;
    } = await request.json();
    const data = await withOutboxWrite(
      addComment,
      comment.mutationId,
      comment.postId,
      comment.authorId,
      comment.content
    );
    return NextResponse.json({ data });
  } catch (error) {
    console.error('failed to add comment', error);
    return NextResponse.json({ message: 'failed to add comment', error }, { status: 500 });
  }
}
```

In the code above, the endpoint parses the request's body to extract the comment properties (`mutationID`, `postId`, `authorId`, `content`). Further, it inserts into Postgres using the `withOutboxWrite` helper function which makes sure to sync it in Postgres and rest of the web clients that are actively looking to stream comments in real-time.

Similarly, you would see the following code in `route.ts` file in the `app/api/posts/[id]` directory:

```tsx
// File: app/api/posts/[id]/route.ts

export const dynamic = 'force-dynamic';

export const fetchCache = 'force-no-store';

import { NextRequest, NextResponse } from 'next/server';
import { getPost } from '@/lib/prisma/api';

export async function GET(_: NextRequest, { params }: { params: Promise<{ id: string }> }) {
  try {
    const id = Number((await params).id);
    const [data, sequenceId] = await getPost(id);
    return NextResponse.json({ sequenceId, data });
  } catch (error) {
    return NextResponse.json({ message: 'failed to get post', error }, { status: 500 });
  }
}
```

In the code above, the endpoint parses the `id` param in the request and returns the `sequenceId` and the comment details associated with that ID in Postgres.

## Deploy to Vercel

The repository is now ready to deploy to Vercel. Use the following steps to deploy:

- Start by creating a GitHub repository containing your app's code.
- Then, navigate to the Vercel Dashboard and create a **New Project**.
- Link the new project to the GitHub repository you've just created.
- In **Settings**, update the **Environment Variables** to match those in your local `.env` file.
- Deploy.

<DetailIconCards>

<a target="_blank" href="https://github.com/neondatabase-labs/ably-livesync-neon" description="A Real-Time Comments Application" icon="github">Real-Time Comments Application</a>

</DetailIconCards>

## Summary

In this guide, you learned how to build a real-time comment system for a Next.js application by integrating Ably LiveSync with a serverless Neon Postgres database. The tutorial covered setting up the database schema, configuring Prisma for streamlined database access, and implementing Ably for real-time updates. You also explored how to handle optimistic updates, ensure data synchronization, and deploy the application to Vercel.

<NeedHelp />


# How to Create a Reliable Testing Dataset with pg_dump and pg_restore

---
author: paul-scanlon
enableTableOfContents: true
createdAt: '2025-02-14T00:00:00.000Z'
updatedOn: '2025-02-14T00:00:00.000Z'
title: How to Create a Reliable Testing Dataset with pg_dump and pg_restore
subtitle: A practical guide to extracting a test dataset from Postgres using pg_dump, pg_restore and psql
---

As your Postgres database grows, you'll likely need a way to generate a smaller, 'good-enough' dataset that preserves the structure and referential integrity of production but is better suited for testing.

There are several ways to do this, but here's a straightforward approach using `pg_dump`, `pg_restore`, `psql` and GitHub Actions.

## Running partial data dumps inside GitHub Actions

You can run `pg_dump`, `pg_restore`, and `psql` from the command line, but sometimes, an automated, reproducible approach is more convenient. To better control when data dumps occur, I use a [scheduled GitHub Action](https://docs.github.com/en/actions/writing-workflows/choosing-when-your-workflow-runs/events-that-trigger-workflows#schedule) to export data from my production database and restore it to a testing database. This method works across different Postgres database providers, but if you're looking for a cost-effective testing environment, consider trying Neon. Check out our [getting started guide](/docs/get-started/signing-up#sign-up) to see how easy it is to set up.

## What is a scheduled GitHub Action?

A scheduled GitHub Action runs automatically at a time you define. Since you're dumping data from a production database, you'll likely want to run this job when the system isn't under heavy load, typically outside of business hours. I usually schedule these jobs for midnight, but since I don't want to stay up that late, a scheduled GitHub Action takes care of it while I sleep.

## Getting started with GitHub Actions

To create a GitHub Action, you'll need a GitHub repository to store it. If you don't have one yet, create one now and clone it to your machine for local development.

In the root of your project, create a `.github` directory. Inside it, add another directory called `workflows`. Then, within `workflows`, create a new file named `dump-test-data.yml`, for example:

```
.github
  |-- workflows
    |-- dump-test-data.yml
```

Now add the following code.

There's a lot happening here, so before I get to the `pg_dump`, `pg_restore` and `psql` steps, let me briefly explain what this first part does.

```yml
name: Dump Test Data

on:
  schedule:
    - cron: '0 0 * * *' # Runs at midnight UTC
  workflow_dispatch:

env:
  PROD_DATABASE_URL: ${{ secrets.PROD_DATABASE_URL }} # Production or staging database
  DEV_DATABASE_URL: ${{ secrets.DEV_DATABASE_URL }} # Development or testing database
  PG_VERSION: '17'

jobs:
  dump-and-restore:
    runs-on: ubuntu-latest

    steps:
      - name: Install PostgreSQL
        run: |
          sudo apt update
          yes '' | sudo /usr/share/postgresql-common/pgdg/apt.postgresql.org.sh
          sudo apt install -y postgresql-${{ env.PG_VERSION }}

      - name: Set PostgreSQL binary path
        run: echo "POSTGRES=/usr/lib/postgresql/${{ env.PG_VERSION }}/bin" >> $GITHUB_ENV
```

### name

This name will appear in the **Actions** section of the GitHub UI. Regardless of what you name your file, this is the name that will be displayed.

## on

This section of the workflow determines when the Action will run. The `schedule` field includes a `cron` expression, which uses [POSIX cron syntax](https://pubs.opengroup.org/onlinepubs/9699919799/utilities/crontab.html#tag_20_25_07) to specify how often the Action should execute.

I've also included the `workflow_dispatch` field, which lets you manually trigger this Action from the GitHub UI—a useful feature while developing and testing the workflow.

### env

This workflow requires two environment variables. The first is the Postgres connection string for the source database, typically your production or staging database. The second is the connection string for the target database, which will serve as your testing database. Both need to use the same version of Postgres. Both of these variables will also need to be added to your GitHub repositories secrets.

To do this, navigate to **Settings** > **Settings and variables** > **Actions** and add them under **Repository secrets**.

![Screenshot of GitHub repository secrets](/guides/images/reliable-testing-dataset-with-pg-dump-and-pg-restore/screenshot-of-github-respository-secrets.jpg)

The last variable defines the Postgres version to install in the Action environment. Since `pg_dump`, `pg_restore`, and `psql` depend on Postgres, you'll need to install it within the Action—I’ll cover this in more detail later. It’s also worth noting the version of Postgres you install here should be the same version used by both your source and target database. In my example, all use [Postgres 17](/blog/postgres-17).

### jobs/steps

The job is named `dump-and-restore`, which will be displayed in the GitHub UI when the Action is running. You can choose any name you prefer.

The first step in the job is to install Postgres. While there are various methods and alternative options available in the [GitHub Marketplace](https://github.com/marketplace?query=Postgres), I prefer to install directly from Apt (Advanced Packaging Tool) for added security, especially since you're providing direct access to your production database.

The next step is to define a variable that is needed when using `pg_dump`, `pg_restore`, and `psql`. This variable is named `POSTGRES` and will be referenced later as `$POSTGRES/pg_dump`.

Before I jump into the dump/restore parts, I'll quickly explain the schema I've used in this example. It’s important to note the foreign key relationships between the tables.

In my example, the foreign key relationships are as follows:

- The **transactions** table has a foreign key `user_id` that references the `user_id` column in the **users** table. This establishes a relationship where each transaction is linked to a specific user.
- The **transactions** table is linked to the **products** table through the `product_id` foreign key. This establishes a relationship where each transaction is associated with a specific product.

### users

This is the schema used to create the `users` table.

```sql
CREATE TABLE users (
    user_id SERIAL PRIMARY KEY,
    first_name VARCHAR(100) NOT NULL,
    last_name VARCHAR(100) NOT NULL,
    email VARCHAR(150) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### products

This is the schema used to create the `products` table.

```sql
CREATE TABLE products (
    product_id SERIAL PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    description TEXT,
    price DECIMAL(10, 2) NOT NULL,
    stock_quantity INT DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### transactions

This is the schema used to create the `transactions` table.

```sql
CREATE TABLE transactions (
    transaction_id SERIAL PRIMARY KEY,
    user_id INT REFERENCES users(user_id) ON DELETE CASCADE,
    product_id INT REFERENCES products(product_id) ON DELETE CASCADE,
    quantity INT NOT NULL,
    total_amount DECIMAL(10, 2) NOT NULL,
    transaction_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    status VARCHAR(50) NOT NULL CHECK (status IN ('pending', 'completed', 'failed'))
);
```

The **transactions** table in my example relies on data from both the **users** and **products** tables. When performing a partial data dump, it's important that transaction rows can reference either a `user_id` from the **users** table or a `product_id` from the **products** table.

With this in mind, I'll start with the `transactions` table when deciding which data to include in the partial dump.

## Dump and restore partial data

Add the following code after the **Set PostgreSQL binary path** step.

```yml {23-46}
name: Dump Test Data
on:
  schedule:
    - cron: '0 0 * * *' # Runs at midnight UTC
  workflow_dispatch:
env:
  PROD_DATABASE_URL: ${{ secrets.PROD_DATABASE_URL }} # Production or staging database
  DEV_DATABASE_URL: ${{ secrets.DEV_DATABASE_URL }} # Development or testing database
  PG_VERSION: '17'
jobs:
  dump-and-restore:
    runs-on: ubuntu-latest
    steps:
      - name: Install PostgreSQL
        run: |
          sudo apt update
          yes '' | sudo /usr/share/postgresql-common/pgdg/apt.postgresql.org.sh
          sudo apt install -y postgresql-${{ env.PG_VERSION }}

      - name: Set PostgreSQL binary path
        run: echo "POSTGRES=/usr/lib/postgresql/${{ env.PG_VERSION }}/bin" >> $GITHUB_ENV

     - name: Dump schema
        run: |
          $POSTGRES/w "${{ github.workspace }}/all-schema.bak" "${{ env.PROD_DATABASE_URL }}"

      - name: Dump data
        run: |
          $POSTGRES/psql "${{ env.PROD_DATABASE_URL }}" -c "\copy (SELECT * FROM transactions ORDER BY transaction_id DESC LIMIT 50) TO '${{ github.workspace }}/transactions-subset.csv' WITH CSV HEADER"
          $POSTGRES/psql "${{ env.PROD_DATABASE_URL }}" -c "\copy (SELECT * FROM products WHERE product_id IN (SELECT product_id FROM transactions ORDER BY transaction_id DESC LIMIT 50)) TO '${{ github.workspace }}/products-subset.csv' WITH CSV HEADER"
          $POSTGRES/psql "${{ env.PROD_DATABASE_URL }}" -c "\copy (SELECT * FROM users WHERE user_id IN (SELECT user_id FROM transactions ORDER BY transaction_id DESC LIMIT 50)) TO '${{ github.workspace }}/users-subset.csv' WITH CSV HEADER"

      - name: Drop tables and schema
        run: |
          $POSTGRES/psql "${{ env.DEV_DATABASE_URL }}" -c "DROP SCHEMA IF EXISTS public CASCADE;"
          $POSTGRES/psql "${{ env.DEV_DATABASE_URL }}" -c "CREATE SCHEMA public;"

      - name: Restore schema
        run: |
          $POSTGRES/pg_restore --clean --no-owner --no-acl --if-exists --schema-only -d "${{ env.DEV_DATABASE_URL }}" "${{ github.workspace }}/all-schema.bak"

      - name: Restore data
        run: |
          $POSTGRES/psql "${{ env.DEV_DATABASE_URL }}" -c "\copy public.users FROM '${{ github.workspace }}/users-subset.csv' WITH CSV HEADER"
          $POSTGRES/psql "${{ env.DEV_DATABASE_URL }}" -c "\copy public.products FROM '${{ github.workspace }}/products-subset.csv' WITH CSV HEADER"
          $POSTGRES/psql "${{ env.DEV_DATABASE_URL }}" -c "\copy public.transactions FROM '${{ github.workspace }}/transactions-subset.csv' WITH CSV HEADER"
```

The above code snippet might look a bit complicated at first, but it’s actually not that bad—let me break it down for you.

### Dump schema

In this step, I use `pg_dump` to export the entire schema from the production database and save it to the GitHub workspace as a file named `all-schema.bak`. This file is stored in memory so it can be accessed later by the **Restore schema** step towards the end of the job.

The flags used in this step are explained below:

| Flag            | Meaning                                                                          |
| --------------- | -------------------------------------------------------------------------------- |
| `-Fc`           | Dumps the database in a custom format.                                           |
| `--schema-only` | Dumps only the schema (table structures, indexes, constraints) without any data. |
| `-f `           | Specifies the output file where the schema dump will be stored.                  |

### Dump data

In this step, I use `psql` to query the data. This is the most complex step, involving three SQL queries, each targeting one of the three tables. The queries are as follows:

#### Transactions query

This query selects the 50 most recent **transactions** from the `transactions` table. Depending on your requirements, you can increase the `LIMIT` or modify the query.

```sql
SELECT * FROM transactions ORDER BY transaction_id DESC LIMIT 50
```

The results are saved to the GitHub workspace memory as a file called `transactions-subset.csv`, which will be used in a later step.

#### Products query

This query selects **products**, but only those with a `product_id` present in the 50 most recent **transactions**:

```sql
SELECT * FROM products WHERE product_id IN (SELECT product_id FROM transactions ORDER BY transaction_id DESC LIMIT 50)
```

The results are saved to the GitHub workspace memory as a file called `products-subset.csv`, which will be used in a later step.

#### Users query

This query selects **users**, but only those with a `user_id` present in the 50 most recent **transactions**:

```sql
SELECT * FROM users WHERE user_id IN (SELECT user_id FROM transactions ORDER BY transaction_id DESC LIMIT 50)
```

The results are saved to the GitHub workspace memory as a file called `users-subset.csv`, which will be used in a later step.

### Drop tables and schema

In this step, I use `psql` to drop the schema and create a fresh one. Since this Action runs on a schedule, this cleanup ensures the target database is ready for new schema and data, avoiding any errors from schema changes since the last run.

The queries used in this step are explained below:

| Step                                    | Effect                                                   |
| --------------------------------------- | -------------------------------------------------------- |
| `DROP SCHEMA IF EXISTS public CASCADE;` | Removes the public schema and everything inside it.      |
| `CREATE SCHEMA public;`                 | Recreates the public schema for a clean restore process. |

### Restore schema

In this step, I use `pg_restore` to restore the schema from the `all-schema.bak` file, which was saved to the GitHub workspace during the Dump schema step.

The flags used in this step are explained below:

| Flag            | Meaning                                                                                                        |
| --------------- | -------------------------------------------------------------------------------------------------------------- |
| `--clean`       | Drops existing database objects before recreating them, ensuring a clean restore.                              |
| `--no-owner`    | Ignores ownership information in the dump file, so restored objects are owned by the user running the restore. |
| `--no-acl`      | Excludes access control (GRANT/REVOKE) statements from the restore, preventing permission changes.             |
| `–if-exits`     | Ensures that DROP commands (used with --clean) only execute if the object exists, preventing errors.           |
| `--schema-only` | Restores only the schema (table structures, indexes, constraints) without inserting any data.                  |
| `-d`            | Specifies the target database to restore into.                                                                 |

## Restore data

In this step, I use `psql` to restore the data to the target database from the `.csv` files generated in the **Dump data** step.

## Finished

Once the Action completes successfully, your target database will have a fresh test data set ready for use!

This Action is part of our [Dev/Test use case](/use-cases/dev-test), widely used by Neon customers who face limitations with traditional databases for testing. By leveraging a dedicated Neon database, while leaving production environments where they are, developers gain access to Neon's full suite of features, including the [built-in SQL editor](/docs/get-started/query-with-neon-sql-editor), [table explorer](/docs/guides/tables), and [branching](/docs/introduction/branching).

If you'd like to learn more about using Neon for testing, check out our [dev/test use case](/use-cases/dev-test) or contact our [sales team](/contact-sales).


# Building AI-powered applications with Replit Agent

---
title: Building AI-powered applications with Replit Agent
subtitle: A guide to building AI applications with Replit Agent
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-03-15T00:00:00.000Z'
updatedOn: '2025-03-15T00:00:00.000Z'
---

[Replit Agent](https://docs.replit.com/replitai/agent) is a newly integrated, AI-powered tool within [Replit](https://replit.com) that simplifies the process of building applications. It allows you to describe the application you want to create using natural language and translates your ideas into a working project. This approach is designed to make application development more accessible and efficient, particularly for those new to coding or looking to quickly test out app concepts.

Replit Agent integrates with Replit's online IDE, hosting features, and package management, offering an end-to-end solution within a familiar environment. This AI integration means you can bypass complex configurations and focus on describing your app's features and functionality. As you start using Replit Agent, you'll notice how tasks like database setup, code generation, and even deployment are handled automatically, freeing you to concentrate on bringing your application ideas to life. This guide introduces you to the basics of Replit Agent by walking through a practical example of creating an AI-powered MCQ Quiz generator from PDF documents.

## Prerequisites

Before you start, ensure you have the following prerequisites in place:

- **Replit Core or Teams subscription:** Access to Replit Agent requires a paid subscription to either Replit Core or Replit Teams for full access including deployments. Sign up at [replit.com/pricing](https://replit.com/pricing).
- **OpenAI API Key:** In this guide, we'll be using OpenAI's `gpt-4o-mini` model for MCQ generation. Sign up for an OpenAI API key at [platform.openai.com](https://platform.openai.com/account/api-keys).

<Admonition type="important" title="Vibe Coding Ahead 😎">
Follow this guide only if you're ready to experience the future of app development through AI-powered tools. You'll be amazed at how quickly you can build full-stack applications with Replit Agent.
</Admonition>

## Building an AI MCQ Quiz Generator app from PDF documents

This app allows users to create MCQ quizzes from uploaded PDF documents. Users can upload PDFs, and the app will generate the questions based on the content using OpenAI's `gpt-4o-mini` model. The generated MCQs will be stored in a database, and users can share a unique link to access the quiz. We'll leverage Replit Agent to build this app in minutes, without writing a single line of code.

You can also follow along with the video below to see the step-by-step process:

<video autoPlay playsInline muted loop width="800" height="600" controls>
  <source type="video/mp4" src="/videos/pages/doc/replit-agent.mp4"/>
</video>

### Create App with Replit Agent

1.  Navigate to [replit.com](https://replit.com) and log into your Replit account, ensuring you are under a Core or Teams subscription.
2.  Click on [Create App](https://replit.com/new) to begin. You'll be presented with the chat interface for Replit Agent.

    ![Replit Agent Create App](/docs/guides/replit-agent-create-new-app.png)

### Describe your app

In the chat interface, describe your app idea to Replit Agent in as much detail as possible. For example, you can say:

```text shouldWrap
Create an AI application that generates multiple-choice questions (MCQs) from uploaded PDFs for students to prepare for exams, using OpenAI's `gpt-4o-mini` model. It should have the following features:

- Ability to upload PDF documents.
- Generation of multiple-choice questions (MCQs) using OpenAI's `gpt-4o-mini` model.
- MCQs directly based on the content of the uploaded PDF.
- Functionality to create and share a link to the generated MCQs
```

Click "Start Building" to initiate the app creation process.

### Review and approve the agent generated plan

Replit Agent will present a development plan, outlining the proposed architecture and features for your application. Carefully review this plan, which details the intended technologies, functionalities, and implementation steps.

Click "Approve Plan & Start" to authorize Replit Agent to proceed with the application build process based on the outlined plan.

### Watch Replit Agent in action

You can now observe Replit Agent as it autonomously generates the application code, creating files within the project explorer and generating code in the editor window in real-time. Agent manages both frontend and backend code generation. You can sit back and watch as the application structure takes shape 😎

<Admonition type="note" title="Iterating with Replit Agent">
Developing applications with AI Agents is inherently an iterative process. As you work with evolving libraries and configurations, occasional unexpected behavior is to be anticipated.  When issues occur, **refer to the video above to see a practical example of iterative debugging.**  Proactively engage Replit Agent by describing the specific problem – detail what you observed, your intended functionality, and any error messages. Replit Agent will then provide guidance and code modifications to address the issue.
</Admonition>

### Run the generated application

Upon completion of the initial code generation phase, Replit Agent will automatically launch your application within the Replit webview. This allows you to interact with the initial MCQ (Multiple Choice Question) Generator.

### Debugging and iterative refinement with Replit Agent

Software development often involves debugging, and Replit Agent is designed to assist in this process. As demonstrated in the video, encountering errors is a normal part of development, and Agent can help diagnose and resolve them.

Should you encounter an error, **carefully examine the error message** displayed in the Replit webview or the console. **Copy the full error message and paste it directly into the Replit Agent chat window.**

Replit Agent is trained to interpret error messages and suggest corrective actions. It will analyze the provided error information, identify the root cause, and propose code modifications to rectify the issue. In our case, Agent accurately identified and resolved a `pdf.js` library error, providing specific code changes.

    ![Replit Agent PDF.js error](/docs/guides/replit-agent-pdfjs-error.png)

Review the Agent's proposed solution and watch as it implements the necessary code modifications. This iterative process of debugging and refinement is a key aspect of developing applications with Replit Agent.

### Add OpenAI API key

To enable the AI-powered MCQ generation, you must integrate your OpenAI API key into the generated application. Replit Agent will prompt you to add this key to integrate with OpenAI's `gpt-4o-mini` model.

    ![Replit Agent OpenAI API key prompt](/docs/guides/replit-agent-openai-key.png)

### Validate MCQ generation functionality

With the OpenAI API key integrated, test the core application feature: generating MCQs from uploaded PDF documents. Upload a sample PDF file to the application and observe the MCQ generation process.

    ![Replit Agent Test app](/docs/guides/replit-agent-test-app.png)

### Review, verify, and share generated MCQs

Examine the generated MCQs, assessing their relevance, accuracy, and overall quality based on the source PDF document. Test the generated shareable link by opening it in a new browser session. This link should direct users to the generated MCQs, enabling them to review and attempt the quiz.

    ![Replit Agent Share Quiz](/docs/guides/replit-agent-share-quiz.png)

### Database integration

A quick review of the generated code will reveal that Replit Agent has defaulted to an in-memory database for simplicity. To confirm this, you can directly ask the Agent about data storage. While in-memory databases are suitable for initial development, they are not ideal for production applications where data persistence is crucial. For a production-grade application, integrating a persistent database like Postgres is essential. You can now instruct Replit Agent to switch your application's data layer to a fully managed Postgres, powered by Neon. Replit Agent will make the necessary changes to the application code to integrate the Postgres database.

    ![Replit Agent Database Integration](/docs/guides/replit-agent-create-database.png)

### Deploying your application to production

After iteratively refining and testing your AI MCQ Generator application, you're ready to deploy it to a production environment. Replit simplifies the deployment process, enabling you to host your application online with just a few clicks.

1.  Click the "Deploy" button situated in the top-right corner of the Replit IDE.
2.  Review and adjust [deployment settings](https://youtu.be/sXP5d0k1atk) as needed. For simple applications, default settings are typically sufficient.
3.  Confirm deployment initiation by clicking "Deploy" again.

    ![Replit Agent Deploy App](/docs/guides/replit-agent-deploy-app.png)

Replit manages the deployment process, making your application publicly accessible via a unique `.replit.app` URL. This URL can be found in the "Deployments" tab within your Replit project. You can also add a [custom domain](https://docs.replit.com/cloud-services/deployments/custom-domains) to your application for a more professional appearance.

    ![Replit Agent Final App](/docs/guides/replit-agent-final-app.png)

Your AI MCQ Generator application should now be live and accessible to users. Share the deployment link with others to showcase what you _vibe coded_ in under 20 minutes with Replit Agent.

## Best practices for building applications with Replit Agent

To optimize your Replit Agent development experience and build applications effectively, consider these best practices:

- **Prompt engineering:**
  - **Improve Prompt**: Use the "Improve Prompt" feature in Replit Agent to refine the prompt and provide additional context. This helps Replit Agent better understand your requirements and generate more accurate code.
    ![Replit Agent Improve Prompt](/docs/guides/replit-agent-improve-prompt.png)
  - **Contextual prompts:** Initiate prompts with clear and comprehensive context. For example, "Modify the MCQ display to show one question at a time."
  - **Incremental iteration:** Decompose complex feature additions into smaller, incremental prompts for greater control and reduced complexity. For instance, when developing a multi-step form, address each form section sequentially.
  - **Specific feedback:** When encountering issues, provide precise and detailed feedback to Replit Agent. Include error messages, descriptions of expected vs. actual behavior, and relevant context for efficient debugging and issue resolution.

- **Leveraging Replit Platform Features:**
  - **Secure secrets management:** Always employ Replit Secrets for storing API keys, database credentials, and other sensitive information.
  - **Deployment**: Use Replit's built-in deployment features to host your applications online. This simplifies the deployment process and makes your applications accessible to a wider audience. You never have to worry about server management, scaling, or maintenance.

## Resources

- [Replit](https://replit.com)
- [Replit Agent Docs](https://docs.replit.com/replitai/agent)
- [Introducing Replit Assistant](https://youtu.be/fxiVDlylORQ)
- [Replit Deployments: Choosing the Right Deployment Type](https://youtu.be/sXP5d0k1atk)
- [Bringing Postgres to Replit with Neon](/blog/neon-replit-integration)

<NeedHelp />


# Track Schema Changes in Production with Postgres Event Triggers

---
title: Track Schema Changes in Production with Postgres Event Triggers
subtitle: Log every schema change with metadata in your Neon database
author: sam-harri
enableTableOfContents: true
createdAt: '2025-07-15T00:00:00.000Z'
---

Event triggers are now fully supported in Neon Postgres databases, and allow you to automatically respond to DDL events like `CREATE`, `ALTER`, `DROP`, or any other statements that define or modify the structure of the database. In this post, we'll show how you can use this feature to build a simple schema audit trail that can record who made schema changes in your production database, what those changes were, and when they occurred.

## Set Up Schema Auditing in Postgres

### Set Up the Audit Schema and Tables

First, we need two tables to store the audit log. To keep our auditing mechanism separate from the main application schema and to simplify permissions later, we'll place it in its own `audit` schema. Here, we have one table for the transactions containing the DDL changes, along with their metadata, then another table for all the DDL changes which reference the transaction they were a part of.

```sql
CREATE SCHEMA IF NOT EXISTS audit;

CREATE TABLE audit.transaction_audit (
  transaction_id         BIGINT PRIMARY KEY,
  event_time             TIMESTAMPTZ NOT NULL DEFAULT now(),
  database_user          TEXT NOT NULL,
  application_user_name  TEXT,
  application_user_email TEXT,
  client_address         INET
);

CREATE TABLE audit.ddl_audit (
  id              BIGSERIAL PRIMARY KEY,
  transaction_id  BIGINT NOT NULL
    REFERENCES audit.transaction_audit(transaction_id)
    ON DELETE CASCADE,
  command_tag     TEXT NOT NULL,
  object_identity TEXT,
  query_text      TEXT NOT NULL
);
```

### Create the Event Trigger Function

In Postgres, event triggers are executed using functions, so we need to create a function that returns the `event_trigger` type. This function will create the transaction entry if it does not already exist, then insert the DDL record. Given each DDL change within the same transaction will fire the trigger, we handle the case where the transaction has already been logged.

The `usr_name` and `usr_email` values are taken from the connection's context using `current_setting()`, though more on these later.

```sql
CREATE OR REPLACE FUNCTION audit.log_schema_changes()
  RETURNS event_trigger
  LANGUAGE plpgsql
AS $$
DECLARE
  obj       record;
  tx        BIGINT := txid_current_if_assigned();
  usr_name  TEXT := current_setting('audit.user_name', true);
  usr_email TEXT := current_setting('audit.user_email', true);
BEGIN
  INSERT INTO audit.transaction_audit (
    transaction_id,
    database_user,
    application_user_name,
    application_user_email,
    client_address
  ) VALUES (
    tx,
    session_user,
    usr_name,
    usr_email,
    inet_client_addr()
  )
  ON CONFLICT (transaction_id) DO NOTHING;

  FOR obj IN SELECT * FROM pg_event_trigger_ddl_commands() LOOP
    INSERT INTO audit.ddl_audit (
      transaction_id,
      command_tag,
      object_identity,
      query_text
    ) VALUES (
      tx,
      obj.command_tag,
      obj.object_identity,
      current_query()
    );
  END LOOP;
END;
$$;
```

### Attach the Trigger to DDL Events

Now, we can attach this function to an event trigger, and have it run after the DDL commands complete.

```sql
CREATE EVENT TRIGGER track_schema_changes
  ON ddl_command_end
  EXECUTE FUNCTION audit.log_schema_changes();
```

## Integrate Audit Logging in Production Workflows

In a production environment, you would rarely apply database migrations manually. Changes would instead be managed through a CI pipeline, which typically require passing a test suite, a staging environment, and review before being able to be merged.

### Create a CI-Only Role

Here, we'll create a dedicated `ci_user` role to run migrations in GitHub Actions. We'll grant this role the minimum permissions necessary, which includes creating objects in the public and audit schemas, referencing users in the Neon Auth schema (if you’re using auth for your project), and inserting records into the log table. This also makes it easy to spot any manual changes made outside of the CI process, since the `database_user` would be something other than `ci_user`, and the application user fields would be empty.

```sql
CREATE ROLE ci_user WITH LOGIN PASSWORD '<some-strong-password>';

GRANT CREATE ON DATABASE neondb TO ci_user;
GRANT USAGE, CREATE ON SCHEMA public TO ci_user;
GRANT USAGE ON SCHEMA neon_auth TO ci_user;
GRANT REFERENCES, SELECT ON TABLE neon_auth.users_sync TO ci_user;
GRANT USAGE, CREATE ON SCHEMA audit TO ci_user;
GRANT INSERT, SELECT ON ALL TABLES IN SCHEMA audit TO ci_user;
GRANT USAGE ON ALL SEQUENCES IN SCHEMA audit TO ci_user;
```

### Configure GitHub Actions

Then, in a `.github/workflows/migrate.yml` file we can define the steps to apply the migration in the pipeline. Likewise, we need to add the `DATABASE_URL` environment variable in the GitHub repo’s Secrets and Variables section.

```yaml
name: Migrate Database

on:
  push:
    branches:
      - master

jobs:
  migrate:
    runs-on: ubuntu-latest
    env:
      DATABASE_URL: ${{ secrets.DATABASE_URL }}

    steps:
      - name: Check out code
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 18
          cache: 'npm'

      - name: Install dependencies
        run: npm install

      - name: Set PGOPTIONS for audit
        run: |
          echo "PGOPTIONS=-c audit.user_name=${{ github.event.head_commit.author.name }} -c audit.user_email=${{ github.event.head_commit.author.email }}" >> $GITHUB_ENV

      - name: Run Drizzle migrations
        run: npx drizzle-kit migrate
```

The key part of this workflow is setting the `PGOPTIONS` environment variable which allows us to set connection parameters and easily pass context from GitHub Actions to our database. Here, we use it to pass `audit.user_name` and `audit.user_email`, and supply information on who the last committer was.

Realistically, schema migrations in production often involve multiple commits, possibly from different authors, and merged by reviewers. Ideally, your audit log should include information about all of these, though this can easily be added based on your needs.

## Visualize the Audit Log Safely with Read-Only Access

Once audit data is collected, you'll want a straightforward way to visualize it. Using the Neon internal tool template seen in a previous blog post, you can quickly build, secure, and host a UI to display these audit entries. Though, given the audit data lives in our production database, it’s a good idea to create a new read-only role that only has access to the `audit` schema to avoid exposing the entire production database to this tool.

```sql
CREATE ROLE audit_reader WITH LOGIN PASSWORD '<some-strong-password>';
REVOKE ALL ON SCHEMA public FROM audit_reader;
REVOKE ALL ON ALL TABLES IN SCHEMA public FROM audit_reader;
REVOKE ALL ON ALL FUNCTIONS IN SCHEMA public FROM audit_reader;
GRANT USAGE ON SCHEMA audit TO audit_reader;
GRANT SELECT ON ALL TABLES IN SCHEMA audit TO audit_reader;
```

From this dashboard, we now have a clear view of who made what changes, and when. DDLs are grouped by transaction, and you can easily search for keywords in the raw SQL.

![Audit log dashboard](/guides/images/schema-change-log/audit_log_frontend.gif)


# Run your own analytics with Umami, Fly.io and Neon

---
title: Run your own analytics with Umami, Fly.io and Neon
subtitle: Self host your Umami analytics on Fly.io and powered by Neon Postgres
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-06-05T00:00:00.000Z'
updatedOn: '2024-06-05T00:00:00.000Z'
---

In this guide, you will learn how to self host your Umami analytics instance on Fly.io and powered by Neon Postgres as the serverless database.

## Prerequisites

To follow along and deploy the application in this guide, you will need the following:

- [flyctl](https://fly.io/docs/getting-started/installing-flyctl/) – A command-line utility that lets you work with the Fly.io platform. You will also need [a fly.io account](https://fly.io/docs/hands-on/sign-up/).
- [A Neon account](https://console.neon.tech/signup) – The self-hosted Umami analytics instance will connect to a Neon serverless Postgres database 🚀

## Steps

- [What is Umami?](#what-is-umami)
- [Provisioning a Postgres database using Neon](#provisioning-a-postgres-database-using-neon)
- [Set up an Umami instance for Fly.io](#set-up-an-umami-instance-for-flyio)
- [Configure Neon Postgres as serverless database for self-hosted Umami analytics](#set-neon-postgres-as-serverless-database-for-self-hosted-umami-analytics)
- [Deploy to Fly.io](#deploy-to-flyio)

## What is Umami?

![Umami Analytics Preview](/guides/images/self-hosting-umami-neon/umami.jpeg)

Umami is a simple, fast, privacy-focused, open-source analytics solution. Umami is a better alternative to Google Analytics because it gives you total control of your data and does not violate the privacy of your users. <sup>[[1](https://umami.is/docs)]</sup>

## Provisioning a Postgres Database using Neon

Using a serverless Postgres database powered by Neon allows you to scale down to zero when the database is not being used, which saves on compute costs.
.

To get started, go to the [Neon Console](https://console.neon.tech/app/projects) and enter a name for your project.

You will be presented with a dialog that provides a connection string of your database. Enable the **Connection pooling** toggle for a pooled connection string.

![](/guides/images/self-hosting-umami-neon/1689d44f-4c5d-4b2a-8d13-32407f9c8781.png)

All Neon connection strings have the following format:

```bash
postgres://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>
```

- `user` is the database user.
- `password` is the database user’s password.
- `endpoint_hostname` is the host with neon.tech as the [top level domain (TLD)](https://www.cloudflare.com/en-gb/learning/dns/top-level-domain/).
- `port` is the Neon port number. The default port number is 5432.
- `dbname` is the name of the database. “neondb” is the default database created with each Neon project if you don't specify your own database name.
- `?sslmode=require&channel_binding=require` are optional query parameters that enforce the [SSL](https://www.cloudflare.com/en-gb/learning/ssl/what-is-ssl/) mode and channel binding for better security when connecting to the Postgres instance.

Please save the connection string somewhere safe. Later, you will use it to configure the `DATABASE_URL` variable.

## Setup Umami analytics instance for Fly.io

To self host your Umami analytics instance, you'll use [Umami's pre-built Docker container for Postgres](https://github.com/umami-software/umami/pkgs/container/umami/157800125?tag=postgresql-latest). This will allow you to self host an Umami analytics instance on Fly.io with a single `fly.toml` file.

In your terminal window, execute the following commands to create a new directory and `cd` to it:

```bash
mkdir self-host-umami-neon
cd self-host-umami-neon
```

In the directory `self-host-umami-neon`, create a file named `fly.toml` with the following content:

```toml
# File: fly.toml

kill_signal = "SIGINT"
kill_timeout = "5s"

[experimental]
    auto_rollback = true

[build]
    image = "ghcr.io/umami-software/umami:postgresql-latest"

[[services]]
    protocol = "tcp"
    internal_port = 3000
    processes = ["app"]

[[services.ports]]
    port = 80
    handlers = ["http"]
    force_https = true

[[services.ports]]
    port = 443
    handlers = ["tls", "http"]

[services.concurrency]
    type = "connections"
    hard_limit = 25
    soft_limit = 20

[[services.tcp_checks]]
    interval = "15s"
    timeout = "2s"
    grace_period = "1s"
```

In the `build` property named `image`, you will see that it's pointing to the latest Postgres compatible pre-built Docker image of Umami.

Next, you need to create an app on Fly.io using the configuration present in `fly.toml` file. In your terminal window, execute the following command to launch a Fly.io app:

```bash
fly launch
```

When prompted by the CLI to allow copying of the existing configuration into a new app, answer with a `y`:

```bash
An existing fly.toml file was found
? Would you like to copy its configuration to the new app? Yes
```

Optional: When asked if you want to tweak the default settings, answer with a `y`:

```bash
Using build strategies '[the "ghcr.io/umami-software/umami:postgresql-latest" docker image]'. Remove [build] from fly.toml to force a rescan
Creating app in /Users/rishi/Desktop/test
We're about to launch your app on Fly.io. Here's what you're getting:

Organization: Rishi Raj Jain         (fly launch defaults to the personal org)
Name:         test                   (derived from your directory name)
Region:       Mumbai, India          (this is the fastest region for you)
App Machines: shared-cpu-1x, 1GB RAM (most apps need about 1GB of RAM)
Postgres:     <none>                 (not requested)
Redis:        <none>                 (not requested)

? Do you want to tweak these settings before proceeding? Yes
Opening https://fly.io/cli/launch/641f1a1d67950614e4e92820ba484310 ...
```

flyctl will then automatically take you to a web page, which allows you to visually edit the default settings. For example, you can change the app name to `self-host-umami-neon`, and change the region to say `ams`.

![Fly.io Deployment Setting](/guides/images/self-hosting-umami-neon/307247099-acca8350-75c8-4007-b486-42c4102dfe40.png)

Click on `Confirm Settings` to set this configuration, and go back to your terminal window. In your terminal window, you should now see output similar to the following:

```bash
Waiting for launch data... Done
Created app 'self-host-umami-neon' in organization 'personal'
Admin URL: https://fly.io/apps/self-host-umami-neon
Hostname: self-host-umami-neon.fly.dev
Wrote config file fly.toml
Validating /Users/rishi/Desktop/test/fly.toml
✓ Configuration is valid
==> Building image
Searching for image 'ghcr.io/umami-software/umami:postgresql-latest' remotely...
image found: img_8rlxp2mjm9g43jqo

Watch your deployment at https://fly.io/apps/self-host-umami-neon/monitoring

Provisioning ips for self-host-umami-neon
  Dedicated ipv6: 2a09:8280:1::2b:b52c:0
  Shared ipv4: 66.241.124.197
  Add a dedicated ipv4 with: fly ips allocate-v4

This deployment will:
 * create 2 "app" machines
```

Once the deployment is ready, you are left with just one step &#8212; to set the `DATABASE_URL` environment variable that we obtained in the previous section. We'll do that in the next section.

## Configure Neon Postgres as serverless database for self-hosted Umami analytics

In your Fly.io [Dashboard > Apps](https://fly.io/dashboard), click on your app name, and you will be taken to the overview of your app on Fly.io.

![](/guides/images/self-hosting-umami-neon/307262264-17f870e2-379f-4d80-b37e-6dac9075174c.png)

Click on `Secrets` in the left sidebar, and then click on `New Secret` on the top right corner to start creating an environment variable for your app.

![](/guides/images/self-hosting-umami-neon/307263442-b835f320-8ae8-4a8a-a4f0-dbeb9e07530f.png)

In the modal, set the name of the secret as `DATABASE_URL`, and set the `Secret` value to be the one that we obtained in the previous section. Click **Set secret** to save the environment variable.

![](/guides/images/self-hosting-umami-neon/307263972-75bef039-f4d1-4b7d-a66d-dd312290a6d1.png)

Great! With that done, you have succesfully ensured that each deployment of your app on Fly.io will have the database URL pointing to the Neon Postgres instance. Let's trigger a deploy to see it all in action.

## Deploy To Fly.io

You can now deploy your app to Fly by running the following command:

```bash
flyctl deploy
```

Once deployed, you will be able to log into your self hosted Umami analytics instance with the default credentials, i.e.; **admin** as the username & **umami** as the password. You will then be able to create new websites and analyze the traffic to those sites.

## Summary

In this guide, you learned how to run your own Umami analytics instance for analytics with Fly.io, powered by Neon postgres as your database.

<NeedHelp />


# Efficiently Syncing 60 Million Rows from Snowflake to Postgres

---
title: Efficiently Syncing 60 Million Rows from Snowflake to Postgres
subtitle: A comprehensive guide on optimizing data transfer from Snowflake to Postgres using chunking and upsert strategies.
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-11-26T00:00:00.000Z'
updatedOn: '2024-11-26T00:00:00.000Z'
---

Transferring large datasets can be challenging, especially when dealing with memory constraints and the need for atomic operations. This guide will provide a structured approach to efficiently sync data from Snowflake to Postgres, ensuring minimal memory usage and the ability to rollback in case of errors. In this guide, we will explore an efficient method to sync a large dataset (60 million rows) from Snowflake to Postgres on a nightly basis. We will discuss the challenges faced with the traditional `COPY INTO` method and provide a robust solution using chunking and upsert strategies.

A common scenario for synchronizing data from Snowflake to Postgres involves integrating third-party datasets sourced from the Snowflake Marketplace that necessitate transformation. This data can subsequently be loaded into Neon, facilitating the creation of relationships between the Snowflake tables and the Postgres tables. Such an integration enables application-facing data to be queried through a unified Object-Relational Mapping (ORM) framework.

## Table of Contents

- [Challenges with the easiest approach](#challenges-with-the-easiest-approach)
- [Proposed Solution](#proposed-solution)
- [When to Upsert or Copy](#when-to-upsert-or-copy)
- [Implementation steps for Chunking](#implementation-steps-for-chunking)

## Challenges with the easiest approach

The easiest method involves saving data from Snowflake to a local CSV file and then performing a massive `COPY INTO` operation to Postgres. This approach has several drawbacks, some of them being:

- **High Memory Usage**: Loading 60 million rows at once can lead to significant memory consumption.
- **Performance Issues**: The single transaction approach can lead to performance bottlenecks.

## Proposed Solution

To address these challenges, we recommend the following strategies:

1. **Chunking**: Split the data into smaller chunks (e.g., 5 million rows) to reduce memory usage.
2. **Upsert Operations**: Instead of truncating and copying, use upsert queries to handle both new and existing records efficiently.
3. **Automated Rollback**: Implement a rollback mechanism to ensure data integrity in case of errors.

## When to Upsert or Copy

If the data from Snowflake contains a mix of new rows and rows that need updates in Postgres, you should opt for the upsert method. This approach allows you to efficiently handle both new and existing records. However, if you choose to use the upsert method, ensure that you create relevant indexes to improve lookup speed during updates and conflict resolution via a key.

On the other hand, if your dataset consists solely of new records, the traditional truncate and copy approach may be more appropriate, as it simplifies the process and can be more performant in such cases.

## Implementation steps for Chunking

### 1. Chunking the Data

Use the `split` command to divide the CSV file into manageable chunks. For example, to split a large CSV file of 60 million rows into chunks of 5 million rows, use the following command:

```shell
split -l 5000000 large_data.csv chunk_
```

### 2. Python Script for Data Transfer

Below is a Python script that connects to Neon and processes each chunk (of 5 million rows). It uses the `psycopg2` library to handle database operations with automatic rollback in case of any errors.

```python
# File: sync_script.py

import glob
import psycopg2
from psycopg2 import sql, DatabaseError

# Database connection parameters
db_params = {
    "dbname": "neondb",
    "user": "neondb_owner",
    "password": "...",
    "host": "ep-...us-east-2.aws.neon.tech",
    "port": 5432
}

tableName = "my_table"

# Read all files that have chunk_ in the present directory
chunk_files = glob.glob("chunk_*")

try:
    # Connect to the database
    conn = psycopg2.connect(**db_params)
    conn.autocommit = False  # Enable manual transaction management
    cur = conn.cursor()

    for chunk in chunk_files:
        with open(chunk, 'r') as f:
            print(f"Processing {chunk}...")
            try:
                cur.copy_expert(sql.SQL("COPY {} FROM STDIN WITH CSV").format(sql.Identifier(tableName)), f)
                # Commit after successfully processing the chunk
                conn.commit()
                print(f"Successfully loaded {chunk}")
            except Exception as e:
                # Rollback all changes if any chunk fails
                conn.rollback()
                print(f"Error processing {chunk}: {e}")
                break  # Stop processing on first error

    cur.close()
    conn.close()
    print("All chunks processed.")

except DatabaseError as db_err:
    print(f"Database connection error: {db_err}")
    if conn:
        conn.rollback()
        conn.close()
```

The script above does the following:

- Uses glob to read all files in the current directory that match the pattern "chunk\_\*".
- Establishes a connection to the PostgreSQL database with manual transaction management enabled.
- Iterates over each chunk file, opening it for reading, and then uses the COPY command to load data from each chunk file into the specified table in the database.
- Commits the transaction after successfully processing each chunk; if an error occurs, it rolls back the transaction and stops further processing.
- Closes the database cursor and connection after processing all chunks or upon encountering an error.

### 3. Running the Script

To execute the script, run the following command in your terminal:

```shell
python3 sync_script.py
```

### 4. Maintenance

After the data transfer, consider running a `VACUUM` command to clean up unnecessary storage and reclaim space:

```sql
VACUUM ANALYZE table_name;
```

## Conclusion

By implementing chunking and upsert strategies, you can efficiently sync large datasets from Snowflake to Neon while minimizing memory usage and ensuring data integrity. This approach not only improves performance but also provides a robust error handling mechanism.

With these strategies in place, you can confidently manage your nightly data syncs without the risk of overwhelming your system resources.

<NeedHelp />


# Database Migrations in Spring Boot with Flyway and Neon

---
title: Database Migrations in Spring Boot with Flyway and Neon
subtitle: Learn how to manage database schema changes in a Spring Boot application using Flyway with Neon Postgres.
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-09-07T00:00:00.000Z'
updatedOn: '2024-09-07T00:00:00.000Z'
---

Database schema management is an essential part of every application development and maintenance process.

As your application grows, you need a reliable way to manage database changes across different environments.

This guide will walk you through setting up and using [Flyway](https://github.com/flyway/flyway) for database migrations in a [Spring Boot](https://github.com/spring-projects/spring-boot) application with Neon Postgres.

## Prerequisites

Before we begin, ensure you have:

- Java Development Kit installed
- [Maven](https://maven.apache.org/) for dependency management
- A [Neon](https://console.neon.tech/signup) account for serverless Postgres
- Basic familiarity with Spring Boot and SQL

Instead of Maven, you can use Gradle for dependency management. The steps will be similar but for this guide, we'll use Maven.

## Setting up the Project

1. Let's create a new Spring Boot project using [Spring Initializr](https://start.spring.io/) with the following dependencies:
   - Spring Web
   - Spring Data JPA
   - PostgreSQL Driver
   - Flyway Migration

   ![](https://imgur.com/KRACyq7.png)

   Once you've selected the dependencies, click "Generate" to download the project. Then, extract the ZIP file and open it in your favorite IDE.

2. If you're using Maven, your `pom.xml` should include these dependencies:

   ```xml
   <dependencies>
       <dependency>
           <groupId>org.springframework.boot</groupId>
           <artifactId>spring-boot-starter-data-jpa</artifactId>
       </dependency>
       <dependency>
           <groupId>org.springframework.boot</groupId>
           <artifactId>spring-boot-starter-web</artifactId>
       </dependency>
       <dependency>
           <groupId>org.flywaydb</groupId>
           <artifactId>flyway-core</artifactId>
       </dependency>
       <dependency>
           <groupId>org.postgresql</groupId>
           <artifactId>postgresql</artifactId>
           <scope>runtime</scope>
       </dependency>
   </dependencies>
   ```

## Configuring the Database Connection

Now that we have our project set up, let's configure the database connection.

To configure your Neon database connection details, open the `application.properties` file in `src/main/resources` and add the following properties:

```properties
spring.datasource.url=jdbc:postgresql://<your-neon-hostname>/<your-database-name>
spring.datasource.username=<your-username>
spring.datasource.password=<your-password>

spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.PostgreSQLDialect
spring.jpa.hibernate.ddl-auto=validate

spring.flyway.enabled=true
spring.flyway.locations=classpath:db/migration
```

Replace the placeholders with your actual Neon database credentials.

Note that we set `spring.jpa.hibernate.ddl-auto=validate` to prevent Hibernate from automatically modifying the schema. Flyway will handle all schema changes.

To learn more about managing your database schema using Hibernate, refer to the [Database Schema Changes with Hibernate, Spring Boot, and Neon](/guides/spring-boot-hibernate) guide.

## Creating Migration Scripts

Flyway uses SQL scripts for migrations. These scripts should be placed in the `src/main/resources/db/migration` directory.

Unlike other migration tools, Flyway uses a version-based naming convention for migration scripts so that it can track the order in which they should be applied. This ensures that migrations are applied in the correct order and only once, but you need to be careful when renaming or modifying existing scripts.

Naming convention for migration scripts:

- `V<VERSION>__<DESCRIPTION>.sql`
- Example: `V2__Create_users_table.sql`

We will start with `V2__` as the first migration script, as Flyway will use `V1__` for its internal schema history table.

Let's create our first migration script:

1. Create a file named `V2__Create_users_table.sql` in `src/main/resources/db/migration`:

   ```sql
   CREATE TABLE users (
       id SERIAL PRIMARY KEY,
       username VARCHAR(50) NOT NULL UNIQUE,
       email VARCHAR(100) NOT NULL UNIQUE,
       created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
   );
   ```

2. Create another file named `V3__Create_posts_table.sql`:

   ```sql
   CREATE TABLE posts (
       id SERIAL PRIMARY KEY,
       title VARCHAR(100) NOT NULL,
       content TEXT,
       user_id INTEGER NOT NULL,
       created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
       FOREIGN KEY (user_id) REFERENCES users(id)
   );
   ```

These scripts will create the `users` and `posts` tables in your Neon database when you run the migrations.

## Running Migrations

Now that we've configured Flyway with our Spring Boot application and Neon database, we can proceed to run the database migrations.

There are two primary methods to execute Flyway migrations: using the Flyway Maven plugin or programmatically through the Flyway API. Let's explore both approaches in detail.

### 1. Using the Flyway Maven Plugin

The Flyway Maven plugin allows you to run migrations directly from the command line, which can be useful for CI/CD pipelines or local development.

First, add the Flyway Maven plugin to your `pom.xml` file:

```xml
<build>
    <plugins>
        <plugin>
            <groupId>org.flywaydb</groupId>
            <artifactId>flyway-maven-plugin</artifactId>
            <version>8.0.0</version>
            <configuration>
				<url>jdbc:postgresql://<your_neon_hostname>/neondb?sslmode=require&channel_binding=require</url>
                <user>${spring.datasource.username}</user>
                <password>${spring.datasource.password}</password>
                <locations>
                    <location>classpath:db/migration</location>
                </locations>
            </configuration>
        </plugin>
    </plugins>
</build>
```

Next, run the following command to create the schema history table in your database:

```bash
mvn flyway:baseline
```

This will create the `flyway_schema_history` table if it doesn't already exist. The table is used by Flyway to track the applied migrations.

Now, you can run the following command to apply pending migrations:

```bash
mvn flyway:migrate
```

This command will execute all pending migrations in the order defined by their version numbers.

Additional useful Flyway Maven plugin commands include:

- `mvn flyway:info`: Displays the status of all migrations. This includes the version, description, type, and state of each migration.
- `mvn flyway:validate`: Validates the applied migrations against the available ones. This ensures that the schema history table is correct and that all migrations were applied successfully.
- `mvn flyway:repair`: Repairs the schema history table. This command is useful if you manually modify the schema history table or if a migration fails.

### 2. Using the Flyway API Programmatically

For more fine-grained control or to integrate migration execution within your application lifecycle, you can use the Flyway API programmatically.

Start by creating a configuration class to set up the Flyway bean:

```java
package com.example.neon;

import org.flywaydb.core.Flyway;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import javax.sql.DataSource;

@Configuration
public class FlywayConfig {

    @Bean(initMethod = "migrate")
    public Flyway flyway(DataSource dataSource) {
        return Flyway.configure()
                .dataSource(dataSource)
                .locations("classpath:db/migration")
                .baselineOnMigrate(true)
                .load();
    }
}
```

This configuration automatically triggers the migration when the application starts. You can now run your Spring Boot application to apply the migrations:

```bash
mvn spring-boot:run
```

Alternatively, if you want more control over when migrations run, you can remove the `initMethod = "migrate"` and call the `migrate()` method manually:

```java
@Service
public class DatabaseMigrationService {

    private final Flyway flyway;

    @Autowired
    public DatabaseMigrationService(Flyway flyway) {
        this.flyway = flyway;
    }

    public void migrateDatabase() {
        flyway.migrate();
    }
}
```

You can then inject this service and call the `migrateDatabase()` method when appropriate, such as during application startup or as part of a maintenance routine.

## Handling Schema Changes

As your application evolves, you'll need to make changes to your database schema. Here's how to handle common scenarios:

### Adding a New Column

To add a new column to an existing table, you will just need to create a new migration script with the `ALTER TABLE` statement.

Create a new migration script, e.g., `V4__Add_user_role.sql`:

```sql
ALTER TABLE users ADD COLUMN role VARCHAR(20);
```

After adding the new migration script, you can run the migration using the Flyway Maven plugin or programmatically by starting the Spring Boot application depending on your preferred method.

### Modifying an Existing Column

To modify an existing column, you can create a new migration script with the `ALTER TABLE` statement, e.g., `V5__Modify_user_role.sql`:

```sql
ALTER TABLE users ALTER COLUMN role SET NOT NULL;
```

After adding the new migration script, you can check the status of your migrations using the Flyway Maven plugin:

```bash
mvn flyway:info
```

You should see the new migration in the list with a `Pending` state indicating that it hasn't been applied yet:

```sql
+-----------+---------+-----------------------+----------+---------------------+----------+
| Category  | Version | Description           | Type     | Installed On        | State    |
+-----------+---------+-----------------------+----------+---------------------+----------+
|           | 1       | << Flyway Baseline >> | BASELINE | 2024-09-07 16:27:26 | Baseline |
| Versioned | 2       | Create users table    | SQL      | 2024-09-07 16:34:20 | Success  |
| Versioned | 3       | Create posts table    | SQL      | 2024-09-07 16:34:23 | Success  |
| Versioned | 4       | Add user role         | SQL      | 2024-09-07 16:40:03 | Success  |
| Versioned | 5       | Modify user role      | SQL      |                     | Pending  |
+-----------+---------+-----------------------+----------+---------------------+----------+
```

Then run the migration using your preferred method to apply the changes.

### Creating a New Table

To create a new table, you would just add a new migration script with the `CREATE TABLE` statement, e.g., `V6__Create_comments_table.sql`:

```sql
CREATE TABLE comments (
    id SERIAL PRIMARY KEY,
    post_id INTEGER NOT NULL,
    user_id INTEGER NOT NULL,
    content TEXT NOT NULL,
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (post_id) REFERENCES posts(id),
    FOREIGN KEY (user_id) REFERENCES users(id)
);
```

## Handling Rollbacks

The Flyway Community Edition doesn't support automatic rollbacks. When you need to roll back a migration, you can create a new migration script to undo the changes. This script should be named with a higher version number than the original migration.

Create a file named `V7__Remove_user_role.sql`:

```sql
ALTER TABLE users DROP COLUMN role;
```

Then run the migration as usual to apply the rollback. This will remove the `role` column from the `users` table as defined in the script.

The Flyway Pro and Enterprise Editions offer additional features like `undo` and `repair` commands for automatic rollback and fixing failed migrations. You can explore these options if you require more advanced rollback capabilities.

## Best Practices

There are several things to keep in mind when managing database migrations:

1. Always keep your migration scripts in version control along with your application code.

2. Make sure that your migrations can be applied multiple times without changing the result beyond the initial application.

3. When possible, write migrations that are backward compatible with the previous version of your application. This will make it easier to roll back changes if needed.

4. Test your migrations thoroughly in a non-production environment before applying them to production. A great way to do this is by using the Neon branching feature to create a separate environment for testing with your production data without affecting the live environment.

5. Once a migration has been applied to any environment, avoid modifying it. Instead, create a new migration to make further changes.

## Conclusion

Using Flyway with Spring Boot and Neon Postgres provides a production ready solution for managing database schema changes. By following these practices, you can ensure that your database schema evolves safely and consistently across all environments.

Remember to always test your migrations thoroughly and have a solid backup and rollback strategy in place. Neon's features like branching and point-in-time recovery can be a great addition to your already existing lifecycle of your database schema.

## Additional Resources

- [Flyway Documentation](https://flywaydb.org/documentation/)
- [Spring Boot Flyway Integration](https://docs.spring.io/spring-boot/docs/current/reference/html/howto.html#howto.data-initialization.migration-tool.flyway)
- [Neon Documentation](/docs)


# Database Schema Changes with Hibernate, Spring Boot, and Neon

---
title: Database Schema Changes with Hibernate, Spring Boot, and Neon
subtitle: Learn how to manage database schema changes with Hibernate, Spring Boot, and Neon Postgres
author: bobbyiliev
enableTableOfContents: true
createdAt: '2024-09-07T00:00:00.000Z'
updatedOn: '2024-09-07T00:00:00.000Z'
---

Managing database schema changes is an important aspect of any application development lifecycle.

When using Hibernate ORM with Spring Boot and Neon Postgres, you have several options for handling schema evolution.

This guide will explore different approaches, their pros and cons, and best practices for managing database schema changes.

## Prerequisites

Before we begin, ensure you have:

- Java Development Kit (JDK) 11 or later
- Maven or Gradle for dependency management
- A [Neon](https://console.neon.tech/signup) account for serverless Postgres
- Basic familiarity with Spring Boot, Hibernate, and JPA concepts

## Setting up the Project

1. Create a new Spring Boot project using [Spring Initializr](https://start.spring.io/) with the following dependencies:
   - Spring Web
   - Spring Data JPA
   - PostgreSQL Driver

2. If you're using Maven, your `pom.xml` should include these dependencies:

   ```xml
   <dependencies>
       <dependency>
           <groupId>org.springframework.boot</groupId>
           <artifactId>spring-boot-starter-data-jpa</artifactId>
       </dependency>
       <dependency>
           <groupId>org.springframework.boot</groupId>
           <artifactId>spring-boot-starter-web</artifactId>
       </dependency>
       <dependency>
           <groupId>org.postgresql</groupId>
           <artifactId>postgresql</artifactId>
           <scope>runtime</scope>
       </dependency>
   </dependencies>
   ```

3. Extract the project and open it in your favorite IDE.

## Configuring the Database Connection

Next, configure your application to connect to a Neon Postgres database. To do that define your Neon database connection in `application.properties`:

```properties
spring.datasource.url=jdbc:postgresql://<your-neon-hostname>/<your-database-name>
spring.datasource.username=<your-username>
spring.datasource.password=<your-password>
```

Replace the placeholders with your actual Neon database credentials.

While modifying the `application.properties` file, you can also configure Hibernate's DDL behavior and other properties:

```
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.PostgreSQLDialect
spring.jpa.hibernate.ddl-auto=update
```

We will revisit the `spring.jpa.hibernate.ddl-auto` property later in this guide.

## Approaches to Schema Management

Before we dive into specific schema changes, let's explore different approaches to managing schema changes with Hibernate and Spring Boot.

### 1. Hibernate Auto DDL (Development Only)

The `spring.jpa.hibernate.ddl-auto` property controls Hibernate's schema generation behavior.

The different possible values are:

- `create`: Drops and recreates the schema on each startup
- `create-drop`: Creates the schema on startup and drops it on shutdown
- `update`: Updates the schema if necessary and doesn't drop existing tables
- `validate`: Validates the schema but makes no changes
- `none`: Disables DDL handling

For development, you might use the `update` strategy in your `application.properties`:

```properties
spring.jpa.hibernate.ddl-auto=update
```

This allows Hibernate to automatically update the schema based on your entity classes.

**Pros:**

- Easy to use during development
- Automatically reflects changes in entity classes

**Cons:**

- Not suitable for production use due to potential data loss or corruption
- Doesn't provide fine-grained control over schema changes

The `update` strategy is useful for development but should be avoided in production environments due to the risk of data loss or corruption.

Another option is to use `validate` in production to prevent accidental schema changes. Or you can disable auto DDL and manage schema changes manually or programmatically.

A handy option is to use `create-drop` for integration tests to recreate the schema before each test run. This ensures a clean database state for each test, however you should not use this in production as it will drop the database on shutdown.

### 2. Schema Generation Scripts

Hibernate can generate schema creation and update scripts based on your entity mappings.

Add these properties to `application.properties`:

```properties
spring.jpa.properties.javax.persistence.schema-generation.scripts.action=create
spring.jpa.properties.javax.persistence.schema-generation.scripts.create-target=create.sql
spring.jpa.properties.javax.persistence.schema-generation.scripts.create-source=metadata
```

This generates a `create.sql` file in your project root, which you can then manually review and apply to your database.

**Pros:**

- Provides a SQL script that you can review and modify
- Allows for version control of schema changes

**Cons:**

- Requires manual application of scripts
- Doesn't handle incremental updates well

### 3. Programmatic Schema Management

You can use Hibernate's `SchemaManagementTool` for more control over schema updates. This allows you to programmatically create, update, or validate the schema.

You can call this method on application startup or trigger it manually when needed.

**Pros:**

- Provides programmatic control over schema updates
- Can be integrated into your application's lifecycle

**Cons:**

- Requires careful management to avoid unintended schema changes
- May not handle all types of schema changes smoothly

### 4. Using a Migration Tool (Recommended for Production)

For production environments, it's recommended to use a dedicated migration tool like Flyway or Liquibase. These tools provide better control, versioning, and rollback capabilities.

To use Flyway, add the dependency to your `pom.xml`:

```xml
<dependency>
    <groupId>org.flywaydb</groupId>
    <artifactId>flyway-core</artifactId>
</dependency>
```

Then create migration scripts in `src/main/resources/db/migration` following Flyway's naming convention (e.g., `V1__Create_user_table.sql`).

**Pros:**

- Provides fine-grained control over schema changes
- Supports versioning and rollbacks
- Works well in production environments

**Cons:**

- Requires manual creation of migration scripts
- Adds complexity to the development process

For more information on using Flyway with Spring Boot, refer to the [Database Migrations in Spring Boot with Flyway and Neon](/guides/spring-boot-flyway) guide.

## Using Hibernate auto DDL

Now that we've covered different approaches to schema management, let's look at how to handle specific schema changes using Hibernate and Spring Boot with Neon Postgres.

As we pointed out earlier, Hibernate's auto DDL feature is convenient for development but not recommended for production use. Let's see how it works and how to handle common schema changes.

Start by setting `spring.jpa.hibernate.ddl-auto=update` in your `application.properties` file and then follow the examples below.

### Creating a New Entity

Once you've set the Hibernate auto DDL property to `update`, Hibernate will automatically create tables based on your entity classes. Start by creating a new entity class in your project called `Product`:

```java
@Entity
public class Product {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(nullable = false)
    private String name;

    @Column(nullable = false)
    private BigDecimal price;
}
```

When you run your Spring Boot application, Hibernate will create the `products` table automatically with the `id`, `name`, and `price` columns based on your entity class.

### Adding a New Column

Now that you have the `Product` entity, let's add a new column to the `products` table.

1. Add the new field to your entity class:

   ```java
   @Entity
   public class Product {
       // ...existing fields

       @Column(name = "description")
       private String description;
   }
   ```

2. Run your application, and Hibernate will automatically add the `description` column to the `products` table. Quite convenient for development!

### Renaming and Dropping Columns

When using `update`, Hibernate doesn't handle column rename or drop operations automatically. You'll need to use migration scripts for these changes.

1. Update the `@Column` annotation in your entity:

   ```java
   @Column(name = "new_column_name")
   private String oldColumnName;
   ```

2. Create a migration script to rename the column:

   ```sql
   ALTER TABLE users RENAME COLUMN old_column_name TO new_column_name;
   ```

The same applies to dropping columns, where you'll need to create a migration script to drop the column from the table:

    ```sql
    ALTER TABLE users DROP COLUMN column_name;
    ```

For those types of changes, you should use a migration tool like Flyway or Liquibase to manage the schema changes.

An alternative approach here is to, use `create` or `create-drop` for `spring.jpa.hibernate.ddl-auto`. This will recreate the schema on each startup, which can be useful for development but can not be used in production as it will lead to data loss.

## Best Practices

With the various approaches to schema management in mind, here are some best practices to follow when managing database schema changes with Hibernate, Spring Boot, and Neon Postgres:

1. While Hibernate's auto DDL is convenient for development, use a dedicated migration tool like [Flyway](/guides/spring-boot-flyway) for production environments.

2. Keep your entity classes and migration scripts in version control.

3. Always test schema changes in a non-production environment before applying them to production. A great way to do this is by using Neon's [branching feature](/docs/introduction/branching).

4. When possible, make schema changes that are backward compatible with the previous version of your application.

5. Make small, incremental changes rather than large, sweeping changes to your schema.

6. When using Hibernate's schema generation, always review the generated SQL before applying it to your database.

## Conclusion

Managing database schema changes with Hibernate, Spring Boot, and Neon requires careful consideration of your development workflow and production requirements.

While Hibernate's auto DDL feature is convenient for development, a more controlled approach using migration tools is recommended for production environments.

Always test your schema changes thoroughly in a non-production environment before applying them to your production database. With careful planning and the right tools, you can maintain a flexible and evolving database schema that supports your application's growth.

## Additional Resources

- [Hibernate ORM Documentation](https://hibernate.org/orm/documentation/5.4/)
- [Spring Boot JPA Documentation](https://docs.spring.io/spring-boot/docs/current/reference/html/data.html#data.sql.jpa-and-spring-data)
- [Flyway Documentation](https://flywaydb.org/documentation/)
- [Neon Documentation](/docs)


# Using Strapi CMS with Neon Postgres and Astro to build a blog

---
title: Using Strapi CMS with Neon Postgres and Astro to build a blog
subtitle: A step-by-step guide for building your own blog in an Astro application with Strapi CMS and Postgres powered by Neon
author: rishi-raj-jain
enableTableOfContents: true
createdAt: '2024-06-06T00:00:00.000Z'
updatedOn: '2024-06-06T00:00:00.000Z'
---

In this guide, you will learn how to set up a serverless Postgres database with Neon, configure Strapi CMS with Postgres, define a blog schema, and author content using Strapi CMS. The guide also covers configuring API read permissions and building a dynamic frontend with Astro to display blog pages based on Strapi content.

## Prerequisites

To follow the steps in this guide, you will need the following:

- [Node.js 18](https://nodejs.org/en) or later
- A [Neon](https://console.neon.tech/signup) account

## Steps

- [Provisioning a serverless Postgres database powered by Neon](#provisioning-a-serverless-postgres-database-powered-by-neon)
- [Setting up Strapi locally with Postgres](#setting-up-strapi-locally-with-postgres)
- [Configure a blog schema in Strapi CMS](#configure-a-blog-schema-in-strapi-cms)
- [Configure API read permissions in Strapi CMS](#configure-api-read-permissions-in-strapi-cms)
- [Create a new Astro application](#create-a-new-astro-application)
- [Integrate Tailwind CSS in your Astro application](#integrate-tailwind-css-in-your-astro-application)
- [Create dynamic blog routes in Astro](#create-dynamic-blog-routes-in-astro)
- [Build and test your Astro application locally](#build-and-test-your-astro-application-locally)
- [Scale-to-zero with Postgres (powered by Neon)](#scale-to-zero-with-postgres-powered-by-neon)

## Provisioning a serverless Postgres database powered by Neon

Using a serverless Postgres database powered by Neon lets you scale compute resources down to zero, which helps you save on compute costs.

To get started, go to the [Neon console](https://console.neon.tech/app/projects) and create a project.

You will then be presented with a dialog that provides a connection string of your database. Enable the **Connection pooling** toggle for a pooled connection string.

![Neon Connection Details](/guides/images/strapi-cms/20b94d5f-aff4-4594-b60b-3a65d4fc884c.png)

All Neon connection strings have the following format:

```bash
postgres://<user>:<password>@<endpoint_hostname>.neon.tech:<port>/<dbname>?sslmode=require&channel_binding=require
```

- `<user>` is the database user.
- `<password>` is the database user’s password.
- `<endpoint_hostname>.neon.tech` is the host with `neon.tech` as the [top-level domain (TLD)](https://www.cloudflare.com/en-gb/learning/dns/top-level-domain/).
- `<port>` is the Neon port number. The default port number is 5432.
- `<dbname>` is the name of the database. **neondb** is the default database created with each Neon project if you do not define your own.
- `?sslmode=require&channel_binding=require` are optional query parameters that enforce [SSL](https://www.cloudflare.com/en-gb/learning/ssl/what-is-ssl/) mode and channel binding for better security when connecting to the Postgres instance.

Each of the above values (except `sslmode`) is used in the next step &#8212; creating a local instance of the Strapi CMS application with Postgres.

## Setting up Strapi locally with Postgres

Let's begin with creating a Strapi CMS backend to serve the content for blog posts. Open your terminal and run the following command:

```bash
npx create-strapi-app@latest blog-api
```

`npx create-strapi-app` is the recommended way to scaffold a Strapi CMS project quickly.

When prompted, choose the following:

![Create Strapi CMS project](/guides/images/strapi-cms/bbdb6810-5336-4a10-9feb-c63b27874a80.png)

- `Custom (manual) settings` as the installation type.
- `TypeScript` as the preferred language.
- `postgres` as the default database client.
- `neondb` as the database name.
- `<endpoint_hostname>.neon.tech` as the host.
- `5432` as the port.
- `neondb_owner` as the username.
- `<password>` as the password.
- `y` to enable an SSL connection.

Once that’s done, change to the project directory and start the app:

```bash
cd blog-api
yarn develop
```

The command `strapi develop` runs, which takes care of creating the minimal schema required by Strapi CMS in the Neon Postgres database. When the setup is complete, you will be taken to `http://localhost:1337/admin` automatically. You will need to create an account for your locally hosted Strapi CMS instance. Strapi CMS makes sure to store the credentials (and all other data) in your Neon Postgres database.

![](/guides/images/strapi-cms/52fbde59-04bd-4af0-9e85-5bb33694d7a5.png)

To proceed, click the **Let's start** button to access the admin dashboard. Now, let's learn how to create the blog schema in Strapi CMS.

## Configure a blog schema in Strapi CMS

Once you have logged into the admin dashboard, it will by default, take you to the **Content-Type Builder** section. Here's where you can start to create the Blog schema. Click on **+ Create new collection type** to get started.

![Content-Type Builder](/guides/images/strapi-cms/55407bd1-68d4-4251-af02-571ef400f943.png)

Enter the **Display name** for your blog schema. For example, name it **Blog**, and click **Continue**.

![Create a collection type](/guides/images/strapi-cms/3f6b865d-100c-46ca-980d-c70ecefb4e86.png)

The application now asks you to create the first field inside the schema. Let's start with the **Title** of the blog first. Select **Text**, and it will automatically take you to the field detail section.

![Select a field](/guides/images/strapi-cms/6118918b-e0f2-493a-a80f-366b1577246c.png)

Enter **Title** as the Name of the first field and click **Finish**.

![Add new title field](/guides/images/strapi-cms/c4570e47-7a3e-4dfe-b855-6c461fe94748.png)

Once that's done, you are taken to an overview of the newly created **Blog** schema. Let's add two more fields. Click on **+ Add another field to this collection type**.

![Add another field](/guides/images/strapi-cms/e7437391-5fdf-4cbd-a922-fd500ab1f6d2.png)

First, you will add an `image` to be associated with the blog. Select **Media** and you are automatically taken to the asset detail section.

![Add media](/guides/images/strapi-cms/9a96ebf1-a6c7-4805-a219-c5b56f7a5b30.png)

Enter **Image** as the Name of the image field and click **Finish**.

![Add image field](/guides/images/strapi-cms/45f62587-a717-4b39-a47e-4d57b4e83517.png)

Next, you will add a `markdown` to be associated with the blog. Select **Rich Text (Markdown)**. You are automatically taken to the field detail section.

![Add Markdown field](/guides/images/strapi-cms/b5a13e5d-139a-44bd-9a36-751785aec8f8.png)

Enter **text** as the Name of the markdown field and click **Finish**.

![Enter markdown field name](/guides/images/strapi-cms/76b789ff-a15f-48ba-84a4-0cfe2cb37eb7.png)

Great! Click **Save** to save the present configuration.

![Configuration saved](/guides/images/strapi-cms/32a0b6ca-e7fd-4f6e-9942-4d63cc66d02b.png)

Click on the **Content Manager** button in the sidebar to start adding your first blog content. Click **+ Create new entry** to get started.

![Content Manager](/guides/images/strapi-cms/c735741c-1b1d-49f0-8f61-4ba210f799a6.png)

Enter the **Title**, select the **Image**, and input the markdown associated with the blog. You are now done writing your first post in the local Strapi CMS instance. All the data is synchronized in Postgres (powered by Neon). To finish off the content creation process, click **Save** and **Publish**.

![Create an entry](/guides/images/strapi-cms/045777e9-a898-490a-bf2b-271858e7ba6a.png)

With that done, let's move on to configuring read permissions for connected clients to access the data corresponding to the blog schema via an API.

## Configure API Read Permissions in Strapi CMS

To be able to fetch the data authored in your local Strapi CMS instance, you will need to configure what is readable and writeable using APIs. Navigate to **Settings > API Tokens** in your admin dashboard. Click on **Create new API Token** to start creating a new API token.

![API tokens](/guides/images/strapi-cms/846d9a45-138c-4474-b34d-33ddc8027e03.png)

Enter a Name to be associated with the token and set **Token Duration** to **unlimited** for the sake of this example. Finally, click **Save** to obtain the API token.

![Create API token](/guides/images/strapi-cms/10f85000-9b9b-428e-ab03-3faaa54cbaf3.png)

Copy the API token and store it somewhere safe as **STRAPI_API_TOKEN**.

Now, let's move on to creating an Astro application to create dynamic blog pages based on blog data that's accessible via your locally hosted instance of Strapi CMS.

## Create a new Astro application

Let’s get started by creating a new Astro project. Open your terminal and run the following command:

```bash
npm create astro@latest blog-ui
```

`npm create astro` is the recommended way to scaffold an Astro project quickly.

When prompted, choose:

- `Empty` when prompted on how to start the new project.
- `Yes` when prompted if plan to write Typescript.
- `Strict` when prompted how strict Typescript should be.
- `Yes` when prompted to install dependencies.
- `Yes` when prompted to initialize a git repository.

Once that’s done, change to the project directory and start the app:

```bash
cd blog-ui
npm run dev
```

The app should be running on [localhost:4321](http://localhost:4321/). Let's close the development server for now.

Next, execute the following command to install the necessary libraries and packages for building the application:

```bash
npm install dotenv marked @tailwindcss/typography
npm install -D @types/node
```

The commands above install the packages, with the `-D` flag specifying the libraries intended for development purposes only.

The libraries installed include:

- [dotenv](https://npmjs.com/package/dotenv): A library for handling environment variables.
- [marked](https://npmjs.com/package/marked): A markdown parser and compiler.
- [@tailwindcss/typography](https://npmjs.com/package/@tailwindcss/typography): A set of `prose` classes for HTML rendered from Markdown or pulled from a CMS.

The development-specific libraries include:

- [@types/node](https://npmjs.com/package/@types/node): Type definitions for node.

Then, add the following lines to your `tsconfig.json` file to make relative imports within the project easier:

```diff
{
  "extends": "astro/tsconfigs/strict",
  "compilerOptions": { // [!code ++]
    "baseUrl": ".", // [!code ++]
    "paths": { // [!code ++]
      "@/*": ["src/*"] // [!code ++]
    } // [!code ++]
  } // [!code ++]
}
```

Now, create a `.env` file. You are going to add the API token obtained earlier.

The `.env` file should contain the following keys:

```bash
# .env

STRAPI_API_TOKEN="..."
```

## Integrate Tailwind CSS in your Astro application

For styling the app, you will use Tailwind CSS. Install and set up Tailwind at the root of your project's directory by running:

```bash
npx astro add tailwind
```

When prompted, choose:

- `Yes` when prompted to install the Tailwind dependencies.
- `Yes` when prompted to generate a minimal `tailwind.config.mjs` file.
- `Yes` when prompted to make changes to the Astro configuration file.

The command finishes integrating TailwindCSS into your Astro project and installs the following dependencies:

- `tailwindcss`: TailwindCSS as a package to scan your project files to generate corresponding styles.
- `@astrojs/tailwind`: The adapter that brings Tailwind's utility CSS classes to every `.astro` file and framework component in your project.

To load pre-configured styles for your HTML (rendered from markdown), update your `tailwind.config.mjs` as follows:

```tsx
/** @type {import('tailwindcss').Config} */
export default {
  content: ['./src/**/*.{astro,html,js,jsx,md,mdx,svelte,ts,tsx,vue}'],
  theme: {
    extend: {},
  },
  plugins: [require('@tailwindcss/typography')], // [!code ++]
};
```

## Create dynamic blog routes in Astro

To programmatically create pages as you keep authoring more content in your locally hosted Strapi CMS, you are going to use [dynamic routes](https://docs.astro.build/en/guides/routing/#dynamic-routes) in Astro. With dynamic routes, you create a single file with a name like `[slug].astro`, where slug represents a [unique and dynamic variable](https://docs.astro.build/en/reference/api-reference/#contextparams) for each blog. Using [getStaticPaths](https://docs.astro.build/en/reference/api-reference/#getstaticpaths), you can programmatically create multiple blog pages with custom data using Strapi CMS as your data source. Let's see this in action. Create a file named `[slug].astro` in the `src/pages` directory with the following code:

```astro
---
// File: src/pages/[slug].astro

import "dotenv/config";
import { marked } from 'marked';

export async function getStaticPaths() {
  const response = await fetch(`http://127.0.0.1:1337/api/blogs?populate=Image`, {
    headers: {
      "Content-Type": "application/json",
      Authorization: `Bearer ${process.env.STRAPI_API_TOKEN}`,
    },
  });
  const {data} = await response.json();
  return data.map(({ id, attributes }) => ({
    params: { slug: id },
    props: attributes,
  }));
}

const { Title, text, Image } = Astro.props

const markdown = marked.parse(text)
---

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" type="image/svg+xml" href="/favicon.svg" />
    <meta name="viewport" content="width=device-width" />
    <meta name="generator" content={Astro.generator} />
    <title>Astro</title>
  </head>
  <body>
    <h1>{Title}</h1>
    <img src={`http://127.0.0.1:1337${Image.data[0].attributes.url}`} />
    <article set:html={markdown} />
  </body>
</html>
```

Let's understand the code above in two parts:

- Inside `getStaticPaths` function, a fetch call is made to the locally hosted Strapi CMS API to get all the blogs with their **Title**, **Image** and **text** values. Looping over each blog item, an array is created that passes all the data obtained as the [**props**](https://docs.astro.build/en/reference/api-reference/#contextprops), and its **id** as the unique variable to be associated with each blog.

- The HTML section represents the content of a particular blog page. The blog data attributes such as Title, Image URL, and markdown are obtained from `Astro.props` (passed in `getStaticPaths` as props). Further, the markdown is parsed into HTML using the `marked` library, and injected into the DOM using `set:html` template directive of Astro.

## Build and Test your Astro application locally

To test the Astro application in action, prepare a build and run the preview server using the following command:

```bash
npm run build && npm run preview
```

## Scale-to-zero with Postgres (powered by Neon)

Interestingly, during the entire process of building this application, you have used Neon's **Scale-to-zero** feature which places your Postgres compute endpoint into an idle state after 5 minutes of inactivity. Click the **Operations** button in your Neon console sidebar to see when the compute was started and automatically suspended to reduce compute usage.

![Neon Monitoring page](/guides/images/strapi-cms/ee753f7d-3da8-4a4c-84c5-be7b6cdce486.png)

## Summary

In this guide, you learned how to build a blog in an Astro application using Strapi CMS and a serverless Postgres database (powered by Neon). Additionally, you learned how to create content collections in Strapi CMS and dynamic blog routes in an Astro application.

<NeedHelp />


# Building a High-Performance Sensor Data API with FastAPI and Postgres' TimescaleDB Extension

---
title: Building a High-Performance Sensor Data API with FastAPI and Postgres' TimescaleDB Extension
subtitle: Create an  API for streaming, storing, and querying sensor data using Postgres TimescaleDB and FastAPI
author: sam-harri
enableTableOfContents: true
createdAt: '2024-10-12T00:00:00.000Z'
updatedOn: '2024-10-12T00:00:00.000Z'
---

In this guide, you'll build a high-performance API for streaming, storing, and querying sensor data using FastAPI and TimescaleDB for efficient time-series data storage.
By combining FastAPI with TimescaleDB's advanced time-series features, you'll be able to maintain low latency queries even at the petabyte scale, making it perfect for things like IoT systems that generate large volumes of sensor data.

## Prerequisites

Before starting, ensure you have the following tools and services ready:

- `pip`: Required for installing and managing Python packages, including [uv](https://docs.astral.sh/uv/) for creating virtual environments. You can check if `pip` is installed by running the following command:
  ```bash
  pip --version
  ```
- Neon serverless Postgres : you will need a Neon account for provisioning and scaling your `PostgreSQL` database. If you don't have an account yet, [sign up here](https://console.neon.tech/signup).

## Setting up the Project

Follow these steps to set up your project and virtual environment:

1.  Create a `uv` project

    If you don't already have uv installed, you can install it with:

    ```bash
    pip install uv
    ```

    Once `uv` is installed, create a new project:

    ```bash
    uv init timescale_fastapi
    ```

    This will create a new project directory called `timescale_fastapi`. Open this directory in your code editor of your choice.

2.  Set up the virtual environment.

        You will now create and activate a virtual environment in which your project's dependencies will beinstalled.

        <CodeTabs labels={["Linux/macOS", "Windows"]}>

            ```bash
            uv venv
            source .venv/bin/activate
            ```

            ```bash
            uv venv
            .venv\Scripts\activate
            ```

        </CodeTabs>

        You should see `(timescale_fastapi)` in your terminal now, this means that your virtual environment is activated.

3.  Install dependencies.

    Next, add all the necessary dependencies for your project:

    ```bash
    uv add python-dotenv asyncpg loguru fastapi uvicorn requests
    ```

    where each package does the following:
    - `FastAPI`: A Web / API framework
    - `AsyncPG`: An asynchronous PostgreSQL client
    - `Uvicorn`: An ASGI server for our app
    - `Loguru`: A logging library
    - `Python-dotenv`: To load environment variables from a .env file

4.  Create the project structure.

    Create the following directory structure to organize your project files:

    ```md
    timescale_fastapi
    ├── src/
    │ ├── database/
    │ │ └── postgres.py
    │ ├── models/
    │ │ └── product_models.py
    │ ├── routes/
    │ │ └── product_routes.py
    │ └── main.py
    ├── .env  
    ├── .python-version
    ├── README.md  
    ├── pyproject.toml  
    └── uv.lock
    ```

## Setting up your Database

In this section, you will set up the `TimescaleDB` extension using Neon's console, add the database's schema, and create the database connection pool and lifecycle management logic in FastAPI. Optionally, you can also add some mock data to test your API endpoints.

Given TimescaleDB is an extension on top of vanilla Postgres, you must first add the extension by running the following SQL in the `SQL Editor` tab of the Neon console.

```sql
CREATE EXTENSION IF NOT EXISTS timescaledb;
```

Next, you will add the necessary tables to your database with:

```sql
CREATE TABLE IF NOT EXISTS sensors (
    sensor_id SERIAL PRIMARY KEY,
    sensor_type VARCHAR(50) NOT NULL,
    description VARCHAR(255),
    location VARCHAR(255)
);


CREATE TABLE IF NOT EXISTS sensor_data (
    sensor_id INT REFERENCES sensors(sensor_id),
    value FLOAT NOT NULL,
    time TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    PRIMARY KEY(sensor_id, time)
);
```

One of TimescaleDB's core features is `Hypertables`, which is an optimized abstraction for handling large time-series data. It partitions your data into chunks based on time, allowing efficient storage, querying, and performance at scale. By converting the sensor_data table into a hypertable, TimescaleDB will manage the underlying chunking and indexing automatically.

To convert the `sensor_data` table into a hypertable, use the following command:

```sql
SELECT create_hypertable('sensor_data', 'time');
```

Now that the schema is ready, you can optionally populate the database with some sample sensor data. First, insert the metadata for two sensors:

```sql
INSERT INTO sensors (sensor_type, description, location)
VALUES
    ('temperature', 'Outdoor temperature sensor', 'Backyard'),
    ('humidity', 'Indoor humidity sensor', 'Living Room');
```

Next, generate time-series data for the past 14 days with one-minute intervals for both sensors. Here's how you can insert random data for each sensor using Timescales `generate_series()` feature.

```sql
INSERT INTO sensor_data (sensor_id, value, time)
SELECT 1 as sensor_id,
       15 + random() * 15 AS value,
       generate_series(
           now() - INTERVAL '14 days',
           now(),
           INTERVAL '1 minute'
       ) AS time;

INSERT INTO sensor_data (sensor_id, value, time)
SELECT 2 as sensor_id,
       40 + random() * 20 AS value,
       generate_series(
           now() - INTERVAL '14 days',
           now(),
           INTERVAL '1 minute'
       ) AS time;
```

With your schema and sample data in place, you're now ready to connect to your database in the FastAPI application. To do this you must create a `.env` file in the root of the project to hold environment-specific variables, such as the connection string to your Neon PostgreSQL database.

```bash
DATABASE_URL=postgres://user:password@your-neon-hostname.neon.tech/neondb?sslmode=require&channel_binding=require
```

Make sure to replace the placeholders (user, password, your-neon-hostname, etc.) with your actual Neon database credentials, which are available in the console.

In your project, the `database.py` file manages the connection to `PostgreSQL` using `asyncpg` and its connection pool, which is a mechanism for managing and reusing database connections efficiently. With this, you can use asynchronous queries, allowing the application to handle multiple requests concurrently.

```python
import os
import asyncpg
import dotenv
from loguru import logger
from typing import Optional

dotenv.load_dotenv()

conn_pool: Optional[asyncpg.Pool] = None


async def init_postgres() -> None:
    """
    Initialize the PostgreSQL connection pool and create the products table if it doesn't exist.
    """
    global conn_pool
    try:
        logger.info("Initializing PostgreSQL connection pool...")

        conn_pool = await asyncpg.create_pool(
            dsn=os.getenv("DATABASE_URL"), min_size=1, max_size=10
        )
        logger.info("PostgreSQL connection pool created successfully.")

    except Exception as e:
        logger.error(f"Error initializing PostgreSQL connection pool: {e}")
        raise


async def get_postgres() -> asyncpg.Pool:
    """
    Get a reference to the PostgreSQL connection pool.

    Returns
    -------
    asyncpg.Pool
        The connection pool object to the PostgreSQL database.
    """
    global conn_pool
    if conn_pool is None:
        logger.error("Connection pool is not initialized.")
        raise ConnectionError("PostgreSQL connection pool is not initialized.")
    try:
        return conn_pool
    except Exception as e:
        logger.error(f"Failed to return PostgreSQL connection pool: {e}")
        raise


async def close_postgres() -> None:
    """
    Close the PostgreSQL connection pool.
    """
    global conn_pool
    if conn_pool is not None:
        try:
            logger.info("Closing PostgreSQL connection pool...")
            await conn_pool.close()
            logger.info("PostgreSQL connection pool closed successfully.")
        except Exception as e:
            logger.error(f"Error closing PostgreSQL connection pool: {e}")
            raise
    else:
        logger.warning("PostgreSQL connection pool was not initialized.")

```

`init_postgres` is responsible for opening the connection pool to the `PostgreSQL` database and `close_postgres` is responsible for gracefully closing all connections in the pool when the `FastAPI` app shuts down to properly manage the lifecycle of the database.

Throughout your API you will also need access to the pool to get connection instances and run queries. `get_postgres` returns the active connection pool. If the pool is not initialized, an error is raised.

## Defining the Pydantic Models

Now, you will create `Pydantic` models to define the structure of the data your API expects and returns, automatically validating incoming requests and responses versus the defined format.

```python
from pydantic import BaseModel
from typing import List
from datetime import datetime, date


class SensorData(BaseModel):
    value: float
    timestamp: datetime


class SensorDataBatch(BaseModel):
    data: List[SensorData]


class SensorCreate(BaseModel):
    sensor_type: str
    description: str
    location: str


class SensorDailyStatsResponse(BaseModel):
    day: date
    sensor_id: int
    avg_value: float
    min_value: float
    max_value: float
    reading_count: int
    median_value: float
    iqr_value: float
```

Each of the models represent the following:

- `SensorData`: A single sensor reading, including the value recorded and the timestamp when the reading occurred
- `SensorDataBatch`: A batch of data points, to support batch streaming in your API
- `SensorCreate`: The fields for creating a new sensor
- `SensorDailyStatsResponse`: The daily sensor statistics

## Creating the API Endpoints

In this section, you will define the FastAPI endpoints that allow you to manage sensor data. These endpoints handle tasks like creating new sensors, streaming sensor data (both single points and batches), and querying daily statistics for a specific sensor. With these endpoints, you can efficiently manage and analyze sensor data using TimescaleDB’s time-series capabilities.

```python
from fastapi import HTTPException, Path, Body, APIRouter, Depends
from database.postgres import get_postgres
from typing import Union, List
from asyncpg import Pool
from loguru import logger
from models.sensor_models import (
    SensorData,
    SensorDataBatch,
    SensorCreate,
    SensorDailyStatsResponse,
)

sensor_router = APIRouter()


@sensor_router.post("/sensors")
async def create_sensor(
    sensor: SensorCreate = Body(...), db: Pool = Depends(get_postgres)
):
    """
    Create a new sensor.

    Parameters
    ----------
    sensor : SensorCreate
        The sensor details (type, description, and location) to create.
    db : asyncpg.Pool
        Database connection pool injected by dependency.

    Returns
    -------
    dict
        A dictionary containing the newly created sensor ID and a success message.
    """
    insert_query = """
    INSERT INTO sensors (sensor_type, description, location)
    VALUES ($1, $2, $3)
    RETURNING sensor_id;
    """

    logger.info(
        f"Creating new sensor with type: {sensor.sensor_type}, location: {sensor.location}"
    )

    async with db.acquire() as conn:
        sensor_id = await conn.fetchval(
            insert_query, sensor.sensor_type, sensor.description, sensor.location
        )

    if sensor_id is None:
        logger.error("Failed to create sensor.")
        raise HTTPException(status_code=500, detail="Failed to create sensor")

    logger.info(f"Sensor created successfully with ID: {sensor_id}")
    return {"sensor_id": sensor_id, "message": "Sensor created successfully."}


@sensor_router.post("/sensor_data/{sensor_id}")
async def stream_sensor_data(
    sensor_id: int = Path(...),
    sensor_data: Union[SensorData, SensorDataBatch] = Body(...),
    db: Pool = Depends(get_postgres),
):
    """
    Stream sensor data (single or batch) for a specific sensor.

    Parameters
    ----------
    sensor_id : int
        The ID of the sensor to associate the data with.
    sensor_data : Union[SensorData, SensorDataBatch]
        The sensor data to stream, which can be either a single data point or a batch.
    db : asyncpg.Pool
        Database connection pool injected by dependency.

    Returns
    -------
    dict
        A success message once the data is streamed.
    """
    insert_query = """
    INSERT INTO sensor_data (sensor_id, value, time)
    VALUES ($1, $2, $3);
    """

    logger.info(f"Streaming data for sensor_id: {sensor_id}")

    async with db.acquire() as conn:
        async with conn.transaction():
            if isinstance(sensor_data, SensorDataBatch):
                for data in sensor_data.data:
                    logger.debug(f"Batch data: {data.value} at {data.timestamp}")
                    await conn.execute(
                        insert_query, sensor_id, data.value, data.timestamp
                    )
            elif isinstance(sensor_data, SensorData):
                logger.debug(
                    f"Single data: {sensor_data.value} at {sensor_data.timestamp}"
                )
                await conn.execute(
                    insert_query, sensor_id, sensor_data.value, sensor_data.timestamp
                )

    logger.info(f"Sensor data streamed successfully for sensor_id: {sensor_id}")
    return {"message": "Sensor data streamed successfully."}


@sensor_router.get(
    "/daily_avg/{sensor_id}", response_model=List[SensorDailyStatsResponse]
)
async def get_sensor_daily_avg(
    sensor_id: int = Path(..., description="The ID of the sensor"),
    db: Pool = Depends(get_postgres),
):
    """
    Query daily statistics (min, max, median, IQR) for a specific sensor over the last 7 days.

    Parameters
    ----------
    sensor_id : int
        The ID of the sensor.
    db : asyncpg.Pool
        Database connection pool injected by dependency.

    Returns
    -------
    List[SensorDailyStatsResponse]
        A list of daily sensor statistics (average, min, max, median, IQR).
    """

    query = """
    WITH sensor_stats AS (
        SELECT
            time_bucket('1 day', time) AS day,
            sensor_id,
            avg(value) AS avg_value,
            min(value) AS min_value,
            max(value) AS max_value,
            count(*) AS reading_count,
            percentile_cont(0.5) WITHIN GROUP (ORDER BY value) AS median_value,
            percentile_cont(0.75) WITHIN GROUP (ORDER BY value) -
            percentile_cont(0.25) WITHIN GROUP (ORDER BY value) AS iqr_value
        FROM sensor_data
        WHERE sensor_id = $1
        GROUP BY day, sensor_id
    )
    SELECT * FROM sensor_stats
    ORDER BY day DESC
    LIMIT 7;
    """

    async with db.acquire() as conn:
        rows = await conn.fetch(query, sensor_id)

    if not rows:
        raise HTTPException(status_code=404, detail="No data found for this sensor.")

    return [
        SensorDailyStatsResponse(
            day=row["day"],
            sensor_id=row["sensor_id"],
            avg_value=row["avg_value"],
            min_value=row["min_value"],
            max_value=row["max_value"],
            reading_count=row["reading_count"],
            median_value=row["median_value"],
            iqr_value=row["iqr_value"],
        )
        for row in rows
    ]

```

The code defines endpoints for:

- `POST /sensors`: This endpoint creates a new sensor by providing the sensor type, description, and location.

- `POST /sensor_data/{sensor_id}`: Streams sensor data for a specific sensor. The data can be a single point or a batch.

- `GET /daily_avg/{sensor_id}`: Retrieves daily statistics (average, min, max, median, IQR) for the given sensor over the last 7 days.

In the query for the sensor statistics, the data is able to be partitioned quickly with Timescale's `time_bucket()` function by using the indexes generated when you created the hypertable. Likewise, you can easily calculate things like the interquartile range (IQR) using Timescale-specific functions.

## Running the Application

After setting up the database, models, and API routes, the next step is to run the `FastAPI` application and test it out.

The `main.py` file defines the `FastAPI` application, manages the database lifecycle, and includes the routes you created above.

```python
from fastapi import FastAPI
from contextlib import asynccontextmanager
from database.postgres import init_postgres, close_postgres
from routes.sensor_routes import sensor_router


@asynccontextmanager
async def lifespan(app: FastAPI):
    await init_postgres()
    yield
    await close_postgres()


app: FastAPI = FastAPI(lifespan=lifespan, title="FastAPI TimescaleDB Sensor Data API")
app.include_router(sensor_router)
```

To run the application, use uvicorn CLI with the following command:

```bash
uvicorn main:app --host 0.0.0.0 --port 8080
```

Once the server is running, you can access the API documentation and test the endpoints directly in your browser:

- Interactive API Docs (Swagger UI):  
  Visit `http://127.0.0.1:8080/docs` to access the automatically generated API documentation where you can test the endpoints.
- Alternative Docs (ReDoc):  
  Visit `http://127.0.0.1:8080/redoc` for another style of API documentation.

## Testing the API

You can test your application using `HTTPie`, a command-line tool for making HTTP requests. The following steps will guide you through creating sensors, streaming data, and querying sensor statistics.

1. Retrieve sensor statistics for pre-generated data (optional).

   If you followed the optional data generation steps, you can retrieve daily statistics for the pre-generated sensors:

   ```bash
   http GET http://127.0.0.1:8080/daily_avg/1
   ```

   ```bash
   http GET http://127.0.0.1:8080/daily_avg/2
   ```

   These commands will return the daily statistics (average, min, max, median, and IQR) for the pre-generated temperature and humidity sensors over the last 7 days.

2. Create a new sensor.

   Start by creating a new sensor (e.g., a temperature sensor for the living room):

   ```bash shouldWrap
   http POST http://127.0.0.1:8080/sensors sensor_type="temperature" description="Living room temperature sensor" location="Living Room"
   ```

   You should see a response confirming the creation of the sensor with a unique ID:

   ```json
   {
     "sensor_id": 3,
     "message": "Sensor created successfully."
   }
   ```

3. Stream a single sensor data point.

   Stream a single data point for the newly created sensor (`sensor_id = 3`):

   ```bash shouldWrap
   http POST http://127.0.0.1:8080/sensor_data/3 value:=23.5 timestamp="2024-10-12T14:29:00"
   ```

   You should get a response indicating success:

   ```json
   {
     "message": "Sensor data streamed successfully."
   }
   ```

4. Stream a batch of sensor data.

   You can also stream multiple sensor data points in a batch for the same sensor:

   ```bash shouldWrap
   http POST http://127.0.0.1:8080/sensor_data/3 data:='[{"value": 22.5, "timestamp": "2024-10-12T14:30:00"}, {"value": 22.7, "timestamp": "2024-10-12T14:31:00"}]'
   ```

   This will send two data points to the sensor. The response will confirm successful streaming of the batch data:

   ```json
   {
     "message": "Sensor data streamed successfully."
   }
   ```

5. Retrieve daily statistics for the new sensor.

   After streaming the sensor data, you can retrieve the daily statistics for the new sensor (`sensor_id = 3`):

   ```
   http GET http://127.0.0.1:8080/daily_avg/3
   ```

   This will return daily statistics (average, min, max, median, and IQR) for the new sensor over the last 7 days:

   ```json
   [
     {
       "day": "2024-10-12",
       "sensor_id": 3,
       "avg_value": 22.6,
       "min_value": 22.5,
       "max_value": 22.7,
       "reading_count": 2,
       "median_value": 22.6,
       "iqr_value": 0.2
     }
   ]
   ```

By following these steps, you can easily create sensors, stream sensor data, and query statistics from your API. For sensors with pre-generated data, you can retrieve the statistics immediately. For new sensors, you can stream data and retrieve their daily stats dynamically.

## Conclusion

Now, you have created and tested an API for managing, streaming, and querying sensor data into `TimescaleDB` using `FastAPI`. By leveraging TimescaleDB for time-series data storage, you now have a high-performance solution for handling sensor data at scale.

As a next step, you can look into streaming data into the database using a distributed event platform like `Kafka` or `Red Panda`, or using `Timescale` to monitor the sensor data with `Apache Superset` or `Grafana`.


# Timeseries Data in Postgres

---
title: Timeseries Data in Postgres
subtitle: A step-by-step guide describing how to use TimescaleDB for timeseries data in Postgres
author: vkarpov15
enableTableOfContents: true
createdAt: '2025-02-24T13:24:36.612Z'
updatedOn: '2025-02-24T13:24:36.612Z'
---

Timeseries queries allow you to efficiently query measurements over time stored in Postgres, like stock prices or temperature measurements.
With the [TimescaleDB extension](https://www.timescale.com/), you can you can efficiently store, query, and analyze time-series data in PostgreSQL using automatic partitioning, compression, and advanced query optimizations.

## Steps

- Install and enable TimescaleDB
- Create a hypertable
- Insert and retrieve timeseries data
- Query time-based aggregations
- Use `last()` to find the most recent value
- Generate histograms for data analysis

### Install and enable TimescaleDB

Before using timeseries queries, you need to install the TimescaleDB extension.
TimescaleDB extends PostgreSQL with hypertables, which optimize storage and queries for timeseries data.
In Neon, TimescaleDB is already installed, you just need to enable it using the following command.

```sql
CREATE EXTENSION IF NOT EXISTS timescaledb;
```

### Create a hypertable

TimescaleDB introduces hypertables, which automatically partition data by time for better performance.
To create a hypertable, first define a Postgres table with a timestamp column as follows.
Note that the timestamp `ts` is part of the primary key.

```sql
CREATE TABLE stock_prices (
  id SERIAL,
  ticker TEXT NOT NULL,
  ts TIMESTAMPTZ NOT NULL,
  price DOUBLE PRECISION NOT NULL,
  PRIMARY KEY (id, ts)
);
```

Then, convert the table into a hypertable:

```sql
SELECT create_hypertable('stock_prices', 'ts');
```

### Insert and retrieve timeseries data

You can insert timeseries data as you would in a normal Postgres table.
The following command inserts multiple stock price records, including multiple entries for AAPL on the same day:

```sql
INSERT INTO stock_prices (ticker, ts, price)
VALUES ('AAPL', '2025-02-22 09:30:00', 174.8),
       ('AAPL', '2025-02-22 10:00:00', 175.3),
       ('AAPL', '2025-02-22 15:59:00', 176.1),
       ('GOOGL', '2025-02-22 10:05:00', 2823.1);
```

You can retrieve rows from the `stock_prices` hypertable using normal SQL queries like the following, which returns all rows where `ts` is after February 22, 2025.

```sql
SELECT * FROM stock_prices WHERE ts >= '2025-02-22';
```

### Query time-based aggregations

TimescaleDB provides efficient ways to aggregate data over time.
The following query calculates the average stock price per day:

```sql
SELECT time_bucket('1 day', ts) AS bucket,
       ticker,
       AVG(price) AS avg_price
FROM stock_prices
GROUP BY bucket, ticker
ORDER BY bucket;
```

The above query outputs the following results.
The `avg_price` for AAPL is the average of the 3 `stock_prices` rows for AAPL on February 22.

| bucket                 | ticker | avg_price |
| ---------------------- | ------ | --------- |
| 2025-02-22 00:00:00+00 | AAPL   | 175.4     |
| 2025-02-22 00:00:00+00 | GOOGL  | 2823.1    |

### Use `last()` to find the most recent value

To find the most recent stock price for each ticker, you can use TimescaleDB's `last()` function as shown below.
There is also a corresponding `first()` function, which would return the first stock price for each ticker.

```sql
SELECT ticker, last(price, ts) AS last_price
FROM stock_prices
GROUP BY ticker;
```

### Generate histograms for data analysis

TimescaleDB's `histogram()` function calculates how many times a value falls into a given "bucket".
In other words, `histogram()` returns how many times a value falls into a given range.
For example, the following query breaks up the price range 170-180 into 10 buckets, and returns how many times the price of AAPL falls into each bucket.

```sql
SELECT ticker, width_bucket(price, 170, 180, 10) AS bucket, COUNT(*) AS frequency
FROM stock_prices
WHERE ticker = 'AAPL'
GROUP BY ticker, bucket
ORDER BY ticker, bucket;
```

The following is the result of the above query.
AAPL has one price in the 5th bucket (174.8), one in the 6th bucket (175.3), and one in the 7th bucket (176.1).
Buckets are 0-indexed, so 170-171 is bucket 0, 171-172 is bucket 1, and so on.

| ticker | bucket | frequency |
| ------ | ------ | --------- |
| AAPL   | 5      | 1         |
| AAPL   | 6      | 1         |
| AAPL   | 7      | 1         |


# Vector Search in Postgres

---
title: Vector Search in Postgres
subtitle: A step-by-step guide describing how to use pgvector for vector search in Postgres
author: vkarpov15
enableTableOfContents: true
createdAt: '2025-02-04T13:24:36.612Z'
updatedOn: '2025-02-04T13:24:36.612Z'
---

Vector search enables you to perform similarity searches on vectors stored in Postgres.
With the [pgvector](https://github.com/pgvector/pgvector) extension, you can store and efficiently query vector embeddings, making Postgres a viable option for AI-driven applications like retrieval-augmented generation (RAG) and semantic search.

## Steps

- Install and enable pgvector
- Create a table with a vector column
- Insert and retrieve vector data
- Perform nearest neighbor searches
- Index using HNSW indexes
- Insert and retrieve embeddings

## Install and enable pgvector

Before using vector search, you need to install the [`pgvector` extension](https://github.com/pgvector/pgvector).
The `pgvector` extension adds a `vector` data type, operators for similarity search (`<->`, `<#>`, `<=>`) , and support for ANN indexes.
In Neon, `pgvector` is already installed, you just need to enable it using the following command.

```sql
CREATE EXTENSION IF NOT EXISTS vector;
```

## Create a table with a vector column

To store vector embeddings, create a table with a vector column.
You must specify the size (also known as _dimensionality_) of the vectors when defining the column.

```sql
CREATE TABLE embeddings (
  id SERIAL PRIMARY KEY,
  data VECTOR(3) -- 3-dimensional vector example
);
```

## Insert and retrieve vector data

You can insert vectors as arrays using the following command.
Under the hood, vectors are just fixed-length arrays of floats.

```sql
INSERT INTO embeddings (data)
VALUES ('[0.1, 0.2, 0.3]'),
       ('[0.5, 0.1, 0.8]');
```

You can retrieve all stored vectors using the following command.

```sql
SELECT * FROM embeddings;
```

## Perform nearest neighbor searches

Vector search typically means finding the closest vectors in the database to a given vector.
There are different distance metrics to calculate which vector is closest, like Euclidean distance (`<->`), cosine similarity (`<#>`), and inner product (`<=>`).

For example, the following command runs nearest neighbor search to find the most similar vector to `[0.2, 0.1, 0.3]` using Euclidean distance, which is `[0.1, 0.2, 0.3]`.

```sql
SELECT * FROM embeddings
ORDER BY data <-> '[0.2, 0.1, 0.3]'
LIMIT 1;
```

## Index using HNSW indexes

For large datasets, exact nearest neighbor search can be slow.
`pgvector` supports two different indexes for nearest neighbor search: HNSW and IVFFlat.

The following command creates a HNSW index.

```sql
CREATE INDEX ON embeddings USING hnsw (data);
```

## Insert and retrieve embeddings

Vector databases are typically used to store _embeddings_.
An embedding is a numerical representation of data in a high-dimensional space that captures semantic relationships and similarities between entities.
First, run the following command to recreate the `embeddings` table to store vectors with dimensionality 512.

```sql
DROP TABLE embeddings;

CREATE TABLE embeddings (
  id SERIAL PRIMARY KEY,
  data VECTOR(512)
);
```

For example, the following command inserts a pair of 512 dimensionality vectors containing text embeddings pulled from the [Nomic API](https://docs.nomic.ai/reference/api/embed-text-v-1-embedding-text-post).
The first embedding represents the string "i like to eat tacos", the second represents the string "An embedding is a numerical representation of data in a high-dimensional space that captures semantic relationships and similarities between entities."

```sql
INSERT INTO embeddings (data)
VALUES
  /* Embedding representation of "i like to eat tacos" */
  ('[0.017120361,0.09112549,-0.24157715,0.0045776367,-0.024642944,0.0062828064,-0.06707764,0.022094727,-0.022232056,-0.019546509,0.010147095,0.05722046,0.027832031,0.07006836,-0.0051574707,-0.041259766,0.0008292198,-0.08605957,0.014213562,0.10180664,-0.045318604,-0.046447754,-0.002002716,-0.04144287,0.11590576,0.0093688965,-0.019638062,0.08929443,-0.057739258,0.031173706,-0.030471802,-0.07293701,0.019317627,0.100097656,0.017288208,-0.053222656,0.082092285,0.018234253,0.024536133,-0.0541687,-0.027191162,0.038635254,0.05657959,-0.050445557,0.06378174,-0.015579224,0.0736084,0.059173584,0.029037476,-0.03451538,-0.030151367,-0.027633667,-0.038604736,-0.06750488,-0.0038433075,-0.06210327,0.055664062,-0.06677246,-0.01828003,0.025848389,0.10809326,0.021942139,0.016067505,0.08532715,0.02708435,0.031311035,-0.046691895,0.078125,-0.07287598,-0.021347046,0.07159424,-0.0037384033,0.03878784,0.014350891,-0.02381897,-0.04309082,-0.031463623,-0.00541687,-0.03274536,-0.015464783,-0.0046539307,-0.017654419,0.08538818,-0.025238037,0.035949707,-0.012565613,0.0625,-0.057647705,-0.0418396,0.052825928,0.024276733,0.002412796,0.051452637,0.01663208,-0.029724121,0.035247803,-0.025817871,0.046081543,-0.007888794,-0.05114746,-0.036346436,0.017074585,0.009651184,-0.010925293,0.103759766,0.022567749,0.056121826,-0.0058555603,0.0362854,0.0031356812,0.03062439,0.042755127,-0.026870728,-0.05215454,-0.006095886,0.00006586313,0.010673523,-0.09136963,0.033721924,0.040740967,-0.01991272,-0.01953125,0.00033140182,0.05831909,0.015686035,0.024383545,-0.005264282,0.022613525,-0.048858643,-0.028945923,-0.002817154,0.03781128,0.014976501,-0.014030457,0.011795044,0.06008911,0.03262329,-0.066101074,0.015686035,0.008361816,0.005657196,0.06335449,0.051635742,-0.015274048,0.02571106,-0.044281006,0.0140686035,-0.09503174,0.011451721,-0.039886475,-0.02571106,0.0073432922,-0.0067329407,0.042541504,-0.022781372,-0.061798096,0.025634766,-0.05718994,-0.0023117065,0.015312195,0.04937744,-0.029815674,-0.009246826,0.05505371,0.014663696,-0.049468994,-0.0051002502,0.06573486,0.030593872,0.07922363,-0.026885986,-0.019348145,-0.051452637,-0.06427002,-0.04324341,-0.076171875,-0.09637451,-0.03753662,0.04888916,-0.017456055,0.02520752,-0.070129395,0.0022792816,0.08203125,-0.038635254,-0.044769287,-0.0020809174,0.025283813,-0.06549072,-0.028427124,0.011878967,0.010292053,-0.07965088,-0.05239868,-0.03062439,0.025115967,0.033081055,0.0035209656,0.014038086,-0.038909912,-0.023147583,-0.03616333,-0.10192871,0.027648926,-0.054382324,0.030395508,-0.05493164,-0.0048446655,-0.03756714,0.022705078,0.06274414,-0.030807495,-0.023605347,-0.02330017,-0.026519775,-0.034210205,-0.004245758,-0.014305115,-0.014213562,0.03845215,0.045684814,-0.014465332,0.009208679,-0.032562256,0.022567749,-0.027557373,-0.0033683777,-0.038085938,-0.04937744,-0.022033691,-0.014198303,-0.07611084,0.14099121,0.003921509,0.034576416,0.05404663,0.066345215,0.0847168,-0.0026435852,-0.051452637,-0.013175964,0.01701355,0.034820557,-0.039642334,-0.05734253,0.039093018,-0.004928589,-0.052215576,-0.027740479,0.050689697,0.049041748,-0.016693115,0.015731812,-0.01158905,0.024597168,-0.01878357,-0.012107849,0.040100098,0.031158447,-0.06994629,0.045135498,-0.10028076,0.033843994,-0.08734131,-0.021850586,-0.009010315,-0.03894043,0.052642822,-0.015525818,-0.07067871,0.023330688,0.011230469,-0.00018012524,0.046447754,-0.06591797,0.019104004,0.02494812,-0.0345459,-0.03277588,-0.0038433075,0.031051636,-0.03744507,-0.011779785,0.031234741,0.0041542053,0.070373535,0.023498535,0.0054016113,-0.011703491,-0.0067710876,0.04724121,0.06185913,-0.025558472,0.040130615,0.03439331,0.013008118,0.08886719,-0.032836914,-0.032958984,-0.043029785,-0.009384155,0.04269409,0.037475586,0.022415161,-0.038513184,0.035064697,0.07702637,-0.057861328,0.06274414,-0.028869629,0.0027332306,-0.024215698,-0.0067977905,0.07885742,-0.047668457,0.03137207,-0.020477295,0.0036449432,0.053375244,-0.002811432,0.03074646,-0.051513672,-0.0021152496,-0.05166626,-0.03869629,0.012924194,0.03878784,0.05831909,0.014884949,-0.07141113,0.001496315,0.01776123,0.03353882,-0.030471802,-0.028747559,0.028167725,0.068725586,0.025894165,-0.030807495,0.05807495,-0.007843018,-0.028762817,0.018737793,-0.04714966,-0.03149414,-0.007259369,-0.057128906,0.014770508,0.095458984,0.016723633,-0.039123535,0.02015686,-0.022628784,0.04852295,-0.0047912598,-0.026687622,0.055267334,-0.048736572,0.014633179,-0.005859375,0.02470398,-0.026916504,0.01083374,-0.010940552,-0.007030487,0.027557373,0.027526855,-0.015853882,0.013328552,0.030960083,-0.048919678,-0.051086426,-0.017242432,0.04147339,-0.004863739,0.017288208,-0.13586426,-0.035247803,0.057891846,-0.037750244,-0.0022220612,0.01576233,-0.057861328,0.039489746,0.055114746,0.037200928,0.04522705,0.0023956299,-0.030136108,-0.004131317,-0.006034851,-0.02619934,-0.07397461,-0.008293152,0.027572632,-0.061828613,0.07537842,-0.038635254,0.031341553,-0.002708435,-0.022384644,-0.057861328,0.00024557114,-0.024810791,-0.047729492,0.06677246,-0.030838013,-0.052520752,0.0579834,-0.03805542,-0.010284424,0.06323242,0.04699707,-0.030380249,-0.0010614395,0.0057678223,0.04824829,-0.014038086,0.016036987,-0.026031494,0.02708435,0.05987549,-0.0025463104,0.030838013,-0.046691895,0.041381836,-0.008102417,0.08227539,0.006324768,-0.07458496,-0.058410645,-0.0014505386,0.04196167,-0.014968872,-0.04714966,-0.03579712,-0.085876465,0.013183594,-0.005340576,0.06896973,0.012649536,-0.029388428,-0.11816406,-0.044128418,0.052124023,0.074645996,0.06384277,-0.023330688,0.019119263,-0.0146865845,0.02279663,0.015640259,0.05090332,0.021072388,0.09814453,-0.066711426,0.02671814,-0.027130127,0.038757324,-0.019317627,0.03741455,0.02746582,-0.03463745,0.00001001358,0.01638794,-0.0362854,0.02861023,0.0057754517,0.045562744,0.013206482,-0.010543823,0.03213501,0.044952393,-0.00018644333,-0.0040397644,-0.027236938,-0.026794434,0.004016876,0.016860962,0.0949707,-0.0129852295,0.024124146,-0.06185913,-0.08068848,-0.005054474,0.037353516,0.028411865,0.008850098,0.04940796,0.018356323,0.008979797,0.016098022,0.013702393,-0.03942871,0.03463745,0.006729126,-0.042541504,0.02607727,0.06451416,-0.029632568,-0.029647827,-0.014167786,-0.01675415,-0.0017442703,0.07269287,0.00013709068,-0.044708252,-0.059417725,-0.097839355,0.013648987,-0.0041923523,0.0025196075]'),
  /* Embedding representation of "An embedding is a numerical representation of data in a high-dimensional space that captures semantic relationships and similarities between entities." */
  ('[0.043701172,0.09063721,-0.24499512,-0.1385498,0.025177002,-0.020385742,0.0074653625,-0.016143799,-0.08294678,-0.03427124,-0.04864502,0.003490448,0.1060791,0.035461426,-0.023712158,0.04220581,-0.016342163,-0.039001465,-0.06008911,0.034362793,-0.048858643,0.023666382,0.012779236,0.012001038,0.057250977,0.038970947,0.08312988,-0.046936035,-0.02229309,0.00674057,0.04751587,-0.015289307,0.0027866364,-0.028762817,-0.043548584,-0.037200928,-0.0045547485,0.09539795,0.026290894,0.018051147,0.0000140070915,-0.002450943,-0.04751587,0.013076782,-0.031982422,-0.0035572052,0.044952393,-0.04220581,0.08660889,-0.037109375,-0.010673523,-0.013221741,-0.015609741,-0.028411865,0.11138916,0.02218628,0.008255005,0.015991211,0.043395996,-0.044189453,0.09460449,0.1005249,-0.06817627,0.09283447,0.0625,0.026916504,-0.097961426,0.05682373,-0.011253357,-0.085510254,0.10241699,-0.010391235,0.03656006,0.028320312,-0.025604248,-0.0149383545,-0.00881958,0.0362854,-0.002401352,0.052734375,0.04220581,0.03640747,0.09686279,-0.040527344,0.09460449,0.045043945,-0.010475159,0.00006771088,-0.06567383,0.060913086,0.016830444,0.009277344,0.02458191,0.05444336,-0.024734497,0.006401062,-0.00166893,0.028289795,-0.033447266,-0.03704834,-0.055389404,-0.01486969,-0.021697998,0.01322937,-0.005695343,0.053649902,-0.000044941902,0.026565552,-0.06561279,0.022399902,-0.022094727,0.015525818,-0.06402588,-0.06585693,-0.0055732727,-0.018295288,0.09020996,-0.07720947,-0.014472961,0.057434082,0.01537323,-0.041870117,0.042419434,0.05392456,0.007080078,0.011199951,-0.020095825,0.007774353,-0.044433594,-0.04031372,-0.016448975,-0.060394287,-0.009780884,0.010131836,0.005207062,0.038879395,-0.048675537,-0.024917603,-0.0069351196,0.08514404,-0.0041885376,-0.015586853,-0.0029888153,-0.0546875,0.008361816,-0.09490967,0.035705566,-0.02935791,0.009742737,-0.015213013,-0.00970459,0.08270264,-0.03753662,-0.045074463,0.01612854,-0.0030441284,0.024749756,0.0041542053,0.064697266,-0.007019043,0.038970947,0.04284668,-0.030029297,0.04623413,0.019699097,-0.074523926,-0.0024147034,0.019836426,0.011489868,0.009597778,-0.04751587,-0.03125,-0.023025513,-0.0064201355,0.0007266998,-0.007888794,0.036834717,-0.068359375,0.056671143,0.006175995,0.021530151,-0.04324341,0.07232666,-0.004169464,-0.025619507,-0.019226074,-0.007259369,-0.01902771,-0.060760498,-0.03161621,-0.055877686,0.012390137,-0.031280518,-0.00705719,-0.019470215,-0.00061893463,0.06774902,-0.034301758,-0.003293991,-0.023925781,-0.007820129,0.011604309,-0.024002075,0.05206299,-0.012214661,0.043304443,-0.04232788,-0.0005931854,-0.050872803,0.04647827,0.06555176,-0.017486572,0.001001358,0.010131836,0.04776001,-0.0076560974,0.0063323975,-0.04147339,-0.02243042,-0.00008791685,0.028717041,-0.01927185,0.039794922,-0.0769043,0.03289795,-0.019439697,-0.03137207,0.047088623,-0.045532227,0.0011854172,-0.03768921,-0.04663086,0.044525146,-0.031173706,0.011817932,0.06109619,-0.01701355,0.06524658,0.006614685,0.037841797,0.018707275,0.053833008,-0.02468872,-0.03387451,-0.02897644,0.05923462,0.024429321,-0.060516357,-0.0435791,0.07159424,-0.04446411,-0.036712646,0.012107849,0.007286072,0.07183838,-0.031829834,-0.047790527,-0.07092285,0.014518738,-0.008964539,0.05621338,-0.017486572,-0.0129470825,-0.036499023,-0.06890869,-0.021835327,0.027175903,-0.007709503,0.02960205,-0.02003479,0.01058197,0.017303467,0.018112183,0.019622803,0.011024475,-0.013412476,0.02229309,-0.012329102,-0.05053711,0.01197052,-0.05316162,-0.018341064,-0.07086182,0.0146865845,-0.018798828,0.021240234,0.036895752,0.020812988,0.025863647,0.031097412,0.037475586,-0.042053223,-0.03768921,0.04321289,-0.00054073334,0.045806885,0.06732178,-0.001572609,0.049682617,-0.064086914,-0.010314941,0.049835205,0.099975586,0.011741638,-0.0135269165,-0.033843994,0.040924072,-0.056121826,0.020202637,-0.04135132,-0.06286621,-0.0056762695,-0.054840088,0.0025749207,-0.0647583,0.051208496,0.027694702,-0.00026249886,0.03201294,-0.07409668,-0.005104065,-0.12463379,0.036010742,-0.031173706,0.0036354065,0.07354736,-0.050201416,0.013839722,-0.01612854,0.021835327,-0.039001465,0.012069702,0.055603027,-0.005886078,-0.034606934,0.017791748,-0.02961731,-0.033721924,0.011795044,0.0029697418,0.08337402,-0.008636475,0.02470398,-0.09301758,0.026794434,0.03869629,-0.061767578,-0.004070282,0.04171753,0.021850586,-0.03186035,0.00680542,0.009895325,-0.032104492,0.022888184,-0.0076675415,0.0440979,-0.00548172,-0.006793976,-0.0138168335,0.060913086,-0.0035152435,-0.02609253,-0.053619385,-0.0090789795,0.012084961,0.03604126,0.040924072,0.020462036,0.031585693,0.0057411194,-0.0006456375,-0.060272217,0.042297363,0.04827881,-0.0340271,-0.087646484,-0.06738281,0.005554199,-0.014373779,-0.017181396,0.03753662,0.015686035,0.005493164,0.037750244,-0.0031909943,0.035125732,0.00712204,0.017791748,0.007865906,0.004673004,-0.015129089,-0.052978516,0.01751709,0.026031494,-0.06939697,-0.018112183,0.010276794,0.03741455,-0.010620117,-0.014030457,-0.066223145,0.0015687943,-0.023376465,-0.0043296814,-0.029556274,-0.008255005,-0.07354736,0.044281006,-0.031341553,-0.0026378632,0.049835205,0.03503418,-0.10040283,0.0003578663,0.039642334,0.037841797,0.040100098,-0.017211914,0.014572144,0.019897461,0.101989746,-0.03503418,0.025268555,0.040802002,-0.015068054,0.006248474,0.0960083,0.016464233,-0.050231934,-0.015098572,0.041625977,0.062927246,0.0340271,-0.034210205,-0.026412964,-0.045013428,-0.0032138824,-0.0058021545,0.07849121,0.009056091,-0.06359863,-0.019699097,-0.016143799,0.016113281,0.12384033,-0.0044937134,-0.01789856,-0.08276367,0.069885254,0.07110596,0.018173218,-0.0017271042,0.033447266,0.12609863,-0.036712646,-0.012878418,-0.042633057,-0.00087690353,-0.00091171265,0.002943039,0.04800415,-0.08984375,0.035003662,-0.004058838,-0.058410645,-0.007270813,-0.07141113,0.068237305,0.10491943,-0.012290955,0.02571106,0.020767212,-0.03253174,0.04916382,0.05633545,-0.008430481,-0.052886963,0.026992798,0.016601562,0.014930725,0.0026130676,-0.07116699,-0.031280518,0.0006713867,0.049682617,0.012771606,0.00046944618,0.034973145,0.02885437,0.020858765,-0.050842285,0.04437256,0.015289307,-0.027572632,-0.11541748,-0.008483887,0.005844116,0.037109375,-0.0057868958,0.03164673,0.06451416,0.000603199,0.004924774,0.053344727,-0.027374268,0.08270264,-0.04724121,-0.11883545,-0.010147095,0.008865356,0.044281006]');
```

You can then query for which embeddings are closest to a new vector.
For example, the following query finds the closest vector to the embedding for "burgers are tasty" using cosine similarity `<#>`.
Unsurprisingly, Postgres returns the "i like to eat tacos" vector.

```sql
SELECT * FROM embeddings
ORDER BY data <#> '[0.001080513,0.08959961,-0.29296875,0.0014181137,-0.019119263,0.021392822,-0.015617371,0.0345459,-0.0690918,0.009246826,-0.018981934,0.091796875,0.0041160583,0.02947998,-0.021835327,-0.03503418,-0.07702637,-0.07989502,-0.021102905,0.09667969,0.024597168,0.0124053955,0.027420044,-0.039001465,0.10235596,0.008583069,-0.06512451,0.08111572,-0.031982422,0.013595581,0.009635925,-0.036315918,-0.08148193,-0.014015198,0.0082092285,-0.0793457,0.0597229,0.024673462,-0.032440186,-0.047332764,0.0021572113,0.037597656,0.009010315,-0.019104004,0.03967285,0.011817932,0.02178955,0.06695557,0.091308594,-0.020004272,-0.022216797,-0.051361084,-0.031402588,-0.076416016,0.050109863,-0.00223732,0.07714844,0.0385437,0.032440186,0.016860962,0.08496094,0.039123535,-0.026733398,0.044921875,0.034698486,-0.025970459,-0.046142578,0.09326172,-0.030349731,0.022888184,0.06933594,-0.04663086,0.049987793,-0.011802673,-0.015655518,0.013885498,-0.04559326,0.00554657,-0.032836914,-0.007724762,0.013305664,0.01574707,0.0519104,0.024887085,0.044128418,0.010795593,0.055603027,-0.054901123,-0.045837402,0.036468506,0.047302246,0.024810791,0.05645752,0.062805176,-0.07165527,0.027282715,-0.011566162,0.040283203,-0.019454956,-0.04058838,-0.032196045,0.024856567,0.0023899078,0.016921997,0.062316895,0.054748535,0.04498291,0.012550354,-0.0043678284,-0.01309967,0.010169983,0.011619568,-0.022766113,-0.032806396,0.016647339,0.049713135,0.03414917,-0.07495117,0.027526855,0.07147217,0.037597656,-0.015792847,-0.0010890961,0.024108887,0.015419006,0.041870117,-0.035705566,0.015434265,-0.038238525,-0.03668213,-0.0011501312,-0.0234375,0.013130188,-0.054748535,0.010276794,0.074523926,0.015075684,0.01109314,0.047698975,0.0103302,0.03717041,0.030288696,0.0032520294,-0.025756836,-0.019424438,-0.05316162,0.04171753,-0.061798096,-0.014389038,0.039855957,0.003578186,0.018844604,0.032562256,0.04837036,-0.0023269653,-0.08227539,0.0446167,-0.042022705,0.016723633,0.023666382,0.03869629,-0.053985596,0.0029525757,0.052124023,0.024337769,-0.03427124,0.0262146,0.057861328,-0.020095825,0.10736084,-0.08642578,-0.07116699,-0.06298828,-0.039398193,0.0034751892,-0.0927124,-0.03677368,-0.026733398,0.044769287,-0.020767212,-0.02268982,-0.020584106,0.015777588,0.1083374,0.015617371,-0.02407837,-0.009567261,0.03842163,-0.0597229,-0.044799805,-0.013885498,0.045318604,-0.070373535,-0.06274414,-0.02027893,0.015327454,0.060546875,-0.0058174133,0.056396484,-0.04336548,-0.036346436,-0.044036865,-0.08642578,-0.017105103,-0.060516357,0.06915283,-0.027557373,0.068359375,0.010856628,-0.007587433,0.043945312,-0.03378296,0.031951904,-0.044433594,0.009857178,-0.013473511,0.00003117323,-0.004611969,-0.006034851,0.021774292,0.018936157,0.036132812,-0.03555298,-0.014785767,0.057800293,-0.036895752,-0.050323486,-0.020721436,-0.016647339,0.030593872,-0.0025672913,-0.05908203,0.06549072,-0.0019426346,0.009986877,0.022094727,0.053375244,0.09222412,0.025772095,0.033935547,-0.0058135986,0.031402588,-0.00065660477,-0.019119263,-0.05419922,0.029510498,0.022735596,-0.04534912,0.018798828,0.04397583,0.03881836,0.0067596436,0.035339355,0.03378296,0.07281494,-0.037597656,-0.035064697,-0.006439209,0.015007019,-0.07867432,-0.015945435,-0.10491943,0.019180298,-0.04498291,0.0010375977,-0.020828247,-0.045959473,0.0075912476,0.007785797,-0.056365967,0.012489319,-0.01574707,-0.014175415,0.07446289,-0.0058784485,-0.015113831,0.0082473755,0.010566711,-0.026016235,-0.007232666,-0.0340271,-0.04611206,-0.05783081,0.027282715,0.00010895729,0.07336426,0.06262207,0.00085544586,0.011199951,0.018447876,0.023620605,0.016677856,0.0335083,0.057250977,0.0013751984,0.0012817383,0.07122803,-0.030960083,-0.0058670044,-0.079589844,0.031188965,-0.023361206,0.054901123,0.03768921,-0.049957275,0.03604126,0.038604736,-0.022659302,0.085510254,-0.03479004,-0.031951904,0.015419006,-0.024887085,0.025009155,-0.06262207,0.01574707,0.015945435,0.031982422,0.09954834,-0.032714844,0.00178051,-0.046905518,-0.011581421,-0.039367676,-0.013847351,0.028335571,0.012199402,0.024276733,0.010948181,-0.03036499,0.0079956055,0.06933594,0.0062446594,-0.055786133,-0.031677246,0.030548096,0.029815674,0.0021915436,-0.032196045,0.07525635,0.022064209,-0.014625549,0.03717041,-0.02999878,-0.072631836,0.0010557175,-0.07318115,-0.05340576,0.050201416,0.016723633,-0.06274414,0.042419434,-0.004119873,0.014839172,-0.042175293,-0.041503906,0.040527344,-0.03591919,0.036987305,-0.0005173683,0.072265625,-0.021835327,-0.01689148,-0.005054474,-0.03012085,-0.002128601,-0.0046463013,-0.011474609,0.041534424,0.026062012,-0.03286743,-0.024169922,-0.005432129,0.014198303,0.024719238,-0.034698486,-0.13830566,-0.037750244,0.0053977966,-0.015106201,-0.012840271,-0.01802063,-0.004131317,0.0043945312,0.00042247772,0.050933838,-0.015274048,-0.0026550293,-0.012771606,0.024887085,-0.012039185,0.014595032,-0.06744385,0.039398193,0.035858154,-0.049957275,0.084106445,-0.014602661,0.009880066,0.04208374,-0.0637207,-0.07525635,-0.015319824,-0.052612305,-0.0031909943,0.038116455,-0.019561768,-0.054351807,0.020751953,-0.024337769,-0.0069236755,0.04043579,0.061676025,-0.052978516,-0.0061302185,0.022094727,0.03894043,-0.03213501,-0.026260376,-0.03050232,0.019104004,0.06640625,0.0076293945,0.040527344,-0.0357666,0.007484436,0.008728027,0.09289551,-0.02178955,-0.07244873,-0.009490967,0.014511108,-0.0084991455,0.0057640076,-0.02168274,0.008926392,-0.084350586,-0.008476257,0.009986877,0.09112549,0.05078125,-0.08001709,-0.046722412,-0.050933838,0.04296875,0.09881592,0.072631836,-0.09197998,0.0047340393,-0.025939941,0.048919678,0.017501831,-0.0037174225,0.046661377,0.13549805,-0.08215332,0.0181427,-0.015930176,0.022705078,0.004009247,-0.030014038,0.014945984,-0.04776001,-0.008041382,-0.054473877,-0.042633057,0.037994385,-0.012489319,0.051116943,0.06933594,0.01612854,0.07757568,0.047698975,-0.019378662,-0.019744873,0.0015363693,-0.021011353,0.0021209717,0.07318115,0.061920166,0.026687622,-0.024475098,-0.027694702,-0.07867432,-0.02645874,0.02218628,0.007041931,-0.018508911,0.05105591,-0.01612854,-0.0007419586,0.018554688,0.025772095,0.0012435913,-0.010955811,-0.024887085,-0.088012695,0.0077705383,0.056915283,-0.011207581,0.0073928833,-0.011749268,-0.0021152496,-0.008453369,-0.019515991,0.020965576,0.042022705,-0.08166504,-0.07745361,0.009712219,-0.03048706,0.026046753]'
LIMIT 1;
```


# Migrate from Vercel Postgres SDK to the Neon serverless driver

---
title: Migrate from Vercel Postgres SDK to the Neon serverless driver
subtitle: Learn how to smoothly transition your application from using Vercel Postgres SDK to the Neon serverless driver
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2024-10-28T00:00:00.000Z'
updatedAt: '2024-10-28T00:00:00.000Z'
---

With Vercel Postgres transitioning to Neon's native integration in the [Vercel Marketplace](https://vercel.com/blog/introducing-the-vercel-marketplace), now is the perfect time to migrate from the Vercel Postgres SDK [(@vercel/postgres)](https://vercel.com/docs/storage/vercel-postgres/sdk) to the [Neon serverless driver](https://github.com/neondatabase/serverless).

## Why migrate?

Switching to the Neon serverless driver provides several advantages. It offers greater flexibility by allowing the choice between HTTP for single queries or WebSockets for transactions and full [node-postgres](https://node-postgres.com/) compatibility. Additionally, it enhances maintainability by relying on Neon's actively maintained, native database driver.

## Prerequisites

To begin, you’ll need:

- An existing application using the Vercel Postgres SDK
- A [Neon account](/docs/get-started/signing-up) (your Vercel Postgres database will automatically migrate to Neon)

## Migration Steps

### 1. Install the Neon serverless driver

Start by installing the Neon serverless driver in your project:

```bash
npm install @neondatabase/serverless
```

<Admonition type="important">
To ensure proper configuration, set your environment variable to `DATABASE_URL` when referencing the database URL in your code, especially if you're following this guide.
</Admonition>

### 2. Update your database connection

Replace your Vercel Postgres SDK imports and connection setup with the Neon serverless driver. You have two options:

#### Option A: Using HTTP (Recommended for simple queries)

```diff
import { sql } from '@vercel/postgres'; // [!code --]

import { neon } from '@neondatabase/serverless'; // [!code ++]
const sql = neon(process.env.DATABASE_URL!); // [!code ++]
```

#### Option B: Using WebSockets (Recommended for transactions)

```diff
import { db } from '@vercel/postgres'; // [!code --]

import ws from 'ws'; // [!code ++]
import { Pool, neonConfig } from '@neondatabase/serverless'; // [!code ++]

const pool = new Pool({ connectionString: process.env.DATABASE_URL }); // [!code ++]
neonConfig.webSocketConstructor = ws; // [!code ++]
```

### 3. Update your queries

Here are common query patterns and how to migrate them:

#### Simple Queries

```diff
# Vercel Postgres SDK
const { rows } = await sql`SELECT * FROM users WHERE id = ${userId}`; // [!code --]

# Neon HTTP
const rows = await sql`SELECT * FROM users WHERE id = ${userId}`; // [!code ++]

# Neon WebSockets
const { rows } = await pool.query('SELECT * FROM users WHERE id = $1', [userId]); // [!code ++]
```

#### Transactions

```diff
 import { db } from '@vercel/postgres'; // [!code --]

async function transferFunds(fromId: number, toId: number, amount: number) { // [!code --]
  const client = await db.connect(); // [!code --]
  try { // [!code --]
    await client.query('BEGIN'); // [!code --]
    await client.query('UPDATE accounts SET balance = balance - $1 WHERE id = $2', [ // [!code --]
      amount, // [!code --]
      fromId, // [!code --]
    ]); // [!code --]
    await client.query('UPDATE accounts SET balance = balance + $1 WHERE id = $2', [amount, toId]); // [!code --]
    await client.query('COMMIT'); // [!code --]
  } catch (e) { // [!code --]
    await client.query('ROLLBACK'); // [!code --]
    throw e; // [!code --]
  } finally { // [!code --]
    client.release(); // [!code --]
  } // [!code --]
} // [!code --]

import { Pool } from '@neondatabase/serverless'; // [!code ++]

async function transferFunds(fromId: number, toId: number, amount: number) { // [!code ++]
  const pool = new Pool({ connectionString: process.env.DATABASE_URL }); // [!code ++]
  try { // [!code ++]
    await pool.query('BEGIN'); // [!code ++]
    await pool.query('UPDATE accounts SET balance = balance - $1 WHERE id = $2', [amount, fromId]); // [!code ++]
    await pool.query('UPDATE accounts SET balance = balance + $1 WHERE id = $2', [amount, toId]); // [!code ++]
    await pool.query('COMMIT'); // [!code ++]
  } catch (e) { // [!code ++]
    await pool.query('ROLLBACK'); // [!code ++]
    throw e; // [!code ++]
  } finally { // [!code ++]
    await pool.end(); // [!code ++]
  } // [!code ++]
} // [!code ++]
```

## Best practices

1.  **Choose the right connection method**:
    - Use HTTP (`neon()`) for single queries and simple transactions.
    - Use WebSockets (`Pool`) for complex transactions and session-based operations.

2.  **Connection management**:
    - For HTTP queries, reuse the `sql` query function.
    - For WebSocket connections in serverless environments, always close connections:

    ```typescript
    const pool = new Pool({ connectionString: process.env.DATABASE_URL });
    try {
      // Your queries here
    } finally {
      await pool.end();
    }
    ```

3.  **Error Handling**:
    ```typescript
    try {
      const result = await sql`SELECT * FROM users`;
      return result;
    } catch (error) {
      console.error('Database error:', error);
      throw new Error('Failed to fetch users');
    }
    ```

## Working with ORMs

Neon's serverless driver is compatible with popular ORMs like Prisma and Drizzle ORM. Check out the following guides to learn more:

<DetailIconCards>

<a href="/docs/guides/prisma" description="Learn how to connect to Neon from Prisma" icon="prisma">Prisma</a>

<a href="https://orm.drizzle.team/docs/tutorials/drizzle-with-neon" description="Learn how to connect to Neon from Drizzle ORM" icon="drizzle">Drizzle ORM</a>

</DetailIconCards>

## Advanced Configuration

For most cases, using neon serverless driver is straightforward without needing advanced configuration. However, for custom setups or troubleshooting, here are the key options:

- **poolQueryViaFetch**: Setting `poolQueryViaFetch` to true sends `Pool.query()` calls as low-latency `HTTP` fetch requests (currently defaults to false).

- **wsProxy**: This option is for connecting via a WebSocket proxy deployed in front of your your own Postgres instance, which allows you to use the Neon serverless driver with a local development environment.

For more information about these options, see [Advanced configuration](https://github.com/neondatabase/serverless/blob/main/CONFIG.md#advanced-configuration).

<NeedHelp/>


# Get started with Windsurf and Neon Postgres MCP Server

---
title: 'Get started with Windsurf and Neon Postgres MCP Server'
subtitle: 'Make schema changes with natural language using Codeium Windsurf and Neon MCP Server'
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-02-22T00:00:00.000Z'
updatedOn: '2025-02-22T00:00:00.000Z'
---

Imagine adjusting your database schema simply by describing the change in plain English. This is possible by combining [Codeium Windsurf](https://codeium.com/windsurf) with the [Neon MCP Server](https://github.com/neondatabase/mcp-server-neon).

This guide demonstrates how to use [Windsurf's Cascade](https://docs.codeium.com/windsurf/cascade) and Neon's MCP server to perform database migrations in your Neon project.

<Admonition type="important" title="Neon MCP Server Security Considerations">
The Neon MCP Server grants powerful database management capabilities through natural language requests. **Always review and authorize actions requested by the LLM before execution.** Ensure that only authorized users and applications have access to the Neon MCP Server.

The Neon MCP Server is intended for local development and IDE integrations only. **We do not recommend using the Neon MCP Server in production environments.** It can execute powerful operations that may lead to accidental or unauthorized changes.

For more information, see [MCP security guidance →](/docs/ai/neon-mcp-server#mcp-security-guidance).
</Admonition>

## Key components

Let's break down the key components in this setup:

- **Codeium Windsurf**: Windsurf is Codeium's next-generation AI IDE, featuring Cascade, an agentic chatbot that supports Model Context Protocol (MCP) for interfacing with external tools.

- **Neon MCP Server**: Neon's MCP server acts as a bridge between MCP clients like Windsurf and [Neon's API](https://api-docs.neon.tech/reference/getting-started-with-neon-api), letting you work with Neon databases using natural language commands.

- **Model Context Protocol (MCP)**: MCP is a lightweight communication standard that allows Windsurf and Neon MCP Server to work together.

## Setting up Neon MCP Server in Windsurf

You have two options for connecting Windsurf to the Neon MCP Server:

1. **Remote MCP Server (Preview):** Connect to Neon's managed MCP server using OAuth for authentication. This method is more convenient as it eliminates the need to manage API keys in Windsurf. Additionally, you will automatically receive the latest features and improvements as soon as they are released.

2. **Local MCP Server:** Run the Neon MCP server locally on your machine, authenticating with a Neon API key.

### Prerequisites

Before you begin, ensure you have the following:

1.  **Codeium Windsurf Editor:** Download and install Windsurf from [codeium.com/windsurf](https://codeium.com/windsurf).
2.  **A Neon Account and Project:** You'll need a Neon account and a project. You can quickly create a new Neon project here [pg.new](https://pg.new)
3.  **Neon API Key (for Local MCP server):** After signing up, get your Neon API Key from the [Neon console](https://console.neon.tech/app/settings/api-keys). This API key is needed to authenticate your application with Neon. For instructions, see [Manage API keys](/docs/manage/api-keys).

    <Admonition type="important" title="Neon API Key Security">
    Keep your Neon API key secure, and never share it publicly. It provides access to your Neon projects.
    </Admonition>

4.  **Node.js (>= v18) and npm:** Ensure Node.js (version 18 or later) and npm are installed. Download them from [nodejs.org](https://nodejs.org).

### Option 1: Setting up the Remote Hosted Neon MCP Server

This method uses Neon's managed server and OAuth authentication.

You can either watch the video below or follow the steps to set up the Neon MCP server in Windsurf.

<video controls playsInline loop width="800" height="600">
  <source type="video/mp4" src="https://neondatabase.wpengine.com/wp-content/uploads/2025/04/neon-hosted-mcp-server.mp4"/>
</video>

<Admonition type="note">
By default, the Remote MCP Server connects to your personal Neon account. To connect to an organization's account, you must authenticate with an API key. For more information, see [API key-based authentication](/docs/ai/neon-mcp-server#api-key-based-authentication).
</Admonition>

1. Open Windsurf.
2. Open Cascade by using `⌘L` on MacOS or `Ctrl+L` on Windows/Linux.
3. Click on the hammer icon (🔨), then click the **"Configure"** button.
   ![Windsurf Configure MCP](/docs/guides/windsurf-configure-mcp.png)
4. This will open up the "Manage MCPs" file in Windsurf.
5. Click on "View raw config" to open the Windsurf's MCP configuration file.
6. Add the "Neon" server entry within the `mcpServers` object:

   ```json
   {
     "mcpServers": {
       "Neon": {
         "command": "npx",
         "args": ["-y", "mcp-remote@latest", "https://mcp.neon.tech/mcp"]
       }
     }
   }
   ```

   If you have other MCP servers configured, you can copy just the Neon part.

7. **Save** the MCP configuration file.
8. Click **"Refresh"** (🔄) in the MCP toolbar in Windsurf Cascade.
9. An OAuth window will open in your browser. Follow the prompts to authorize Windsurf to access your Neon account.
   ![Neon OAuth window](/docs/guides/neon-oauth-window.png)
10. You can verify that the connection is successful by checking the available MCP servers in Cascade. The toolbar should indicate that you have MCP servers available, and you should see "1 available MCP server" (or more if you configured additional servers).

    ![Windsurf MCP Toolbar with Server Available](/docs/guides/windsurf-mcp-server-available.png)

11. Windsurf is now connected to the Neon MCP server.

<Admonition type="note">
The remote hosted MCP server is in preview due to the [new OAuth MCP specification](https://spec.modelcontextprotocol.io/specification/2025-03-26/basic/authorization/), expect potential changes as we continue to refine the OAuth integration.
</Admonition>

<Admonition type="tip" title="Troubleshooting OAuth Errors">
If you encounter an error message like `{"code":"invalid_request","error":"invalid redirect uri"}` when starting Windsurf with the remote MCP server, this is typically due to cached OAuth credentials. To fix this issue:

1. Remove the MCP authentication cache by running: `rm -rf ~/.mcp-auth`
2. Restart Windsurf
3. The OAuth flow will start fresh, allowing you to properly authenticate

This error commonly occurs when there are changes to the OAuth configuration or when cached credentials become invalid.
</Admonition>

### Option 2: Setting up the Local Neon MCP Server

This method runs the Neon MCP server locally on your machine, using a Neon API
key for authentication.

1. Open Windsurf.
2. Open Cascade by using `⌘L` on MacOS or `Ctrl+L` on Windows/Linux.
3. Click on the hammer icon (🔨), then click the **"Configure"** button.
   ![Windsurf Configure MCP](/docs/guides/windsurf-configure-mcp.png)
4. This will open up the "Manage MCPs" file in Windsurf.
5. Click on "View raw config" to open the Windsurf's MCP configuration file.
6. Add the "Neon" server entry within the `mcpServers` object:

   <CodeTabs labels={["MacOS/Linux", "Windows", "Windows (WSL)"]}>

   ```json
   {
     "mcpServers": {
       "Neon": {
         "command": "npx",
         "args": ["-y", "@neondatabase/mcp-server-neon", "start", "<YOUR_NEON_API_KEY>"]
       }
     }
   }
   ```

   ```json
   {
     "mcpServers": {
       "Neon": {
         "command": "cmd",
         "args": [
           "/c",
           "npx",
           "-y",
           "@neondatabase/mcp-server-neon",
           "start",
           "<YOUR_NEON_API_KEY>"
         ]
       }
     }
   }
   ```

   ```json
   {
     "mcpServers": {
       "Neon": {
         "command": "wsl",
         "args": ["npx", "-y", "@neondatabase/mcp-server-neon", "start", "<YOUR_NEON_API_KEY>"]
       }
     }
   }
   ```

   </CodeTabs>

   > Replace `<YOUR_NEON_API_KEY>` with your actual Neon API key which you obtained from the [prerequisites](#prerequisites) section:

   If you have other MCP servers configured, you can copy just the `Neon` part.

7. **Save** the MCP configuration file.
8. Click **"Refresh"** (🔄) in the MCP toolbar in Windsurf Cascade to refresh the configuration.
9. You can verify that the connection is successful by checking the available MCP servers in Cascade. The toolbar should indicate that you have MCP servers available, and you should see "1 available MCP server" (or more if you configured additional servers).
   ![Windsurf MCP Toolbar with Server Available](/docs/guides/windsurf-mcp-server-available.png)

   You've now configured Neon MCP Server in Windsurf and can manage your Neon Postgres databases using AI.

<MCPTools />

These actions enable any MCP client like Windsurf to interact with various functionalities of the **Neon platform via the Neon API.** Certain tools, especially database migration ones, are tailored for AI agent and LLM usage, leveraging Neon's branching for safe preview and commit.

## Development Workflow: Adding a Column with Windsurf and Neon MCP

Let's walk through a typical development scenario: Quickly adding a column for prototyping using natural language within Windsurf's Cascade. The following chat log demonstrates a real interaction with Windsurf and the Neon MCP server.

**Scenario:** During development, you decide to track timestamps for entries in your `playing_with_neon` table. You want to quickly add a `created_at` column.

<Admonition type="tip" title="Security Reminder">
Be aware that Cascade currently executes commands directly from your prompts without confirmation unlike other IDE's and apps like [Cursor](/guides/cursor-mcp-neon) and [Claude](/guides/neon-mcp-server).  Review your requests thoroughly to avoid unintended or unwanted actions.
</Admonition>

Following is a sample interaction with Cascade where you can see how it uses the Neon MCP server to add a new column to your database table:

```text shouldWrap
User: in my neon project id: fancy-bush-59303206, list all the tables
Cascade: I'll help you list all the tables in your Neon project. I'll use the default database name 'neondb' since it wasn't specified.
> MCP Tool: neon / get_database_tables
> I found 1 table in your Neon project:
> - playing_with_neon (in the public schema)
> Would you like to see more details about this table or perform any operations on it?

User: can you show me first 5 rows?
Cascade: I'll query the first 5 rows from the playing_with_neon table.
> MCP Tool: neon / run_sql
> Here are the first 5 rows from the table. The table has three columns:
> id: An integer identifier
> name: A string column containing what appears to be hash-like values
> value: A numeric column with decimal values between 0 and 1
> Would you like to perform any other operations on this table?

User: can you add created_at column?
Cascade: I'll help you add a created_at column to the playing_with_neon table. I'll create a migration to add this column with a timestamp data type.
> MCP Tool: neon / prepare_database_migration
> Let me verify the changes in the temporary branch to make sure the column was added correctly.
> I've verified that the created_at column has been added successfully in the temporary branch. Would you like me to commit this migration to the production branch?
> The migration has been completed successfully! The created_at column has been added to the playing_with_neon table. Let's verify the change in the production branch:
> Perfect! The created_at column has been successfully added and populated with timestamps. For existing rows, it was set to the time when the column was added. Any new rows added to the table will automatically get their creation timestamp.
```

**Key takeaways:**

- **Natural language interaction:** You can use simple, conversational English to interact with your database through Windsurf's Cascade.
- **MCP Tools in Action:** The underlying MCP tools (`get_database_tables`, `run_sql`, `prepare_database_migration`, `complete_database_migration`) illustrate the automated process.
- **Branching for safety:** The agent automatically creates a temporary branch for schema changes and tests them before committing to the production branch.

You can verify the changes in your Neon Console or by querying the database directly.

![Neon Console with added column](/docs/guides/neon-cursor-mcp-added-column.png)

<Admonition type="note">
While the Neon MCP server allows you to utilize all of Neon's functionalities, you can also use a [generic Postgres MCP server](https://github.com/modelcontextprotocol/servers/tree/main/src/postgres) to connect to Neon.
</Admonition>

## Bonus: Creating a project and branch

Beyond schema changes, you can also manage your Neon projects and branches directly within Windsurf using natural language commands. This can be useful for quickly setting up a new development or test environment.

### Creating a new project

Let's say you want to create a new Neon project named "my-new-project". You can simply tell Cascade:

```text shouldWrap
User: Create a Neon project named "my-new-project"
Cascade: I'll help you create a new Neon project with the specified name.
> MCP Tool: neon / create_project
> Tool Arguments: { "name": "my-new-project" }
> Great! I've created a new Neon project for you with the following details:
> Project Name: `my-new-project`
> Project ID: `restless-butterfly-08800888`
> Default Branch: `main`
> Default Database: `neondb`
> The project is now ready for use. You can start creating tables, running queries, or creating additional databases as needed. Would you like to do anything specific with your new project?
```

![Windsurf creating a new Neon project](/docs/guides/windsurf-mcp-create-new-project.png)

Cascade will then use the `create_project` MCP tool to initiate the project creation. It will provide you with a project ID and name.

### Creating a New Branch

Once you have a project, you can easily create new branches for development or experimentation. For example, to create a branch named "feature-x" in your "my-new-project" project:

```text shouldWrap
User: Create a branch named "feature-x"
Cascade: I'll create a new branch named "feature-x" in your Neon project.
> MCP Tool: neon / create_branch
> Tool Arguments: { "projectId": "restless-butterfly-08800888", "branchName": "feature-x" }
> Perfect! I've created a new branch with the following details:
> Branch Name: `feature-x`
> Branch ID: `br-aged-sun-a5woam3c`
> Project ID: `restless-butterfly-08800888`
> The branch has been created successfully and is ready for use. You can now make changes to this branch without affecting the production branch. Would you like to do anything with this new branch?
```

![Windsurf creating a new Neon branch](/docs/guides/windsurf-mcp-create-new-branch.png)

Cascade will use the `create_branch` MCP tool to create the branch and provide you with the branch name and ID. Notice how we don't need to specify the project ID, as Cascade remembers the active project context.

## Conclusion

Windsurf with Neon MCP Server lets you use natural language to interact with your database and take advantage of Neon's branching capabilities for fast iteration. This approach is ideal for quickly testing database ideas and making schema changes during development.

## Resources

- [MCP Protocol](https://modelcontextprotocol.org)
- [Codeium Windsurf Documentation](https://docs.codeium.com/windsurf/getting-started)
- [Neon Docs](/docs)
- [Neon API Keys](/docs/manage/api-keys#creating-api-keys)
- [Neon MCP server GitHub](https://github.com/neondatabase/mcp-server-neon)

<NeedHelp/>


# Using Neon Postgres with Zapier

---
title: Using Neon Postgres with Zapier
subtitle: Automate workflows by connecting Neon Postgres to hundreds of apps with Zapier, triggering actions from database events or pushing data into Neon from other services.
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-05-29T00:00:00.000Z'
updatedOn: '2025-05-29T00:00:00.000Z'
---

Zapier is a powerful no-code automation platform that allows you to connect Neon Postgres to thousands of other web services. By linking your Neon database with apps like Slack, Google Sheets, Gmail, Stripe, or Typeform, you can automate actions based on database events (e.g., a new row is added) or push data into Neon from these external systems.

This guide will walk you through setting up two common automation scenarios:

1.  Triggering an action when a new row is added to a table.
2.  Adding data to a table based on an event in an external service.

These examples will illustrate the core concepts, which you can then adapt to a wide variety of other use cases.

## Prerequisites

Before you begin, ensure you have the following:

- **Zapier Account:** A Zapier account is required to create and manage Zaps. Please note that the PostgreSQL integration is a Pro feature on Zapier and requires a [paid plan](https://zapier.com/pricing).

* **Neon Account and Project:** A Neon account and a project with a running Postgres database. You can create a free Neon account and project at [pg.new](https://pg.new).
* **Database tables (for examples):** For the examples in this guide, we'll be using the following tables to demonstrate the functionality. Create these tables in your Neon database if you intend to follow along:
  - A table named `users` to demonstrate triggering actions from new rows.
  - A table named `form_submissions` to demonstrate adding data from an external source.

    You can create these tables using the [Neon SQL Editor](/docs/get-started/query-with-neon-sql-editor) or any Postgres client such as [`psql`](/docs/connect/query-with-psql-editor)

    **Example SQL for `users` table:**

        ```sql
        CREATE TABLE users (
            id SERIAL PRIMARY KEY,
            name VARCHAR(255),
            email VARCHAR(255) UNIQUE,
            signed_up_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
        );
        ```

    **Example SQL for `form_submissions` table:**

        ```sql
        CREATE TABLE form_submissions (
            id SERIAL PRIMARY KEY,
            submitter_email VARCHAR(255),
            feedback_text TEXT,
            submitted_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
        );
        ```

<Admonition type="important">
For Zapier's "New Row" trigger to reliably detect new entries in Neon, your table should have an auto-incrementing `PRIMARY KEY` (like `SERIAL` or `BIGSERIAL`) or a column that strictly orders new rows (like a `created_at` timestamp). Zapier uses this "Ordering Column" to check for new entries.
</Admonition>

## Connecting Neon Postgres to Zapier

Before creating Zaps, you need to connect your Neon database to Zapier. Zapier uses a generic "PostgreSQL" app integration. Due to how [Neon uses Server Name Indication (SNI) for routing connections](/docs/connect/connection-errors#the-endpoint-id-is-not-specified) and how some clients handle SNI, a specific format is required for the password field in Zapier to ensure a successful connection.

1.  **Log in to Zapier.**
2.  Navigate to "**App Connections**" from the left sidebar.
    ![App Connections in Zapier](/docs/guides/zapier-app-connections.png)
3.  Click "**Add connection**" and search for "**PostgreSQL**".
    ![Add connection page in Zapier](/docs/guides/zapier-add-connection.png)
4.  A pop-up window will appear asking for connection details. You can find most of these in your Neon Console on the **Dashboard** page, by clicking on the **Connect** button for your database. Fill in the following fields:
    - **Host:** Your Neon host (e.g., `ep-tight-boat-a6aplura-pooler.us-west-2.aws.neon.tech`)
    - **Port:** `5432`
    - **Database:** Your Neon database name (e.g., `neondb`)
    - **Username:** Your Neon database user (e.g., `neon_user`)
    - **Password:** **This is where the special format is needed.** See the important note below.

    <Admonition type="important" title="Password Format for Neon Postgres in Zapier">
    To connect Zapier to Neon successfully, you must include your Neon **Endpoint ID** within the password field. This is because Neon uses SNI to route connections, and some clients like Zapier's PostgreSQL connector do not pass SNI information in a way that Neon can use directly without this workaround.
    1.  Find your **Endpoint ID**. It's the first part of your Neon hostname (e.g., if your host is `ep-tight-boat-a6aplura-pooler.us-west-2.aws.neon.tech`, your endpoint ID is `ep-tight-boat-a6aplura`).
    2.  In Zapier's **Password** field, enter the following string, replacing `[endpoint_id]` with your actual endpoint ID and `[your_actual_password]` with your database user's password:

        `endpoint=[endpoint_id]$[your_actual_password]`

    **Example:** If your endpoint ID is `ep-tight-boat-a6aplura` and your password is `MySecurePassword`, you would enter:

    `endpoint=ep-tight-boat-a6aplura$MySecurePassword`

    This format allows Zapier to connect to your Neon database while adhering to the SNI requirements.
    </Admonition>

    <Admonition type="important" title="Security Best Practice">
    It is strongly recommend to create a dedicated database user/role in Neon specifically for Zapier. Grant this user only the minimum necessary permissions on the specific tables Zapier will interact with.
    </Admonition>

5.  After filling in all fields, including the specially formatted password, click "**Yes, Continue to PostgreSQL**". Zapier will test the connection.
6.  If the connection is successful, you should see **PostgreSQL** listed under your connected apps in Zapier. You can now use this connection in your Zaps.
    ![PostgreSQL connection in Zapier](/docs/guides/zapier-postgresql-connection.png)

## Use case 1: Notify on new Database entries

Let's create a Zap that sends a Slack message whenever a new user is added to `users` table in Neon.

### Step 1: Setting up the Trigger (new row in Neon)

1. Navigate to your Zapier dashboard and click "**Create > Zap**" to create a new Zap.
   ![Create Zap in Zapier](/docs/guides/zapier-create-zap.png)

2. **Trigger Setup:**
   - Search for and select "**PostgreSQL**" as the trigger app.
   - For "Event", choose "**New Row**".
   - For "Account", select the Neon PostgreSQL connection you configured earlier. Click "Continue".
     ![PostgreSQL trigger in Zapier](/docs/guides/zapier-postgresql-trigger.png)
   - You will be prompted to set up the trigger. Fill in your trigger details.
   - **Table:** Select or type the name of your table (e.g., `users`).
   - **Order By:** Select the column Zapier should use to find new rows. An auto-incrementing primary key like `id` or a timestamp like `signed_up_at` is ideal. `id` column is used in this example.
3. Click "**Continue**".
   ![PostgreSQL trigger setup in Zapier](/docs/guides/zapier-postgresql-trigger-setup.png)

4. Add sample data to your `users` table in Neon if you haven't already. Use the following SQL command in the Neon SQL Editor or any Postgres client to insert a sample user:

   ```sql
   INSERT INTO users (name, email) VALUES ('John Doe', 'john@doe.com');
   ```

5. **Test trigger:** Click "**Test trigger**". Zapier will attempt to find a recent row in your `users` table.
   You should see data from the row appear. Click "**Continue with selected record**".
   ![PostgreSQL trigger test in Zapier](/docs/guides/zapier-postgresql-trigger-test.png)

### Step 2: Setting up the Action (Send Slack message)

1.  **Action Setup:**
    - Search for and select "**Slack**" as the action app.
    - For "Event", choose "**Send Channel Message**".
    - **Connect Slack Account:** If you haven't already, connect your Slack account and grant Zapier permissions.
    - You will be prompted to set up the action. Fill in your action details.
    - **Channel:** Choose the Slack channel where you want to send notifications.
    - **Message Text:** Compose your message. You can insert data from the Neon trigger step.
    - You can use the "/" button to insert fields from the trigger data. For example, you might write:

      ```
      New user signed up: *{{name}}* ({{email}})
      ```

      This will dynamically insert the user's name and email from the Neon trigger data. Refer to the image below for an example.

      ![Slack action setup in Zapier](/docs/guides/zapier-slack-action-setup.png)

    - Configure other options like "Bot Name", "Bot Icon", etc., as desired.

2.  Click "**Continue**".
3.  **Test action:** Click "**Test step**". Zapier will send a sample message to your selected Slack channel using the data from the trigger test.
    ![Zapier message test in Slack](/docs/guides/zapier-slack-test-message.png)
4.  If the test is successful, click "**Publish Zap**".

Now, whenever a new row is added to your `users` table in Neon Postgres, a message will automatically be posted to your specified Slack channel.

<Admonition type="note" title="Trigger frequency">
Zapier uses a polling system for its "New Row" trigger, checking Postgres for new data every 2-15 minutes (depending on your Zapier plan), not in real-time. This means a new row added to Neon may take a few minutes to trigger your Zap and send the Slack message.
</Admonition>

## Use case 2: Log Form submissions into Neon

Let's create a Zap that adds a new row to our `form_submissions` table in Neon whenever a Google Form is submitted.

### Step 1: Setting up the Trigger (New Google Form response)

1.  In Zapier, click "**Create Zap**".
2.  Search for and select "**Google Forms**" as the trigger app.
3.  For "Trigger Event", choose "**New Form Response**". Click "Continue".
4.  **Connect Google Forms Account:** If you haven't already, connect your Google account and grant Zapier permissions.
5.  Select the Google Form you want to use.
6.  Click "**Continue**".
7.  Make a test submission in your Google Form to ensure there is data for Zapier to work with. You can do this by filling out the form and submitting it.
8.  **Test trigger:** Click "**Test trigger**". Zapier will attempt to find a recent form submission. Click "**Continue with selected record**".

### Step 2: Setting up the Action (Create row in Neon)

1.  **Action Setup:**
    - Search for and select "**PostgreSQL**" as the action app.
    - For "Event", choose "**New Row**". Click "Continue".
    - For "Account", select the Neon PostgreSQL connection you configured earlier. Click "Continue".
    - You will be prompted to set up the action. Fill in your action details.
    - **Table:** Select or type the name of your table (e.g., `form_submissions`).
    - **Map columns:** You will see a list of columns from your `form_submissions` table. For each column, you need to map the corresponding data from the Google Forms trigger.
      - For `submitter_email`, select the Google Forms field that collects the email (e.g., `{{Email}}`).
      - For `feedback_text`, select the Google Forms field for feedback (e.g., `{{Feedback}}`).
      - The `id` and `submitted_at` columns in our example `form_submissions` table have default values (SERIAL and CURRENT_TIMESTAMP respectively), so you can often leave them unmapped in Zapier, and Postgres will handle them. If your table structure requires them, map them accordingly.
        ![Google Forms to Neon mapping in Zapier](/docs/guides/zapier-google-forms-to-neon-mapping.png)
2.  Click "**Continue**".
3.  **Test action:** Click "**Test step**". Zapier will attempt to create a new row in your `form_submissions` table in Neon using the data from the Google Forms test.
4.  **Verify in Neon:** Check your `form_submissions` table in Neon to confirm the new row was added.
5.  If the test is successful and the data appears in Neon, click "**Publish Zap**".

Now, every time your Google Form is submitted, the data will be automatically logged into your Neon Postgres database.

## Expanding to other use cases

The two examples above demonstrate the fundamental patterns for integrating Neon with other services via Zapier:

1.  **Neon as a Trigger:** An event in your Neon database (like a new row or an updated row) initiates actions in other apps.
2.  **Neon as an Action:** An event in an external app (like a new email, a new Stripe payment, an Airtable update) results in data being created or updated in your Neon database.

You can adapt these patterns to automate a vast array of tasks. Here are some additional use cases you might consider:

- **Notify on new Database entries:** For example: Neon -> Email, Neon -> Discord.
- **Log Form submissions into Neon:** For example: Typeform -> Neon, JotForm -> Neon.
- **Update Google Sheets from Neon:**
  - Trigger: New/Updated Row in Neon.
  - Action: Create/Update Row in Google Sheets.
- **Sync Stripe payments or subscriptions:**
  - Trigger: New Stripe charge/subscription.
  - Action: Create row in Neon.
- **Send custom emails from data in Neon:**
  - Trigger: New/Updated Row in Neon (e.g., a user signs up).
  - Action: Create a personalized email using OpenAI and send it via Mailchip/Resend etc.

The process for building these Zaps will be very similar:

1.  Choose your **Trigger** app and event.
2.  Choose your **Action** app and event.
3.  Connect your app accounts.
4.  Configure the trigger and action, mapping data fields between the services.
5.  Test and publish your Zap.

## Troubleshooting

If you encounter issues connecting Neon to Zapier or if your Zaps involving Neon are not working as expected, consider the following:

- **Verify password format:** Ensure you are using the correct password format when connecting Neon to Zapier, which includes the `endpoint=[endpoint_id]$` prefix before your actual password. Refer to the details in the [Connecting Neon Postgres to Zapier](#connecting-neon-postgres-to-zapier) section for the exact structure. An incorrect password format is a common reason for connection failures.

- **Specific Errors:**

  If you encounter an error message stating: "**Your Zap could not be turned on - AppVersions using SQL Zero require static-ip pool types**" when trying to activate or run a Zap involving the PostgreSQL connection.
  - **Observation:** This issue appears to be related to the Zapier platform's handling of PostgreSQL connections and can sometimes occur without any changes made to your Zap configuration. It has been [reported by users in the Zapier community](https://community.zapier.com/troubleshooting-99/your-zap-could-not-be-turned-on-appversions-using-sql-zero-require-static-ip-pool-types-47107).
  - **Recommended Action:** If you encounter this specific error, and you've confirmed your connection details (including the password format) are correct, the most effective course of action is to **contact Zapier Support directly** as described in the above community post. You can contact them through the [Zapier Support page](https://zapier.com/app/get-help).

## Conclusion

Zapier provides a user-friendly way to connect your Neon Postgres database to the wider ecosystem of cloud applications, enabling powerful automations without writing code. By understanding the trigger and action model, you can streamline workflows, synchronize data, and save significant time.

## Resources

- [Zapier](https://zapier.com)
- [PostgreSQL App on Zapier](https://zapier.com/apps/postgresql/integrations)
- [Zapier example templates for PostgreSQL](https://zapier.com/apps/postgresql/integrations#zap-template-list)
- [Neon Documentation](/docs)

<NeedHelp/>


# Get started with Zed and Neon Postgres MCP Server

---
title: 'Get started with Zed and Neon Postgres MCP Server'
subtitle: 'Make schema changes with natural language using Zed and Neon MCP Server'
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-04-10T00:00:00.000Z'
updatedOn: '2025-04-10T00:00:00.000Z'
---

Imagine you could interact with your database using plain English, whether you're asking for specific data or changing its schema. That's what the [Neon MCP Server](https://github.com/neondatabase/mcp-server-neon) allows you to do. It lets you manage your Neon Postgres databases using everyday language, simplifying tasks like running queries and performing database migrations.

In this guide, we'll explore how to set up the Neon MCP Server within [Zed](https://zed.dev), a next-generation AI-powered code editor, to handle various database operations. These include creating projects, managing database branches, running SQL queries, and performing safe database migrations.

<Admonition type="important" title="Neon MCP Server Security Considerations">
The Neon MCP Server grants powerful database management capabilities through natural language requests. **Always review and authorize actions requested by the LLM before execution.** Ensure that only authorized users and applications have access to the Neon MCP Server.

The Neon MCP Server is intended for local development and IDE integrations only. **We do not recommend using the Neon MCP Server in production environments.** It can execute powerful operations that may lead to accidental or unauthorized changes.

For more information, see [MCP security guidance →](/docs/ai/neon-mcp-server#mcp-security-guidance).
</Admonition>

<Admonition type="note">
MCP support in Zed is currently in **preview**. Ensure you're using the Preview version of Zed to add MCP servers. You can download the **Preview** version from [zed.dev/releases/preview](https://zed.dev/releases/preview).
</Admonition>

## Setting up Neon MCP Server in Zed

You have two options for connecting Zed to the Neon MCP Server:

1.  **Remote MCP Server (Preview):** Connect to Neon's managed MCP server using OAuth for authentication. This method is more convenient as it eliminates the need to manage API keys in Zed. Additionally, you will automatically receive the latest features and improvements as soon as they are released.

2.  **Local MCP Server:** Run the Neon MCP server locally on your machine, authenticating with a Neon API key.

### Prerequisites

Before you begin, ensure you have the following:

1.  **Zed editor:** Download and install preview version of Zed from [zed.dev/releases/preview](https://zed.dev/releases/preview).
2.  **A Neon account and project:** You'll need a Neon account and a project. You can quickly create a new Neon project here [pg.new](https://pg.new)
3.  **Neon API Key (for Local MCP server):** After signing up, get your Neon API Key from the [Neon console](https://console.neon.tech/app/settings/api-keys). This API key is needed to authenticate your application with Neon. For instructions, see [Manage API keys](/docs/manage/api-keys).

    <Admonition type="warning" title="Neon API Key Security">
    Keep your Neon API key secure, and never share it publicly. It provides access to your Neon projects.
    </Admonition>

4.  **Node.js (>= v18) and npm:** Ensure Node.js (version 18 or later) and npm are installed. Download them from [nodejs.org](https://nodejs.org).

<Admonition type="note">
In Zed, MCP servers are known as **context servers**. These context servers enable context-aware capabilities, facilitating Zed's interaction with external systems such as Neon.
</Admonition>

### Option 1: Setting up the Remote Hosted Neon MCP Server

<Admonition type="note">
By default, the Remote MCP Server connects to your personal Neon account. To connect to an organization's account, you must authenticate with an API key. For more information, see [API key-based authentication](/docs/ai/neon-mcp-server#api-key-based-authentication).
</Admonition>

This method uses Neon's managed server and OAuth authentication.

1.  Open Zed.
2.  Click the Assistant (✨) icon in the bottom right corner of Zed.
    ![Zed Assistant icon](/docs/guides/zed/assistant-icon.png)
3.  Click **Settings** in the top right panel of the Assistant.
    ![Zed Assistant settings](/docs/guides/zed/assistant-settings.png)
4.  In the **Context Servers** section, click **+ Add Context Server**.
    ![Zed add context server](/docs/guides/zed/add-context-server.png)
5.  Configure Neon Server:
    - Enter **Neon** in the **Name** field.
    - In the **Command** field, enter:
      ```bash
      npx -y mcp-remote https://mcp.neon.tech/sse
      ```
      ![Zed add Neon Remote MCP server](/docs/guides/zed/add-neon-remote-mcp-server.png)
    - Click **Add Server**.
6.  An OAuth window will open. Follow the prompts to authorize Zed to access your Neon account
    ![Neon OAuth window](/docs/guides/neon-oauth-window.png)

7.  Check the Context Servers section in Zed **Settings** to ensure the connection is successful. Neon should be listed as a context server.
    ![Zed with Neon MCP Tools](/docs/guides/zed/with-neon-mcp-tools.png)

8.  Zed is now connected to Neon's remote MCP server.

<Admonition type="note">
The remote hosted MCP server is in preview due to the [new OAuth MCP specification](https://spec.modelcontextprotocol.io/specification/2025-03-26/basic/authorization/), expect potential changes as we continue to refine the OAuth integration.
</Admonition>

### Option 2: Setting up the Local Neon MCP Server

This method runs the Neon MCP server locally on your machine, using a Neon API key for authentication.

1.  Open Zed.
2.  Click the Assistant (✨) icon in the bottom right corner of Zed.
    ![Zed Assistant icon](/docs/guides/zed/assistant-icon.png)
3.  Click **Settings** in the top right panel of the Assistant.
    ![Zed Assistant settings](/docs/guides/zed/assistant-settings.png)
4.  In the **Context Servers** section, click **+ Add Context Server**.
    ![Zed add context server](/docs/guides/zed/add-context-server.png)
5.  Configure Neon Server:
    - Enter **Neon** in the **Name** field.
    - In the **Command** field, enter:
      ```bash
      npx -y @neondatabase/mcp-server-neon start <YOUR_NEON_API_KEY>
      ```
      ![Zed add Neon Local MCP server](/docs/guides/zed/add-neon-local-mcp-server.png)
    - Click **Add Server**.

6.  Check the Context Servers section in Zed Settings to ensure the connection is successful. Neon should be listed as a context server.
    ![Zed with Neon MCP Tools](/docs/guides/zed/with-neon-mcp-tools.png)

7.  Zed is now connected to Neon's local MCP server.

### Verification

With the Neon MCP server set up either remotely or locally, you can now verify the connection and test the available tools.

1.  Open Zed Assistant.

2.  Type `List my Neon projects` in the Assistant text field.
    ![Zed list neon projects](/docs/guides/zed/list_neon_projects.png)

3.  When prompted, click **Allow** to authorize the tool to access your Neon project.
    ![Zed allow tool access](/docs/guides/zed/allow_tool_access.png)

4.  Zed will use the `list_projects` MCP tool to retrieve and display your Neon projects, including project ID, name, and other details.
    ![Zed list projects](/docs/guides/zed/list_projects.png)

5.  Try other commands such as listing all tables, creating a new branch, creating a new project, or running SQL queries.

### Troubleshooting

If you experience issues adding an MCP server from the Assistant panel, you can also do so through the **Settings**. Follow these steps:

1.  Open the `~/.config/zed/settings.json` file. You can access this file by clicking on **Settings** in the Zed menu bar.
2.  Add the following `context_servers` section to the file.

<CodeTabs labels={["Remote MCP server", "Local MCP server"]}>

```json
"context_servers": {
   "neon": {
      "command": {
         "path": "npx",
         "args": ["-y", "mcp-remote@latest", "https://mcp.neon.tech/sse"],
         "env": null
      },
      "settings": {}
   }
}
```

```json
"context_servers": {
   "neon": {
      "command": {
         "path": "npx",
         "args": ["-y", "@neondatabase/mcp-server-neon", "start", "<YOUR_NEON_API_KEY>"],
         "env": null
      },
      "settings": {}
   }
}
```

</CodeTabs>

#### Troubleshooting on Windows

If you are using Windows, and you encounter issues with the command line, you may need to adjust the command to use `cmd` or `wsl` to run the MCP server. For example, here's how you can set it up:

<CodeTabs labels={["Windows", "Windows (WSL)"]}>

```json
"context_servers": {
   "neon": {
      "command": {
         "path": "cmd",
         "args": ["/c", "npx", "-y", "@neondatabase/mcp-server-neon", "start", "<YOUR_NEON_API_KEY>"],
         "env": null
      },
      "settings": {}
   }
}
```

```json
"context_servers": {
   "neon": {
      "command": {
         "path": "wsl",
         "args": ["npx", "-y", "@neondatabase/mcp-server-neon", "start", "<YOUR_NEON_API_KEY>"],
         "env": null
      },
      "settings": {}
   }
}
```

</CodeTabs>

<MCPTools />

These actions enable any MCP client like Zed to interact with various functionalities of the **Neon platform via the Neon API.** Certain tools, especially database migration ones, are tailored for AI agent and LLM usage, leveraging Neon's branching for safe preview and commit.

## Development Workflow: Adding a Column with Zed and Neon MCP

Let's walk through a typical development scenario: Quickly adding a column for prototyping using natural language within Zed. The following chat log demonstrates a real interaction with Zed and the Neon MCP server.

**Scenario:** During development, you decide to track timestamps for entries in your `playing_with_neon` table. You want to quickly add a `created_at` column.

<Admonition type="tip" title="Security Reminder">
For your security, review the tool's purpose before permitting the operation to proceed. Remember that LLMs can sometimes produce unexpected results, so careful monitoring is always recommended.
</Admonition>

Here's the conversation log between the user and Zed:

![Zed listing projects](/docs/guides/zed/mcp-neon-adding-column-1.png)

```text shouldWrap
User: List my neon projects
Zed: I'll use the list_projects tool to show your Neon projects.
> Run MCP tool `list_projects`
> You have one Neon project:
> Name: mcp
> Project ID: red-math-82356803
> Region: aws-ap-southeast-1
> ...other details...

User: what tables are present in it?
Zed: I'll use the `get_database_tables` tool to list the tables in your project. I'll use the project ID we just found.
> Run MCP tool `get_database_tables`
> There is one table in your project's database:
> Schema: public
> Table Name: `playing_with_neon`
> Type: Base Table
> Let me fetch the details of this table's schema so you can see its structure:
> Run MCP tool `describe_table_schema`
> The "playing_with_neon" table has three columns:
> 1. id: An auto-incrementing integer (primary key)
> 2. value: A real (floating-point) number that can be null
> 3. name: A text field that cannot be null

Would you like to see the data in this table or perform any operations on it?
```

![Zed showing table schema](/docs/guides/zed/mcp-neon-adding-column-2.png)

```text
User: Can you show me first 5 rows in it?
Zed: I'll query the first 5 rows from the playing_with_neon table:
> Run MCP tool `run_sql`
> xxx (table rows are returned by Zed)

User: can you add a created_at column to the table?
Zed: I'll use the prepare_database_migration tool to add a created_at column to the playing_with_neon table:
> Run MCP tool `prepare_database_migration`
> I've added the `created_at` column in a temporary branch. Let me verify the change by checking the table structure:
> Run MCP tool `run_sql`
> I've verified that the created_at column has been successfully added in the temporary branch.
> Migration Details:
> - Migration ID: a78e2a3b-8498-48a0-9c7a-2dec30da84bc
> - Temporary Branch Name: br-royal-mode-a1nfvzag
> - Temporary Branch ID: br-royal-mode-a1nfvzag
> - Migration Result: Success
> Would you like to commit this migration to the production branch?

User: yes do it
Zed: I'll complete the migration using the provided migration ID:
> Run MCP tool `complete_database_migration`
> The migration has been successfully completed. The `created_at` column has been added to the `playing_with_neon` table in the production branch, and the temporary branch has been deleted.
```

![Zed completing migration](/docs/guides/zed/mcp-neon-adding-column-3.png)

**Key takeaways:**

- **Natural language interaction:** You can use simple, conversational English to interact with your database.
- **Step-by-step guidance:** Zed (Claude) confirms each step and provides details like branch names and migration IDs.
- **MCP Tools in action:** The underlying MCP tools (`get_database_tables`, `run_sql`, `prepare_database_migration`, `complete_database_migration`) illustrate the automated process.
- **Branching for safety:** The agent automatically creates a temporary branch for schema changes. The user will be prompted to confirm the migration before applying it to the production branch.

You can verify the changes in your Neon Console or by querying the database directly.

![Neon Console with added column](/docs/guides/neon-cursor-mcp-added-column.png)

<Admonition type="note">
While the Neon MCP server allows you to utilize all of Neon's functionalities, you can also use a [generic Postgres MCP server](https://github.com/modelcontextprotocol/servers/tree/main/src/postgres) to connect to Neon.
</Admonition>

## Bonus: Creating a project and branch

Beyond schema changes, you can also manage your Neon projects and branches directly within Zed using natural language commands. This can be useful for quickly setting up a new development or test environment.

### Creating a new project

Let's say you want to create a new Neon project named "my-new-project". You can simply tell Zed:

```text shouldWrap
User: Create a Neon project named "my-new-project"
Zed: I'll create a new Neon project with the specified name.
> Run MCP tool `create_project`
> I've successfully created a new Neon project named "my-new-project". Here are the key details:
> - Project ID: red-frog-93550744
> - Default Branch: main (ID: br-shiny-water-a5us05x3)
> - Default Database: neondb
> The project is ready to use. You can now create tables, add data, or perform other database operations. Would you like to perform any specific operations with your new project?
```

![Zed creating a new Neon project](/docs/guides/zed/mcp-create-new-project.png)

Zed will then use the `create_project` MCP tool to initiate the project creation. It will provide you with a project ID, default branch, and database details. You can then proceed to create tables or perform other operations within this project.

### Creating a New Branch

Once you have a project, you can easily create new branches for development or experimentation. For example, to create a branch named "feature-x" in your "my-new-project" project:

```text shouldWrap
User: Create a branch named "feature-x"
Zed: I'll create a new branch named "feature-x" in your project.
> Run MCP tool `create_branch`
> I've successfully created a new branch named "feature-x" in your project. Here are the details:
> - Branch Name: feature-x
> - Branch ID: br-gentle-dream-a5u9fmm4
> - Parent Branch: br-shiny-water-a5us05x3
> The branch is ready for use. Would you like to perform any specific operations in this branch?
```

![Zed creating a new Neon branch](/docs/guides/zed/mcp-create-new-branch.png)

Zed will use the `create_branch` MCP tool to create the branch and provide you with the branch name and ID. Notice how we don't need to specify the project ID, as Zed remembers the active project context.

## Conclusion

Zed combined with the Neon MCP Server, whether using the **Remote Hosted (Preview)** option or the **Local Server** setup, lets you use natural language to interact with your database and take advantage of Neon's branching capabilities for fast iteration. This approach is ideal for quickly testing
database ideas and making schema changes during development.

## Resources

- [MCP Protocol](https://modelcontextprotocol.org)
- [Context Servers in Zed](https://zed.dev/docs/assistant/context-servers)
- [Neon Docs](/docs)
- [Neon API Keys](/docs/manage/api-keys#creating-api-keys)
- [Neon MCP server GitHub](https://github.com/neondatabase/mcp-server-neon)

<NeedHelp/>


# Getting started with Zero and Neon

---
title: Getting started with Zero and Neon
subtitle: A step-by-step guide to integrating Zero with Neon Postgres
author: dhanush-reddy
enableTableOfContents: true
createdAt: '2025-05-01T00:00:00.000Z'
updatedOn: '2025-05-01T00:00:00.000Z'
---

This guide demonstrates how to integrate [Zero](https://zero.rocicorp.dev/) by [Rocicorp](https://rocicorp.dev/) with Neon Postgres. Zero allows you to build reactive, real-time applications by writing queries directly in your client code against your backend database schema. It synchronizes query results efficiently to a client-side cache, enabling instant UI updates and a local-first feel.

Zero achieves this using its custom streaming query engine, [ZQL](https://zero.rocicorp.dev/docs/reading-data), and a stateful middleware service called `zero-cache`. `zero-cache` maintains a SQLite replica of your upstream Postgres database and serves ZQL queries to clients over WebSockets.

This guide provides a step-by-step walkthrough of setting up Zero with Neon Postgres. You will learn how to:

- Prepare your Neon Postgres database for Zero integration.
- Clone and run the Zero `hello-zero` quickstart application as a practical example.
- Test the integration to ensure data syncs correctly between the application, `zero-cache`, and Neon.

## Prerequisites

Before you begin, ensure you have the following prerequisites installed and configured:

- **Neon Account:** Sign up for a free [Neon account](https://console.neon.tech/signup) if you don't have one already. Neon provides a Postgres database for your Zero application.
- **Node.js v20+:** Node.js (version 20 or higher) is required to run the `hello-zero` example application and Zero tooling. Download and install it from [nodejs.org](https://nodejs.org).

## Setting up Neon Database

Zero requires a Postgres database (version 15+) with logical replication enabled. You'll configure your Neon project accordingly.

1.  **Create a Neon Project:** If you haven't already, create a new Neon project using [pg.new](https://pg.new).
2.  **Enable Logical Replication:** Zero uses Postgres logical replication (`wal_level = logical`) to receive changes from your database.
    - Navigate to your Neon Project using the [Neon Console](https://console.neon.tech/).
    - Open the **Settings** menu.
    - Click on **Logical Replication**.
    - Click the **Enable** button to enable logical replication.
      ![Neon dashboard settings with option to enable logical replication](/docs/guides/neon-console-settings-logical-replication.png)

3.  **Retrieve Connection String:**
    - Navigate to the **Dashboard** of your Neon project.
    - Click on the **Connect** button which opens a modal.
    - Select your database and branch, and copy the connection string with connection pooling disabled.
      <Admonition type="important">
      Make sure to turn off connection pooling in the connection string modal. This is essential for `zero-cache` to maintain a persistent connection to the Neon database.
      </Admonition>
      ![Neon direct connection string modal](/docs/guides/neon-console-direct-connection-string.png)

## Setting up the `hello-zero` example application

With your Neon database prepared, let's set up the `hello-zero` example application from [Zero's Quickstart](https://zero.rocicorp.dev/docs/quickstart) to connect to it.

1.  **Clone the `hello-zero` repository and install dependencies:**
    In a terminal window, navigate to the directory where you want to clone the `hello-zero` repository. Run the following commands:

    ```bash
    git clone https://github.com/rocicorp/hello-zero.git
    cd hello-zero
    npm install
    ```

    This clones the `hello-zero` repository and installs the necessary Node.js dependencies.

    <Admonition type="note" title="Using non-npm package managers?">
    If you are using `pnpm` or `bun` instead of `npm`, you might need to explicitly allow the postinstall script for `@rocicorp/zero-sqlite3`, which installs native binaries. Follow the instructions on [Zero's Docs](https://zero.rocicorp.dev/docs/quickstart#not-npm) to configure your package manager correctly.
    </Admonition>

2.  **Apply database schema/seed data:**
    To run the example application, you need to set up the database schema and seed initial data by running the SQL migrations. The project includes the necessary SQL commands in the `docker/seed.sql` file.

    You can execute this file using `psql` (ensure it's installed locally) or the [Neon SQL Editor](/docs/get-started/query-with-neon-sql-editor).

    Using `psql`, run the following command. Replace `YOUR_NEON_CONNECTION_STRING` with your database connection string copied from the Neon Console:

    ```bash
    psql "YOUR_NEON_CONNECTION_STRING" -f docker/seed.sql
    ```

    > Alternatively, you can run the SQL commands directly in the Neon SQL Editor. Copy the contents of `docker/seed.sql` and paste them into the SQL Editor in the Neon Console. Click **Run** to execute the commands.

3.  **Configure environment variables:**
    Open the `.env` file and modify the `ZERO_UPSTREAM_DB` variable to point to your Neon database. It should look something like this:

    ```env
    # other environment variables...
    ZERO_UPSTREAM_DB="YOUR_NEON_CONNECTION_STRING"
    ```

    > Replace `YOUR_NEON_CONNECTION_STRING` with the actual connection string from your Neon project dashboard which you copied earlier.

4.  **Run the `zero-cache` service:**
    Now, start the `zero-cache` service using the provided npm script. In your terminal, still within the `hello-zero` directory, run:

    ```bash
    npm run dev:zero-cache
    ```

    This command starts the `zero-cache` process. It connects to your Neon database, applies necessary [permissions](https://zero.rocicorp.dev/docs/permissions) required for the `hello-zero` application, and starts the replication process. The terminal will display logs indicating the connection status and replication progress. Keep this terminal window open as it runs the `zero-cache` service.

    <Admonition type="tip" title="Topology">
    To optimize performance, deploy your `zero-cache` service in close proximity to your Neon database to minimize latency in production. During local development, you might notice logs indicating a higher ping time if your `zero-cache` service and Neon database are not in the same region. This is expected and can be disregarded in a development environment. However, for production, it is crucial to deploy the `zero-cache` service in the same region as your Neon database to achieve minimal latency. For more information on deployment, refer to [Deploying Zero](https://zero.rocicorp.dev/docs/deployment#topology).
    </Admonition>

5.  **Run the `hello-zero` UI:**
    Open a _new_ terminal window, navigate back to the `hello-zero` directory, and run the following command to start the frontend application:
    ```bash
    npm run dev:ui
    ```
    This command starts the Vite development server, making the application available at `http://localhost:5173`. Open this URL in your browser.

## Using the demo application

You should now have the `hello-zero` application running in your browser. It connects to the `zero-cache` process running in your first terminal window, which synchronizes data with your Neon Postgres database.

1.  **Access the application:** Open `http://localhost:5173` in your browser.
2.  **Test functionality:** Try the features described in the [Zero Quickstart Overview](https://zero.rocicorp.dev/docs/quickstart#quick-overview):
    - Click **Add Messages**. New messages should appear instantly.
    - Open the app in a second browser tab or window. Changes made in one window should appear nearly instantaneously in the other.
    - Click **Login**. You'll be logged in as a random user.
    - Try **Remove Messages**. This should work now that you are logged in.
      ![Demo of the hello-zero app](/docs/guides/hello-zero-demo.gif)
    - Try editing a message (pencil icon). You should only be able to edit messages created by the user you are logged in as.
    - Use the **From** or **Contains** filters.
3.  **Verify data in Neon (Optional):** In the Neon Console, navigate to **Tables** and select the `message` table. You should see the messages you added in the application. This confirms that data is being synchronized correctly between the application, `zero-cache`, and Neon.
    ![Neon messages table](/docs/guides/zero-message-table.png)

Congratulations! You have successfully set up Rocicorp Zero with Neon Postgres using the `hello-zero` example application. Check out [Canvas](https://github.com/neondatabase-labs/canvas), a collaborative drawing app built with Zero and Neon, for a more complex example of Zero in action.

<Admonition type="note" title="Schema Changes">
Zero uses Postgres event triggers for efficient schema migration handling. While Neon now supports event triggers, Zero may still perform a **full reset of the `zero-cache` and all connected client states** whenever schema changes are detected to ensure correctness.

This reset mechanism can be inefficient for larger databases (e.g., > 1GB) or applications undergoing frequent schema evolution. For smaller databases or projects with stable schemas, the impact is typically acceptable. Consider this behavior when managing schema changes for your Zero application, especially for larger projects.
</Admonition>

## Resources

- [Zero Documentation](https://zero.rocicorp.dev/docs)
- [Zero Quickstart](https://zero.rocicorp.dev/docs/quickstart)
- [Zero Deployment Guide](https://zero.rocicorp.dev/docs/deployment)
- [`hello-zero` Repository](https://github.com/rocicorp/hello-zero)
- [Neon Documentation](/docs)
- [Canvas - A collaborative drawing app built with Zero and Neon](https://github.com/neondatabase-labs/canvas)

<NeedHelp/>


